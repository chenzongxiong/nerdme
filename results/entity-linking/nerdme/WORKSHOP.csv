sentence,entity_text,entity_type
"- [Paper Acceptance] Oct, 2023: [""FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets""](https://arxiv.org/abs/2310.04793) is acceptedüéâ  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023   - [Paper Acceptance] Oct, 2023: [""FinGPT: Democratizing Internet-scale Data for Financial Large Language Models""](https://arxiv.org/abs/2307.10485) is acceptedüéâ  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023  - [Model Release] Oct, 2023: We release the [financial multi-task LLMs](https://huggingface.co/FinGPT) üî•  produced when evaluating base-LLMs on [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)  - [Paper Acceptance] Sep, 2023: [""Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models""](https://arxiv.org/abs/2310.04027) is acceptedüéâ  by [ACM International Conference on AI in Finance (ICAIF-23)](https://ai-finance.org/icaif-23-accepted-papers/)  - [Model Release] Aug, 2023: We release the [financial sentiment analysis model](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) üî•   - [Paper Acceptance] Jul, 2023: [""Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models""](https://arxiv.org/abs/2306.12659) is acceptedüéâ  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Paper Acceptance] Jul, 2023: [""FinGPT: Open-Source Financial Large Language Models""](https://arxiv.org/abs/2306.06031) is acceptedüéâ  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Medium Blog] Jun 2023: [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)  ## Why FinGPT?",Instruction Workshop,WORKSHOP
"- [Paper Acceptance] Oct, 2023: [""FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets""](https://arxiv.org/abs/2310.04793) is acceptedüéâ  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023   - [Paper Acceptance] Oct, 2023: [""FinGPT: Democratizing Internet-scale Data for Financial Large Language Models""](https://arxiv.org/abs/2307.10485) is acceptedüéâ  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023  - [Model Release] Oct, 2023: We release the [financial multi-task LLMs](https://huggingface.co/FinGPT) üî•  produced when evaluating base-LLMs on [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)  - [Paper Acceptance] Sep, 2023: [""Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models""](https://arxiv.org/abs/2310.04027) is acceptedüéâ  by [ACM International Conference on AI in Finance (ICAIF-23)](https://ai-finance.org/icaif-23-accepted-papers/)  - [Model Release] Aug, 2023: We release the [financial sentiment analysis model](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) üî•   - [Paper Acceptance] Jul, 2023: [""Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models""](https://arxiv.org/abs/2306.12659) is acceptedüéâ  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Paper Acceptance] Jul, 2023: [""FinGPT: Open-Source Financial Large Language Models""](https://arxiv.org/abs/2306.06031) is acceptedüéâ  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Medium Blog] Jun 2023: [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)  ## Why FinGPT?",Instruction Workshop,WORKSHOP
"- [Paper Acceptance] Oct, 2023: [""FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets""](https://arxiv.org/abs/2310.04793) is acceptedüéâ  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023   - [Paper Acceptance] Oct, 2023: [""FinGPT: Democratizing Internet-scale Data for Financial Large Language Models""](https://arxiv.org/abs/2307.10485) is acceptedüéâ  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023  - [Model Release] Oct, 2023: We release the [financial multi-task LLMs](https://huggingface.co/FinGPT) üî•  produced when evaluating base-LLMs on [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)  - [Paper Acceptance] Sep, 2023: [""Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models""](https://arxiv.org/abs/2310.04027) is acceptedüéâ  by [ACM International Conference on AI in Finance (ICAIF-23)](https://ai-finance.org/icaif-23-accepted-papers/)  - [Model Release] Aug, 2023: We release the [financial sentiment analysis model](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) üî•   - [Paper Acceptance] Jul, 2023: [""Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models""](https://arxiv.org/abs/2306.12659) is acceptedüéâ  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Paper Acceptance] Jul, 2023: [""FinGPT: Open-Source Financial Large Language Models""](https://arxiv.org/abs/2306.06031) is acceptedüéâ  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Medium Blog] Jun 2023: [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)  ## Why FinGPT?",FinLLM 2023,WORKSHOP
"- [Paper Acceptance] Oct, 2023: [""FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets""](https://arxiv.org/abs/2310.04793) is acceptedüéâ  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023   - [Paper Acceptance] Oct, 2023: [""FinGPT: Democratizing Internet-scale Data for Financial Large Language Models""](https://arxiv.org/abs/2307.10485) is acceptedüéâ  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023  - [Model Release] Oct, 2023: We release the [financial multi-task LLMs](https://huggingface.co/FinGPT) üî•  produced when evaluating base-LLMs on [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)  - [Paper Acceptance] Sep, 2023: [""Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models""](https://arxiv.org/abs/2310.04027) is acceptedüéâ  by [ACM International Conference on AI in Finance (ICAIF-23)](https://ai-finance.org/icaif-23-accepted-papers/)  - [Model Release] Aug, 2023: We release the [financial sentiment analysis model](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) üî•   - [Paper Acceptance] Jul, 2023: [""Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models""](https://arxiv.org/abs/2306.12659) is acceptedüéâ  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Paper Acceptance] Jul, 2023: [""FinGPT: Open-Source Financial Large Language Models""](https://arxiv.org/abs/2306.06031) is acceptedüéâ  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Medium Blog] Jun 2023: [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)  ## Why FinGPT?",FinLLM 2023,WORKSHOP
"v=x4dIx9VYQoM) --->   ## Citing FinGPT ``` @article{yang2023fingpt,   title={FinGPT: Open-Source Financial Large Language Models},   author={Yang, Hongyang and Liu, Xiao-Yang and Wang, Christina Dan},   journal={FinLLM Symposium at IJCAI 2023},   year={2023} } @article{zhang2023instructfingpt,       title={Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models},        author={Boyu Zhang and Hongyang Yang and Xiao-Yang Liu},       journal={FinLLM Symposium at IJCAI 2023},       year={2023} } @article{zhang2023fingptrag,   title={Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models},   author={Zhang, Boyu and Yang, Hongyang and Zhou, tianyu and Babar, Ali and Liu, Xiao-Yang},  journal = {ACM International Conference on AI in Finance (ICAIF)},   year={2023} }  @article{wang2023fingptbenchmark,   title={FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets},   author={Wang, Neng and Yang, Hongyang and Wang, Christina Dan},   journal={NeurIPS Workshop on Instruction Tuning and Instruction Following},   year={2023} } @article{2023finnlp,   title={Data-centric FinGPT: Democratizing Internet-scale Data for Financial Large Language Models},   author={Liu, Xiao-Yang and Wang, Guoxuan and Yang, Hongyang and Zha, Daochen},   journal={NeurIPS Workshop on Instruction Tuning and Instruction Following},   year={2023} }  ```  <div align=""center""> <a href=""https://finllm.github.io/workshop/#/fcb"" target=""_blank""> <img align=""center"" src=figs/fingpt_best_presentation.png width=""65%""> </div>   ## LICENSE  MIT License  **Disclaimer: We are sharing codes for academic purposes under the MIT education license.",NeurIPS Workshop,WORKSHOP
"v=x4dIx9VYQoM) --->   ## Citing FinGPT ``` @article{yang2023fingpt,   title={FinGPT: Open-Source Financial Large Language Models},   author={Yang, Hongyang and Liu, Xiao-Yang and Wang, Christina Dan},   journal={FinLLM Symposium at IJCAI 2023},   year={2023} } @article{zhang2023instructfingpt,       title={Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models},        author={Boyu Zhang and Hongyang Yang and Xiao-Yang Liu},       journal={FinLLM Symposium at IJCAI 2023},       year={2023} } @article{zhang2023fingptrag,   title={Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models},   author={Zhang, Boyu and Yang, Hongyang and Zhou, tianyu and Babar, Ali and Liu, Xiao-Yang},  journal = {ACM International Conference on AI in Finance (ICAIF)},   year={2023} }  @article{wang2023fingptbenchmark,   title={FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets},   author={Wang, Neng and Yang, Hongyang and Wang, Christina Dan},   journal={NeurIPS Workshop on Instruction Tuning and Instruction Following},   year={2023} } @article{2023finnlp,   title={Data-centric FinGPT: Democratizing Internet-scale Data for Financial Large Language Models},   author={Liu, Xiao-Yang and Wang, Guoxuan and Yang, Hongyang and Zha, Daochen},   journal={NeurIPS Workshop on Instruction Tuning and Instruction Following},   year={2023} }  ```  <div align=""center""> <a href=""https://finllm.github.io/workshop/#/fcb"" target=""_blank""> <img align=""center"" src=figs/fingpt_best_presentation.png width=""65%""> </div>   ## LICENSE  MIT License  **Disclaimer: We are sharing codes for academic purposes under the MIT education license.",NeurIPS Workshop,WORKSHOP
## Multi-granular Legal Topic Classification on Greek Legislation  Code and data repo for the paper: [Multi-granular Legal Topic Classification on Greek Legislation](https://arxiv.org/abs/2109.15298)\ presented at NLLP 2021 workshop co-located with EMNLP 2021.,NLLP 2021,WORKSHOP
"-- Old workshop links: ## The 2015 MAF Workshop, Bremerton *August 19-21, 2015.* * [Meeting website](http://lsstsciencecollaborations.github.io/ObservingStrategy/) * [List of registered attendees](https://project.lsst.org/meetings/lsst2015/cadence-registrations) * [Hack session products](https://github.com/LSSTScienceCollaborations/ObservingStrategy/issues?",2015 MAF Workshop,WORKSHOP
"%22+) * [Feedback form](http://goo.gl/forms/xmVBIWynm4) ## Face-to-Face White Paper Workshop, Tucscon *November 19-20, 2015.* * [Program](workshop/Tucson2015.md) End of old workshop links -->  ## Contacts  This effort is being coordinated by [Zeljko Ivezic](https://github.com/LSSTScienceCollaborations/ObservingStrategy/issues/new?",Face-to-Face White Paper Workshop,WORKSHOP
"# arXiv public datasets  This project is part of a submission to an ICLR 2019 workshop, RLGM Representation Learning on Graphs and Manifolds.",RLGM Representation Learning on Graphs and Manifolds,WORKSHOP
# Benchmarking methods for label error detection in token classification data  Code to reproduce results from the paper:  [**Detecting Label Errors in Token Classification Data**](https://arxiv.org/abs/2210.03920)   *NeurIPS 2022 Workshop on Interactive Learning for Natural Language Processing (InterNLP)*  This repository is only for intended for scientific purposes.,NeurIPS 2022 Workshop on Interactive Learning for Natural Language Processing (InterNLP),WORKSHOP
p=universal-instance-perception-as-object)  ## News - :trophy: We are the runner-up in [Segmentation in the Wild challenge](https://eval.ai/web/challenges/challenge-page/1931/leaderboard/4567). - :trophy: We are the winner of [BDD100K MOT Challenge](https://eval.ai/web/challenges/challenge-page/1989/leaderboard/4696) and the runner-up of [BDD MOTS Challenge](https://eval.ai/web/challenges/challenge-page/1996/leaderboard/4718) on CVPR2023 workshop.  ## Highlight - UNINEXT is accepted by **CVPR2023**. - UNINEXT reformulates diverse instance perception tasks into **a unified object discovery and retrieval paradigm** and can flexibly perceive different types of objects by simply changing the input prompts. - UNINEXT achieves **superior performance on 20 challenging benchmarks using a single model with the same model parameters**.   ## Introduction  !,CVPR2023 workshop,WORKSHOP
"Our paper is accepted for publication at the [2019 IEEE Workshop on Applications of Signal Processing to Audio  and Acoustics (WASPAA), Mohonk Mountain House, New Paltz, NY](https://www.waspaa.com/).",2019 IEEE Workshop on Applications of Signal Processing to Audio  and Acoustics,WORKSHOP
"Our paper is accepted for publication at the [2019 IEEE Workshop on Applications of Signal Processing to Audio  and Acoustics (WASPAA), Mohonk Mountain House, New Paltz, NY](https://www.waspaa.com/).",WASPAA,WORKSHOP
"Our paper is accepted for publication at the [2019 IEEE Workshop on Applications of Signal Processing to Audio  and Acoustics (WASPAA), Mohonk Mountain House, New Paltz, NY](https://www.waspaa.com/).",waspaa,WORKSHOP
"[taxonomy](taxonomy1.png)](https://arxiv.org/abs/2404.00673)  ----------  ## Approaches  | **Title** | **Year** | **Venue** | **Target Explanations** | **Attacks** | **Defenses** | **Code** | | --------------- | :----: | ---- | :----: | :----: | :----: | :----: | | [Please Tell Me More: Privacy Impact of Explainability through the Lens of Membership Inference Attack](https://www.computer.org/csdl/proceedings-article/sp/2024/313000a120/1Ub23teQ7PG) | 2024 | _SP_ | Feature-based | Membership Inference | Differential Privacy, Privacy-Preserving Models, DP-SGD | - | | [Towards a Game-theoretic Understanding of Explanation-based Membership Inference Attacks](https://arxiv.org/abs/2404.07139) | 2024 | _arXiv_ | Feature-based | Membership Inference | Game Theory | - | | [On the Privacy Risks of Algorithmic Recourse](https://proceedings.mlr.press/v206/pawelczyk23a.html) | 2023 | _AISTATS_ | Counterfactual | Membership Inference | Differential Privacy | - | | [The Privacy Issue of Counterfactual Explanations: Explanation Linkage Attacks](https://dl.acm.org/doi/full/10.1145/3608482) | 2023 | _TIST_ | Counterfactual | Linkage | Anonymisaion | - | | [Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations](https://dl.acm.org/doi/abs/10.1145/3580305.3599343) | 2023 | _KDD_ | Counterfactual | - | Perturbation | [[Code]](https://github.com/isVy08/L2C/) | | [Private Graph Extraction via Feature Explanations](https://petsymposium.org/popets/2023/popets-2023-0041.pdf) | 2023 | _PETS_ | Feature-based | Graph Extraction | Perturbation | [[Code]](https://github.com/iyempissy/graph-stealing-attacks-with-explanation) | | [Privacy-Preserving Algorithmic Recourse](https://arxiv.org/abs/2311.14137) | 2023 | _ICAIF_ |  Counterfactual | - | Differential Privacy | - | | [Accurate, Explainable, and Private Models: Providing Recourse While Minimizing Training Data Leakage](https://arxiv.org/abs/2308.04341) | 2023 | _ICML-Workshop_ | Counterfactual | Membership Inference | Differential Privacy | - | | [Probabilistic Dataset Reconstruction from Interpretable Models](https://arxiv.org/abs/2308.15099) | 2023 | _arXiv_ | Interpretable Surrogates | Data Reconstruction | - | [[Code]](https://github.com/ferryjul/ProbabilisticDatasetsReconstruction) | | [DeepFixCX: Explainable privacy-preserving image compression for medical image analysis](https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1495) | 2023 | _WIREs-DMKD_ | Case-based | Identity recognition | Anonymisation | [[Code]](https://github.com/adgaudio/DeepFixCX) | | [XorSHAP: Privacy-Preserving Explainable AI for Decision Tree Models](https://eprint.iacr.org/2023/1859) | 2023 | _Preprint_ | Shapley | - | Multi-party Computation | - | | DP-XAI | 2023 | _Github_ | ALE plot | - | Differential Privacy | [[Code]](https://github.com/lange-martin/dp-global-xai) | | [Inferring Sensitive Attributes from Model Explanations](https://dl.acm.org/doi/abs/10.1145/3511808.3557362) | 2022 | _CIKM_ | Gradient-based, Perturbation-based | Attribute Inference | - | [[Code]](https://github.com/vasishtduddu/AttInfExplanations) | | [Model explanations with differential privacy](https://dl.acm.org/doi/abs/10.1145/3531146.3533235) | 2022 | _FAccT_ | Feature-based | - | Differential Privacy | - | | [DualCF: Efficient Model Extraction Attack from Counterfactual Explanations](https://dl.acm.org/doi/10.1145/3531146.3533188) | 2022 | _FAccT_ | Counterfactual | Model Extraction | - | - | | [Feature Inference Attack on Shapley Values](https://dl.acm.org/doi/abs/10.1145/3548606.3560573) | 2022 | _CCS_ | Shapley | Attribute/Feature Inference | Low-dimensional | - | | [Evaluating the privacy exposure of interpretable global explainers](https://ieeexplore.ieee.org/abstract/document/10063510/), [Privacy Risk of Global Explainers](https://ebooks.iospress.nl/doi/10.3233/FAIA220206) | 2022 | _CogMI_ | Interpretable Surrogates | Membership Inference | - | - | | [Privacy-Preserving Case-Based Explanations: Enabling Visual Interpretability by Protecting Privacy](https://ieeexplore.ieee.org/document/9729808/) | 2022 | _IEEE Access_ | Example-based | - | Anonymisation | - | | [On the amplification of security and privacy risks by post-hoc explanations in machine learning models](https://arxiv.org/abs/2206.14004) | 2022 | _arXiv_ | Feature-based | Membership Inference | - | - | | [Differentially Private Counterfactuals via Functional Mechanism](https://arxiv.org/abs/2208.02878) | 2022 | _arXiv_ | Counterfactual | - | Differential Privacy | - | | [Differentially Private Shapley Values for Data Evaluation](https://arxiv.org/abs/2206.00511) | 2022 | _arXiv_ | Shapley | - | Differential Privacy | [[Code]](https://github.com/amiratag/DataShapley) | | [Exploiting Explanations for Model Inversion Attacks](https://openaccess.thecvf.com/content/ICCV2021/html/Zhao_Exploiting_Explanations_for_Model_Inversion_Attacks_ICCV_2021_paper.html) | 2021 | _ICCV_ | Gradient-based, Interpretable Surrogates | Model Inversion | - | - | | [On the Privacy Risks of Model Explanations](https://dl.acm.org/doi/abs/10.1145/3461702.3462533) | 2021 | AIES | Feature-based, Shapley, Counterfactual | Membership Inference | - | - | | [Adversarial XAI Methods in Cybersecurity](https://ieeexplore.ieee.org/abstract/document/9555622) | 2021 | TIFS | Counterfactual | Membership Inference | - | - | | [MEGEX: Data-Free Model Extraction Attack against Gradient-Based Explainable AI](https://arxiv.org/abs/2107.08909) | 2021 | _arXiv_ | Gradient-based | Model Extraction | - | [[Code]](https://github.com/cake-lab/datafree-model-extraction) | | [Robust Counterfactual Explanations for Privacy-Preserving SVM](https://www.diva-portal.org/smash/record.jsf?",ICML-Workshop,WORKSHOP
"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ‚â§ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ‚â§ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ‚â§ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ‚â§ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ‚â§ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ‚â§ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",ICML-Workshop,WORKSHOP
"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ‚â§ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ‚â§ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ‚â§ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ‚â§ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ‚â§ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ‚â§ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",RCV-CVPR,WORKSHOP
IEEE 2011 workshop on automatic speech recognition and understanding.,IEEE 2011 workshop on automatic speech recognition and understanding,WORKSHOP
"For a detailed description of the annotation process, quality control procedures, summary statistics and baseline modelling results, see our [paper](https://arxiv.org/abs/2010.07410), to appear in the [Fourth Workshop on Online Abuse and Harms 2020](https://www.workshopononlineabuse.com/).",Fourth Workshop on Online Abuse and Harms 2020,WORKSHOP
"The three major files for reproducing the results in [arXiv:2408.08106](https://www.arxiv.org/abs/2408.08106) (accepted as a workshop paper in [the Machine Learning and the Physical Sciences workshop, NeurIPS 2024](https://ml4physicalsciences.github.io/2024/)) are in the following",Machine Learning and the Physical Sciences,WORKSHOP
"Stay tuned.  ## Contact Yin Li (yin.li@wisc.edu)  ## References If you are using our code, please consider citing our paper. ``` @inproceedings{zhang2022actionformer,   title={ActionFormer: Localizing Moments of Actions with Transformers},   author={Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},   booktitle={European Conference on Computer Vision},   series={LNCS},   volume={13664},   pages={492-510},   year={2022} } ```  If you cite our results on Ego4D, please consider citing our tech report in addition to the main paper. ``` @article{mu2022actionformerego4d,   title={Where a Strong Backbone Meets Strong Features -- ActionFormer for Ego4D Moment Queries Challenge},   author={Mu, Fangzhou and Mo, Sicheng and Wang, Gillian, and Li, Yin},   journal={arXiv e-prints},   year={2022} } ```  If you are using TSP features, please cite ``` @inproceedings{alwassel2021tsp,   title={{TSP}: Temporally-sensitive pretraining of video encoders for localization tasks},   author={Alwassel, Humam and Giancola, Silvio and Ghanem, Bernard},   booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},   pages={3173--3183},   year={2021} } ```",the IEEE/CVF International Conference on Computer Vision Workshops,WORKSHOP
"# Implicit Neural Representation in Medical Imaging: A Comparative Survey <br> <span style=""float: right""><sub><sup>ICCV 2023 CVAMD Workshop</sup></sub></span>    [!",CVAMD,WORKSHOP
"+ Meta + Sum loss: `model_zoo/meta_sum`   + Base Sum loss: `model_zoo/base_sum`   # Requirements  - Tensorflow 1.4 - python 3.6 - [Stanza](https://github.com/stanfordnlp/stanza)   # Citation  If you use the code in your paper, then please cite it as:    ``` @inproceedings{pshuang2018PT-MAML,   author    = {Po{-}Sen Huang and                Chenglong Wang and                Rishabh Singh and                Wen-tau Yih and                Xiaodong He},   title     = {Natural Language to Structured Query Generation via Meta-Learning},   booktitle = {NAACL},   year      = {2018}, } ```   ``` @inproceedings{2018executionguided,   author    = {Chenglong Wang and                Po{-}Sen Huang and                Alex Polozov and                Marc Brockschmidt and                 Rishabh Singh},   title = ""{Execution-Guided Neural Program Decoding}"",   booktitle = {ICML workshop on Neural Abstract Machines & Program Induction v2 (NAMPI)},   year = {2018} } ```   and   ``` @techreport{chenglong,   author = {Wang, Chenglong and Brockschmidt, Marc and Singh, Rishabh},   title = {Pointing Out {SQL} Queries From Text},   number = {MSR-TR-2017-45},   year = {2017},   month = {November},   url = {https://www.microsoft.com/en-us/research/publication/pointing-sql-queries-text/}, } ```    # Contributing  This project welcomes contributions and suggestions.",ICML workshop,WORKSHOP
"positional arguments:   pred                  path to predictions file (accepts multiple paths)  optional arguments:   -h, --help            show this help message and exit   --task {bin,class,full}                         evaluation mode (see above; default: ""cat"")   --true T              path to gold standard file (default: data/test.bio) ```  Once you have trained your own model(s) and decoded the test set, you may evaluate the results simply by doing:  ```shell script python3 eval.py /PATH/TO/PREDICTION-1 /PATH/TO/PREDICTION-2 /PATH/TO/PREDICTION-3 ```  If you do:  ```shell script python3 eval.py data/test.bio ```  you should obtain perfect results (because you will be evaluating the gold labels against themselves).  ## Citation  If you use NUBes, IULA+ or any of the provided material in your publications, please cite us appropriately:  ```bibtex @inproceedings{lima2020nubes,   author      = {Salvador Lima Lopez and Naiara Perez and Montse Cuadros and German Rigau},   title       = ""{NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts}"",   booktitle   = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC2020)},   month       = {May},   year        = {2020},   address     = {Marseille, France},   publisher   = {European Language Resources Association},   pages       = {5772--5781} }  ```  If you use IULA+, please cite also the paper describing the original corpus, IULA-SCRC:  ```bibtex @inproceedings{marimon2017annotation,   author      = {Montserrat Marimon and Jorge Vivaldi and N{\'u}ria Bel Rafecas},   title       = ""{Annotation of negation in the IULA Spanish Clinical Record Corpus}"",   booktitle   = {Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR)},   month       = {Apr},   year        = {2017},   address     = {Valencia, Spain},   publisher   = {Association for Computational Linguistics},   pages       = {43--52} } ```  ## License  The resources [NUBes](.",Workshop Computational Semantics Beyond Events and Roles,WORKSHOP
"positional arguments:   pred                  path to predictions file (accepts multiple paths)  optional arguments:   -h, --help            show this help message and exit   --task {bin,class,full}                         evaluation mode (see above; default: ""cat"")   --true T              path to gold standard file (default: data/test.bio) ```  Once you have trained your own model(s) and decoded the test set, you may evaluate the results simply by doing:  ```shell script python3 eval.py /PATH/TO/PREDICTION-1 /PATH/TO/PREDICTION-2 /PATH/TO/PREDICTION-3 ```  If you do:  ```shell script python3 eval.py data/test.bio ```  you should obtain perfect results (because you will be evaluating the gold labels against themselves).  ## Citation  If you use NUBes, IULA+ or any of the provided material in your publications, please cite us appropriately:  ```bibtex @inproceedings{lima2020nubes,   author      = {Salvador Lima Lopez and Naiara Perez and Montse Cuadros and German Rigau},   title       = ""{NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts}"",   booktitle   = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC2020)},   month       = {May},   year        = {2020},   address     = {Marseille, France},   publisher   = {European Language Resources Association},   pages       = {5772--5781} }  ```  If you use IULA+, please cite also the paper describing the original corpus, IULA-SCRC:  ```bibtex @inproceedings{marimon2017annotation,   author      = {Montserrat Marimon and Jorge Vivaldi and N{\'u}ria Bel Rafecas},   title       = ""{Annotation of negation in the IULA Spanish Clinical Record Corpus}"",   booktitle   = {Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR)},   month       = {Apr},   year        = {2017},   address     = {Valencia, Spain},   publisher   = {Association for Computational Linguistics},   pages       = {43--52} } ```  ## License  The resources [NUBes](.",SemBEaR,WORKSHOP
"* *Sep 2024*: Support MS-HuBERT (see [MS-HuBERT](https://arxiv.org/pdf/2406.05661)) * *Dec 2023*: Support Multi-resolution HuBERT (MR-HuBERT, see [Multiresolution HuBERT](https://arxiv.org/pdf/2310.02720.pdf)) * *Oct 2023*: Support ESPnet pre-trained upstream models (see [ESPnet HuBERT](https://arxiv.org/abs/2306.06672) and [WavLabLM](https://arxiv.org/abs/2309.15317)) * *Sep 2022*: In [JSALT 2022](https://jsalt-2022-ssl.github.io/member), We upgrade the codebase to support testing, documentation and a new [S3PRL PyPI package](https://pypi.org/project/s3prl/) for easy installation and usage for upstream models.",JSALT 2022,WORKSHOP
"* *Sep 2024*: Support MS-HuBERT (see [MS-HuBERT](https://arxiv.org/pdf/2406.05661)) * *Dec 2023*: Support Multi-resolution HuBERT (MR-HuBERT, see [Multiresolution HuBERT](https://arxiv.org/pdf/2310.02720.pdf)) * *Oct 2023*: Support ESPnet pre-trained upstream models (see [ESPnet HuBERT](https://arxiv.org/abs/2306.06672) and [WavLabLM](https://arxiv.org/abs/2309.15317)) * *Sep 2022*: In [JSALT 2022](https://jsalt-2022-ssl.github.io/member), We upgrade the codebase to support testing, documentation and a new [S3PRL PyPI package](https://pypi.org/project/s3prl/) for easy installation and usage for upstream models.",jsalt-2022,WORKSHOP
/s3prl/upstream/distiller/README.md) for more info. * *Sep 2021:* We host a *challenge* in [*AAAI workshop: The 2nd Self-supervised Learning for Audio and Speech Processing*](https://aaai-sas-2022.github.io/)!,AAAI workshop,WORKSHOP
/s3prl/upstream/distiller/README.md) for more info. * *Sep 2021:* We host a *challenge* in [*AAAI workshop: The 2nd Self-supervised Learning for Audio and Speech Processing*](https://aaai-sas-2022.github.io/)!,aaai-sas-2022,WORKSHOP
"Ozgur, ""[Accelerating SVRG via second-order information](http://www.opt-ml.org/papers/OPT2015_paper_41.pdf),"" OPT2015, 2015",OPT2015,WORKSHOP
"Ozgur, ""[Accelerating SVRG via second-order information](http://www.opt-ml.org/papers/OPT2015_paper_41.pdf),"" OPT2015, 2015",OPT2015,WORKSHOP
"Ozgur, ""[Accelerating SVRG via second-order information](http://www.opt-ml.org/papers/OPT2015_paper_41.pdf),"" OPT2015, 2015",OPT2015,WORKSHOP
"Ozgur, ""[Accelerating SVRG via second-order information](http://www.opt-ml.org/papers/OPT2015_paper_41.pdf),"" OPT2015, 2015",OPT2015,WORKSHOP
"- (2023.10) Code with LLaMA2 has been released. - (2023.10) Prompt-OIRL has been featured in a positioning [paper](https://arxiv.org/pdf/2310.06147.pdf) as an example of **inverse alignment**. - (2023.9) Prompt-OIRL has been selected as an **oral presentation** at the ENLSP workshop at NeurIPS'2023.  ## üìñ  Abstract  > In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization.",ENLSP,WORKSHOP
**[CVPR'22]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2022/papers/Quan_Which_Images_To_Label_for_Few-Shot_Medical_Landmark_Detection_CVPR_2022_paper.pdf)**  Towards Fewer Annotations: Active Learning via Region Impurity and Prediction Uncertainty for Domain Adaptive Semantic Segmentation   **[CVPR'22]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_Towards_Fewer_Annotations_Active_Learning_via_Region_Impurity_and_Prediction_CVPR_2022_paper.pdf)** **[[Code]](https://github.com/BIT-DA/RIPU)**  Learning Distinctive Margin Toward Active Domain Adaptation   **[CVPR'22]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_Learning_Distinctive_Margin_Toward_Active_Domain_Adaptation_CVPR_2022_paper.pdf)** **[[Code]](https://github.com/TencentYoutuResearch/ActiveLearning-SDM)**  BoostMIS: Boosting Medical Image Semi-Supervised Learning With Adaptive Pseudo Labeling and Informative Active Annotation   **[CVPR'22]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_BoostMIS_Boosting_Medical_Image_Semi-Supervised_Learning_With_Adaptive_Pseudo_Labeling_CVPR_2022_paper.pdf)** **[[Code]](https://github.com/wannature/BoostMIS)**  One-Bit Active Query With Contrastive Pairs   **[CVPR'22]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_One-Bit_Active_Query_With_Contrastive_Pairs_CVPR_2022_paper.pdf)**  Revisiting Superpixels for Active Learning in Semantic Segmentation With Realistic Annotation Costs   **[CVPR'21]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2021/papers/Cai_Revisiting_Superpixels_for_Active_Learning_in_Semantic_Segmentation_With_Realistic_CVPR_2021_paper.pdf)** **[[Code]](https://github.com/cailile/Revisiting-Superpixels-for-Active-Learning)**  Sequential Graph Convolutional Network for Active Learning   **[CVPR'21]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2021/papers/Caramalau_Sequential_Graph_Convolutional_Network_for_Active_Learning_CVPR_2021_paper.pdf)** **[[Code]](https://github.com/razvancaramalau/Sequential-GCN-for-Active-Learning)**  VaB-AL: Incorporating Class Imbalance and Difficulty With Variational Bayes for Active Learning   **[CVPR'21]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2021/papers/Choi_VaB-AL_Incorporating_Class_Imbalance_and_Difficulty_With_Variational_Bayes_for_CVPR_2021_paper.pdf)** **[[Code]](https://github.com/jongwon20000/vabal)**  Transferable Query Selection for Active Domain Adaptation   **[CVPR'21]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2021/papers/Fu_Transferable_Query_Selection_for_Active_Domain_Adaptation_CVPR_2021_paper.pdf)** **[[Code]](https://github.com/thuml/Transferable-Query-Selection)**  Exploring Data-Efficient 3D Scene Understanding With Contrastive Scene Contexts   **[CVPR'21]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2021/papers/Hou_Exploring_Data-Efficient_3D_Scene_Understanding_With_Contrastive_Scene_Contexts_CVPR_2021_paper.pdf)** **[[Code]](https://github.com/facebookresearch/ContrastiveSceneContexts)**  Task-Aware Variational Adversarial Active Learning   **[CVPR'21]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2021/papers/Kim_Task-Aware_Variational_Adversarial_Active_Learning_CVPR_2021_paper.pdf)** **[[Code]](https://github.com/cubeyoung/TA-VAAL)**  Multiple Instance Active Learning for Object Detection   **[CVPR'21]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2021/papers/Yuan_Multiple_Instance_Active_Learning_for_Object_Detection_CVPR_2021_paper.pdf)** **[[Code]](https://github.com/yuantn/MI-AOD)**  Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision   üïù  **[CVPR'20]** **[[PDF]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Gudovskiy_Deep_Active_Learning_for_Biased_Datasets_via_Fisher_Kernel_Self-Supervision_CVPR_2020_paper.pdf)** **[[Code]](https://github.com/gudovskiy/al-fk-self-supervision)**  State-Relabeling Adversarial Active Learning   **[CVPR'20]** **[[PDF]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_State-Relabeling_Adversarial_Active_Learning_CVPR_2020_paper.pdf)** **[[Code]](https://github.com/Beichen1996/SRAAL)**  ViewAL: Active Learning with Viewpoint Entropy for Semantic Segmentation   **[CVPR'20]** **[[PDF]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Siddiqui_ViewAL_Active_Learning_With_Viewpoint_Entropy_for_Semantic_Segmentation_CVPR_2020_paper.pdf)** **[[Code]](https://github.com/nihalsid/ViewAL)**  Learning Loss for Active Learning   **[CVPR'19]** **[[PDF]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Yoo_Learning_Loss_for_Active_Learning_CVPR_2019_paper.pdf)**  Reducing Uncertainty in Undersampled MRI Reconstruction with Active Acquisition   **[CVPR'19]** **[[PDF]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Reducing_Uncertainty_in_Undersampled_MRI_Reconstruction_With_Active_Acquisition_CVPR_2019_paper.pdf)**  The Power of Ensembles for Active Learning in Image Classification   **[CVPR'18]** **[[PDF]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Beluch_The_Power_of_CVPR_2018_paper.pdf)**  Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation   **[CVPR'18]** **[[PDF]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Quantization_of_Fully_CVPR_2018_paper.pdf)**  Fine-Tuning Convolutional Neural Networks for Biomedical Image Analysis: Actively and Incrementally   **[CVPR'17]** **[[PDF]](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Fine-Tuning_Convolutional_Neural_CVPR_2017_paper.pdf)** **[[Code]](https://github.com/MrGiovanni/Active-Learning)**   ### International Conference on Computer Vision (ICCV) HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling   üïù  **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_HAL3D_Hierarchical_Active_Learning_for_Fine-Grained_3D_Part_Labeling_ICCV_2023_paper.pdf)**  Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation   üïù  **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Hierarchical_Point-based_Active_Learning_for_Semi-supervised_Point_Cloud_Semantic_Segmentation_ICCV_2023_paper.pdf)** **[[Code]](https://github.com/SmiletoE/HPAL)**  ALWOD: Active Learning for Weakly-Supervised Object Detection   üïù  **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_ALWOD_Active_Learning_for_Weakly-Supervised_Object_Detection_ICCV_2023_paper.pdf)** **[[Code]](https://github.com/seqam-lab/ALWOD)**  You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation   üïù  **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Samet_You_Never_Get_a_Second_Chance_To_Make_a_Good_ICCV_2023_paper.pdf)** **[[Code]](https://github.com/nerminsamet/seedal)**  Heterogeneous Diversity Driven Active Learning for Multi-Object Tracking   üïù  **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Heterogeneous_Diversity_Driven_Active_Learning_for_Multi-Object_Tracking_ICCV_2023_paper.pdf)**  TiDAL: Learning Training Dynamics for Active Learning   üïù  **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Kye_TiDAL_Learning_Training_Dynamics_for_Active_Learning_ICCV_2023_paper.pdf)** **[[Code]](https://github.com/hyperconnect/TiDAL)**  Knowledge-Aware Federated Active Learning with Non-IID Data   üïù  **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_Knowledge-Aware_Federated_Active_Learning_with_Non-IID_Data_ICCV_2023_paper.pdf)** **[[Code]](https://github.com/ycao5602/KAFAL)**  Adaptive Superpixel for Active Learning in Semantic Segmentation   **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Adaptive_Superpixel_for_Active_Learning_in_Semantic_Segmentation_ICCV_2023_paper.pdf)** **[[Code]](https://github.com/ml-postech/adaptive-superpixel-for-active-learning-in-semantic-segmentation)**  Active Universal Domain Adaptation   üïù  **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_Active_Universal_Domain_Adaptation_ICCV_2021_paper.pdf)**  Semi-Supervised Active Learning for Semi-Supervised Models: Exploit Adversarial Examples With Graph-Based Virtual Labels   üïù  **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_Semi-Supervised_Active_Learning_for_Semi-Supervised_Models_Exploit_Adversarial_Examples_With_ICCV_2021_paper.pdf)**  Active Learning for Deep Object Detection via Probabilistic Modeling   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Choi_Active_Learning_for_Deep_Object_Detection_via_Probabilistic_Modeling_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/NVlabs/AL-MDN)**  Contrastive Coding for Active Learning Under Class Distribution Mismatch   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Du_Contrastive_Coding_for_Active_Learning_Under_Class_Distribution_Mismatch_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/RUC-DWBI-ML/CCAL)**  Semi-Supervised Active Learning With Temporal Output Discrepancy   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Semi-Supervised_Active_Learning_With_Temporal_Output_Discrepancy_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/siyuhuang/TOD)**  Influence Selection for Active Learning   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Influence_Selection_for_Active_Learning_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/dragonlzm/ISAL)**  Multi-Anchor Active Domain Adaptation for Semantic Segmentation   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Ning_Multi-Anchor_Active_Domain_Adaptation_for_Semantic_Segmentation_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/munanning/MADA)**  Active Learning for Lane Detection: A Knowledge Distillation Approach   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Peng_Active_Learning_for_Lane_Detection_A_Knowledge_Distillation_Approach_ICCV_2021_paper.pdf)**  Active Domain Adaptation via Clustering Uncertainty-Weighted Embeddings   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Prabhu_Active_Domain_Adaptation_via_Clustering_Uncertainty-Weighted_Embeddings_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/virajprabhu/CLUE)**  S3VAADA: Submodular Subset Selection for Virtual Adversarial Active Domain Adaptation   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Rangwani_S3VAADA_Submodular_Subset_Selection_for_Virtual_Adversarial_Active_Domain_Adaptation_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/val-iisc/s3vaada)**  LabOR: Labeling Only If Required for Domain Adaptive Semantic Segmentation   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Shin_LabOR_Labeling_Only_if_Required_for_Domain_Adaptive_Semantic_Segmentation_ICCV_2021_paper.pdf)**  ReDAL: Region-Based and Diversity-Aware Active Learning for Point Cloud Semantic Segmentation   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_ReDAL_Region-Based_and_Diversity-Aware_Active_Learning_for_Point_Cloud_Semantic_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/tsunghan-wu/ReDAL)**  Active Learning for Deep Detection Neural Networks   **[ICCV'19]** **[[PDF]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Aghdam_Active_Learning_for_Deep_Detection_Neural_Networks_ICCV_2019_paper.pdf)** **[[Code]](https://gitlab.com/haghdam/deep_active_learning)**  Deep Reinforcement Active Learning for Human-in-the-Loop Person Re-Identification   **[ICCV'19]** **[[PDF]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Deep_Reinforcement_Active_Learning_for_Human-in-the-Loop_Person_Re-Identification_ICCV_2019_paper.pdf)**  Variational Adversarial Active Learning   **[ICCV'19]** **[[PDF]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Sinha_Variational_Adversarial_Active_Learning_ICCV_2019_paper.pdf)** **[[Code]](https://github.com/sinhasam/vaal)**   ### ICCV Workshop Computational Evaluation of the Combination of Semi-Supervised and Active Learning for Histopathology Image Segmentation with Missing Annotations   **[ICCVW'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/papers/Jimenez_Computational_Evaluation_of_the_Combination_of_Semi-Supervised_and_Active_Learning_ICCVW_2023_paper.pdf)**  Reducing Label Effort: Self-Supervised Meets Active Learning   **[ICCVW'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Bengar_Reducing_Label_Effort_Self-Supervised_Meets_Active_Learning_ICCVW_2021_paper.pdf)**  Joint semi-supervised and active learning for segmentation of gigapixel pathology images with cost-effective labeling   **[ICCVW'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Lai_Joint_Semi-Supervised_and_Active_Learning_for_Segmentation_of_Gigapixel_Pathology_ICCVW_2021_paper.pdf)**   ### European Conference on Computer Vision (ECCV) Learn from the Learnt: Source-Free Active Domain Adaptation via Contrastive Sampling and Visual Persistence   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00073.pdf)**  Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02596.pdf)**  Bidirectional Uncertainty-Based Active Learning for Open-Set Annotation   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04026.pdf)**  Active Coarse-to-Fine Segmentation of Moveable Parts from Real Images   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04970.pdf)**  Efficient Active Domain Adaptation for Semantic Segmentation by Selecting Information-rich Superpixels   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05060.pdf)**  Two-Stage Active Learning for Efficient Temporal Action Segmentation   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06348.pdf)**  Active Generation for Image Classification   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06459.pdf)**  Dataset Quantization with Active Learning based Adaptive Sampling   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07772.pdf)**  MetaAT: Active Testing for Label-Efficient Evaluation of Dense Recognition Tasks   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10062.pdf)**  Generalized Coverage for More Robust Low-Budget Active Learning   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11051.pdf)**  Exploring Active Learning in Meta-Learning: Enhancing Context Set Labeling   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/12574.pdf)**  Optical Flow Training under Limited Label Budget via Active Learning   üïù  **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-20047-2_24)** **[[Code]](https://github.com/duke-vision/optical-flow-active-learning-release)**  ActiveNeRF: Learning where to See with Uncertainty Estimation   üïù  **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19827-4_14)** **[[Code]](https://github.com/LeapLabTHU/ActiveNeRF)**  Active Label Correction Using Robust Parameter Update and Entropy Propagation   üïù  **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19803-8_1)**  LiDAL: Inter-Frame Uncertainty Based Active Learning for 3D LiDAR Semantic Segmentation   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19812-0_15)** **[[Code]](https://github.com/hzykent/LiDAL)**  Combating Label Distribution Shift for Active Domain Adaptation   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19827-4_32)** **[[Code]](https://github.com/sehyun03/ADA-label-distribution-matching)**  Unsupervised Selective Labeling for More Effective Semi-Supervised Learning   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-20056-4_25)** **[[Code]](https://github.com/TonyLianLong/UnsupervisedSelectiveLabeling)**  D2ADA: Dynamic Density-Aware Active Domain Adaptation for Semantic Segmentation   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19818-2_26)** **[[Code]](https://github.com/tsunghan-wu/D2ADA)**  When Active Learning Meets Implicit Semantic Data Augmentation   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-20056-4_25)**  Talisman: Targeted Active Learning for Object Detection with Rare Classes and Slices Using Submodular Mutual Information   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19839-7_1)** **[[Code]](https://github.com/surajkothawade/talisman)**  PT4AL: Using Self-Supervised Pretext Tasks for Active Learning   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19809-0_34)** **[[Code]](https://github.com/johnsk95/PT4AL)**  Active learning strategies for weakly-supervised object detection   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-20056-4_13)** **[[Code]](https://github.com/huyvvo/BiB)**  Active Pointly-Supervised Instance Segmentation   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19815-1_35)** **[[Code]](https://github.com/chufengt/APIS)**  Contextual Diversity for Active Learning   **[ECCV'20]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-030-58517-4_9)** **[[Code]](https://github.com/sharat29ag/CDAL)**  Active Crowd Counting with Limited Supervision   üïù  **[ECCV'20]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-030-58565-5_34)**  Weight Decay Scheduling and Knowledge Distillation for Active Learning   üïù  **[ECCV'20]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-030-58574-7_26)**  Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Cost   **[ECCV'20]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-030-58607-2_30)**  Two Stream Active Query Suggestion for Active Learning in Connectomics   **[ECCV'20]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-030-58523-5_7)**  Dual Adversarial Network for Deep Active Learning   **[ECCV'20]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-030-58586-0_40)**  What do I Annotate Next?,ICCV Workshop,WORKSHOP
"Tong, [Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set](https://arxiv.org/abs/1903.08527), IEEE Computer Vision and Pattern Recognition Workshop (CVPRW) on Analysis and Modeling of Faces and Gestures (AMFG), 2019.",Computer Vision and Pattern Recognition Workshop,WORKSHOP
"Tong, [Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set](https://arxiv.org/abs/1903.08527), IEEE Computer Vision and Pattern Recognition Workshop (CVPRW) on Analysis and Modeling of Faces and Gestures (AMFG), 2019.",CVPRW,WORKSHOP
"If you have any further questions, please contact Yu Deng (dengyu2008@hotmail.com) and Jiaolong Yang (jiaoyan@microsoft.com).   ## Citation  Please cite the following paper if this model helps your research:   @inproceedings{deng2019accurate,      title={Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set},      author={Yu Deng and Jiaolong Yang and Sicheng Xu and Dong Chen and Yunde Jia and Xin Tong},      booktitle={IEEE Computer Vision and Pattern Recognition Workshops},      year={2019}  } ## The face images on this page are from the public [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset released by MMLab, CUHK.",Computer Vision and Pattern Recognition Workshops,WORKSHOP
[Schema of Robustness Validation Framework](img/workshop_framework_2.png)  More details can be found in the paper accepted to NIPS IRASL workshop https://arxiv.org/abs/1812.02205  *This framework is in progress and will be updated.*,NIPS IRASL workshop,WORKSHOP
"The sixth and last movement is the chorale entitled *""Wie bin ich doch so herzlich froh""*.   ## Citations  ``` @article{poltronieri2022HaMSEOntology,           author    = {Andrea Poltronieri and                        Aldo Gangemi},           title     = {The HaMSE Ontology: Using Semantic Technologies to support Music Representation                        Interoperability and Musicological Analysis},           journal   = {CoRR},           volume    = {abs/2202.05817},           year      = {2022},           url       = {https://arxiv.org/abs/2202.05817},           eprinttype = {arXiv},           eprint    = {2202.05817},           timestamp = {Fri, 18 Feb 2022 12:23:53 +0100},           biburl    = {https://dblp.org/rec/journals/corr/abs-2202-05817.bib},           bibsource = {dblp computer science bibliography, https://dblp.org} }  ```  A more advanced version of this work can be found in:  ``` @inproceedings{poltronieri2021musicnoteontology,           title={The Music Note Ontology},           author={Poltronieri, Andrea and Gangemi, Aldo},           booktitle={Proceedings of the 12th Workshop on Ontology Design and Patterns (WOP 2021), Online, October 24, 2021.},           journal={CEUR-WS},           editor={Hammar, Karl and Shimizu, Cogan and K√º√ß√ºk McGinty, Hande and Asprino, Luigi and Carriero, Valentina Anita},           year={2021},           month={11} } ```  ## License MIT License  Copyright (c) 2021 Andrea Poltronieri  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",Workshop on Ontology Design and Patterns,WORKSHOP
"The sixth and last movement is the chorale entitled *""Wie bin ich doch so herzlich froh""*.   ## Citations  ``` @article{poltronieri2022HaMSEOntology,           author    = {Andrea Poltronieri and                        Aldo Gangemi},           title     = {The HaMSE Ontology: Using Semantic Technologies to support Music Representation                        Interoperability and Musicological Analysis},           journal   = {CoRR},           volume    = {abs/2202.05817},           year      = {2022},           url       = {https://arxiv.org/abs/2202.05817},           eprinttype = {arXiv},           eprint    = {2202.05817},           timestamp = {Fri, 18 Feb 2022 12:23:53 +0100},           biburl    = {https://dblp.org/rec/journals/corr/abs-2202-05817.bib},           bibsource = {dblp computer science bibliography, https://dblp.org} }  ```  A more advanced version of this work can be found in:  ``` @inproceedings{poltronieri2021musicnoteontology,           title={The Music Note Ontology},           author={Poltronieri, Andrea and Gangemi, Aldo},           booktitle={Proceedings of the 12th Workshop on Ontology Design and Patterns (WOP 2021), Online, October 24, 2021.},           journal={CEUR-WS},           editor={Hammar, Karl and Shimizu, Cogan and K√º√ß√ºk McGinty, Hande and Asprino, Luigi and Carriero, Valentina Anita},           year={2021},           month={11} } ```  ## License MIT License  Copyright (c) 2021 Andrea Poltronieri  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",WOP 2021,WORKSHOP
"## Fully Convolutional Instance-aware Semantic Segmentation  The major contributors of this repository include [Haozhi Qi](https://github.com/Oh233), [Yi Li](https://github.com/liyi14), [Guodong Zhang](https://github.com/gd-zhang), [Haochen Zhang](https://github.com/Braininvat), [Jifeng Dai](https://github.com/daijifeng001), and [Yichen Wei](https://github.com/YichenWei).  ### Introduction  **FCIS** is a fully convolutional end-to-end solution for instance segmentation, which won the first place in COCO segmentation challenge 2016.",COCO segmentation challenge 2016,WORKSHOP
Slides in [ImageNet ILSVRC and COCO workshop 2016](http://image-net.org/challenges/ilsvrc+coco2016): [OneDrive](https://onedrive.live.com/?,ImageNet ILSVRC and COCO workshop 2016,WORKSHOP
Slides in [ImageNet ILSVRC and COCO workshop 2016](http://image-net.org/challenges/ilsvrc+coco2016): [OneDrive](https://onedrive.live.com/?,ilsvrc+coco2016,WORKSHOP
# UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?,TSAR-2022,WORKSHOP
"**Find the arXiv version of our paper here: https://arxiv.org/abs/2301.01764**  In case you find these results useful, please consider citing our work in addition to the shared task paper (see below).  ``` @article{aumiller-gertz-2023-unihd, author = {Aumiller, Dennis and Gertz, Michael}, title = {{UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?}}",TSAR-2022,WORKSHOP
"# Original README: TSAR-2022-Shared-Task Datasets and Evaluation Scripts TSAR2022 Shared Task on Lexical Simplification for English (en), Spanish (es) and Portuguese (pt) - Datasets and Evaluation scripts  Please look at the website of the Shared Task for more details about the Evaluation Benchmark, Guidelines, Registration Form, etc...",TSAR-2022,WORKSHOP
"# Original README: TSAR-2022-Shared-Task Datasets and Evaluation Scripts TSAR2022 Shared Task on Lexical Simplification for English (en), Spanish (es) and Portuguese (pt) - Datasets and Evaluation scripts  Please look at the website of the Shared Task for more details about the Evaluation Benchmark, Guidelines, Registration Form, etc...",TSAR2022,WORKSHOP
<br/>[TSAR-2022 Shared-Task website](https://taln.upf.edu/pages/tsar2022-st/)  ## Datasets  There is no training dataset for the TSAR-2022 Shared Task.,TSAR-2022,WORKSHOP
<br/>[TSAR-2022 Shared-Task website](https://taln.upf.edu/pages/tsar2022-st/)  ## Datasets  There is no training dataset for the TSAR-2022 Shared Task.,tsar2022,WORKSHOP
<br/>[TSAR-2022 Shared-Task website](https://taln.upf.edu/pages/tsar2022-st/)  ## Datasets  There is no training dataset for the TSAR-2022 Shared Task.,TSAR-2022,WORKSHOP
- /datasets/trial/tsar2022_en_trial_none.tsv - /datasets/trial/tsar2022_en_trial_gold.tsv - /datasets/trial/tsar2022_es_trial_none.tsv - /datasets/trial/tsar2022_es_trial_gold.tsv - /datasets/trial/tsar2022_pt_trial_none.tsv - /datasets/trial/tsar2022_pt_trial_gold.tsv  <br/>    ### Test dataset   The *test_none* files (used for the evaluation benchmark) contain the instances with the sentences and target complex words,tsar2022,WORKSHOP
- /datasets/trial/tsar2022_en_trial_none.tsv - /datasets/trial/tsar2022_en_trial_gold.tsv - /datasets/trial/tsar2022_es_trial_none.tsv - /datasets/trial/tsar2022_es_trial_gold.tsv - /datasets/trial/tsar2022_pt_trial_none.tsv - /datasets/trial/tsar2022_pt_trial_gold.tsv  <br/>    ### Test dataset   The *test_none* files (used for the evaluation benchmark) contain the instances with the sentences and target complex words,tsar2022,WORKSHOP
- /datasets/trial/tsar2022_en_trial_none.tsv - /datasets/trial/tsar2022_en_trial_gold.tsv - /datasets/trial/tsar2022_es_trial_none.tsv - /datasets/trial/tsar2022_es_trial_gold.tsv - /datasets/trial/tsar2022_pt_trial_none.tsv - /datasets/trial/tsar2022_pt_trial_gold.tsv  <br/>    ### Test dataset   The *test_none* files (used for the evaluation benchmark) contain the instances with the sentences and target complex words,tsar2022,WORKSHOP
- /datasets/trial/tsar2022_en_trial_none.tsv - /datasets/trial/tsar2022_en_trial_gold.tsv - /datasets/trial/tsar2022_es_trial_none.tsv - /datasets/trial/tsar2022_es_trial_gold.tsv - /datasets/trial/tsar2022_pt_trial_none.tsv - /datasets/trial/tsar2022_pt_trial_gold.tsv  <br/>    ### Test dataset   The *test_none* files (used for the evaluation benchmark) contain the instances with the sentences and target complex words,tsar2022,WORKSHOP
- /datasets/trial/tsar2022_en_trial_none.tsv - /datasets/trial/tsar2022_en_trial_gold.tsv - /datasets/trial/tsar2022_es_trial_none.tsv - /datasets/trial/tsar2022_es_trial_gold.tsv - /datasets/trial/tsar2022_pt_trial_none.tsv - /datasets/trial/tsar2022_pt_trial_gold.tsv  <br/>    ### Test dataset   The *test_none* files (used for the evaluation benchmark) contain the instances with the sentences and target complex words,tsar2022,WORKSHOP
- /datasets/trial/tsar2022_en_trial_none.tsv - /datasets/trial/tsar2022_en_trial_gold.tsv - /datasets/trial/tsar2022_es_trial_none.tsv - /datasets/trial/tsar2022_es_trial_gold.tsv - /datasets/trial/tsar2022_pt_trial_none.tsv - /datasets/trial/tsar2022_pt_trial_gold.tsv  <br/>    ### Test dataset   The *test_none* files (used for the evaluation benchmark) contain the instances with the sentences and target complex words,tsar2022,WORKSHOP
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",tsar2022,WORKSHOP
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",tsar2022,WORKSHOP
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",tsar2022,WORKSHOP
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",tsar2022,WORKSHOP
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",tsar2022,WORKSHOP
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",tsar2022,WORKSHOP
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",TSAR-2022,WORKSHOP
The datasets (under the /datasets directory) are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License [CC-BY-NC-SA-4.0](CC-BY-NC-SA-4.0).  ## Contact https://taln.upf.edu/pages/tsar2022-st/#contact,tsar2022,WORKSHOP
"*In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP*, pages 62‚Äì79, Abu Dhabi, United Arab Emirates (Hybrid).",Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,WORKSHOP
Please refer to [text2sql-data](https://github.com/jkkummerfeld/text2sql-data/tree/master/data) for details. * main/res/data/scan * main/res/data/geography * main/res/data/advising  ### Notes + main/res/data/iwslt14 - both vocabulary augmentation set and the entire dataset for IWSLT14 + main/res/data/iwslt15 - both vocabulary augmentation set and the entire dataset for IWSLT15 + main/res/data/prepare-iwslt14.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for IWSLT14 + main/res/data/prepare-iwslt15.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for IWSLT15 + main/res/data/geo_vars.txt - the entity augmentation set for Grography + main/res/data/adv_vars.txt - the entity augmentation set for Advising  ## Setup Please ensure required packages are already installed.,IWSLT14,WORKSHOP
Please refer to [text2sql-data](https://github.com/jkkummerfeld/text2sql-data/tree/master/data) for details. * main/res/data/scan * main/res/data/geography * main/res/data/advising  ### Notes + main/res/data/iwslt14 - both vocabulary augmentation set and the entire dataset for IWSLT14 + main/res/data/iwslt15 - both vocabulary augmentation set and the entire dataset for IWSLT15 + main/res/data/prepare-iwslt14.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for IWSLT14 + main/res/data/prepare-iwslt15.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for IWSLT15 + main/res/data/geo_vars.txt - the entity augmentation set for Grography + main/res/data/adv_vars.txt - the entity augmentation set for Advising  ## Setup Please ensure required packages are already installed.,IWSLT15,WORKSHOP
Please refer to [text2sql-data](https://github.com/jkkummerfeld/text2sql-data/tree/master/data) for details. * main/res/data/scan * main/res/data/geography * main/res/data/advising  ### Notes + main/res/data/iwslt14 - both vocabulary augmentation set and the entire dataset for IWSLT14 + main/res/data/iwslt15 - both vocabulary augmentation set and the entire dataset for IWSLT15 + main/res/data/prepare-iwslt14.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for IWSLT14 + main/res/data/prepare-iwslt15.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for IWSLT15 + main/res/data/geo_vars.txt - the entity augmentation set for Grography + main/res/data/adv_vars.txt - the entity augmentation set for Advising  ## Setup Please ensure required packages are already installed.,IWSLT14,WORKSHOP
Please refer to [text2sql-data](https://github.com/jkkummerfeld/text2sql-data/tree/master/data) for details. * main/res/data/scan * main/res/data/geography * main/res/data/advising  ### Notes + main/res/data/iwslt14 - both vocabulary augmentation set and the entire dataset for IWSLT14 + main/res/data/iwslt15 - both vocabulary augmentation set and the entire dataset for IWSLT15 + main/res/data/prepare-iwslt14.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for IWSLT14 + main/res/data/prepare-iwslt15.sh - [fairseq](https://github.com/facebookresearch/fairseq) preprocess script for IWSLT15 + main/res/data/geo_vars.txt - the entity augmentation set for Grography + main/res/data/adv_vars.txt - the entity augmentation set for Advising  ## Setup Please ensure required packages are already installed.,IWSLT15,WORKSHOP
"Please find the example pipeline shown below.  ### Models + LSTM - lstm_luong_wmt_en_de + Transformer - transformer_iwslt_de_en + Dynamic Conv. - lightconv_iwslt_de_en  ### BPE ``` examples/translation/subword-nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code <iwslt14.tokenized.de-en/iwslt14.vocab.en> iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe ```  ### Preprocessing ``` TEXT=examples/translation/iwslt14.tokenized.de-en fairseq-preprocess --source-lang en --target-lang de \     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \     --destdir data-bin/iwslt14.tokenized.de-en \     --workers 20 ```  ### Training LSTM ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lstm_luong_wmt_en_de --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.2 --weight-decay 0.0 \     --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Transformer ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch transformer_iwslt_de_en --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Dynamic Conv. ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lightconv_iwslt_de_en \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ```  ### Evaluation BLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring bleu --remove-bpe --cpu >bleu.log 2>&1 & ``` ScareBLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring sacrebleu --remove-bpe --cpu >sacrebleu.log 2>&1 & ```  ## Authors * **Ning Shi** - mrshininnnnn@gmail.com  ## BibTex ``` @inproceedings{shi-etal-2022-revisit,     title = ""Revisit Systematic Generalization via Meaningful Learning"",     author = ""Shi, Ning  and       Wang, Boxin  and       Wang, Wei  and       Liu, Xiangyu  and       Lin, Zhouhan"",     booktitle = ""Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP"",     month = dec,     year = ""2022"",     address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.blackboxnlp-1.6"",     pages = ""62--79"",     abstract = ""Humans can systematically generalize to novel compositions of existing concepts.",Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,WORKSHOP
"E.},     title = {{EarthPT: a time series foundation model for Earth Observation}},     journal = {arXiv},     year = {2023},     eprint = {2309.07207},     doi = {10.48550/arXiv.2309.07207} } ```  This work is also in the [proceedings](https://www.climatechange.ai/papers/neurips2023/2) of the 2023 CCAI NeurIPS workshop.",2023 CCAI NeurIPS workshop,WORKSHOP
"| https://www.doi.org/10.1108/MRR-05-2020-0286      | | ""There is No Corona; It‚Äôs a Conspiracy"": Addressing the Perceptions of People about COVID-19 through the Narrative of Their Comments on Social Media | Creator    |   2021 | Journal of Consumer Health on the Internet                                                                                | Routledge                                            | https://www.doi.org/10.1080/15398285.2020.1867412 | | Raising the flag: Monitoring user perceived disinformation on reddit                                                                                 | Creator    |   2021 | Information (Switzerland)                                                                                                 | MDPI AG                                              | https://www.doi.org/10.3390/info12010004          | | SAMS: Human-in-the-loop approach to combat the sharing of digital misinformation                                                                     | Creator    |   2021 | CEUR Workshop Proceedings                                                                                                 | CEUR-WS                                              | https://arodes.hes-so.ch/record/8922              | | Fighting disaster misinformation in Latin America: the #19S Mexican earthquake case study                                                            | Creator    |   2021 | Personal and Ubiquitous Computing                                                                                         | Springer Science and Business Media Deutschland GmbH | https://www.doi.org/10.1007/s00779-020-01411-5    | | Dynamics of social corrections to peers sharing COVID-19 misinformation on WhatsApp in Brazil                                                        | Creator    |   2021 | Journal of the American Medical Informatics Association : JAMIA                                                           | NLM (Medline)                                        | https://www.doi.org/10.1093/jamia/ocab219         |   ## 2020 | Title                                                                                                                                                                                    | Category   |   Year | Venue                                                                           | Publisher                                              | URL                                                            | |:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------|-------:|:--------------------------------------------------------------------------------|:-------------------------------------------------------|:---------------------------------------------------------------| | Investigating Differences in Crowdsourced News Credibility Assessment: Raters, Tasks, and Expert Criteria                                                                                | Annotator  |   2020 | Proceedings of the ACM on Human-Computer Interaction                            | Association for Computing Machinery                    | https://www.doi.org/10.1145/3415164                            | | WhistleBlower: Towards A Decentralized and Open Platform for Spotting Fake News                                                                                                          | Annotator  |   2020 | Proceedings - 2020 IEEE International Conference on Blockchain, Blockchain 2020 | Institute of Electrical and Electronics Engineers Inc. | https://www.doi.org/10.1109/Blockchain50366.2020.00026         | | A reliable weighting scheme for the aggregation of crowd intelligence to detect fake news                                                                                                | Annotator  |   2020 | Information (Switzerland)                                                       | MDPI AG                                                | https://www.doi.org/10.3390/INFO11060319                       | | ‚ÄúIs It the Message or the Messenger?‚Äù",CEUR,WORKSHOP
"| https://www.doi.org/10.2196/19458                              | | Misinformation debunking and cross-platform information sharing through Twitter during Hurricanes Harvey and Irma: a case study on shelters and ID checks                                | Creator    |   2020 | Natural Hazards                                                                 | Springer                                               | https://www.doi.org/10.1007/s11069-020-04016-6                 | | Characterizing COVID-19 misinformation communities using a novel twitter dataset                                                                                                         | Creator    |   2020 | CEUR Workshop Proceedings                                                       | CEUR-WS                                                | https://arxiv.org/abs/2008.00791                               | | Refuting fake news on social media: nonprofits, crisis response strategies and issue involvement                                                                                         | Creator    |   2020 | Journal of Product and Brand Management                                         | Emerald Group Holdings Ltd",CEUR,WORKSHOP
"[https://arxiv.org/abs/2311.16613](https://arxiv.org/abs/2311.16613)  BibTex: ``` @inproceedings{Gkrispanis_WACVW2024, author={Gkrispanis, Konstantinos and Gkalelis, Nikolaos and Mezaris, Vasileios}, title={Filter-Pruning of Lightweight Face Detectors Using a Geometric Median Criterion}, year={2024}, month={Jan.}, booktitle={IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW 2024)} } ```  ## Acknowledgements  This work was supported by the EU Horizon 2020 programme under grant agreement H2020-951911 AI4Media.",WACVW2024,WORKSHOP
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",MLGWorkshop 2017,WORKSHOP
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",ICML 2019 LRGSD Workshop,WORKSHOP
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",NeurIPS RRL Workshop 2018,WORKSHOP
