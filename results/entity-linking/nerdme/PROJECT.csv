sentence,entity_text,entity_type
"# Integrated multimodal artificial intelligence framework for healthcare applications  This repository contains the code to replicate the data processing, modeling and reporting of our Holistic AI in Medicine (HAIM) in Nature's NPJ Digital Medicine.",Integrated multimodal artificial intelligence framework for healthcare applications,PROJECT
"# Integrated multimodal artificial intelligence framework for healthcare applications  This repository contains the code to replicate the data processing, modeling and reporting of our Holistic AI in Medicine (HAIM) in Nature's NPJ Digital Medicine.",Holistic AI in Medicine (HAIM),PROJECT
"In this work, we propose and evaluate a unified Holistic AI in Medicine (HAIM) framework to facilitate the generation and testing of AI systems that leverage multimodal inputs.",Holistic AI in Medicine,PROJECT
"In this work, we propose and evaluate a unified Holistic AI in Medicine (HAIM) framework to facilitate the generation and testing of AI systems that leverage multimodal inputs.",HAIM,PROJECT
"We evaluate our HAIM framework by training and characterizing 14,324 independent models based on HAIM-MIMIC-MM, a multimodal clinical database (N=34,537 samples) containing 7,279 unique hospitalizations and 6,485 patients, spanning all possible input combinations of 4 data modalities (i.e., tabular, time-series, text, and images), 11 unique data sources and 12 predictive tasks.",HAIM,PROJECT
The generalizable properties and flexibility of our Holistic AI in Medicine (HAIM) framework could offer a promising pathway for future multimodal predictive systems in clinical and operational healthcare settings.  ## Code  The code uses Python3.6.9 and is separated into four sections:  0 - Software Package requirement  1 - Data Preprocessing.,Holistic AI in Medicine,PROJECT
The generalizable properties and flexibility of our Holistic AI in Medicine (HAIM) framework could offer a promising pathway for future multimodal predictive systems in clinical and operational healthcare settings.  ## Code  The code uses Python3.6.9 and is separated into four sections:  0 - Software Package requirement  1 - Data Preprocessing.,HAIM,PROJECT
"# TANL: Structured Prediction as Translation between Augmented Natural Languages  Code for the paper ""[Structured Prediction as Translation between Augmented Natural Languages](http://arxiv.org/abs/2101.05779)"" (ICLR 2021) and [fine-tuned multi-task model](#fine-tuned-multi-task-model).",TANL,PROJECT
"If you use this code, please cite the paper using the bibtex reference below. ``` @inproceedings{tanl,     title={Structured Prediction as Translation between Augmented Natural Languages},     author={Giovanni Paolini and Ben Athiwaratkun and Jason Krone and Jie Ma and Alessandro Achille and Rishita Anubhai and Cicero Nogueira dos Santos and Bing Xiang and Stefano Soatto},     booktitle={9th International Conference on Learning Representations, {ICLR} 2021},     year={2021}, } ```   ## Requirements  - Python 3.6+ - PyTorch (tested with version 1.7.1) - Transformers (tested with version 4.0.0) - NetworkX (tested with version 2.5, only used in coreference resolution)  You can install all required Python packages with `pip install -r requirements.txt`   ## Datasets  By default, datasets are expected to be in `data/DATASET_NAME`.",tanl,PROJECT
"`t5-base`) - `do_train` (bool): whether to run training (default is False) - `do_eval` (bool): whether to run evaluation on the `dev` set (default is False) - `do_predict` (bool): whether to run evaluation on the `test` set (default is False) - `train_split` (str): comma-separated list of data splits for training (default is `train`) - `num_train_epochs` (int): number of train epochs - `learning_rate` (float): initial learning rate (default is 5e-4) - `train_subset` (float > 0 and <=1): portion of training data to effectively use during training (default is 1, i.e., use all training data) - `per_device_train_batch_size` (int): batch size per GPU during training (default is 8) - `per_device_eval_batch_size` (int): batch size during evaluation (default is 8; only one GPU is used for evaluation) - `max_seq_length` (int): maximum input sequence length after tokenization; longer sequences are truncated - `max_output_seq_length` (int): maximum output sequence length (default is `max_seq_length`) - `max_seq_length_eval` (int): maximum input sequence length for evaluation (default is `max_seq_length`) - `max_output_seq_length_eval` (int): maximum output sequence length for evaluation (default is `max_output_seq_length` or `max_seq_length_eval` or `max_seq_length`) - `episodes` (str): episodes to run (default is `0`; an interval can be specified, such as `1-4`; the episode number is used as the random seed) - `num_beams` (int): number of beams for beam search during generation (default is 1) - `multitask` (bool): if True, the name of the dataset is prepended to each input sentence (default is False)  See [arguments.py](arguments.py) and [transformers.TrainingArguments](https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py) for additional config arguments.   ## Fine-tuned multi-task model  The weights of our multi-task model (released under the [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/)) can be downloaded here: https://tanl.s3.amazonaws.com/tanl-multitask.zip  Extract the zip file in the `experiments/` directory.",tanl,PROJECT
"`t5-base`) - `do_train` (bool): whether to run training (default is False) - `do_eval` (bool): whether to run evaluation on the `dev` set (default is False) - `do_predict` (bool): whether to run evaluation on the `test` set (default is False) - `train_split` (str): comma-separated list of data splits for training (default is `train`) - `num_train_epochs` (int): number of train epochs - `learning_rate` (float): initial learning rate (default is 5e-4) - `train_subset` (float > 0 and <=1): portion of training data to effectively use during training (default is 1, i.e., use all training data) - `per_device_train_batch_size` (int): batch size per GPU during training (default is 8) - `per_device_eval_batch_size` (int): batch size during evaluation (default is 8; only one GPU is used for evaluation) - `max_seq_length` (int): maximum input sequence length after tokenization; longer sequences are truncated - `max_output_seq_length` (int): maximum output sequence length (default is `max_seq_length`) - `max_seq_length_eval` (int): maximum input sequence length for evaluation (default is `max_seq_length`) - `max_output_seq_length_eval` (int): maximum output sequence length for evaluation (default is `max_output_seq_length` or `max_seq_length_eval` or `max_seq_length`) - `episodes` (str): episodes to run (default is `0`; an interval can be specified, such as `1-4`; the episode number is used as the random seed) - `num_beams` (int): number of beams for beam search during generation (default is 1) - `multitask` (bool): if True, the name of the dataset is prepended to each input sentence (default is False)  See [arguments.py](arguments.py) and [transformers.TrainingArguments](https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py) for additional config arguments.   ## Fine-tuned multi-task model  The weights of our multi-task model (released under the [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/)) can be downloaded here: https://tanl.s3.amazonaws.com/tanl-multitask.zip  Extract the zip file in the `experiments/` directory.",tanl,PROJECT
[PyPI version](https://img.shields.io/pypi/v/GRANDE)](https://pypi.org/project/GRANDE/) [!,GRANDE,PROJECT
[PyPI version](https://img.shields.io/pypi/v/GRANDE)](https://pypi.org/project/GRANDE/) [!,GRANDE,PROJECT
"id=XEFWBxi075  ## Installation To download the latest official release of the package, use the pip command below: ```bash pip install GRANDE ``` More details can be found under: https://pypi.org/project/GRANDE/  ## Cite us  ``` @inproceedings{ marton2024grande, title={{GRANDE}: Gradient-Based Decision Tree Ensembles}, author={Sascha Marton and Stefan L{\""u}dtke and Christian Bartelt and Heiner Stuckenschmidt}, booktitle={The Twelfth International Conference on Learning Representations}, year={2024}, url={https://openreview.net/forum?",GRANDE,PROJECT
"<h1 align=""center"">Ask2Transformers</h1> <h3 align=""center"">A Framework for Textual Entailment based Zero Shot text classification</h3> <p align=""center"">  <a href=""https://paperswithcode.com/sota/domain-labelling-on-babeldomains?",Ask2Transformers,PROJECT
"url=https://paperswithcode.com/badge/ask2transformers-zero-shot-domain-labelling/domain-labelling-on-babeldomains"">  </a> </p>  This repository contains the code for out of the box ready to use zero-shot classifiers among different tasks, such as Topic Labelling or Relation Extraction.",paperswithcode,PROJECT
# Installation  By using Pip (check the last release)  ```shell script pip install a2t ```  By clonning the repository  ```shell script git clone https://github.com/osainz59/Ask2Transformers.git cd Ask2Transformers pip install . ```  Or directly by ```shell script pip install git+https://github.com/osainz59/Ask2Transformers ```  <!,Ask2Transformers,PROJECT
# Installation  By using Pip (check the last release)  ```shell script pip install a2t ```  By clonning the repository  ```shell script git clone https://github.com/osainz59/Ask2Transformers.git cd Ask2Transformers pip install . ```  Or directly by ```shell script pip install git+https://github.com/osainz59/Ask2Transformers ```  <!,Ask2Transformers,PROJECT
# Installation  By using Pip (check the last release)  ```shell script pip install a2t ```  By clonning the repository  ```shell script git clone https://github.com/osainz59/Ask2Transformers.git cd Ask2Transformers pip install . ```  Or directly by ```shell script pip install git+https://github.com/osainz59/Ask2Transformers ```  <!,Ask2Transformers,PROJECT
"To train your own model, first, you will need to convert your actual dataset in some sort of NLI data, we recommend you to have a look to [tacred2mnli.py](https://github.com/osainz59/Ask2Transformers/blob/master/scripts/tacred2mnli.py) script that serves as an example",Ask2Transformers,PROJECT
If you need help read the new [documentation](https://osainz59.github.io/Ask2Transformers) or post an Issue on GitHub,Ask2Transformers,PROJECT
"<div align=""center""> <img align=""center"" width=""30%"" alt=""image"" src=""https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139""> </div>  # FinGPT: Open-Source Financial Large Language Models [!",FinGPT,PROJECT
"<div align=""center""> <img align=""center"" width=""30%"" alt=""image"" src=""https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139""> </div>  # FinGPT: Open-Source Financial Large Language Models [!",FinGPT,PROJECT
[Downloads](https://static.pepy.tech/badge/fingpt)](https://pepy.tech/project/fingpt) [!,fingpt,PROJECT
[Downloads](https://static.pepy.tech/badge/fingpt)](https://pepy.tech/project/fingpt) [!,fingpt,PROJECT
[Downloads](https://static.pepy.tech/badge/fingpt/week)](https://pepy.tech/project/fingpt) [!,fingpt,PROJECT
[Downloads](https://static.pepy.tech/badge/fingpt/week)](https://pepy.tech/project/fingpt) [!,fingpt,PROJECT
[PyPI](https://img.shields.io/pypi/v/fingpt.svg)](https://pypi.org/project/fingpt/) !,fingpt,PROJECT
[](https://img.shields.io/github/issues-raw/AI4Finance-Foundation/fingpt?,fingpt,PROJECT
"user=AI4Finance-Foundation&repo=FinGPT&countColor=%23B17A)   ## What's New:  - [Model Release] Nov, 2023: We release [FinGPT-Forecaster](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster)!",FinGPT,PROJECT
"user=AI4Finance-Foundation&repo=FinGPT&countColor=%23B17A)   ## What's New:  - [Model Release] Nov, 2023: We release [FinGPT-Forecaster](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster)!",FinGPT,PROJECT
"ðŸ”¥ [Demo](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster), [Medium Blog](https://medium.datadriveninvestor.com/introducing-fingpt-forecaster-the-future-of-robo-advisory-services-50add34e3d3c) & [Model](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) are available on HuggingfaceðŸ¤— !",FinGPT,PROJECT
"ðŸ”¥ [Demo](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster), [Medium Blog](https://medium.datadriveninvestor.com/introducing-fingpt-forecaster-the-future-of-robo-advisory-services-50add34e3d3c) & [Model](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) are available on HuggingfaceðŸ¤— !",fingpt,PROJECT
"ðŸ”¥ [Demo](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster), [Medium Blog](https://medium.datadriveninvestor.com/introducing-fingpt-forecaster-the-future-of-robo-advisory-services-50add34e3d3c) & [Model](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) are available on HuggingfaceðŸ¤— !",FinGPT,PROJECT
"- [Paper Acceptance] Oct, 2023: [""FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets""](https://arxiv.org/abs/2310.04793) is acceptedðŸŽ‰  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023   - [Paper Acceptance] Oct, 2023: [""FinGPT: Democratizing Internet-scale Data for Financial Large Language Models""](https://arxiv.org/abs/2307.10485) is acceptedðŸŽ‰  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023  - [Model Release] Oct, 2023: We release the [financial multi-task LLMs](https://huggingface.co/FinGPT) ðŸ”¥  produced when evaluating base-LLMs on [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)  - [Paper Acceptance] Sep, 2023: [""Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models""](https://arxiv.org/abs/2310.04027) is acceptedðŸŽ‰  by [ACM International Conference on AI in Finance (ICAIF-23)](https://ai-finance.org/icaif-23-accepted-papers/)  - [Model Release] Aug, 2023: We release the [financial sentiment analysis model](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) ðŸ”¥   - [Paper Acceptance] Jul, 2023: [""Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models""](https://arxiv.org/abs/2306.12659) is acceptedðŸŽ‰  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Paper Acceptance] Jul, 2023: [""FinGPT: Open-Source Financial Large Language Models""](https://arxiv.org/abs/2306.06031) is acceptedðŸŽ‰  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Medium Blog] Jun 2023: [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)  ## Why FinGPT?",FinGPT,PROJECT
"- [Paper Acceptance] Oct, 2023: [""FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets""](https://arxiv.org/abs/2310.04793) is acceptedðŸŽ‰  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023   - [Paper Acceptance] Oct, 2023: [""FinGPT: Democratizing Internet-scale Data for Financial Large Language Models""](https://arxiv.org/abs/2307.10485) is acceptedðŸŽ‰  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023  - [Model Release] Oct, 2023: We release the [financial multi-task LLMs](https://huggingface.co/FinGPT) ðŸ”¥  produced when evaluating base-LLMs on [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)  - [Paper Acceptance] Sep, 2023: [""Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models""](https://arxiv.org/abs/2310.04027) is acceptedðŸŽ‰  by [ACM International Conference on AI in Finance (ICAIF-23)](https://ai-finance.org/icaif-23-accepted-papers/)  - [Model Release] Aug, 2023: We release the [financial sentiment analysis model](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) ðŸ”¥   - [Paper Acceptance] Jul, 2023: [""Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models""](https://arxiv.org/abs/2306.12659) is acceptedðŸŽ‰  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Paper Acceptance] Jul, 2023: [""FinGPT: Open-Source Financial Large Language Models""](https://arxiv.org/abs/2306.06031) is acceptedðŸŽ‰  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Medium Blog] Jun 2023: [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)  ## Why FinGPT?",FinGPT,PROJECT
"- [Paper Acceptance] Oct, 2023: [""FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets""](https://arxiv.org/abs/2310.04793) is acceptedðŸŽ‰  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023   - [Paper Acceptance] Oct, 2023: [""FinGPT: Democratizing Internet-scale Data for Financial Large Language Models""](https://arxiv.org/abs/2307.10485) is acceptedðŸŽ‰  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023  - [Model Release] Oct, 2023: We release the [financial multi-task LLMs](https://huggingface.co/FinGPT) ðŸ”¥  produced when evaluating base-LLMs on [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)  - [Paper Acceptance] Sep, 2023: [""Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models""](https://arxiv.org/abs/2310.04027) is acceptedðŸŽ‰  by [ACM International Conference on AI in Finance (ICAIF-23)](https://ai-finance.org/icaif-23-accepted-papers/)  - [Model Release] Aug, 2023: We release the [financial sentiment analysis model](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) ðŸ”¥   - [Paper Acceptance] Jul, 2023: [""Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models""](https://arxiv.org/abs/2306.12659) is acceptedðŸŽ‰  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Paper Acceptance] Jul, 2023: [""FinGPT: Open-Source Financial Large Language Models""](https://arxiv.org/abs/2306.06031) is acceptedðŸŽ‰  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Medium Blog] Jun 2023: [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)  ## Why FinGPT?",FinGPT,PROJECT
"- [Paper Acceptance] Oct, 2023: [""FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets""](https://arxiv.org/abs/2310.04793) is acceptedðŸŽ‰  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023   - [Paper Acceptance] Oct, 2023: [""FinGPT: Democratizing Internet-scale Data for Financial Large Language Models""](https://arxiv.org/abs/2307.10485) is acceptedðŸŽ‰  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023  - [Model Release] Oct, 2023: We release the [financial multi-task LLMs](https://huggingface.co/FinGPT) ðŸ”¥  produced when evaluating base-LLMs on [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)  - [Paper Acceptance] Sep, 2023: [""Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models""](https://arxiv.org/abs/2310.04027) is acceptedðŸŽ‰  by [ACM International Conference on AI in Finance (ICAIF-23)](https://ai-finance.org/icaif-23-accepted-papers/)  - [Model Release] Aug, 2023: We release the [financial sentiment analysis model](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) ðŸ”¥   - [Paper Acceptance] Jul, 2023: [""Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models""](https://arxiv.org/abs/2306.12659) is acceptedðŸŽ‰  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Paper Acceptance] Jul, 2023: [""FinGPT: Open-Source Financial Large Language Models""](https://arxiv.org/abs/2306.06031) is acceptedðŸŽ‰  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Medium Blog] Jun 2023: [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)  ## Why FinGPT?",FinGPT,PROJECT
"- [Paper Acceptance] Oct, 2023: [""FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets""](https://arxiv.org/abs/2310.04793) is acceptedðŸŽ‰  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023   - [Paper Acceptance] Oct, 2023: [""FinGPT: Democratizing Internet-scale Data for Financial Large Language Models""](https://arxiv.org/abs/2307.10485) is acceptedðŸŽ‰  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023  - [Model Release] Oct, 2023: We release the [financial multi-task LLMs](https://huggingface.co/FinGPT) ðŸ”¥  produced when evaluating base-LLMs on [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)  - [Paper Acceptance] Sep, 2023: [""Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models""](https://arxiv.org/abs/2310.04027) is acceptedðŸŽ‰  by [ACM International Conference on AI in Finance (ICAIF-23)](https://ai-finance.org/icaif-23-accepted-papers/)  - [Model Release] Aug, 2023: We release the [financial sentiment analysis model](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) ðŸ”¥   - [Paper Acceptance] Jul, 2023: [""Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models""](https://arxiv.org/abs/2306.12659) is acceptedðŸŽ‰  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Paper Acceptance] Jul, 2023: [""FinGPT: Open-Source Financial Large Language Models""](https://arxiv.org/abs/2306.06031) is acceptedðŸŽ‰  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Medium Blog] Jun 2023: [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)  ## Why FinGPT?",FinGPT,PROJECT
"- [Paper Acceptance] Oct, 2023: [""FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets""](https://arxiv.org/abs/2310.04793) is acceptedðŸŽ‰  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023   - [Paper Acceptance] Oct, 2023: [""FinGPT: Democratizing Internet-scale Data for Financial Large Language Models""](https://arxiv.org/abs/2307.10485) is acceptedðŸŽ‰  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023  - [Model Release] Oct, 2023: We release the [financial multi-task LLMs](https://huggingface.co/FinGPT) ðŸ”¥  produced when evaluating base-LLMs on [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)  - [Paper Acceptance] Sep, 2023: [""Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models""](https://arxiv.org/abs/2310.04027) is acceptedðŸŽ‰  by [ACM International Conference on AI in Finance (ICAIF-23)](https://ai-finance.org/icaif-23-accepted-papers/)  - [Model Release] Aug, 2023: We release the [financial sentiment analysis model](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) ðŸ”¥   - [Paper Acceptance] Jul, 2023: [""Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models""](https://arxiv.org/abs/2306.12659) is acceptedðŸŽ‰  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Paper Acceptance] Jul, 2023: [""FinGPT: Open-Source Financial Large Language Models""](https://arxiv.org/abs/2306.06031) is acceptedðŸŽ‰  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Medium Blog] Jun 2023: [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)  ## Why FinGPT?",fingpt,PROJECT
"- [Paper Acceptance] Oct, 2023: [""FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets""](https://arxiv.org/abs/2310.04793) is acceptedðŸŽ‰  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023   - [Paper Acceptance] Oct, 2023: [""FinGPT: Democratizing Internet-scale Data for Financial Large Language Models""](https://arxiv.org/abs/2307.10485) is acceptedðŸŽ‰  by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023  - [Model Release] Oct, 2023: We release the [financial multi-task LLMs](https://huggingface.co/FinGPT) ðŸ”¥  produced when evaluating base-LLMs on [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)  - [Paper Acceptance] Sep, 2023: [""Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models""](https://arxiv.org/abs/2310.04027) is acceptedðŸŽ‰  by [ACM International Conference on AI in Finance (ICAIF-23)](https://ai-finance.org/icaif-23-accepted-papers/)  - [Model Release] Aug, 2023: We release the [financial sentiment analysis model](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) ðŸ”¥   - [Paper Acceptance] Jul, 2023: [""Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models""](https://arxiv.org/abs/2306.12659) is acceptedðŸŽ‰  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Paper Acceptance] Jul, 2023: [""FinGPT: Open-Source Financial Large Language Models""](https://arxiv.org/abs/2306.06031) is acceptedðŸŽ‰  by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023  - [Medium Blog] Jun 2023: [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)  ## Why FinGPT?",FinGPT,PROJECT
"BloombergGPT has privileged data access and APIs, while FinGPT presents a more accessible alternative.",FinGPT,PROJECT
"For detailed and more customized implementation, please refer to [FinGPT-Forecaster](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster)   ## FinGPT Demos:   ### Current State-of-the-arts for Financial Sentiment Analysis  * [FinGPT V3 (Updated on 10/12/2023)](.",FinGPT,PROJECT
"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc.",FinGPT,PROJECT
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginnerâ€™s Guide to FinGPT: Training with LoRA and ChatGLM2â€“6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT,PROJECT
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginnerâ€™s Guide to FinGPT: Training with LoRA and ChatGLM2â€“6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",fingpt,PROJECT
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginnerâ€™s Guide to FinGPT: Training with LoRA and ChatGLM2â€“6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT,PROJECT
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginnerâ€™s Guide to FinGPT: Training with LoRA and ChatGLM2â€“6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT,PROJECT
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginnerâ€™s Guide to FinGPT: Training with LoRA and ChatGLM2â€“6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",fingpt,PROJECT
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginnerâ€™s Guide to FinGPT: Training with LoRA and ChatGLM2â€“6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT,PROJECT
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginnerâ€™s Guide to FinGPT: Training with LoRA and ChatGLM2â€“6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT,PROJECT
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginnerâ€™s Guide to FinGPT: Training with LoRA and ChatGLM2â€“6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT,PROJECT
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginnerâ€™s Guide to FinGPT: Training with LoRA and ChatGLM2â€“6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT,PROJECT
"Or you may refer to the [wiki](https://ai4finance-foundation.github.io/FinNLP/)  <div align=""center""> <img align=""center"" src=figs/FinGPT_FinNLP_data_source.png> </div>  * [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark): We introduce a novel Instruction Tuning paradigm optimized for open-source Large Language Models (LLMs) in finance, enhancing their adaptability to diverse financial datasets while also facilitating cost-effective, systematic benchmarking from task-specific, multi-task, and zero-shot instruction tuning tasks.",FinGPT,PROJECT
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| Worldâ€™s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",FinGPT,PROJECT
"repo=AI4Finance-Foundation/FinGPT"" /> </a>  ## News  + [Columbia Perspectives on ChatGPT](https://datascience.columbia.edu/news/2023/columbia-perspectives-on-chatgpt/?",FinGPT,PROJECT
"v=x4dIx9VYQoM) --->   ## Citing FinGPT ``` @article{yang2023fingpt,   title={FinGPT: Open-Source Financial Large Language Models},   author={Yang, Hongyang and Liu, Xiao-Yang and Wang, Christina Dan},   journal={FinLLM Symposium at IJCAI 2023},   year={2023} } @article{zhang2023instructfingpt,       title={Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models},        author={Boyu Zhang and Hongyang Yang and Xiao-Yang Liu},       journal={FinLLM Symposium at IJCAI 2023},       year={2023} } @article{zhang2023fingptrag,   title={Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models},   author={Zhang, Boyu and Yang, Hongyang and Zhou, tianyu and Babar, Ali and Liu, Xiao-Yang},  journal = {ACM International Conference on AI in Finance (ICAIF)},   year={2023} }  @article{wang2023fingptbenchmark,   title={FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets},   author={Wang, Neng and Yang, Hongyang and Wang, Christina Dan},   journal={NeurIPS Workshop on Instruction Tuning and Instruction Following},   year={2023} } @article{2023finnlp,   title={Data-centric FinGPT: Democratizing Internet-scale Data for Financial Large Language Models},   author={Liu, Xiao-Yang and Wang, Guoxuan and Yang, Hongyang and Zha, Daochen},   journal={NeurIPS Workshop on Instruction Tuning and Instruction Following},   year={2023} }  ```  <div align=""center""> <a href=""https://finllm.github.io/workshop/#/fcb"" target=""_blank""> <img align=""center"" src=figs/fingpt_best_presentation.png width=""65%""> </div>   ## LICENSE  MIT License  **Disclaimer: We are sharing codes for academic purposes under the MIT education license.",FinGPT,PROJECT
"For more details, please visit our [project page](http://www.sysu-hcp.net/hierarchical-semantic-embedding/).",hierarchical-semantic-embedding,PROJECT
"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1.",HSE,PROJECT
"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1.",HSE,PROJECT
"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1.",HSE,PROJECT
Clone the repository Clone the Hierarchical Semantic Embedding project by: ``` git clone https://github.com/HCPLab-SYSU/HSE.git ``` and we denote the folder `hse-mm2018` as `$HSE_ROOT`.,Hierarchical Semantic Embedding,PROJECT
Clone the repository Clone the Hierarchical Semantic Embedding project by: ``` git clone https://github.com/HCPLab-SYSU/HSE.git ``` and we denote the folder `hse-mm2018` as `$HSE_ROOT`.,HSE,PROJECT
Clone the repository Clone the Hierarchical Semantic Embedding project by: ``` git clone https://github.com/HCPLab-SYSU/HSE.git ``` and we denote the folder `hse-mm2018` as `$HSE_ROOT`.,hse,PROJECT
Clone the repository Clone the Hierarchical Semantic Embedding project by: ``` git clone https://github.com/HCPLab-SYSU/HSE.git ``` and we denote the folder `hse-mm2018` as `$HSE_ROOT`.,HSE,PROJECT
"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . â”œâ”€â”€ code â”‚   â”œâ”€â”€ Butterfly200 â”‚   â”‚   â”œâ”€â”€ baseline â”‚   â”‚   â””â”€â”€ HSE â”‚   â”œâ”€â”€ CUB_200_2011 â”‚   â”‚   â”œâ”€â”€ baseline â”‚   â”‚   â””â”€â”€ HSE â”‚   â””â”€â”€ Vegfru â”‚       â”œâ”€â”€ baseline â”‚       â””â”€â”€ HSE â”œâ”€â”€ data â”‚   â”œâ”€â”€ Butterfly200 â”‚   â”‚   â””â”€â”€ images â”‚   â”œâ”€â”€ CUB_200_2011 â”‚   â”‚   â””â”€â”€ images â”‚   â””â”€â”€ Vegfru â”‚       â””â”€â”€ images â”œâ”€â”€ models â”‚   â”œâ”€â”€ Butterfly200 â”‚   â”œâ”€â”€ CUB_200_2011 â”‚   â””â”€â”€ Vegfru â””â”€â”€ scripts  ```  ## 2.",HSE,PROJECT
"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . â”œâ”€â”€ code â”‚   â”œâ”€â”€ Butterfly200 â”‚   â”‚   â”œâ”€â”€ baseline â”‚   â”‚   â””â”€â”€ HSE â”‚   â”œâ”€â”€ CUB_200_2011 â”‚   â”‚   â”œâ”€â”€ baseline â”‚   â”‚   â””â”€â”€ HSE â”‚   â””â”€â”€ Vegfru â”‚       â”œâ”€â”€ baseline â”‚       â””â”€â”€ HSE â”œâ”€â”€ data â”‚   â”œâ”€â”€ Butterfly200 â”‚   â”‚   â””â”€â”€ images â”‚   â”œâ”€â”€ CUB_200_2011 â”‚   â”‚   â””â”€â”€ images â”‚   â””â”€â”€ Vegfru â”‚       â””â”€â”€ images â”œâ”€â”€ models â”‚   â”œâ”€â”€ Butterfly200 â”‚   â”œâ”€â”€ CUB_200_2011 â”‚   â””â”€â”€ Vegfru â””â”€â”€ scripts  ```  ## 2.",HSE,PROJECT
"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . â”œâ”€â”€ code â”‚   â”œâ”€â”€ Butterfly200 â”‚   â”‚   â”œâ”€â”€ baseline â”‚   â”‚   â””â”€â”€ HSE â”‚   â”œâ”€â”€ CUB_200_2011 â”‚   â”‚   â”œâ”€â”€ baseline â”‚   â”‚   â””â”€â”€ HSE â”‚   â””â”€â”€ Vegfru â”‚       â”œâ”€â”€ baseline â”‚       â””â”€â”€ HSE â”œâ”€â”€ data â”‚   â”œâ”€â”€ Butterfly200 â”‚   â”‚   â””â”€â”€ images â”‚   â”œâ”€â”€ CUB_200_2011 â”‚   â”‚   â””â”€â”€ images â”‚   â””â”€â”€ Vegfru â”‚       â””â”€â”€ images â”œâ”€â”€ models â”‚   â”œâ”€â”€ Butterfly200 â”‚   â”œâ”€â”€ CUB_200_2011 â”‚   â””â”€â”€ Vegfru â””â”€â”€ scripts  ```  ## 2.",HSE,PROJECT
"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . â”œâ”€â”€ code â”‚   â”œâ”€â”€ Butterfly200 â”‚   â”‚   â”œâ”€â”€ baseline â”‚   â”‚   â””â”€â”€ HSE â”‚   â”œâ”€â”€ CUB_200_2011 â”‚   â”‚   â”œâ”€â”€ baseline â”‚   â”‚   â””â”€â”€ HSE â”‚   â””â”€â”€ Vegfru â”‚       â”œâ”€â”€ baseline â”‚       â””â”€â”€ HSE â”œâ”€â”€ data â”‚   â”œâ”€â”€ Butterfly200 â”‚   â”‚   â””â”€â”€ images â”‚   â”œâ”€â”€ CUB_200_2011 â”‚   â”‚   â””â”€â”€ images â”‚   â””â”€â”€ Vegfru â”‚       â””â”€â”€ images â”œâ”€â”€ models â”‚   â”œâ”€â”€ Butterfly200 â”‚   â”œâ”€â”€ CUB_200_2011 â”‚   â””â”€â”€ Vegfru â””â”€â”€ scripts  ```  ## 2.",HSE,PROJECT
"[Vegfru](https://github.com/ustc-vim/vegfru) is proposed by [Hou et al., ICCV2017](http://home.ustc.edu.cn/~saihui/project/vegfru/iccv17_vegfru.pdf), and it covers two-level categories.  ## 3.",vegfru,PROJECT
"Deployment Firstly, make sure the working directory is `$HSE_ROOT`, or ``` cd $HSE_ROOT ``` then, run the deployment script: ## deploy HSE ``` .",HSE,PROJECT
"Deployment Firstly, make sure the working directory is `$HSE_ROOT`, or ``` cd $HSE_ROOT ``` then, run the deployment script: ## deploy HSE ``` .",HSE,PROJECT
"Deployment Firstly, make sure the working directory is `$HSE_ROOT`, or ``` cd $HSE_ROOT ``` then, run the deployment script: ## deploy HSE ``` .",HSE,PROJECT
"Word2Vec embeddings available at: http://legislation.di.uoa.gr/publications/ner_word2vec  **Note:**   NLP training scripts based on ""Large-Scale Multi-Label Text Classification on EU Legislation"" project.",Large-Scale Multi-Label Text Classification on EU Legislation,PROJECT
"For full code and project structure, follow lmtc-eurlex57k project instructions at: https://github.com/iliaschalkidis/lmtc-eurlex57k",lmtc-eurlex57k,PROJECT
"For full code and project structure, follow lmtc-eurlex57k project instructions at: https://github.com/iliaschalkidis/lmtc-eurlex57k",lmtc-eurlex57k,PROJECT
"Related Mashups ---------------- [Ontology-based math formula search](https://github.com/CLLKazan/MathSearch)  Links to External LOD Datasets ------------------------------ * [A mapping to DBpedia](https://github.com/CLLKazan/OntoMathPro/blob/master/external.links.dbpedia.nt) * [A mapping to ScienceWISE](https://github.com/CLLKazan/OntoMathPro/blob/master/external.links.sciencewise.nt)    License ---------------------  Licensed under the Apache License, Version 2.0: [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)",MathSearch,PROJECT
# EORB-SLAM: An Event-based ORB-SLAM  This project is a feature-based odometry/SLAM algorithm that is used to estimate the 6DoF pose of a robot and reconstruct the 3D point cloud of the scene using a variety of sensors.,EORB-SLAM,PROJECT
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,Pangolin,PROJECT
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,Pangolin,PROJECT
"Integration of this project with ROS is possible, but it requires compiling and installing the core project first and then linking it to your ROS logic.  ## Installation  Once you installed all the dependencies, you can download and install the project:  ```bash git clone https://github.com/m-dayani/EORB_SLAM.git cd EORB_SLAM  mkdir build cd build cmake .. make -j4 ```  To make life easier for you, I included a bash script (`build_eorb_slam.sh`) and a Docker file that contains all the necessary commands to download and install the required packages.  ## Usage  1.",EORB_SLAM,PROJECT
```bash â”œâ”€ conf                    # All configurations live there â”œâ”€ notebooks               # Notebooks to get started with multimodal datasets and models â”œâ”€ eval.py                 # Eval script â”œâ”€ insall.sh               # Installation script for DeepViewAgg â”œâ”€ scripts                 # Some scripts to help manage the project â”œâ”€ torch_points3d     â”œâ”€ core                # Core components     â”œâ”€ datasets            # All code related to datasets     â”œâ”€ metrics             # All metrics and trackers     â”œâ”€ models              # All models     â”œâ”€ modules             # Basic modules that can be used in a modular way     â”œâ”€ utils               # Various utils     â””â”€ visualization       # Visualization â””â”€ train.py                # Main script to launch a training ```  Several changes were made to extend the original project to multimodal learning on point clouds with images.,DeepViewAgg,PROJECT
"<br>  ## ðŸ’³  Credits - This implementation of DeepViewAgg largely relies on the  [Torch-Points3D framework](https://github.com/nicolas-chaulet/torch-points3d), although not merged with the official project  at this point",torch-points3d,PROJECT
"# RumexLeaves-CenterNet This repository contains the official implementation for  [Zoom in on the Plant: Fine-grained Analysis of Leaf, Stem and Vein Instances.]",RumexLeaves-CenterNet,PROJECT
The full description of this work is [available on arXiv](https://arxiv.org/abs/2110.05812).  ## Application on the Ampli ANR project  ### Goal This repo was used as part of the [Ampli ANR projet](https://projet.liris.cnrs.fr/ampli/).,Ampli ANR,PROJECT
The full description of this work is [available on arXiv](https://arxiv.org/abs/2110.05812).  ## Application on the Ampli ANR project  ### Goal This repo was used as part of the [Ampli ANR projet](https://projet.liris.cnrs.fr/ampli/).,Ampli ANR,PROJECT
"usp=sharing).  ``` # single-gpu testing python tools/test.py <CONFIG_FILE> <SEG_CHECKPOINT_FILE> --eval mIoU  # multi-gpu testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --eval mIoU  # multi-gpu, multi-scale testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --aug-test --eval mIoU ```  Example on the Ampli ANR project:   ``` # Evaluate checkpoint on a single GPU python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --eval mIoU  # Display segmentation results python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --show ```  ### Training  To train with pre-trained models, run: ``` # single-gpu training python tools/train.py <CONFIG_FILE> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  # multi-gpu training tools/dist_train.sh <CONFIG_FILE> <GPU_NUM> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  ```  Example on the Ampli ANR project with the ImageNet-22K pretrained model (available [here](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth)) :   ``` python tools/train.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py --options model.pretrained="".",Ampli ANR,PROJECT
"usp=sharing).  ``` # single-gpu testing python tools/test.py <CONFIG_FILE> <SEG_CHECKPOINT_FILE> --eval mIoU  # multi-gpu testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --eval mIoU  # multi-gpu, multi-scale testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --aug-test --eval mIoU ```  Example on the Ampli ANR project:   ``` # Evaluate checkpoint on a single GPU python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --eval mIoU  # Display segmentation results python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --show ```  ### Training  To train with pre-trained models, run: ``` # single-gpu training python tools/train.py <CONFIG_FILE> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  # multi-gpu training tools/dist_train.sh <CONFIG_FILE> <GPU_NUM> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  ```  Example on the Ampli ANR project with the ImageNet-22K pretrained model (available [here](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth)) :   ``` python tools/train.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py --options model.pretrained="".",Ampli ANR,PROJECT
"If you use this work, please cite it: ``` @misc{guerin2021satellite,       title={Satellite Image Semantic Segmentation},        author={Eric GuÃ©rin and Killian Oechslin and Christian Wolf and BenoÃ®t Martinez},       year={2021},       eprint={2110.05812},       archivePrefix={arXiv},       primaryClass={cs.CV} } ```  ## Other Links  > **Image Classification**: See [Swin Transformer for Image Classification](https://github.com/microsoft/Swin-Transformer)",Swin Transformer for Image Classification,PROJECT
> **Object Detection**: See [Swin Transformer for Object Detection](https://github.com/SwinTransformer/Swin-Transformer-Object-Detection),Swin Transformer for Object Detection,PROJECT
> **Object Detection**: See [Swin Transformer for Object Detection](https://github.com/SwinTransformer/Swin-Transformer-Object-Detection),Swin-Transformer-Object-Detection,PROJECT
> **Self-Supervised Learning**: See [MoBY with Swin Transformer](https://github.com/SwinTransformer/Transformer-SSL),MoBY with Swin Transformer,PROJECT
"> **Video Recognition**, See [Video Swin Transformer](https://github.com/SwinTransformer/Video-Swin-Transformer).",Video Swin Transformer,PROJECT
"> **Video Recognition**, See [Video Swin Transformer](https://github.com/SwinTransformer/Video-Swin-Transformer).",Video-Swin-Transformer,PROJECT
"usp=sharing) provided by [AlignPS](https://github.com/daodaofr/AlignPS).   #####  Train with a single GPU ```shell python tools/train.py ${CONFIG_FILE} --no-validate ``` - CONFIG_FILE about PSTR is in [configs/PSTR](configs/pstr)  #####  Test with a single GPU  ```shell PRW: sh run_test_prw.sh  CUHK: sh run_test_cuhk.sh   ```  - If you want to output the results of different models, please  change CONFIGPATH, MODELPATH, OUTPATH for diffferent models   ## Results  We provide some models with different backbones and results on PRW and CUHK-SYSU datasets, which have a little difference to CVPR version due to jitter",AlignPS,PROJECT
"usp=sharing) provided by [AlignPS](https://github.com/daodaofr/AlignPS).   #####  Train with a single GPU ```shell python tools/train.py ${CONFIG_FILE} --no-validate ``` - CONFIG_FILE about PSTR is in [configs/PSTR](configs/pstr)  #####  Test with a single GPU  ```shell PRW: sh run_test_prw.sh  CUHK: sh run_test_cuhk.sh   ```  - If you want to output the results of different models, please  change CONFIGPATH, MODELPATH, OUTPATH for diffferent models   ## Results  We provide some models with different backbones and results on PRW and CUHK-SYSU datasets, which have a little difference to CVPR version due to jitter",AlignPS,PROJECT
"- \+ indicates adding a re-scoring module during evaluation, where we modify the final matching score as the weighted score of [CBGM](https://github.com/serend1p1ty/SeqNet) score and originial matching scores.   ## Citation If the project helps your research, please cite this paper.  ``` @article{Cao_PSTR_CVPR_2022,   author =       {Jiale Cao and Yanwei Pang and Rao Muhammad Anwer and Hisham Cholakkal and Jin Xie and Mubarak Shah and Fahad Shahbaz Khan},   title =        {PSTR: End-to-End One-Step Person Search With Transformers},   journal =      {Proc.",SeqNet,PROJECT
"IEEE Conference on Computer Vision and Pattern Recognition},   year =         {2022} } ```  ## Acknowledgement Many thanks to the open source codes: [mmdetection](https://github.com/open-mmlab/mmdetection), [AlignPS](https://github.com/daodaofr/AlignPS), and [SeqNet](https://github.com/serend1p1ty/SeqNet).",mmdetection,PROJECT
"IEEE Conference on Computer Vision and Pattern Recognition},   year =         {2022} } ```  ## Acknowledgement Many thanks to the open source codes: [mmdetection](https://github.com/open-mmlab/mmdetection), [AlignPS](https://github.com/daodaofr/AlignPS), and [SeqNet](https://github.com/serend1p1ty/SeqNet).",mmdetection,PROJECT
"IEEE Conference on Computer Vision and Pattern Recognition},   year =         {2022} } ```  ## Acknowledgement Many thanks to the open source codes: [mmdetection](https://github.com/open-mmlab/mmdetection), [AlignPS](https://github.com/daodaofr/AlignPS), and [SeqNet](https://github.com/serend1p1ty/SeqNet).",AlignPS,PROJECT
"IEEE Conference on Computer Vision and Pattern Recognition},   year =         {2022} } ```  ## Acknowledgement Many thanks to the open source codes: [mmdetection](https://github.com/open-mmlab/mmdetection), [AlignPS](https://github.com/daodaofr/AlignPS), and [SeqNet](https://github.com/serend1p1ty/SeqNet).",AlignPS,PROJECT
"IEEE Conference on Computer Vision and Pattern Recognition},   year =         {2022} } ```  ## Acknowledgement Many thanks to the open source codes: [mmdetection](https://github.com/open-mmlab/mmdetection), [AlignPS](https://github.com/daodaofr/AlignPS), and [SeqNet](https://github.com/serend1p1ty/SeqNet).",SeqNet,PROJECT
"IEEE Conference on Computer Vision and Pattern Recognition},   year =         {2022} } ```  ## Acknowledgement Many thanks to the open source codes: [mmdetection](https://github.com/open-mmlab/mmdetection), [AlignPS](https://github.com/daodaofr/AlignPS), and [SeqNet](https://github.com/serend1p1ty/SeqNet).",SeqNet,PROJECT
# DejaVu ## Table of Contents =================    * [Code](#code)     * [Install Requirements](#install-requirements)     * [Usage](#usage)     * [Example](#example)   * [Datasets](#datasets)   * [Deployment and Failure Injection Scripts of Train-Ticket](#deployment-and-failure-injection-scripts-of-train-ticket)   * [Citation](#citation)   * [Supplementary details](#supplementary-details)    ## Paper A preprint version: https://arxiv.org/abs/2207.09021 ## Code ### Install 1.,DejaVu,PROJECT
Pull the code from GitHub    ```bash    git pull https://github.com/NetManAIOps/DejaVu.git DejaVu    ``` 3.,DejaVu,PROJECT
Pull the code from GitHub    ```bash    git pull https://github.com/NetManAIOps/DejaVu.git DejaVu    ``` 3.,DejaVu,PROJECT
"Since 2019, the BLEBeacon dataset is part of the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD): https://crawdad.org/unm/blebeacon/20190312/  To cite the dataset:  > Dimitrios Sikeridis, Ioannis Papapanagiotou, Michael Devetsikiotis, CRAWDAD dataset unm/blebeacon (v. 2019â€‘03â€‘12), downloaded from https://crawdad.org/unm/blebeacon/20190312, Mar 2019",Community Resource for Archiving Wireless Data At Dartmouth,PROJECT
"Since 2019, the BLEBeacon dataset is part of the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD): https://crawdad.org/unm/blebeacon/20190312/  To cite the dataset:  > Dimitrios Sikeridis, Ioannis Papapanagiotou, Michael Devetsikiotis, CRAWDAD dataset unm/blebeacon (v. 2019â€‘03â€‘12), downloaded from https://crawdad.org/unm/blebeacon/20190312, Mar 2019",crawdad,PROJECT
"Since 2019, the BLEBeacon dataset is part of the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD): https://crawdad.org/unm/blebeacon/20190312/  To cite the dataset:  > Dimitrios Sikeridis, Ioannis Papapanagiotou, Michael Devetsikiotis, CRAWDAD dataset unm/blebeacon (v. 2019â€‘03â€‘12), downloaded from https://crawdad.org/unm/blebeacon/20190312, Mar 2019",CRAWDAD,PROJECT
"Since 2019, the BLEBeacon dataset is part of the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD): https://crawdad.org/unm/blebeacon/20190312/  To cite the dataset:  > Dimitrios Sikeridis, Ioannis Papapanagiotou, Michael Devetsikiotis, CRAWDAD dataset unm/blebeacon (v. 2019â€‘03â€‘12), downloaded from https://crawdad.org/unm/blebeacon/20190312, Mar 2019",crawdad,PROJECT
> [Cite using BibTeX](https://crawdad.org/unm/blebeacon/20190312/)    # Technical Summary  ## Operation     Users carried off-the-shelf Gimbal Series 10 iBeacons that continuously transmit BLE advertisement packets.,crawdad,PROJECT
"/README_CN.md"">ç®€ä½“ä¸­æ–‡</a></div>  <div align=""center"">     <img src=""docs/figs/logo.png"" align=""center"" width=""34%""> </div>  <div align=""center"">     <h2><strong>FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation</strong></h2> </div>  <div align=""center"">     <a href=""https://scholar.google.com/citations?",FRNet,PROJECT
"page_id=Xiangxu-0103.FRNet&left_color=gray&right_color=purple"">     </a> </div>  ## About  **FRNet** is a simple yet efficient network for LiDAR segmentation.",FRNet,PROJECT
"page_id=Xiangxu-0103.FRNet&left_color=gray&right_color=purple"">     </a> </div>  ## About  **FRNet** is a simple yet efficient network for LiDAR segmentation.",FRNet,PROJECT
"**FRNet** achieves an appealing balance between accuracy and efficiency, enabling real-time LiDAR segmentation",FRNet,PROJECT
"| <img src=""docs/figs/teaser1.png"" align=""center"" width=""91%""> | <img src=""docs/figs/teaser2.png"" align=""center"" width=""89%""> | | :----------------------------------------------------------: | :----------------------------------------------------------: | |                     Speed *vs.* Accuracy                     |                    Speed *vs.* Robustness                    |  Visit our [project page](https://xiangxu-0103.github.io/FRNet) to explore more examples.",FRNet,PROJECT
"usp=sharing).  ## License  This work is under the [Apache 2.0 license](LICENSE).  ## Citation  If you find this work helpful, please kindly consider citing our paper:  ```bibtex @article{xu2023frnet,     title = {FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation},     author = {Xu, Xiang and Kong, Lingdong and Shuai, Hui and Liu, Qingshan},     journal = {arXiv preprint arXiv:2312.04484},     year = {2023} } ```  ## Acknowledgements  This work is developed based on the [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) codebase",FRNet,PROJECT
"usp=sharing).  ## License  This work is under the [Apache 2.0 license](LICENSE).  ## Citation  If you find this work helpful, please kindly consider citing our paper:  ```bibtex @article{xu2023frnet,     title = {FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation},     author = {Xu, Xiang and Kong, Lingdong and Shuai, Hui and Liu, Qingshan},     journal = {arXiv preprint arXiv:2312.04484},     year = {2023} } ```  ## Acknowledgements  This work is developed based on the [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) codebase",MMDetection3D,PROJECT
"usp=sharing).  ## License  This work is under the [Apache 2.0 license](LICENSE).  ## Citation  If you find this work helpful, please kindly consider citing our paper:  ```bibtex @article{xu2023frnet,     title = {FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation},     author = {Xu, Xiang and Kong, Lingdong and Shuai, Hui and Liu, Qingshan},     journal = {arXiv preprint arXiv:2312.04484},     year = {2023} } ```  ## Acknowledgements  This work is developed based on the [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) codebase",mmdetection3d,PROJECT
"> <img src=""https://github.com/open-mmlab/mmdetection3d/blob/main/resources/mmdet3d-logo.png"" width=""30%""/><br> > MMDetection3D is an open-source object detection toolbox based on PyTorch, towards the next-generation platform for general 3D perception.",mmdetection3d,PROJECT
"> <img src=""https://github.com/open-mmlab/mmdetection3d/blob/main/resources/mmdet3d-logo.png"" width=""30%""/><br> > MMDetection3D is an open-source object detection toolbox based on PyTorch, towards the next-generation platform for general 3D perception.",MMDetection3D,PROJECT
It is a part of the OpenMMLab project developed by MMLab.,OpenMMLab,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-lasot-ext)](https://paperswithcode.com/sota/visual-object-tracking-on-lasot-ext?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-lasot-ext)](https://paperswithcode.com/sota/visual-object-tracking-on-lasot-ext?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-lasot)](https://paperswithcode.com/sota/visual-object-tracking-on-lasot?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-lasot)](https://paperswithcode.com/sota/visual-object-tracking-on-lasot?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-tracking-on-tnl2k)](https://paperswithcode.com/sota/visual-tracking-on-tnl2k?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-tracking-on-tnl2k)](https://paperswithcode.com/sota/visual-tracking-on-tnl2k?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-trackingnet)](https://paperswithcode.com/sota/visual-object-tracking-on-trackingnet?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-trackingnet)](https://paperswithcode.com/sota/visual-object-tracking-on-trackingnet?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/multi-object-tracking-and-segmentation-on-3)](https://paperswithcode.com/sota/multi-object-tracking-and-segmentation-on-3?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/multi-object-tracking-and-segmentation-on-3)](https://paperswithcode.com/sota/multi-object-tracking-and-segmentation-on-3?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/multiple-object-tracking-on-bdd100k-val)](https://paperswithcode.com/sota/multiple-object-tracking-on-bdd100k-val?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/multiple-object-tracking-on-bdd100k-val)](https://paperswithcode.com/sota/multiple-object-tracking-on-bdd100k-val?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/video-instance-segmentation-on-youtube-vis-1)](https://paperswithcode.com/sota/video-instance-segmentation-on-youtube-vis-1?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/video-instance-segmentation-on-youtube-vis-1)](https://paperswithcode.com/sota/video-instance-segmentation-on-youtube-vis-1?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/video-instance-segmentation-on-ovis-1)](https://paperswithcode.com/sota/video-instance-segmentation-on-ovis-1?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/video-instance-segmentation-on-ovis-1)](https://paperswithcode.com/sota/video-instance-segmentation-on-ovis-1?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refer-1)](https://paperswithcode.com/sota/referring-expression-segmentation-on-refer-1?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refer-1)](https://paperswithcode.com/sota/referring-expression-segmentation-on-refer-1?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-davis)](https://paperswithcode.com/sota/referring-expression-segmentation-on-davis?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-davis)](https://paperswithcode.com/sota/referring-expression-segmentation-on-davis?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refcoco)](https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refcoco)](https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refcoco-3)](https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco-3?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on-refcoco)](https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on-refcoco)](https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on)](https://paperswithcode.com/sota/referring-expression-comprehension-on?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on)](https://paperswithcode.com/sota/referring-expression-comprehension-on?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on-refcoco-1)](https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco-1?,paperswithcode,PROJECT
"# UNDAW Repository  ### Welcome to the repository of UNDAW (Unsupervised Adversarial Domain Adaptation Based on the Wasserstein Distance)  This is the repository for the method presented in the paper:  ""Unsupervised Adversarial Domain Adaptation Based on the Wasserstein Distance,""  by [K.",UNDAW,PROJECT
"# UNDAW Repository  ### Welcome to the repository of UNDAW (Unsupervised Adversarial Domain Adaptation Based on the Wasserstein Distance)  This is the repository for the method presented in the paper:  ""Unsupervised Adversarial Domain Adaptation Based on the Wasserstein Distance,""  by [K.",UNDAW,PROJECT
* The research leading to these results has received funding from the [European Research  Council](https://erc.europa.eu/) under the European Unionâ€™s H2020 Framework Programme  through ERC Grant Agreement 637422 EVERYSOUND,European Unionâ€™s H2020 Framework Programme,PROJECT
"The paper is available on: - *Computer Vision Foundation (CVF)*: https://openaccess.thecvf.com/content/WACV2021/html/Muller-Budack_Ontology-Driven_Event_Type_Classification_in_Images_WACV_2021_paper.html - *arXiv*: https://arxiv.org/pdf/2011.04714.pdf  Further information can be found on the **EventKG** website: http://eventkg.l3s.uni-hannover.de/VisE   ## Content  - [Ontology-driven Event Type Classification in Images](#ontology-driven-event-type-classification-in-images)   - [Content](#content)   - [Setup](#setup)     - [Setup with Singularity (for Reproducibility)](#setup-with-singularity-for-reproducibility)     - [Setup with Virtual Environment](#setup-with-virtual-environment)     - [Setup with Docker](#setup-with-docker)   - [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models)   - [Models](#models)   - [Inference](#inference)   - [Test](#test)   - [VisE-D: Visual Event Classification Dataset](#vise-d-visual-event-classification-dataset)   - [VisE-O: Visual Event Ontology](#vise-o-visual-event-ontology)   - [Benchmark Ontologies](#benchmark-ontologies)   - [Supplemental Material](#supplemental-material)   - [LICENSE](#license)   ## Setup  We provide three different ways to setup the project.",EventKG,PROJECT
"The paper is available on: - *Computer Vision Foundation (CVF)*: https://openaccess.thecvf.com/content/WACV2021/html/Muller-Budack_Ontology-Driven_Event_Type_Classification_in_Images_WACV_2021_paper.html - *arXiv*: https://arxiv.org/pdf/2011.04714.pdf  Further information can be found on the **EventKG** website: http://eventkg.l3s.uni-hannover.de/VisE   ## Content  - [Ontology-driven Event Type Classification in Images](#ontology-driven-event-type-classification-in-images)   - [Content](#content)   - [Setup](#setup)     - [Setup with Singularity (for Reproducibility)](#setup-with-singularity-for-reproducibility)     - [Setup with Virtual Environment](#setup-with-virtual-environment)     - [Setup with Docker](#setup-with-docker)   - [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models)   - [Models](#models)   - [Inference](#inference)   - [Test](#test)   - [VisE-D: Visual Event Classification Dataset](#vise-d-visual-event-classification-dataset)   - [VisE-O: Visual Event Ontology](#vise-o-visual-event-ontology)   - [Benchmark Ontologies](#benchmark-ontologies)   - [Supplemental Material](#supplemental-material)   - [LICENSE](#license)   ## Setup  We provide three different ways to setup the project.",eventkg,PROJECT
[Release](https://img.shields.io/github/release/falkenber9/falcon)](https://github.com/falkenber9/falcon/releases) [!,falcon,PROJECT
[Release](https://img.shields.io/github/release/falkenber9/falcon)](https://github.com/falkenber9/falcon/releases) [!,falcon,PROJECT
[License](https://img.shields.io/github/license/falkenber9/falcon)](LICENSE) [!,falcon,PROJECT
v=Va_aZYxRu3U)  **FALCON** is an open-source software collection for real-time analysis of radio resources in private or commercial LTE/LTE-A networks.,FALCON,PROJECT
FALCON enables an exact determination of the current network load and the identification of bottlenecks.,FALCON,PROJECT
"[DFG](gfx/DFG_small.png ""DFG"")  The research around this project has been supported by *Deutsche Forschungsgemeinschaft* (DFG) within the Collaborative Research Center SFB 876 â€œProviding Information by Resource-Constrained Analysisâ€, project A4 at TU Dortmund University.",Providing Information by Resource-Constrained Analysis,PROJECT
[FALCON Video Presentation](http://img.youtube.com/vi/Va_aZYxRu3U/0.jpg)](http://www.youtube.com/watch?,FALCON,PROJECT
"# FALCON git clone https://aur.archlinux.org/tudo-falcon.git cd tudo-falcon makepkg -si ```  ## SDR Hardware FALCON has been tested with the following Software Defined Radios (SDRs):  * Ettus Research: USRP B210 * Ettus Research: USRP B205mini * Lime Microsystems: LimeSDR Mini  In addition, any SDR supported by [srsLTE library][srslte] should work as well.  ## Computer Hardware Requirements / Performance Real-time decoding of LTE signals requires a mature multicore CPU, especially when monitoring busy cells and large bandwidths (i.e. 15MHz and 20MHz cells).",FALCON,PROJECT
"[FALCON Screenshot](gfx/FalconGUI.png ""Falcon GUI Screenshot"")   ### FALCON Eye A command-line version of FALCON Decoder.",FALCON,PROJECT
# GLIB  [!,GLIB,PROJECT
[DOI](https://zenodo.org/badge/342098687.svg)](https://zenodo.org/badge/latestdoi/342098687) <br> CNN-based visual understanding for detecting UI glitches in game Apps > GLIB: Towards Automated Test Oracle for Graphically-Rich Applications <br> > Paper URL: https://arxiv.org/abs/2106.10507  ## Architecture  !,GLIB,PROJECT
".. raw:: html   The COBRA Toolbox |br| COnstraint-Based Reconstruction and Analysis Toolbox ---------------------------------------------------------------------------  .. raw:: html     <table>      <tr>      <td><div align=""center""><a href=""https://opencobra.github.io/cobratoolbox/latest/tutorials/index.html""><img src=""https://img.shields.io/badge/COBRA-tutorials-blue.svg?",opencobra,PROJECT
"maxAge=0""></a>        <a href=""https://opencobra.github.io/cobratoolbox/latest""><img src=""https://img.shields.io/badge/COBRA-docs-blue.svg?",opencobra,PROJECT
"maxAge=0""></a></div></td>        <td><div align=""center""><a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-linux/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/linux.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-macOS/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/macOS.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-windows7/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/windows.svg""></a>        <a href=""http://opencobra.github.io/cobratoolbox/docs/builds.html""><img src=""http://concordion.org/img/benefit-links.png?",opencobra,PROJECT
"You may install ``TOMLAB``, ``IBM ILOG CPLEX``, ``GUROBI``, or ``MOSEK`` by following these `detailed instructions <https://opencobra.github.io/cobratoolbox/docs/solvers.html>`__.  .. end-requirements-marker  Installation ------------  .. begin-installation-marker  1.",opencobra,PROJECT
You can clone the repository using:     .. code-block:: console        $ git clone --depth=1 https://github.com/opencobra/cobratoolbox.git cobratoolbox      |warning| Please note the ``--depth=1`` in the clone command.,opencobra,PROJECT
All tutorials can be run from    the    `/tutorials <https://github.com/opencobra/cobratoolbox/tree/master/tutorials>`__    directory,opencobra,PROJECT
-  Answers to Frequently Asked Questions (**FAQ**) are    `here <https://opencobra.github.io/cobratoolbox/stable/faq.html>`__.,opencobra,PROJECT
"How to contribute -----------------  .. begin-how-to-contribute-marker  |thumbsup| |tada| First off, thanks for taking the time to contribute to `The COBRA Toolbox <https://github.com/opencobra/cobratoolbox>`__!",opencobra,PROJECT
"|tada| |thumbsup|  .. raw:: html     <p align=""center"">    <img src=""https://cdn.jsdelivr.net/gh/opencobra/MATLAB.devTools@e735bd91310e8ef10fab4d3c21833a85bf4b8159/docs/source/_static/img/logo_devTools.png"" height=""120px"" alt=""devTools""/>    </p>   You can install the `MATLAB.devTools <https://github.com/opencobra/MATLAB.devTools>`__ from within MATLAB by typing:  .. code-block:: matlab      >> installDevTools()  |bulb| Check out `MATLAB.devTools <https://github.com/opencobra/MATLAB.devTools>`__ - and contribute the smart way!",opencobra,PROJECT
"|tada| |thumbsup|  .. raw:: html     <p align=""center"">    <img src=""https://cdn.jsdelivr.net/gh/opencobra/MATLAB.devTools@e735bd91310e8ef10fab4d3c21833a85bf4b8159/docs/source/_static/img/logo_devTools.png"" height=""120px"" alt=""devTools""/>    </p>   You can install the `MATLAB.devTools <https://github.com/opencobra/MATLAB.devTools>`__ from within MATLAB by typing:  .. code-block:: matlab      >> installDevTools()  |bulb| Check out `MATLAB.devTools <https://github.com/opencobra/MATLAB.devTools>`__ - and contribute the smart way!",opencobra,PROJECT
The **official documentation** is `here <https://opencobra.github.io/MATLAB.devTools/ >`__.,opencobra,PROJECT
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,opencobra,PROJECT
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,opencobra,PROJECT
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,opencobra,PROJECT
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,opencobra,PROJECT
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,opencobra,PROJECT
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,opencobra,PROJECT
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,opencobra,PROJECT
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,opencobra,PROJECT
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,opencobra,PROJECT
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,opencobra,PROJECT
"Check the compatibility `here <https://opencobra.github.io/cobratoolbox/docs/compatibility.html>`__.  .. begin-binaries-marker  For convenience, we provide `glpk_mex <https://github.com/blegat/glpkmex>`__ and `libSBML-5.17+ <http://sbml.org/Software/libSBML>`__ in ``/external``.",opencobra,PROJECT
"`Binaries <https://github.com/opencobra/COBRA.binary>`__ for these libraries are provided in a submodule for Mac OS X 10.6 or later (64-bit), GNU/Linux Ubuntu 14.0+ (64-bit), and Microsoft Windows 7+ (64-bit).",opencobra,PROJECT
Read more on the compatibility with SBML-FBCv2 `here <https://opencobra.github.io/cobratoolbox/docs/notes.html>`__.  .. end-binaries-marker  Disclaimer ----------  *The software provided by the openCOBRA Project is distributed under the GNU GPLv3 or later.,openCOBRA,PROJECT
If the user so chooses to use the software provided by the openCOBRA project for commercial endeavors then it is solely the userâ€™s responsibility to license any patents that may exist and respond in full to any legal actions taken by the patent holder.*   .. icon-marker   ..,openCOBRA,PROJECT
"|tutorials| raw:: html     <a href=""https://opencobra.github.io/cobratoolbox/latest/tutorials/index.html""><img src=""https://img.shields.io/badge/COBRA-tutorials-blue.svg?",opencobra,PROJECT
"|latest| raw:: html     <a href=""https://opencobra.github.io/cobratoolbox/latest""><img src=""https://img.shields.io/badge/COBRA-docs-blue.svg?",opencobra,PROJECT
"# An Empirical Framework for Domain Generalization In Clinical Settings  ## Paper If you use this code in your research, please cite the following publication: ``` @inproceedings{zhang2021empirical,   title={An empirical framework for domain generalization in clinical settings},   author={Zhang, Haoran and Dullerud, Natalie and Seyyed-Kalantari, Laleh and Morris, Quaid and Joshi, Shalmali and Ghassemi, Marzyeh},   booktitle={Proceedings of the Conference on Health, Inference, and Learning},   pages={279--290},   year={2021} } ```  This paper can also be found on arxiv: https://arxiv.org/abs/2103.11163    ## Acknowledgements  Our implementation is a modified version of the excellent [DomainBed](https://github.com/facebookresearch/DomainBed) framework (from commit [a10458a](https://github.com/facebookresearch/DomainBed/tree/a10458a2adfd8aec0fda2d617f710e5044e5dc60)).",DomainBed,PROJECT
"We also make use of some code from [eICU Benchmarks](https://github.com/mostafaalishahi/eICU_Benchmark).   ## To replicate the experiments in the paper:  ### Step 0: Environment and Prerequisites Run the following commands to clone this repo and create the Conda environment:  ``` git clone https://github.com/MLforHealth/ClinicalDG.git cd ClinicalDG/ conda env create -f environment.yml conda activate clinicaldg ```  ### Step 1: Obtaining the Data See [DataSources.md](DataSources.md) for detailed instructions.  ### Step 2: Running Experiments  Experiments can be ran using the same procedure as for the [DomainBed framework](https://github.com/facebookresearch/DomainBed), with a few additional adjustable data hyperparameters which should be passed in as a JSON formatted dictionary.",MLforHealth,PROJECT
usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` â””â”€â”€ ytvis_2019     â”œâ”€â”€ train     â”‚   â”œâ”€â”€ Annotations     â”‚   â”œâ”€â”€ JPEGImages     â”‚   â””â”€â”€ meta.json     â”œâ”€â”€ valid     â”‚   â”œâ”€â”€ Annotations     â”‚   â”œâ”€â”€ JPEGImages     â”‚   â””â”€â”€ meta.json     â”œâ”€â”€ test     â”‚   â”œâ”€â”€ Annotations     â”‚   â”œâ”€â”€ JPEGImages     â”‚   â””â”€â”€ meta.json     â”œâ”€â”€ train.json     â”œâ”€â”€ valid.json     â””â”€â”€ test.json ```  3.,VISOLO,PROJECT
usp=sharing) |  ## Video Comparisons The overall flow of our VISOLO and the comparison of different VIS methods on the YouTube-VIS 2019 dataset are provided at https://youtu.be/j33H7vcJ2uU  ## License  VISOLO is released under the [Apache 2.0 license](LICENSE).,VISOLO,PROJECT
MCG: [matlab code](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/) 5.,MCG,PROJECT
RIGOR: [matlab code](http://cpl.cc.gatech.edu/projects/RIGOR/)  Apologies if I've left your method off this list.,RIGOR,PROJECT
RIGOR: [matlab code](http://cpl.cc.gatech.edu/projects/RIGOR/)  Apologies if I've left your method off this list.,RIGOR,PROJECT
"Solatorio},     journal={arXiv preprint arXiv:2402.16829},     year={2024},     URL={https://arxiv.org/abs/2402.16829}     eprint={2402.16829},     archivePrefix={arXiv},     primaryClass={cs.LG} } ```  # Acknowledgements  This work is supported by the ""KCP IV - Exploring Data Use in the Development Economics Literature using Large Language Models (AI and LLMs)"" project funded by the [Knowledge for Change Program (KCP)](https://www.worldbank.org/en/programs/knowledge-for-change) of the World Bank - RA-P503405-RESE-TF0C3444.",KCP IV - Exploring Data Use in the Development Economics Literature using Large Language Models (AI and LLMs),PROJECT
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",unirxn,PROJECT
/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).,ActionFormer,PROJECT
Please refer to Ego4D and EgoVLP's documentation for more details on feature extraction.,Ego4D,PROJECT
Please refer to Ego4D and EgoVLP's documentation for more details on feature extraction.,EgoVLP,PROJECT
/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,ActionFormer,PROJECT
/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,ActionFormer,PROJECT
"/_assets/reward-reports-banner.png"" width=""600""/>     <br> <p> <p align=""center"">     <a href=""https://github.com/RewardReports/reward-reports/blob/main/LICENSE"">         <img alt=""GitHub"" src=""https://img.shields.io/github/license/RewardReports/reward-reports"">     </a>     <a href=""CODE_OF_CONDUCT.md"">         <img alt=""Contributor Covenant"" src=""https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg"">     </a> </p>  Welcome to Reward Reports!",RewardReports,PROJECT
"/_assets/reward-reports-banner.png"" width=""600""/>     <br> <p> <p align=""center"">     <a href=""https://github.com/RewardReports/reward-reports/blob/main/LICENSE"">         <img alt=""GitHub"" src=""https://img.shields.io/github/license/RewardReports/reward-reports"">     </a>     <a href=""CODE_OF_CONDUCT.md"">         <img alt=""Contributor Covenant"" src=""https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg"">     </a> </p>  Welcome to Reward Reports!",RewardReports,PROJECT
"/_assets/reward-reports-banner.png"" width=""600""/>     <br> <p> <p align=""center"">     <a href=""https://github.com/RewardReports/reward-reports/blob/main/LICENSE"">         <img alt=""GitHub"" src=""https://img.shields.io/github/license/RewardReports/reward-reports"">     </a>     <a href=""CODE_OF_CONDUCT.md"">         <img alt=""Contributor Covenant"" src=""https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg"">     </a> </p>  Welcome to Reward Reports!",Reward Reports,PROJECT
"Building on the documentation frameworks for [model cards](https://arxiv.org/abs/1810.03993) and [datasheets](https://arxiv.org/abs/1803.09010) proposed by Mitchell et al. and Gebru et al., we argue the need for Reward Reports for AI systems.",Reward Reports,PROJECT
"In a [whitepaper](http://arxiv.org/abs/2202.05716) recently published by the Center for Long-Term Cybersecurity, we introduced Reward Reports as living documents for proposed RL deployments that demarcate design choices.",Reward Reports,PROJECT
"At a minimum, Reward Reports are an opportunity for RL practitioners to deliberate on these questions and begin the work of deciding how to resolve them in practice.",Reward Reports,PROJECT
"# **Novel Class Discovery for 3D Point Cloud Semantic Segmentation [CVPR 2023]** The official implementation of our work ""Novel Class Discovery for 3D Point Cloud Semantic Segmentation"".  !",Novel Class Discovery for 3D Point Cloud Semantic Segmentation,PROJECT
"This work was also partially supported by the PRIN project LEGO-AI (Prot. 2020TA3K9N), the EU ISFP PROTECTOR (101034216) project and the EU H2020 MARVEL (957337) project and, it was carried out in the Vision and Learning joint laboratory of Fondazione Bruno Kessler and UNITN.  ## TODOS :soon: - Add code for $\text{EUMS}^\dagger$",LEGO-AI,PROJECT
"This work was also partially supported by the PRIN project LEGO-AI (Prot. 2020TA3K9N), the EU ISFP PROTECTOR (101034216) project and the EU H2020 MARVEL (957337) project and, it was carried out in the Vision and Learning joint laboratory of Fondazione Bruno Kessler and UNITN.  ## TODOS :soon: - Add code for $\text{EUMS}^\dagger$",EU ISFP PROTECTOR,PROJECT
"This work was also partially supported by the PRIN project LEGO-AI (Prot. 2020TA3K9N), the EU ISFP PROTECTOR (101034216) project and the EU H2020 MARVEL (957337) project and, it was carried out in the Vision and Learning joint laboratory of Fondazione Bruno Kessler and UNITN.  ## TODOS :soon: - Add code for $\text{EUMS}^\dagger$",EU H2020 MARVEL,PROJECT
"usp=share_link) |  **Note:** The result of DAB-DETR-R50 w/ Team-DETR under the 50-epoch setting is different from which we report in the paper because we lost this checkpoint, and here is the one we retrained.  ## Usage  ### Installation  Our code contains three projects, Team-DAB-DETR, Team-DN-DETR, and Team-DINO, based on DAB-DETR, DN-DETR, and DINO, respectively, and no extra dependency is needed.",Team-DAB-DETR,PROJECT
"usp=share_link) |  **Note:** The result of DAB-DETR-R50 w/ Team-DETR under the 50-epoch setting is different from which we report in the paper because we lost this checkpoint, and here is the one we retrained.  ## Usage  ### Installation  Our code contains three projects, Team-DAB-DETR, Team-DN-DETR, and Team-DINO, based on DAB-DETR, DN-DETR, and DINO, respectively, and no extra dependency is needed.",Team-DN-DETR,PROJECT
"usp=share_link) |  **Note:** The result of DAB-DETR-R50 w/ Team-DETR under the 50-epoch setting is different from which we report in the paper because we lost this checkpoint, and here is the one we retrained.  ## Usage  ### Installation  Our code contains three projects, Team-DAB-DETR, Team-DN-DETR, and Team-DINO, based on DAB-DETR, DN-DETR, and DINO, respectively, and no extra dependency is needed.",Team-DINO,PROJECT
"```bash # Team-DAB-DETR and Team-DN-DETR # multi-gpu python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --resume /path/to/your/checkpoint \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --matcher team \   --q_splits 65 20 15 \   --eval  # single-gpu python main.py \   --coco_path /path/to/your/COCODIR \   --resume /path/to/your/checkpoint \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --matcher team \   --q_splits 65 20 15 \   --eval  # --------------------------------------------  # Team-DINO # You need to write config and .sh files in advance. # multi-gpu bash scripts/DINO_4scale_1stage_team_r50_e12_eval.sh /path/to/your/COCODIR /path/to/your/output/dir /path/to/your/checkpoint ```  ### Training  In default, we divide the queries into three groups, with the proportions of 65%, 20%, and 15%, corresponding to the relative scales of (0, 0.2], (0.2, 0.4], and (0.4, 1], respectively.",Team-DAB-DETR,PROJECT
"```bash # Team-DAB-DETR and Team-DN-DETR # multi-gpu python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --resume /path/to/your/checkpoint \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --matcher team \   --q_splits 65 20 15 \   --eval  # single-gpu python main.py \   --coco_path /path/to/your/COCODIR \   --resume /path/to/your/checkpoint \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --matcher team \   --q_splits 65 20 15 \   --eval  # --------------------------------------------  # Team-DINO # You need to write config and .sh files in advance. # multi-gpu bash scripts/DINO_4scale_1stage_team_r50_e12_eval.sh /path/to/your/COCODIR /path/to/your/output/dir /path/to/your/checkpoint ```  ### Training  In default, we divide the queries into three groups, with the proportions of 65%, 20%, and 15%, corresponding to the relative scales of (0, 0.2], (0.2, 0.4], and (0.4, 1], respectively.",Team-DN-DETR,PROJECT
"```bash # Team-DAB-DETR and Team-DN-DETR # multi-gpu python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --resume /path/to/your/checkpoint \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --matcher team \   --q_splits 65 20 15 \   --eval  # single-gpu python main.py \   --coco_path /path/to/your/COCODIR \   --resume /path/to/your/checkpoint \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --matcher team \   --q_splits 65 20 15 \   --eval  # --------------------------------------------  # Team-DINO # You need to write config and .sh files in advance. # multi-gpu bash scripts/DINO_4scale_1stage_team_r50_e12_eval.sh /path/to/your/COCODIR /path/to/your/output/dir /path/to/your/checkpoint ```  ### Training  In default, we divide the queries into three groups, with the proportions of 65%, 20%, and 15%, corresponding to the relative scales of (0, 0.2], (0.2, 0.4], and (0.4, 1], respectively.",Team-DINO,PROJECT
"If you want to change the responsible scale range of each group, you can modify matcher.py for Team-DAB-DETR and Team-DN-DETR or the config file for Team-DINO.",Team-DAB-DETR,PROJECT
"If you want to change the responsible scale range of each group, you can modify matcher.py for Team-DAB-DETR and Team-DN-DETR or the config file for Team-DINO.",Team-DN-DETR,PROJECT
"```bash # Team-DAB-DETR and Team-DN-DETR # multi-gpu (12-epoch setting / 1x setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # multi-gpu (50-epoch setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # single-gpu (12-epoch setting / 1x setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # single-gpu (50-epoch setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # --------------------------------------------  # Team-DINO # You need to write config and .sh files in advance. # multi-gpu bash scripts/DINO_4scale_1stage_team_r50_e12.sh /path/to/your/COCODIR /path/to/your/output/dir ```  ## How to integrate query teamwork into your model  The query teamwork contains three parts: scale-wise grouping, position constraint, and preference extraction.",Team-DAB-DETR,PROJECT
"```bash # Team-DAB-DETR and Team-DN-DETR # multi-gpu (12-epoch setting / 1x setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # multi-gpu (50-epoch setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # single-gpu (12-epoch setting / 1x setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # single-gpu (50-epoch setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # --------------------------------------------  # Team-DINO # You need to write config and .sh files in advance. # multi-gpu bash scripts/DINO_4scale_1stage_team_r50_e12.sh /path/to/your/COCODIR /path/to/your/output/dir ```  ## How to integrate query teamwork into your model  The query teamwork contains three parts: scale-wise grouping, position constraint, and preference extraction.",Team-DN-DETR,PROJECT
"```bash # Team-DAB-DETR and Team-DN-DETR # multi-gpu (12-epoch setting / 1x setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # multi-gpu (50-epoch setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # single-gpu (12-epoch setting / 1x setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # single-gpu (50-epoch setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # --------------------------------------------  # Team-DINO # You need to write config and .sh files in advance. # multi-gpu bash scripts/DINO_4scale_1stage_team_r50_e12.sh /path/to/your/COCODIR /path/to/your/output/dir ```  ## How to integrate query teamwork into your model  The query teamwork contains three parts: scale-wise grouping, position constraint, and preference extraction.",Team-DINO,PROJECT
"# OntoED and OntoEvent  <p align=""center"">     <font size=4><strong>OntoED: A Model for Low-resource Event Detection with Ontology Embedding</strong></font> </p>   ðŸŽ  The project is an official implementation for [**OntoED**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoED) model and a repository for [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) dataset, which has firstly been proposed in the paper [OntoED: Low-resource Event Detection with Ontology Embedding](https://arxiv.org/pdf/2105.10922.pdf) accepted by ACL 2021",OntoED,PROJECT
"# OntoED and OntoEvent  <p align=""center"">     <font size=4><strong>OntoED: A Model for Low-resource Event Detection with Ontology Embedding</strong></font> </p>   ðŸŽ  The project is an official implementation for [**OntoED**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoED) model and a repository for [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) dataset, which has firstly been proposed in the paper [OntoED: Low-resource Event Detection with Ontology Embedding](https://arxiv.org/pdf/2105.10922.pdf) accepted by ACL 2021",OntoED,PROJECT
"# OntoED and OntoEvent  <p align=""center"">     <font size=4><strong>OntoED: A Model for Low-resource Event Detection with Ontology Embedding</strong></font> </p>   ðŸŽ  The project is an official implementation for [**OntoED**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoED) model and a repository for [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) dataset, which has firstly been proposed in the paper [OntoED: Low-resource Event Detection with Ontology Embedding](https://arxiv.org/pdf/2105.10922.pdf) accepted by ACL 2021",OntoED,PROJECT
"# OntoED and OntoEvent  <p align=""center"">     <font size=4><strong>OntoED: A Model for Low-resource Event Detection with Ontology Embedding</strong></font> </p>   ðŸŽ  The project is an official implementation for [**OntoED**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoED) model and a repository for [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) dataset, which has firstly been proposed in the paper [OntoED: Low-resource Event Detection with Ontology Embedding](https://arxiv.org/pdf/2105.10922.pdf) accepted by ACL 2021",OntoED,PROJECT
ðŸ¤—  The implementations are based on [Huggingface's Transformers](https://github.com/huggingface/transformers) and remanagement is referred to [MAVEN's baselines](https://github.com/THU-KEG/MAVEN-dataset/) & [DeepKE](https://github.com/zjunlp/DeepKE),DeepKE,PROJECT
ðŸ¤—  The implementations are based on [Huggingface's Transformers](https://github.com/huggingface/transformers) and remanagement is referred to [MAVEN's baselines](https://github.com/THU-KEG/MAVEN-dataset/) & [DeepKE](https://github.com/zjunlp/DeepKE),DeepKE,PROJECT
"If you use or extend our work, please cite the following paper:  ``` @inproceedings{ACL2021_OntoED,     title = ""{O}nto{ED}: Low-resource Event Detection with Ontology Embedding"",     author = ""Deng, Shumin  and       Zhang, Ningyu  and       Li, Luoqiu  and       Hui, Chen  and       Huaixiao, Tou  and       Chen, Mosha  and       Huang, Fei  and       Chen, Huajun"",     booktitle = ""Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"",     month = aug,     year = ""2021"",     address = ""Online"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2021.acl-long.220"",     doi = ""10.18653/v1/2021.acl-long.220"",     pages = ""2828--2839"" } ```",OntoED,PROJECT
"If you use or extend our work, please cite the following paper:  ``` @inproceedings{ACL2021_OntoED,     title = ""{O}nto{ED}: Low-resource Event Detection with Ontology Embedding"",     author = ""Deng, Shumin  and       Zhang, Ningyu  and       Li, Luoqiu  and       Hui, Chen  and       Huaixiao, Tou  and       Chen, Mosha  and       Huang, Fei  and       Chen, Huajun"",     booktitle = ""Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)"",     month = aug,     year = ""2021"",     address = ""Online"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2021.acl-long.220"",     doi = ""10.18653/v1/2021.acl-long.220"",     pages = ""2828--2839"" } ```",{O}nto{ED},PROJECT
"<div id=""top""></div>  # ABIDES: Agent-Based Interactive Discrete Event Simulation environment  <!",ABIDES: Agent-Based Interactive Discrete Event Simulation environment,PROJECT
"-- TABLE OF CONTENTS --> <ol>   <li>     <a href=""#about-the-project"">About The Project</a>   </li>   <li><a href=""#citing-abides"">Citing ABIDES</a></li>   <li>     <a href=""#getting-started"">Getting Started</a>     <ul>       <li><a href=""#installation"">Installation</a></li>     </ul>   </li>   <li>     <a href=""#usage-regular"">Usage (regular)</a>     <ul>       <li><a href=""#using-a-python-script"">Using a Python Script</a></li>       <li><a href=""#using-the-abides-command"">Using the `abides` Command</a></li>     </ul>   </li>   <li><a href=""#usage-gym"">Usage (Gym)</a></li>   <li><a href=""#default-available-markets-configurations"">Default Available Markets Configurations</a></li>   <li><a href=""#contributing"">Contributing</a></li>   <li><a href=""#license"">License</a></li>   <li><a href=""#acknowledgments"">Acknowledgments</a></li> </ol>  <!",ABIDES,PROJECT
-- ABOUT THE PROJECT --> ## About The Project  ABIDES (Agent Based Interactive Discrete Event Simulator) is a general purpose multi-agent discrete event simulator.,ABIDES (Agent Based Interactive Discrete Event Simulator),PROJECT
"The project is currently broken down into 3 parts: ABIDES-Core, ABIDES-Markets and ABIDES-Gym",ABIDES,PROJECT
"The project is currently broken down into 3 parts: ABIDES-Core, ABIDES-Markets and ABIDES-Gym",ABIDES,PROJECT
"The project is currently broken down into 3 parts: ABIDES-Core, ABIDES-Markets and ABIDES-Gym",ABIDES,PROJECT
* ABIDES-Core: Core general purpose simulator that be used as a base to build simulations of various systems. * ABIDES-Markets: Extension of ABIDES-Core to financial markets.,ABIDES,PROJECT
* ABIDES-Core: Core general purpose simulator that be used as a base to build simulations of various systems. * ABIDES-Markets: Extension of ABIDES-Core to financial markets.,ABIDES,PROJECT
* ABIDES-Core: Core general purpose simulator that be used as a base to build simulations of various systems. * ABIDES-Markets: Extension of ABIDES-Core to financial markets.,ABIDES,PROJECT
"Contains implementation of an exchange mimicking NASDAQ, stylised trading agents and configurations. * ABIDES-Gym: Extra layer to wrap the simulator into an OpenAI Gym environment for reinforcement learning use. 2 ready to use trading environments available.",ABIDES,PROJECT
"-- CITING --> ## Citing ABIDES  [ABIDES-Gym: Gym Environments for Multi-Agent Discrete Event Simulation and Application to Financial Markets](https://arxiv.org/pdf/2110.14771.pdf) or use the following BibTeX:  ``` @misc{amrouni2021abidesgym,       title={ABIDES-Gym: Gym Environments for Multi-Agent Discrete Event Simulation and Application to Financial Markets},        author={Selim Amrouni and Aymeric Moulin and Jared Vann and Svitlana Vyetrenko and Tucker Balch and Manuela Veloso},       year={2021},       eprint={2110.14771},       archivePrefix={arXiv},       primaryClass={cs.MA} } ```  [ABIDES: Towards High-Fidelity Market Simulation for AI Research](https://arxiv.org/abs/1904.12066) or by using the following BibTeX:  ``` @misc{byrd2019abides,       title={ABIDES: Towards High-Fidelity Market Simulation for AI Research},        author={David Byrd and Maria Hybinette and Tucker Hybinette Balch},       year={2019},       eprint={1904.12066},       archivePrefix={arXiv},       primaryClass={cs.MA} } ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  <!",ABIDES,PROJECT
"Download the ABIDES source code, either directly from GitHub or with git:      ```bash     git clone https://github.com/jpmorganchase/abides-jpmc-public     ```      **Note:** The latest stable version is contained within the `main` branch.  2.",abides,PROJECT
"-- USAGE EXAMPLES --> ## Usage (regular) Regular ABIDES simulations can be run either directly in python or through the command line  _For more examples, please refer to the [Documentation](https://example.com)_  ### Using a Python Script:  ```python from abides_markets.configs import rmsc04 from abides_core import abides  config_state = rmsc04.build_config(seed = 0, end_time = '10:00:00') end_state = abides.run(config_state) ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ### Using the abides Command:  The config can be loaded and the simulation run using the `abides` command in the terminal (from directory root):  ``` $ abides abides-markets/abides_markets/configs/rmsc04.py --end_time ""10:00:00"" ```  The first argument is a path to a valid ABIDES configuration file.",abides,PROJECT
"-- USAGE EXAMPLES --> ## Usage (regular) Regular ABIDES simulations can be run either directly in python or through the command line  _For more examples, please refer to the [Documentation](https://example.com)_  ### Using a Python Script:  ```python from abides_markets.configs import rmsc04 from abides_core import abides  config_state = rmsc04.build_config(seed = 0, end_time = '10:00:00') end_state = abides.run(config_state) ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ### Using the abides Command:  The config can be loaded and the simulation run using the `abides` command in the terminal (from directory root):  ``` $ abides abides-markets/abides_markets/configs/rmsc04.py --end_time ""10:00:00"" ```  The first argument is a path to a valid ABIDES configuration file.",abides,PROJECT
"-- USAGE EXAMPLES --> ## Usage (regular) Regular ABIDES simulations can be run either directly in python or through the command line  _For more examples, please refer to the [Documentation](https://example.com)_  ### Using a Python Script:  ```python from abides_markets.configs import rmsc04 from abides_core import abides  config_state = rmsc04.build_config(seed = 0, end_time = '10:00:00') end_state = abides.run(config_state) ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ### Using the abides Command:  The config can be loaded and the simulation run using the `abides` command in the terminal (from directory root):  ``` $ abides abides-markets/abides_markets/configs/rmsc04.py --end_time ""10:00:00"" ```  The first argument is a path to a valid ABIDES configuration file.",abides,PROJECT
"-- USAGE EXAMPLES --> ## Usage (regular) Regular ABIDES simulations can be run either directly in python or through the command line  _For more examples, please refer to the [Documentation](https://example.com)_  ### Using a Python Script:  ```python from abides_markets.configs import rmsc04 from abides_core import abides  config_state = rmsc04.build_config(seed = 0, end_time = '10:00:00') end_state = abides.run(config_state) ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ### Using the abides Command:  The config can be loaded and the simulation run using the `abides` command in the terminal (from directory root):  ``` $ abides abides-markets/abides_markets/configs/rmsc04.py --end_time ""10:00:00"" ```  The first argument is a path to a valid ABIDES configuration file.",abides,PROJECT
"-- USAGE EXAMPLES --> ## Usage (regular) Regular ABIDES simulations can be run either directly in python or through the command line  _For more examples, please refer to the [Documentation](https://example.com)_  ### Using a Python Script:  ```python from abides_markets.configs import rmsc04 from abides_core import abides  config_state = rmsc04.build_config(seed = 0, end_time = '10:00:00') end_state = abides.run(config_state) ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ### Using the abides Command:  The config can be loaded and the simulation run using the `abides` command in the terminal (from directory root):  ``` $ abides abides-markets/abides_markets/configs/rmsc04.py --end_time ""10:00:00"" ```  The first argument is a path to a valid ABIDES configuration file.",abides,PROJECT
"<p align=""right"">(<a href=""#top"">back to top</a>)</p>  ## Usage (Gym) ABIDES can also be run through a Gym interface using ABIDES-Gym environments.",ABIDES,PROJECT
"```python import gym import abides_gym  env = gym.make(     ""markets-daily_investor-v0"",     background_config=""rmsc04"", )  env.seed(0) initial_state = env.reset() for i in range(5):     state, reward, done, info = env.step(0) ```  ## Default Available Markets Configurations  ABIDES currently has the following available background Market Simulation Configuration:  * RMSC03: 1 Exchange Agent, 1 POV Market Maker Agent, 100 Value Agents, 25 Momentum Agents, 5000 Noise Agents   * RMSC04: 1 Exchange Agent, 2 Market Maker Agents, 102 Value Agents, 12 Momentum Agents, 1000  Noise Agents  <p align=""right"">(<a href=""#top"">back to top</a>)</p>  <!",abides,PROJECT
"-- ACKNOWLEDGMENTS --> ## Acknowledgments ABIDES was originally developed by David Byrd and Tucker Balch: https://github.com/abides-sim/abides ABIDES is currently developed and maintained by [Jared Vann](https://github.com/jaredvann) (aka @jaredvann), [Selim Amrouni](https://github.com/selimamrouni) (aka @selimamrouni), and [Aymeric Moulin](https://github.com/AymericCAMoulin) (@AymericCAMoulin).",abides,PROJECT
"-- ACKNOWLEDGMENTS --> ## Acknowledgments ABIDES was originally developed by David Byrd and Tucker Balch: https://github.com/abides-sim/abides ABIDES is currently developed and maintained by [Jared Vann](https://github.com/jaredvann) (aka @jaredvann), [Selim Amrouni](https://github.com/selimamrouni) (aka @selimamrouni), and [Aymeric Moulin](https://github.com/AymericCAMoulin) (@AymericCAMoulin).",abides,PROJECT
"# Introduction This is PointSQL, the source codes of [Natural Language to Structured Query Generation via Meta-Learning](https://arxiv.org/abs/1803.02400)  and [Pointing Out SQL Queries From Text](https://www.microsoft.com/en-us/research/publication/pointing-sql-queries-text) from Microsoft Research.",PointSQL,PROJECT
Download pretrained embeddings from [glove](https://nlp.stanford.edu/projects/glove/) and [character n-gram embeddings](http://www.logos.t.u-tokyo.ac.jp/~hassy/publications/arxiv2016jmt/) and put them under ``input/``   #### Note we use a new preprocessed dataset (v2) in the [Execute-Guided Decoding](https://arxiv.org/abs/1807.03100) paper - A preprocessed dataset can be found [here](https://1drv.ms/u/s!,glove,PROJECT
Download pretrained embeddings from [glove](https://nlp.stanford.edu/projects/glove/) and [character n-gram embeddings](http://www.logos.t.u-tokyo.ac.jp/~hassy/publications/arxiv2016jmt/) and put them under ``input/``   #### Note we use a new preprocessed dataset (v2) in the [Execute-Guided Decoding](https://arxiv.org/abs/1807.03100) paper - A preprocessed dataset can be found [here](https://1drv.ms/u/s!,glove,PROJECT
"Moreover, it is based on the [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project) projects.",ThingML,PROJECT
"Moreover, it is based on the [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project) projects.",ThingML,PROJECT
"Moreover, it is based on the [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project) projects.",HEADS,PROJECT
"Moreover, it is based on the [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project) projects.",HEADS,PROJECT
"Similar to ThingML/HEADS, ML2 is built using the [Eclipse Modeling Framework (EMF)](https://www.eclipse.org/modeling/emf/), as well as the [Xtext framework](https://www.eclipse.org/Xtext/), and is released under the terms of the Apache 2.0 permissive open source license.",ThingML,PROJECT
"Similar to ThingML/HEADS, ML2 is built using the [Eclipse Modeling Framework (EMF)](https://www.eclipse.org/modeling/emf/), as well as the [Xtext framework](https://www.eclipse.org/Xtext/), and is released under the terms of the Apache 2.0 permissive open source license.",HEADS,PROJECT
"The name ML-Quadrat (""Quadrat"" is the German word for ""square"" / Ë†2) refers to the fact that the project is about two MLs simultaneously: (i) ML for Modeling Language (as in ThingML); (ii) ML for Machine Learning, i.e., a sub-discipline of Artificial Intelligence (AI).",ML-Quadrat,PROJECT
"The name ML-Quadrat (""Quadrat"" is the German word for ""square"" / Ë†2) refers to the fact that the project is about two MLs simultaneously: (i) ML for Modeling Language (as in ThingML); (ii) ML for Machine Learning, i.e., a sub-discipline of Artificial Intelligence (AI).",ThingML,PROJECT
"However, in what follows, we refer to the project name as ML2 for simplicity.    ## Why ML2?",ML2,PROJECT
"ThingML/HEADS and other Model-Driven Software Engineering (MDSE) tools for the IoT/CPS, that we are aware of, do not support Data Analytics and Machine Learning (DAML) at the modeling layer.",ThingML,PROJECT
"ThingML/HEADS and other Model-Driven Software Engineering (MDSE) tools for the IoT/CPS, that we are aware of, do not support Data Analytics and Machine Learning (DAML) at the modeling layer.",HEADS,PROJECT
"In the original version from [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project), the server used to reply to every ping message of the client with a pong response.",ThingML,PROJECT
"In the original version from [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project), the server used to reply to every ping message of the client with a pong response.",ThingML,PROJECT
"In the original version from [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project), the server used to reply to every ping message of the client with a pong response.",HEADS,PROJECT
"In the original version from [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project), the server used to reply to every ping message of the client with a pong response.",HEADS,PROJECT
"Note that the software model instances in ML2 have the .thingml extension, similar to the ThingML/HEADS projects, although the meta-models/grammars of the DSMLs are different (ML2 is backward-compatible).      ### How to generate code out of the sample model instance?",ThingML,PROJECT
"Note that the software model instances in ML2 have the .thingml extension, similar to the ThingML/HEADS projects, although the meta-models/grammars of the DSMLs are different (ML2 is backward-compatible).      ### How to generate code out of the sample model instance?",HEADS,PROJECT
"Note that the software model instances in ML2 have the .thingml extension, similar to the ThingML/HEADS projects, although the meta-models/grammars of the DSMLs are different (ML2 is backward-compatible).      ### How to generate code out of the sample model instance?",ML2,PROJECT
"Once installed, you shall create a new workspace and then import the ML2 project there.",ML2,PROJECT
"Finall, please go to Help -> Install New Software and install the following software through the default update site (e.g., http://download.eclipse.org/releases/2021-03): Under the Modeling category, select and install the MWE2 Language SDK, MWE2 Runtime SDK and Xtext Complete SDK.      #### Running the GenerateThingML.mwe2 workflow in the Eclipse IDE  One of the projects in the workspace, called thingml.ide might still have errors.",thingml,PROJECT
"Please run the [GenerateThingML.mwe2](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/language/thingml/src/org/thingml/xtext/GenerateThingML.mwe2) workflow, which resides in the thingml project in the workspace (under src -> org.thingml.xtext) from within the Eclipse IDE by right-clicking on it and choosing Run as -> MWE2 Workflow from the context menu.",thingml,PROJECT
"This is a model instance, which shall conform to the meta-model/grammar of the DSML of ML2.",ML2,PROJECT
"[Back to top](#toc)    ### Full documentation of the DSML  As mentioned, ML2 is based on the [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project) projects.",ThingML,PROJECT
"[Back to top](#toc)    ### Full documentation of the DSML  As mentioned, ML2 is based on the [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project) projects.",ThingML,PROJECT
"[Back to top](#toc)    ### Full documentation of the DSML  As mentioned, ML2 is based on the [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project) projects.",HEADS,PROJECT
"[Back to top](#toc)    ### Full documentation of the DSML  As mentioned, ML2 is based on the [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project) projects.",HEADS,PROJECT
The **abstract syntax** of the DSML of ML2 is implemented through the [Xtext grammar](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/language/thingml/src/org/thingml/xtext/ThingML.xtext).,ML2,PROJECT
"Following the semantics of [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project), the communication between the ""things"" in ML2 is carried out through asynchronous message-passing.",ThingML,PROJECT
"Following the semantics of [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project), the communication between the ""things"" in ML2 is carried out through asynchronous message-passing.",ThingML,PROJECT
"Following the semantics of [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project), the communication between the ""things"" in ML2 is carried out through asynchronous message-passing.",HEADS,PROJECT
"Following the semantics of [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project), the communication between the ""things"" in ML2 is carried out through asynchronous message-passing.",HEADS,PROJECT
"Following the semantics of [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project), the communication between the ""things"" in ML2 is carried out through asynchronous message-passing.",ML2,PROJECT
"Further, each thing can have local variables, called **properties**.    ###### Subsection 3.2: Data Analytics (and Machine Learning)  This is the main innovation of ML2 compared to [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project).",ThingML,PROJECT
"Further, each thing can have local variables, called **properties**.    ###### Subsection 3.2: Data Analytics (and Machine Learning)  This is the main innovation of ML2 compared to [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project).",ThingML,PROJECT
"Further, each thing can have local variables, called **properties**.    ###### Subsection 3.2: Data Analytics (and Machine Learning)  This is the main innovation of ML2 compared to [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project).",HEADS,PROJECT
"Further, each thing can have local variables, called **properties**.    ###### Subsection 3.2: Data Analytics (and Machine Learning)  This is the main innovation of ML2 compared to [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project).",HEADS,PROJECT
A **platform-independent imperative action language** is adopted and adapted from the DSML of [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project).,ThingML,PROJECT
A **platform-independent imperative action language** is adopted and adapted from the DSML of [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project).,ThingML,PROJECT
A **platform-independent imperative action language** is adopted and adapted from the DSML of [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project).,HEADS,PROJECT
A **platform-independent imperative action language** is adopted and adapted from the DSML of [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project).,HEADS,PROJECT
"However, another key innovation of ML2, compared to [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project) is the extension of this action language to support using the DAML components (explained above) in the state machines.",ML2,PROJECT
"However, another key innovation of ML2, compared to [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project) is the extension of this action language to support using the DAML components (explained above) in the state machines.",ThingML,PROJECT
"However, another key innovation of ML2, compared to [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project) is the extension of this action language to support using the DAML components (explained above) in the state machines.",ThingML,PROJECT
"However, another key innovation of ML2, compared to [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project) is the extension of this action language to support using the DAML components (explained above) in the state machines.",HEADS,PROJECT
"However, another key innovation of ML2, compared to [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project) is the extension of this action language to support using the DAML components (explained above) in the state machines.",HEADS,PROJECT
"Below, we briefly explain the new action types that are introduced in ML2, but did not exist in [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project):    1.",ML2,PROJECT
"Below, we briefly explain the new action types that are introduced in ML2, but did not exist in [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project):    1.",ThingML,PROJECT
"Below, we briefly explain the new action types that are introduced in ML2, but did not exist in [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project):    1.",ThingML,PROJECT
"Below, we briefly explain the new action types that are introduced in ML2, but did not exist in [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project):    1.",HEADS,PROJECT
"Below, we briefly explain the new action types that are introduced in ML2, but did not exist in [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project):    1.",HEADS,PROJECT
"Maven Artifacts  You can find the Maven artifacts of ML2 at https://oss.sonatype.org (e.g., search for the groupid: io.github.arminmoin).",ML2,PROJECT
"Below, we briefly explain each of them.    ### Contributing to the Grammar/Meta-model of the DSML    The Xtext grammar is the core of the DSML of ML2.",ML2,PROJECT
"After any modifications, please build the entire project again using Maven in the terminal as follows:    ```bash  cd ML-Quadrat  mvn clean install -X  cd ML2/language  mvn clean install -X  ```  The -X option is optional and enables the debugging mode, thus resulting in a more detailed output.",ML-Quadrat,PROJECT
"Open ThingML.genmodel that resides in the project called ""thingml"" in the workspace at model -> generated.   2.",ThingML,PROJECT
"Right-click on ThingML and select the options **Generate Model Code**, **Generate Edit Code** and **Generate Editor Code** one after another.",ThingML,PROJECT
# Hybrid-Self-Attention-NEAT  ### Abstract  This repository contains the code to reproduce the results presented in the original [paper](https://link.springer.com/article/10.1007/s12530-023-09510-3).,Hybrid-Self-Attention-NEAT,PROJECT
"<br/> In this article, we present a â€œHybrid Self-Attention NEATâ€ method to improve the original NeuroEvolution of Augmenting Topologies (NEAT) algorithm in high-dimensional inputs.",Hybrid Self-Attention NEAT,PROJECT
Hybrid self-attention NEAT: a novel evolutionary self-attention approach to improve the NEAT algorithm in high dimensional inputs.,Hybrid self-attention NEAT,PROJECT
"- **Standard Benchmarks.**   OpenSTL will support standard benchmarks of STL algorithms image with training and evaluation as many open-source projects (e.g., [MMDetection](https://github.com/open-mmlab/mmdetection) and [USB](https://github.com/microsoft/Semi-supervised-learning)).",MMDetection,PROJECT
"- **Standard Benchmarks.**   OpenSTL will support standard benchmarks of STL algorithms image with training and evaluation as many open-source projects (e.g., [MMDetection](https://github.com/open-mmlab/mmdetection) and [USB](https://github.com/microsoft/Semi-supervised-learning)).",USB,PROJECT
This work was the very start of the S3PRL project which established lots of foundamental modules and coding styles.,S3PRL,PROJECT
"# SGDLibrary : Stochastic Optimization Algorithm Library in MATLAB/Octave ----------  Authors: [Hiroyuki Kasai](http://kasai.comm.waseda.ac.jp/kasai/)  Last page update: November 20, 2020  Latest library version: 1.0.20 (see Release notes for more info)  <br />  Announcement ---------- We are very welcome to your contribution.",SGDLibrary : Stochastic Optimization Algorithm Library in MATLAB/Octave,PROJECT
Note that this SGDLibrary internally contains the [GDLibrary](https://github.com/hiroyuki-kasai/GDLibrary).,GDLibrary,PROJECT
"- Softmax classification problem does not support ""Hessian-vector product"" type algorithms, i.e., SQN, SVRG-SQN and SVRG-LBFGS. - This SGDLibrary internally contains the [GDLibrary](https://github.com/hiroyuki-kasai/GDLibrary).",GDLibrary,PROJECT
/data/general_data.json): 5K human-annotated samples for ChatGPT responses to general user queries from [Alpaca](https://github.com/tatsu-lab/stanford_alpaca).,Alpaca,PROJECT
/data/general_data.json): 5K human-annotated samples for ChatGPT responses to general user queries from [Alpaca](https://github.com/tatsu-lab/stanford_alpaca).,alpaca,PROJECT
"He is currently (2018) working in the     [TESCON project](https://www.cl.cam.ac.uk/~cm770/tescon/tescon.html     ""TESCON webpage"") (EPSRC grant Grant: RG90413 NRAG/536)",TESCON,PROJECT
"He is currently (2018) working in the     [TESCON project](https://www.cl.cam.ac.uk/~cm770/tescon/tescon.html     ""TESCON webpage"") (EPSRC grant Grant: RG90413 NRAG/536)",tescon,PROJECT
"He is currently (2018) working in the     [TESCON project](https://www.cl.cam.ac.uk/~cm770/tescon/tescon.html     ""TESCON webpage"") (EPSRC grant Grant: RG90413 NRAG/536)",tescon,PROJECT
"He is currently (2018) working in the     [TESCON project](https://www.cl.cam.ac.uk/~cm770/tescon/tescon.html     ""TESCON webpage"") (EPSRC grant Grant: RG90413 NRAG/536)",TESCON,PROJECT
[View SafeML: Safety Monitoring of Machine Learning Classifiers on File Exchange](https://www.mathworks.com/matlabcentral/images/matlab-file-exchange.svg)](https://uk.mathworks.com/matlabcentral/fileexchange/76793-safeml-safety-monitoring-of-machine-learning-classifiers) [!,SafeML,PROJECT
[View SafeML: Safety Monitoring of Machine Learning Classifiers on File Exchange](https://www.mathworks.com/matlabcentral/images/matlab-file-exchange.svg)](https://uk.mathworks.com/matlabcentral/fileexchange/76793-safeml-safety-monitoring-of-machine-learning-classifiers) [!,safeml,PROJECT
[Documentation Status](https://readthedocs.org/projects/safeml/badge/?,safeml,PROJECT
"[arxiv badge](https://img.shields.io/badge/arXiv-2005.13166-red)](https://arxiv.org/abs/2005.13166)   <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/SafeML_Logo.png"" alt=""SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety""> </p>    # SafeML Exploring techniques for safety monitoring of machine learning classifiers. ## Abstract <p align=""justify""> Ensuring safety and explainability of machine learning (ML) is a topic of increasing relevance as data-driven applications venture into safety-critical application domains, traditionally committed to high safety standards that are not satisfied with an exclusive testing approach of otherwise inaccessible black-box systems.",SafeML,PROJECT
"[arxiv badge](https://img.shields.io/badge/arXiv-2005.13166-red)](https://arxiv.org/abs/2005.13166)   <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/SafeML_Logo.png"" alt=""SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety""> </p>    # SafeML Exploring techniques for safety monitoring of machine learning classifiers. ## Abstract <p align=""justify""> Ensuring safety and explainability of machine learning (ML) is a topic of increasing relevance as data-driven applications venture into safety-critical application domains, traditionally committed to high safety standards that are not satisfied with an exclusive testing approach of otherwise inaccessible black-box systems.",SafeML,PROJECT
"[arxiv badge](https://img.shields.io/badge/arXiv-2005.13166-red)](https://arxiv.org/abs/2005.13166)   <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/SafeML_Logo.png"" alt=""SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety""> </p>    # SafeML Exploring techniques for safety monitoring of machine learning classifiers. ## Abstract <p align=""justify""> Ensuring safety and explainability of machine learning (ML) is a topic of increasing relevance as data-driven applications venture into safety-critical application domains, traditionally committed to high safety standards that are not satisfied with an exclusive testing approach of otherwise inaccessible black-box systems.",SafeAI,PROJECT
"[arxiv badge](https://img.shields.io/badge/arXiv-2005.13166-red)](https://arxiv.org/abs/2005.13166)   <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/SafeML_Logo.png"" alt=""SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety""> </p>    # SafeML Exploring techniques for safety monitoring of machine learning classifiers. ## Abstract <p align=""justify""> Ensuring safety and explainability of machine learning (ML) is a topic of increasing relevance as data-driven applications venture into safety-critical application domains, traditionally committed to high safety standards that are not satisfied with an exclusive testing approach of otherwise inaccessible black-box systems.",SafeML,PROJECT
"-- ### Keywords * Artificial Intelligence Safety, Safe AI, AI Safety, SafeAI  * Machine Learning Safety, Safe ML, ML Safety, SafeML * Deep Learning Safety, Safe DL, DL Safety, SafeDL * Empirical Statistical Distance, Data Separability Measures * Safety Monitoring @Runtime, Safety Monitoring in Open Adaptive Systems-->  ## Description <p align=""justify""> The following figure illustrates the flowchart of the proposed approach.",SafeML,PROJECT
"</p> <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/FlowChart.png"" alt=""FlowChart"">  <figcaption>Figure 1.",SafeML,PROJECT
Flowchart of the proposed approach</figcaption> </p>  -------------------  # SafeML Applications The SafeML idea can be used for different applications.,SafeML,PROJECT
Flowchart of the proposed approach</figcaption> </p>  -------------------  # SafeML Applications The SafeML idea can be used for different applications.,SafeML,PROJECT
"Three aplication of the SafeML project have been illustrated as follows:  ## SafeML in Security <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/Security_Example_v3.png"" alt=""Application of SafeML for Security, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 2.",SafeML,PROJECT
"Three aplication of the SafeML project have been illustrated as follows:  ## SafeML in Security <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/Security_Example_v3.png"" alt=""Application of SafeML for Security, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 2.",SafeML,PROJECT
"Three aplication of the SafeML project have been illustrated as follows:  ## SafeML in Security <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/Security_Example_v3.png"" alt=""Application of SafeML for Security, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 2.",SafeML,PROJECT
"Three aplication of the SafeML project have been illustrated as follows:  ## SafeML in Security <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/Security_Example_v3.png"" alt=""Application of SafeML for Security, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 2.",SafeML,PROJECT
"Three aplication of the SafeML project have been illustrated as follows:  ## SafeML in Security <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/Security_Example_v3.png"" alt=""Application of SafeML for Security, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 2.",SafeML,PROJECT
"Three aplication of the SafeML project have been illustrated as follows:  ## SafeML in Security <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/Security_Example_v3.png"" alt=""Application of SafeML for Security, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 2.",SafeAI,PROJECT
"Application of the SafeML for Security Intrusion Detection</figcaption> </p>  ## SafeML for Medical Applications <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Medical_Example_v1.png"" alt=""Application of SafeML in medical,SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 3.",SafeML,PROJECT
"Application of the SafeML for Security Intrusion Detection</figcaption> </p>  ## SafeML for Medical Applications <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Medical_Example_v1.png"" alt=""Application of SafeML in medical,SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 3.",SafeML,PROJECT
"Application of the SafeML for Security Intrusion Detection</figcaption> </p>  ## SafeML for Medical Applications <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Medical_Example_v1.png"" alt=""Application of SafeML in medical,SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 3.",SafeML,PROJECT
"Application of the SafeML for Security Intrusion Detection</figcaption> </p>  ## SafeML for Medical Applications <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Medical_Example_v1.png"" alt=""Application of SafeML in medical,SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 3.",SafeML,PROJECT
"Application of the SafeML for Security Intrusion Detection</figcaption> </p>  ## SafeML for Medical Applications <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Medical_Example_v1.png"" alt=""Application of SafeML in medical,SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 3.",SafeML,PROJECT
"Application of the SafeML for Security Intrusion Detection</figcaption> </p>  ## SafeML for Medical Applications <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Medical_Example_v1.png"" alt=""Application of SafeML in medical,SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 3.",SafeML,PROJECT
"Application of the SafeML for Security Intrusion Detection</figcaption> </p>  ## SafeML for Medical Applications <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Medical_Example_v1.png"" alt=""Application of SafeML in medical,SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 3.",SafeAI,PROJECT
"Application of the SafeML in ML/DL based Disease Detection or Diagnosis</figcaption> </p>  ## SafeML for Autonomous Vehicles and Self-Driving Cars <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Autonomous_Vehicle_Example_v1.png"" alt=""Application_of_SafeML_for_Autonomous_Vehicle_or_Self-driving_cars, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 4.",SafeML,PROJECT
"Application of the SafeML in ML/DL based Disease Detection or Diagnosis</figcaption> </p>  ## SafeML for Autonomous Vehicles and Self-Driving Cars <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Autonomous_Vehicle_Example_v1.png"" alt=""Application_of_SafeML_for_Autonomous_Vehicle_or_Self-driving_cars, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 4.",SafeML,PROJECT
"Application of the SafeML in ML/DL based Disease Detection or Diagnosis</figcaption> </p>  ## SafeML for Autonomous Vehicles and Self-Driving Cars <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Autonomous_Vehicle_Example_v1.png"" alt=""Application_of_SafeML_for_Autonomous_Vehicle_or_Self-driving_cars, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 4.",SafeML,PROJECT
"Application of the SafeML in ML/DL based Disease Detection or Diagnosis</figcaption> </p>  ## SafeML for Autonomous Vehicles and Self-Driving Cars <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Autonomous_Vehicle_Example_v1.png"" alt=""Application_of_SafeML_for_Autonomous_Vehicle_or_Self-driving_cars, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 4.",SafeML,PROJECT
"Application of the SafeML in ML/DL based Disease Detection or Diagnosis</figcaption> </p>  ## SafeML for Autonomous Vehicles and Self-Driving Cars <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Autonomous_Vehicle_Example_v1.png"" alt=""Application_of_SafeML_for_Autonomous_Vehicle_or_Self-driving_cars, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 4.",SafeML,PROJECT
"Application of the SafeML in ML/DL based Disease Detection or Diagnosis</figcaption> </p>  ## SafeML for Autonomous Vehicles and Self-Driving Cars <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Autonomous_Vehicle_Example_v1.png"" alt=""Application_of_SafeML_for_Autonomous_Vehicle_or_Self-driving_cars, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 4.",SafeML,PROJECT
"Application of the SafeML in ML/DL based Disease Detection or Diagnosis</figcaption> </p>  ## SafeML for Autonomous Vehicles and Self-Driving Cars <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Autonomous_Vehicle_Example_v1.png"" alt=""Application_of_SafeML_for_Autonomous_Vehicle_or_Self-driving_cars, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 4.",SafeAI,PROJECT
"Application of the SafeML for Traffic Sign Detection in Autonomous Vehicles/Self-driving Cars</figcaption> </p>  ## SafeML for Autonomous Railway Systems <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Railway_Example_v3.png"" alt=""Application of SafeML for Autonomous Railway Systems, Intelligent Railway, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 5.",SafeML,PROJECT
"Application of the SafeML for Traffic Sign Detection in Autonomous Vehicles/Self-driving Cars</figcaption> </p>  ## SafeML for Autonomous Railway Systems <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Railway_Example_v3.png"" alt=""Application of SafeML for Autonomous Railway Systems, Intelligent Railway, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 5.",SafeML,PROJECT
"Application of the SafeML for Traffic Sign Detection in Autonomous Vehicles/Self-driving Cars</figcaption> </p>  ## SafeML for Autonomous Railway Systems <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Railway_Example_v3.png"" alt=""Application of SafeML for Autonomous Railway Systems, Intelligent Railway, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 5.",SafeML,PROJECT
"Application of the SafeML for Traffic Sign Detection in Autonomous Vehicles/Self-driving Cars</figcaption> </p>  ## SafeML for Autonomous Railway Systems <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Railway_Example_v3.png"" alt=""Application of SafeML for Autonomous Railway Systems, Intelligent Railway, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 5.",SafeML,PROJECT
"Application of the SafeML for Traffic Sign Detection in Autonomous Vehicles/Self-driving Cars</figcaption> </p>  ## SafeML for Autonomous Railway Systems <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Railway_Example_v3.png"" alt=""Application of SafeML for Autonomous Railway Systems, Intelligent Railway, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 5.",SafeML,PROJECT
"Application of the SafeML for Traffic Sign Detection in Autonomous Vehicles/Self-driving Cars</figcaption> </p>  ## SafeML for Autonomous Railway Systems <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Railway_Example_v3.png"" alt=""Application of SafeML for Autonomous Railway Systems, Intelligent Railway, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 5.",SafeML,PROJECT
"Application of the SafeML for Traffic Sign Detection in Autonomous Vehicles/Self-driving Cars</figcaption> </p>  ## SafeML for Autonomous Railway Systems <p align=""center"">  <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_Railway_Example_v3.png"" alt=""Application of SafeML for Autonomous Railway Systems, Intelligent Railway, SafeML, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety"">  <figcaption>Figure 5.",SafeAI,PROJECT
"Application of the SafeML for Obstacle Detection in Autonomous Railway Systems</figcaption> </p>  -------------------  ## From SafeML Toward Explainable AI (XAI) <p align=""justify""> The proposed method is not only suitable for safety evaluation of machine learning classifiers but also can be used @Run-Time as an eXplainable AI (XAI).",SafeML,PROJECT
"Application of the SafeML for Obstacle Detection in Autonomous Railway Systems</figcaption> </p>  -------------------  ## From SafeML Toward Explainable AI (XAI) <p align=""justify""> The proposed method is not only suitable for safety evaluation of machine learning classifiers but also can be used @Run-Time as an eXplainable AI (XAI).",SafeML,PROJECT
"In one of our <a href = ""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/Explainable_AI"">examples for security dataset</a>, we showed how SafeML can be used as XAI.",SafeML,PROJECT
"In one of our <a href = ""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/Explainable_AI"">examples for security dataset</a>, we showed how SafeML can be used as XAI.",SafeML,PROJECT
"--Similar to the idea of <a href = ""https://github.com/slundberg/shap"">SHAP XAI Python Package</a>, the SafeML idea can be used for <a href ""https://en.wikipedia.org/wiki/Explainable_artificial_intelligence"">AI explainability and interpretability</a>.",SafeML,PROJECT
"The following figure shows the idea using SafeML as the XAI tool: </p>  <p align=""justify""> <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_XAI.png"" alt=""SafeML as XAI, AI explainability, AI interpretability, explanable AI, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety""> <figcaption>Figure 6.",SafeML,PROJECT
"The following figure shows the idea using SafeML as the XAI tool: </p>  <p align=""justify""> <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_XAI.png"" alt=""SafeML as XAI, AI explainability, AI interpretability, explanable AI, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety""> <figcaption>Figure 6.",SafeML,PROJECT
"The following figure shows the idea using SafeML as the XAI tool: </p>  <p align=""justify""> <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_XAI.png"" alt=""SafeML as XAI, AI explainability, AI interpretability, explanable AI, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety""> <figcaption>Figure 6.",SafeML,PROJECT
"The following figure shows the idea using SafeML as the XAI tool: </p>  <p align=""justify""> <img src=""https://github.com/ISorokos/SafeML/blob/master/Pictures/SafeML_XAI.png"" alt=""SafeML as XAI, AI explainability, AI interpretability, explanable AI, SafeAI, AIsafety, AI safety, SafeDL, machine learning safety""> <figcaption>Figure 6.",SafeML,PROJECT
"Application of the SafeML in AI explainability and interpretability</figcaption>  </p> -->  ## Case Studies <ul>   <li><b>Security Datasets</b></li> <p align=""justify""> A Intrusion Detection Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2017.html"">CICIDS2017</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",SafeML,PROJECT
"Application of the SafeML in AI explainability and interpretability</figcaption>  </p> -->  ## Case Studies <ul>   <li><b>Security Datasets</b></li> <p align=""justify""> A Intrusion Detection Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2017.html"">CICIDS2017</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",SafeML,PROJECT
"</li></p> <p align=""justify""> Tor-nonTor Dataset (<a href=""https://www.unb.ca/cic/datasets/tor.html"">ISCXTor2016</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",SafeML,PROJECT
"</p> <p align=""justify""> DDoS Attach Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2018.html"">CSE-CIC-IDS2018</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",SafeML,PROJECT
"</p> <p align=""justify""> NSL version of KDD Security Dataset  (<a href=""https://www.unb.ca/cic/datasets/nsl.html"">NSL-KDD</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/NSL_KDD_Security_Dataset"">MATLAB implementation</a>].",SafeML,PROJECT
"</p> <p align=""justify""> DDoS Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ddos-2019.html"">CICDDoS2019</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/CICDDoS2019_Security_Dataset"">MATLAB implementation</a>].",SafeML,PROJECT
"</p> <p align=""justify""> CIC DoS Dataset (<a href=""https://www.unb.ca/cic/datasets/dos-dataset.html"">CICDoS2017</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",SafeML,PROJECT
"</p>  <p align=""justify""> VPN-nonVPN Dataset (<a href=""https://www.unb.ca/cic/datasets/vpn.html"">ISCXVPN2016</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",SafeML,PROJECT
"</p>  <p align=""justify""> Botnet Dataset (<a href=""https://www.unb.ca/cic/datasets/botnet.html"">BotNet2014</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",SafeML,PROJECT
"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",SafeML,PROJECT
"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",SafeML,PROJECT
"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",SafeML,PROJECT
"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",SafeML,PROJECT
"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",SafeML,PROJECT
"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>].",SafeML,PROJECT
"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>].",SafeML,PROJECT
"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>].",safeml,PROJECT
"</li>   <li><a href = ""https://www.cs.toronto.edu/~kriz/cifar.html""><b>CIFAR 10/100 Datasets</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_MATLAB/CIFAR_10_Dataset/README.md"">MATLAB implementation</a>]</li>   <li>More datasets will be tested.",SafeML,PROJECT
</a> (2020) <b>SafeML: Safety Monitoring of Machine Learning Classifiers through Statistical Difference Measure<b>.,SafeML,PROJECT
"[<a href = ""https://github.com/ISorokos/SafeML/blob/master/Documents/SafeML%20Safety%20Monitoring%20of%20Machine%20Learning%20Classifiers%20through%20Statistical%20Difference%20Measure.pdf"">PDF</a>][<a href=""https://arxiv.org/abs/2005.13166v1"">arXiv</a>][<a href=""https://www.researchgate.net/publication/341699548_SafeML_Safety_Monitoring_of_Machine_Learning_Classifiers_through_Statistical_Difference_Measure/stats"">ResearchGate</a>][<a href = ""https://deepai.org/publication/safeml-safety-monitoring-of-machine-learning-classifiers-through-statistical-difference-measure"">DeepAI</a>][<a href= ""https://doi.org/10.1007/978-3-030-58920-2_13"">Springer</a>].",SafeML,PROJECT
"[<a href = ""https://github.com/ISorokos/SafeML/blob/master/Documents/SafeML%20Safety%20Monitoring%20of%20Machine%20Learning%20Classifiers%20through%20Statistical%20Difference%20Measure.pdf"">PDF</a>][<a href=""https://arxiv.org/abs/2005.13166v1"">arXiv</a>][<a href=""https://www.researchgate.net/publication/341699548_SafeML_Safety_Monitoring_of_Machine_Learning_Classifiers_through_Statistical_Difference_Measure/stats"">ResearchGate</a>][<a href = ""https://deepai.org/publication/safeml-safety-monitoring-of-machine-learning-classifiers-through-statistical-difference-measure"">DeepAI</a>][<a href= ""https://doi.org/10.1007/978-3-030-58920-2_13"">Springer</a>].",SafeML,PROJECT
"[<a href = ""https://github.com/ISorokos/SafeML/blob/master/Documents/SafeML%20Safety%20Monitoring%20of%20Machine%20Learning%20Classifiers%20through%20Statistical%20Difference%20Measure.pdf"">PDF</a>][<a href=""https://arxiv.org/abs/2005.13166v1"">arXiv</a>][<a href=""https://www.researchgate.net/publication/341699548_SafeML_Safety_Monitoring_of_Machine_Learning_Classifiers_through_Statistical_Difference_Measure/stats"">ResearchGate</a>][<a href = ""https://deepai.org/publication/safeml-safety-monitoring-of-machine-learning-classifiers-through-statistical-difference-measure"">DeepAI</a>][<a href= ""https://doi.org/10.1007/978-3-030-58920-2_13"">Springer</a>].",SafeML,PROJECT
"[<a href = ""https://github.com/ISorokos/SafeML/blob/master/Documents/SafeML%20Safety%20Monitoring%20of%20Machine%20Learning%20Classifiers%20through%20Statistical%20Difference%20Measure.pdf"">PDF</a>][<a href=""https://arxiv.org/abs/2005.13166v1"">arXiv</a>][<a href=""https://www.researchgate.net/publication/341699548_SafeML_Safety_Monitoring_of_Machine_Learning_Classifiers_through_Statistical_Difference_Measure/stats"">ResearchGate</a>][<a href = ""https://deepai.org/publication/safeml-safety-monitoring-of-machine-learning-classifiers-through-statistical-difference-measure"">DeepAI</a>][<a href= ""https://doi.org/10.1007/978-3-030-58920-2_13"">Springer</a>].",safeml,PROJECT
"</p> <p align=""justify"">  [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Documents/SafeML_IMBSA2020_Presentation.pdf"">Presentation at the 7th International Symposium on Model-Based Safety and Assessment (IMBSA2020)</a>].",SafeML,PROJECT
"</p> <p align=""justify"">  [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Documents/SafeML_IMBSA2020_Presentation.pdf"">Presentation at the 7th International Symposium on Model-Based Safety and Assessment (IMBSA2020)</a>].",SafeML,PROJECT
"[<a href = ""https://github.com/ISorokos/SafeML/blob/master/Documents/Toward%20Improving%20Confidence%20in%20Autonomous%20Vehicle%20Software%20A%20Study%20on%20Traffic%20Sign%20Recognition%20Systems.pdf"">PDF</a>][<a href = ""https://doi.org/10.1109/MC.2021.3075054"">IEEE</a>]</p>    ## Medium Posts <p align=""justify""> <a href = ""https://towardsdatascience.com/how-to-make-your-classifier-safe-46d55f39f1ad"">How to Make Your Classifier Safe</a>, (2020, June) Published in Medium (Towards Data Science).",SafeML,PROJECT
"[<a href = ""https://www.kaggle.com/kooaslansefat/concept-drift-adversarial-validation-safeml"">Kaggle Version</a>] </p>   ## Talks <p align=""justify""> SafeML - A Human-in-the-loop Approach for Safety Monitoring of Machine Learning Classifiers (2020, November), in <a href = ""https://openethics.ai/events/"">Open Ethics Series S01E06 Human-in-the-loop AI Ðgency & oversight.",safeml,PROJECT
"[<a href = ""https://www.kaggle.com/kooaslansefat/concept-drift-adversarial-validation-safeml"">Kaggle Version</a>] </p>   ## Talks <p align=""justify""> SafeML - A Human-in-the-loop Approach for Safety Monitoring of Machine Learning Classifiers (2020, November), in <a href = ""https://openethics.ai/events/"">Open Ethics Series S01E06 Human-in-the-loop AI Ðgency & oversight.",SafeML,PROJECT
"v=m4szeYSIuRc"">YouTube</a>] </p>  <p align=""justify""> Trustworthy and Explainable Machine Learning with SafeML (2021, February), in <a href = ""https://www.deeplearning.ai"">DeepLearning.ai - Pai and AI Series.",SafeML,PROJECT
"</a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Documents/SafeML_Pie_and_AI.pptx"">Presentation File</a>]   ## Cite as <pre> @article{Aslansefat2020SafeML,    author  = {{Aslansefat}, Koorosh and {Sorokos}, Ioannis and {Whiting}, Declan and               {Tavakoli Kolagari}, Ramin and {Papadopoulos}, Yiannis},    title   = ""{SafeML: Safety Monitoring of Machine Learning Classifiers through Statistical Difference Measure}"",    journal = {arXiv e-prints},    year    = {2020},    url     = {https://arxiv.org/abs/2005.13166},    eprint  = {2005.13166}, } </pre>  ## Related Works <p align=""justify""> Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & ManÃ©, D. (2016).",SafeML,PROJECT
"</a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Documents/SafeML_Pie_and_AI.pptx"">Presentation File</a>]   ## Cite as <pre> @article{Aslansefat2020SafeML,    author  = {{Aslansefat}, Koorosh and {Sorokos}, Ioannis and {Whiting}, Declan and               {Tavakoli Kolagari}, Ramin and {Papadopoulos}, Yiannis},    title   = ""{SafeML: Safety Monitoring of Machine Learning Classifiers through Statistical Difference Measure}"",    journal = {arXiv e-prints},    year    = {2020},    url     = {https://arxiv.org/abs/2005.13166},    eprint  = {2005.13166}, } </pre>  ## Related Works <p align=""justify""> Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & ManÃ©, D. (2016).",SafeML,PROJECT
"</a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Documents/SafeML_Pie_and_AI.pptx"">Presentation File</a>]   ## Cite as <pre> @article{Aslansefat2020SafeML,    author  = {{Aslansefat}, Koorosh and {Sorokos}, Ioannis and {Whiting}, Declan and               {Tavakoli Kolagari}, Ramin and {Papadopoulos}, Yiannis},    title   = ""{SafeML: Safety Monitoring of Machine Learning Classifiers through Statistical Difference Measure}"",    journal = {arXiv e-prints},    year    = {2020},    url     = {https://arxiv.org/abs/2005.13166},    eprint  = {2005.13166}, } </pre>  ## Related Works <p align=""justify""> Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & ManÃ©, D. (2016).",SafeML,PROJECT
"[<a href = ""https://doi.org/10.1109/SP.2018.00058"">IEEE</a>] </p>    ## Related Projects <p align=""justify""> <a href = ""https://github.com/koo-ec/SafeNN"">SafeNN Project: </a>This porject relies on the idea of SafeML and aimed to evaluate safety of Deep Neural Networks using statistical distance measures (will be public soon).",SafeNN,PROJECT
"[<a href = ""https://doi.org/10.1109/SP.2018.00058"">IEEE</a>] </p>    ## Related Projects <p align=""justify""> <a href = ""https://github.com/koo-ec/SafeNN"">SafeNN Project: </a>This porject relies on the idea of SafeML and aimed to evaluate safety of Deep Neural Networks using statistical distance measures (will be public soon).",SafeNN,PROJECT
"[<a href = ""https://doi.org/10.1109/SP.2018.00058"">IEEE</a>] </p>    ## Related Projects <p align=""justify""> <a href = ""https://github.com/koo-ec/SafeNN"">SafeNN Project: </a>This porject relies on the idea of SafeML and aimed to evaluate safety of Deep Neural Networks using statistical distance measures (will be public soon).",SafeML,PROJECT
"</p> <p align=""justify""> <a href = ""https://github.com/dependable-ai/nn-dependability-kit"">NN-Dependability-KIT Project: </a>Toolbox for software dependability engineering of artificial neural networks.",nn-dependability-kit,PROJECT
"</p> <p align=""justify""> <a href = ""https://github.com/dependable-ai/nn-dependability-kit"">NN-Dependability-KIT Project: </a>Toolbox for software dependability engineering of artificial neural networks.",NN-Dependability-KIT,PROJECT
"</p> <p align=""justify""> <a href = ""https://github.com/cfinlay/confident-nn"">Confident-NN Project: </a>Toolbox for empirical confidence estimation in neural networks-based classification.",confident-nn,PROJECT
"</p> <p align=""justify""> <a href = ""https://github.com/cfinlay/confident-nn"">Confident-NN Project: </a>Toolbox for empirical confidence estimation in neural networks-based classification.",Confident-NN,PROJECT
"</p> <p align=""justify""> <a href = ""https://eth-sri.github.io/research/safeai"">SafeAI Project: </a>Different toolboxes like <a href = ""https://github.com/eth-sri/diffai"">DiffAI<a>, <a href = ""https://github.com/eth-sri/dl2"">DL2<a> and <a href = ""https://github.com/eth-sri/eran"">ERAN</a> from SRILab ETH ZÃ¼rich focusing on robust, safe and interpretable AI.",safeai,PROJECT
"</p> <p align=""justify""> <a href = ""https://eth-sri.github.io/research/safeai"">SafeAI Project: </a>Different toolboxes like <a href = ""https://github.com/eth-sri/diffai"">DiffAI<a>, <a href = ""https://github.com/eth-sri/dl2"">DL2<a> and <a href = ""https://github.com/eth-sri/eran"">ERAN</a> from SRILab ETH ZÃ¼rich focusing on robust, safe and interpretable AI.",SafeAI,PROJECT
"</p>  <p align=""justify""> <a href = ""https://openai.com/blog/debate/"">AI Safety via Debate:</a> This project aims to evaluate the AI Safety through <a href = ""https://github.com/openai/pixel"">Debate Games</a>.",AI Safety via Debate,PROJECT
"</p>  <p align=""justify""> <a href =""https://github.com/safednn-nasa"">SafeDNN:</a> A research project from NASA that focses on the property inference from Deep Neural Networks (DNNs).",safednn,PROJECT
"</p>  <p align=""justify""> <a href =""https://github.com/safednn-nasa"">SafeDNN:</a> A research project from NASA that focses on the property inference from Deep Neural Networks (DNNs).",SafeDNN,PROJECT
"</p> <p align=""justify""> <b>A1:</b> The ""buffer-size"" in SafeML algorithm should be defined by an expert in the design time.",SafeML,PROJECT
"</p> <p align=""justify""> <b>A2:</b> It is also another hyper-parameter that should be defined in the offline phase of the SafeML and after training the classifier.",SafeML,PROJECT
"</p>   ## Future Extensions <p align=""justify""> Currently SafeML has designed for Deep Learning (DL) and Machine Learning (ML) Classifiers.",SafeML,PROJECT
We are also trying to improve SafeML for classification tasks in time series.,SafeML,PROJECT
[SafeML contributors](https://contrib.rocks/image?,SafeML,PROJECT
repo=ISorokos/SafeML&max=2000)](https://github.com/ISorokos/SafeML/graphs/contributors),SafeML,PROJECT
repo=ISorokos/SafeML&max=2000)](https://github.com/ISorokos/SafeML/graphs/contributors),SafeML,PROJECT
"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",LongBench,PROJECT
"# sql4ml  Sql4ml is a Haskell project that takes as input SQL code which defines the objective/cost/loss function of a supervised machine learning model (ML), as well as a set of parameters, and generates TensorFlow (Python API) code that trains this model.",sql4ml,PROJECT
"# sql4ml  Sql4ml is a Haskell project that takes as input SQL code which defines the objective/cost/loss function of a supervised machine learning model (ML), as well as a set of parameters, and generates TensorFlow (Python API) code that trains this model.",Sql4ml,PROJECT
"It is the prototype that is used in the paper ""Nantia Makrynioti, Ruy Ley-Wild and Vasilis Vassalos. sql4ml: a declarative end-to-end workflow for machine learning.""",sql4ml,PROJECT
"See also the section on installation on how to build dependent projects.  ## Installation  Sql4ml uses the open source project queryparser (https://github.com/uber/queryparser), also in Haskell.",Sql4ml,PROJECT
"See also the section on installation on how to build dependent projects.  ## Installation  Sql4ml uses the open source project queryparser (https://github.com/uber/queryparser), also in Haskell.",queryparser,PROJECT
"See also the section on installation on how to build dependent projects.  ## Installation  Sql4ml uses the open source project queryparser (https://github.com/uber/queryparser), also in Haskell.",queryparser,PROJECT
The sql4ml module is in file sql4ml_translator.hs.,sql4ml,PROJECT
The sql4ml module is in file sql4ml_translator.hs.,sql4ml,PROJECT
"To compile main.hs, run in a terminal:      ghc -o main main.hs  Sql4ml uses the MySQL database (https://www.mysql.com/) for storing data.",Sql4ml,PROJECT
"You can also try to translate individual SQL queries by loading the sql4ml_translator module in ghci and type:      translateToTensorFlowCommand (L.pack ""CREATE VIEW squaredErrors AS SELECT POW(errors.errorValue, 2) AS squaredErrorValue, errors.observationID AS observationID FROM errors;"") [""features""] [""weights""] [[""f1"", ""f2""]]  where      translateToTensorFlowCommand :: L.Text -> [String] -> [String] -> [[String]] -> String     translateToTensorFlowCommand sql_statement feature_tables variable_tables feature_names  and * sql_statement: a SQL create view query in type Text * feature_tables: a list of names of the tables storing the features of the model. * variable_tables: a list of names of the tables storing the weights of the model. * feature_names: a list of lists, each of which has the actual names of features stored in each table.",sql4ml,PROJECT
"<p align=""center""> <img src=""https://user-images.githubusercontent.com/51358498/152991504-005a1daa-2900-4f48-8bec-d163d6336ed2.png"" width=""400""> </p>  # OpenWeedLocator  Welcome to the OpenWeedLocator (OWL) project, an opensource hardware and software weed detector that uses entirely off-the-shelf componentry, very simple green-detection algorithms (with capacity to upgrade to in-crop detection) and 3D printable parts.",OpenWeedLocator,PROJECT
"<p align=""center""> <img src=""https://user-images.githubusercontent.com/51358498/152991504-005a1daa-2900-4f48-8bec-d163d6336ed2.png"" width=""400""> </p>  # OpenWeedLocator  Welcome to the OpenWeedLocator (OWL) project, an opensource hardware and software weed detector that uses entirely off-the-shelf componentry, very simple green-detection algorithms (with capacity to upgrade to in-crop detection) and 3D printable parts.",OpenWeedLocator,PROJECT
"<p align=""center""> <img src=""https://user-images.githubusercontent.com/51358498/152991504-005a1daa-2900-4f48-8bec-d163d6336ed2.png"" width=""400""> </p>  # OpenWeedLocator  Welcome to the OpenWeedLocator (OWL) project, an opensource hardware and software weed detector that uses entirely off-the-shelf componentry, very simple green-detection algorithms (with capacity to upgrade to in-crop detection) and 3D printable parts.",OWL,PROJECT
"OWL integrates weed detection on a Raspberry Pi with a relay control board or custom driver board, in a custom designed case so you can attach any 12V solenoid, relay, lightbulb or  device for low-cost, simple and open-source site-specific weed control.",OWL,PROJECT
"Projects to date have seen OWL mounted on robots, vehicles and bicycles for spot spraying.",OWL,PROJECT
"For the latest ideas and news, check out the [Discussion](https://github.com/geezacoleman/OpenWeedLocator/discussions) tab.  ### News **08-05-2024** - Check out the latest Compact OWL enclosures!",OpenWeedLocator,PROJECT
"For the latest ideas and news, check out the [Discussion](https://github.com/geezacoleman/OpenWeedLocator/discussions) tab.  ### News **08-05-2024** - Check out the latest Compact OWL enclosures!",OWL,PROJECT
|                                           | Front                                                                                                                    | Back                                                                                                                    | |-------------------------------------------|--------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------| | Official OWL extruded aluminium enclosure | !,OWL,PROJECT
[front_enclosure](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/c5d8c4d7-21d0-4987-9691-cd9a8615b65a) | !,OpenWeedLocator,PROJECT
[enclosure_back](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/4ddf0538-265e-4e6b-aebd-040336d1562b) | | 3D printable enclosure                    |  !,OpenWeedLocator,PROJECT
[20240502_214051_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/beb40fca-723d-4f2b-9555-46bc0587cd8d)                                                                                                                        |        !,OpenWeedLocator,PROJECT
[3d-printed_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/aa4898ff-5481-4d89-98ba-656302ac70b1)                                                                                                                 |  Find all the 3D printable files [on the OWL repository](#3d-printing) or download them from  [Printables](https://www.printables.com/model/875853-raspberry-pi-rugged-imaging-enclosure).  **13-04-2024** - OpenWeedLocator now supports Raspberry Pi 5 and picamera2!,OpenWeedLocator,PROJECT
[3d-printed_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/aa4898ff-5481-4d89-98ba-656302ac70b1)                                                                                                                 |  Find all the 3D printable files [on the OWL repository](#3d-printing) or download them from  [Printables](https://www.printables.com/model/875853-raspberry-pi-rugged-imaging-enclosure).  **13-04-2024** - OpenWeedLocator now supports Raspberry Pi 5 and picamera2!,OpenWeedLocator,PROJECT
[robotti_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/99df0188-a850-4753-ac48-ab743c46d563) |                   !,OpenWeedLocator,PROJECT
[OWL - on robot agerris](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/9cb73514-dffc-4c53-969e-c1c816610f1b)                  |          !,OWL,PROJECT
[OWL - on robot agerris](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/9cb73514-dffc-4c53-969e-c1c816610f1b)                  |          !,OpenWeedLocator,PROJECT
"[bike_owl_cropped](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/17ad4ead-429e-4384-9e74-b050a536897f)        |  ### Official Publications  #### OpenWeedLocator (OWL): An open-source, low-cost device for fallow weed detection  This is the original OWL publication, released in [Scientific Reports (open access)](https://www.nature.com/articles/s41598-021-03858-9).",OpenWeedLocator,PROJECT
"[bike_owl_cropped](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/17ad4ead-429e-4384-9e74-b050a536897f)        |  ### Official Publications  #### OpenWeedLocator (OWL): An open-source, low-cost device for fallow weed detection  This is the original OWL publication, released in [Scientific Reports (open access)](https://www.nature.com/articles/s41598-021-03858-9).",OpenWeedLocator,PROJECT
"[bike_owl_cropped](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/17ad4ead-429e-4384-9e74-b050a536897f)        |  ### Official Publications  #### OpenWeedLocator (OWL): An open-source, low-cost device for fallow weed detection  This is the original OWL publication, released in [Scientific Reports (open access)](https://www.nature.com/articles/s41598-021-03858-9).",OWL,PROJECT
If you use the OWL in your research please consider citing this publication.  #### Investigating image-based fallow weed detection performance on Raphanus sativus and Avena sativa at speeds up to 30 km/h  The performance of the OWL from 5 - 30 km/h with different cameras and on broadleaf and grass 'weeds' was tested and published in [Computers and Electronics in Agriculture](https://www.sciencedirect.com/science/article/pii/S0168169923008074).,OWL,PROJECT
If you use the OWL in your research please consider citing this publication.  #### Investigating image-based fallow weed detection performance on Raphanus sativus and Avena sativa at speeds up to 30 km/h  The performance of the OWL from 5 - 30 km/h with different cameras and on broadleaf and grass 'weeds' was tested and published in [Computers and Electronics in Agriculture](https://www.sciencedirect.com/science/article/pii/S0168169923008074).,OWL,PROJECT
"[DOI](https://zenodo.org/badge/399194159.svg)](https://zenodo.org/badge/latestdoi/399194159)  # Overview  * [OWL Use Cases](#owl-use-cases) * [Community Development](#community-development-and-contribution) * [Hardware Requirements](#hardware-requirements)     - [Hardware Assembly](#hardware-assembly)     - [Single Board Computer (SBC) Options](#sbc-options) * [Software Installation](#software)     - [Quick Method](#quick-method)     - [Detailed Method](#detailed-method)     - [Changing Detection Settings](#changing-detection-settings)     - [Green-on-Green (almost) :eyes::dart::seedling:](#green-on-green)     - [Installing on non-Raspberry Pi Computers](#non-raspberry-pi-installation) * [Controller](#connecting-a-controller) * [3D Printing](#3d-printing) * [Updating OWL](#updating-owl)     - [Version History](#version-history) * [Troubleshooting](#troubleshooting) * [Citing OWL](#citing-owl) * [Acknowledgements](#acknowledgements) * [References](#references)  ### Manuals  If you prefer a hardcopy version of these instructions, you can view and download the PDF using one of the links below.",OWL,PROJECT
"[DOI](https://zenodo.org/badge/399194159.svg)](https://zenodo.org/badge/latestdoi/399194159)  # Overview  * [OWL Use Cases](#owl-use-cases) * [Community Development](#community-development-and-contribution) * [Hardware Requirements](#hardware-requirements)     - [Hardware Assembly](#hardware-assembly)     - [Single Board Computer (SBC) Options](#sbc-options) * [Software Installation](#software)     - [Quick Method](#quick-method)     - [Detailed Method](#detailed-method)     - [Changing Detection Settings](#changing-detection-settings)     - [Green-on-Green (almost) :eyes::dart::seedling:](#green-on-green)     - [Installing on non-Raspberry Pi Computers](#non-raspberry-pi-installation) * [Controller](#connecting-a-controller) * [3D Printing](#3d-printing) * [Updating OWL](#updating-owl)     - [Version History](#version-history) * [Troubleshooting](#troubleshooting) * [Citing OWL](#citing-owl) * [Acknowledgements](#acknowledgements) * [References](#references)  ### Manuals  If you prefer a hardcopy version of these instructions, you can view and download the PDF using one of the links below.",OWL,PROJECT
"[DOI](https://zenodo.org/badge/399194159.svg)](https://zenodo.org/badge/latestdoi/399194159)  # Overview  * [OWL Use Cases](#owl-use-cases) * [Community Development](#community-development-and-contribution) * [Hardware Requirements](#hardware-requirements)     - [Hardware Assembly](#hardware-assembly)     - [Single Board Computer (SBC) Options](#sbc-options) * [Software Installation](#software)     - [Quick Method](#quick-method)     - [Detailed Method](#detailed-method)     - [Changing Detection Settings](#changing-detection-settings)     - [Green-on-Green (almost) :eyes::dart::seedling:](#green-on-green)     - [Installing on non-Raspberry Pi Computers](#non-raspberry-pi-installation) * [Controller](#connecting-a-controller) * [3D Printing](#3d-printing) * [Updating OWL](#updating-owl)     - [Version History](#version-history) * [Troubleshooting](#troubleshooting) * [Citing OWL](#citing-owl) * [Acknowledgements](#acknowledgements) * [References](#references)  ### Manuals  If you prefer a hardcopy version of these instructions, you can view and download the PDF using one of the links below.",owl,PROJECT
"Solenoids | Goyen solenoid 3QH/3662 with Teejet body | Many alternatives exist as outlined in [#2](https://github.com/geezacoleman/OpenWeedLocator/issues/2) Spray tips | Teejet TP4003E-SS | 40 degree, flat fan nozzles, stainless steel Strainer | TeeJet 50 mesh strainer | Protect spray tip from clogging/damage Pump/tank | Northstar 12V 60L ATV Sprayer | 8.3 LPM 12V pump, 60L capacity, tray mounted  ## Robot-mounted spot spraying  A second system, identical to the first, was developed for the University of Sydney's Digifarm robot, the Agerris Digital Farm Hand.",OpenWeedLocator,PROJECT
"Check out the OWL [Discussion](https://github.com/geezacoleman/OpenWeedLocator/discussions) page for any questions, suggestions ideas or feedback.",OWL,PROJECT
"Check out the OWL [Discussion](https://github.com/geezacoleman/OpenWeedLocator/discussions) page for any questions, suggestions ideas or feedback.",OpenWeedLocator,PROJECT
[Finished OWL - small](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/accae1b1-b00d-40f9-ab95-743b732df2a0)      | !,OWL,PROJECT
[Finished OWL - small](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/accae1b1-b00d-40f9-ab95-743b732df2a0)      | !,OpenWeedLocator,PROJECT
[3D Printed OWL](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/a9771aa2-355d-40db-ac15-f6da037b63ed) | !,OWL,PROJECT
[3D Printed OWL](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/a9771aa2-355d-40db-ac15-f6da037b63ed) | !,OpenWeedLocator,PROJECT
[Extruded OWL](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/d153db2d-624b-427e-832e-599a3a841623) |  All 3D models and hardware assembly guides are provided in subsequent sections.,OWL,PROJECT
[Extruded OWL](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/d153db2d-624b-427e-832e-599a3a841623) |  All 3D models and hardware assembly guides are provided in subsequent sections.,OpenWeedLocator,PROJECT
"The parts list is substantially reduced:  | **Component**                                                        | **Quantity**      | **Link**                                                                                                                                                                                                                                                                                                                                                                                                          | |----------------------------------------------------------------------|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | **Enclosure**                                                        |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | **OFFICIAL OWL ENCLOSURE** - aluminium                               | 1                 | TBD                                                                                                                                                                                                                                                                                                                                                                                                               | | Extrusion - 3D printed                                               | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Main%20Body.stl)                                                                                                                                                                                                                                                                                                   | | Front plate                                                          | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Frontplate.stl)                                                                                                                                                                                                                                                                                                    | | Tray                                                                 | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/picamera2/3D%20Models/Compact%20OWL/Tray.stl)                                                                                                                                                                                                                                                                                                     | | Back plate - Amphenol, Adafruit RJ45                                 | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Receptacle%20and%20Ethernet.stl)                                                                                                                                                                                                                                                                   | | Back plate - Amphenol only                                           | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Receptacle%20Only.stl)                                                                                                                                                                                                                                                                             | | Back plate - 16 mm cable gland                                       | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Gland.stl)                                                                                                                                                                                                                                                                                         | | Lens mount                                                           | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Lens%20Mount.stl)                                                                                                                                                                                                                                                                                                  | | Camera mount                                                         | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/picamera2/3D%20Models/Compact%20OWL/Camera%20Mount.stl)                                                                                                                                                                                                                                                                                           | | **Computing**                                                        |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | Raspberry Pi 5 4GB (or Pi 4 or 3B+)                                  | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-5-model-b-4gb.html)                                                                                                                                                                                                                                                                                                                                           | | *Green-on-Green ONLY - Google Coral USB Accelerator                  | 1                 | [Link](https://coral.ai/products/accelerator)                                                                                                                                                                                                                                                                                                                                                                     | | 64GB SD Card (min. 16 GB)                                            | 1                 | [Link](https://core-electronics.com.au/extreme-sd-microsd-memory-card-64gb-class-10-adapter-included.html)                                                                                                                                                                                                                                                                                                        | | **Camera** (choose one)                                              |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | RECOMMENDED: Raspberry Pi Global Shutter Camera                      | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-global-shutter-camera.html)                                                                                                                                                                                                                                                                                                                                   | | CCTV 6mm Wide Angle Lens                                             | 1 (GS or HQ only) | [Link](https://core-electronics.com.au/raspberry-pi-6mm-wide-angle-lens.html)                                                                                                                                                                                                                                                                                                                                     | | SUPPORTED: Raspberry Pi 12MP HQ Camera                               | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-hq-camera.html)                                                                                                                                                                                                                                                                                                                                               | | SUPPORTED: Raspberry Pi Camera Module 3                              | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-3.html)                                                                                                                                                                                                                                                                                                                                                | | SUPPORTED: Raspberry Pi V2 Camera (NOT RECOMMENDED)                  | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-board-v2-8-megapixels-38552.html)                                                                                                                                                                                                                                                                                                                      | | âš ï¸NOTEâš ï¸ If you use the RPi 5, make sure you have the right camera cable | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-fpc-adapter-cable-200mm.html)                                                                                                                                                                                                                                                                                                                          | | **Power Management** * items only needed in place of OWL driver board | 1                 |                                                                                                                                                                                                                                                                                                                                                                                                                   | | **OFFICIAL OWL DRIVER BOARD** (incl. power mgmt, relay control)      | 1                 | TBD                                                                                                                                                                                                                                                                                                                                                                                                               | | * 5V 5A Step Down Voltage Regulator                                  | 1                 | [Link](https://core-electronics.com.au/pololu-5v-5a-step-down-voltage-regulator-d24v50f5.html)                                                                                                                                                                                                                                                                                                                    | | * 4 Channel, Relay Control Board HAT                                 | 1                 | [Link](https://core-electronics.com.au/pirelay-v2-relay-board-for-raspberry-pi-1.html?",OWL,PROJECT
"The parts list is substantially reduced:  | **Component**                                                        | **Quantity**      | **Link**                                                                                                                                                                                                                                                                                                                                                                                                          | |----------------------------------------------------------------------|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | **Enclosure**                                                        |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | **OFFICIAL OWL ENCLOSURE** - aluminium                               | 1                 | TBD                                                                                                                                                                                                                                                                                                                                                                                                               | | Extrusion - 3D printed                                               | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Main%20Body.stl)                                                                                                                                                                                                                                                                                                   | | Front plate                                                          | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Frontplate.stl)                                                                                                                                                                                                                                                                                                    | | Tray                                                                 | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/picamera2/3D%20Models/Compact%20OWL/Tray.stl)                                                                                                                                                                                                                                                                                                     | | Back plate - Amphenol, Adafruit RJ45                                 | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Receptacle%20and%20Ethernet.stl)                                                                                                                                                                                                                                                                   | | Back plate - Amphenol only                                           | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Receptacle%20Only.stl)                                                                                                                                                                                                                                                                             | | Back plate - 16 mm cable gland                                       | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Gland.stl)                                                                                                                                                                                                                                                                                         | | Lens mount                                                           | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Lens%20Mount.stl)                                                                                                                                                                                                                                                                                                  | | Camera mount                                                         | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/picamera2/3D%20Models/Compact%20OWL/Camera%20Mount.stl)                                                                                                                                                                                                                                                                                           | | **Computing**                                                        |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | Raspberry Pi 5 4GB (or Pi 4 or 3B+)                                  | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-5-model-b-4gb.html)                                                                                                                                                                                                                                                                                                                                           | | *Green-on-Green ONLY - Google Coral USB Accelerator                  | 1                 | [Link](https://coral.ai/products/accelerator)                                                                                                                                                                                                                                                                                                                                                                     | | 64GB SD Card (min. 16 GB)                                            | 1                 | [Link](https://core-electronics.com.au/extreme-sd-microsd-memory-card-64gb-class-10-adapter-included.html)                                                                                                                                                                                                                                                                                                        | | **Camera** (choose one)                                              |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | RECOMMENDED: Raspberry Pi Global Shutter Camera                      | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-global-shutter-camera.html)                                                                                                                                                                                                                                                                                                                                   | | CCTV 6mm Wide Angle Lens                                             | 1 (GS or HQ only) | [Link](https://core-electronics.com.au/raspberry-pi-6mm-wide-angle-lens.html)                                                                                                                                                                                                                                                                                                                                     | | SUPPORTED: Raspberry Pi 12MP HQ Camera                               | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-hq-camera.html)                                                                                                                                                                                                                                                                                                                                               | | SUPPORTED: Raspberry Pi Camera Module 3                              | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-3.html)                                                                                                                                                                                                                                                                                                                                                | | SUPPORTED: Raspberry Pi V2 Camera (NOT RECOMMENDED)                  | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-board-v2-8-megapixels-38552.html)                                                                                                                                                                                                                                                                                                                      | | âš ï¸NOTEâš ï¸ If you use the RPi 5, make sure you have the right camera cable | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-fpc-adapter-cable-200mm.html)                                                                                                                                                                                                                                                                                                                          | | **Power Management** * items only needed in place of OWL driver board | 1                 |                                                                                                                                                                                                                                                                                                                                                                                                                   | | **OFFICIAL OWL DRIVER BOARD** (incl. power mgmt, relay control)      | 1                 | TBD                                                                                                                                                                                                                                                                                                                                                                                                               | | * 5V 5A Step Down Voltage Regulator                                  | 1                 | [Link](https://core-electronics.com.au/pololu-5v-5a-step-down-voltage-regulator-d24v50f5.html)                                                                                                                                                                                                                                                                                                                    | | * 4 Channel, Relay Control Board HAT                                 | 1                 | [Link](https://core-electronics.com.au/pirelay-v2-relay-board-for-raspberry-pi-1.html?",OpenWeedLocator,PROJECT
"The parts list is substantially reduced:  | **Component**                                                        | **Quantity**      | **Link**                                                                                                                                                                                                                                                                                                                                                                                                          | |----------------------------------------------------------------------|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | **Enclosure**                                                        |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | **OFFICIAL OWL ENCLOSURE** - aluminium                               | 1                 | TBD                                                                                                                                                                                                                                                                                                                                                                                                               | | Extrusion - 3D printed                                               | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Main%20Body.stl)                                                                                                                                                                                                                                                                                                   | | Front plate                                                          | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Frontplate.stl)                                                                                                                                                                                                                                                                                                    | | Tray                                                                 | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/picamera2/3D%20Models/Compact%20OWL/Tray.stl)                                                                                                                                                                                                                                                                                                     | | Back plate - Amphenol, Adafruit RJ45                                 | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Receptacle%20and%20Ethernet.stl)                                                                                                                                                                                                                                                                   | | Back plate - Amphenol only                                           | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Receptacle%20Only.stl)                                                                                                                                                                                                                                                                             | | Back plate - 16 mm cable gland                                       | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Gland.stl)                                                                                                                                                                                                                                                                                         | | Lens mount                                                           | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Lens%20Mount.stl)                                                                                                                                                                                                                                                                                                  | | Camera mount                                                         | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/picamera2/3D%20Models/Compact%20OWL/Camera%20Mount.stl)                                                                                                                                                                                                                                                                                           | | **Computing**                                                        |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | Raspberry Pi 5 4GB (or Pi 4 or 3B+)                                  | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-5-model-b-4gb.html)                                                                                                                                                                                                                                                                                                                                           | | *Green-on-Green ONLY - Google Coral USB Accelerator                  | 1                 | [Link](https://coral.ai/products/accelerator)                                                                                                                                                                                                                                                                                                                                                                     | | 64GB SD Card (min. 16 GB)                                            | 1                 | [Link](https://core-electronics.com.au/extreme-sd-microsd-memory-card-64gb-class-10-adapter-included.html)                                                                                                                                                                                                                                                                                                        | | **Camera** (choose one)                                              |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | RECOMMENDED: Raspberry Pi Global Shutter Camera                      | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-global-shutter-camera.html)                                                                                                                                                                                                                                                                                                                                   | | CCTV 6mm Wide Angle Lens                                             | 1 (GS or HQ only) | [Link](https://core-electronics.com.au/raspberry-pi-6mm-wide-angle-lens.html)                                                                                                                                                                                                                                                                                                                                     | | SUPPORTED: Raspberry Pi 12MP HQ Camera                               | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-hq-camera.html)                                                                                                                                                                                                                                                                                                                                               | | SUPPORTED: Raspberry Pi Camera Module 3                              | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-3.html)                                                                                                                                                                                                                                                                                                                                                | | SUPPORTED: Raspberry Pi V2 Camera (NOT RECOMMENDED)                  | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-board-v2-8-megapixels-38552.html)                                                                                                                                                                                                                                                                                                                      | | âš ï¸NOTEâš ï¸ If you use the RPi 5, make sure you have the right camera cable | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-fpc-adapter-cable-200mm.html)                                                                                                                                                                                                                                                                                                                          | | **Power Management** * items only needed in place of OWL driver board | 1                 |                                                                                                                                                                                                                                                                                                                                                                                                                   | | **OFFICIAL OWL DRIVER BOARD** (incl. power mgmt, relay control)      | 1                 | TBD                                                                                                                                                                                                                                                                                                                                                                                                               | | * 5V 5A Step Down Voltage Regulator                                  | 1                 | [Link](https://core-electronics.com.au/pololu-5v-5a-step-down-voltage-regulator-d24v50f5.html)                                                                                                                                                                                                                                                                                                                    | | * 4 Channel, Relay Control Board HAT                                 | 1                 | [Link](https://core-electronics.com.au/pirelay-v2-relay-board-for-raspberry-pi-1.html?",OpenWeedLocator,PROJECT
"The parts list is substantially reduced:  | **Component**                                                        | **Quantity**      | **Link**                                                                                                                                                                                                                                                                                                                                                                                                          | |----------------------------------------------------------------------|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | **Enclosure**                                                        |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | **OFFICIAL OWL ENCLOSURE** - aluminium                               | 1                 | TBD                                                                                                                                                                                                                                                                                                                                                                                                               | | Extrusion - 3D printed                                               | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Main%20Body.stl)                                                                                                                                                                                                                                                                                                   | | Front plate                                                          | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Frontplate.stl)                                                                                                                                                                                                                                                                                                    | | Tray                                                                 | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/picamera2/3D%20Models/Compact%20OWL/Tray.stl)                                                                                                                                                                                                                                                                                                     | | Back plate - Amphenol, Adafruit RJ45                                 | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Receptacle%20and%20Ethernet.stl)                                                                                                                                                                                                                                                                   | | Back plate - Amphenol only                                           | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Receptacle%20Only.stl)                                                                                                                                                                                                                                                                             | | Back plate - 16 mm cable gland                                       | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Gland.stl)                                                                                                                                                                                                                                                                                         | | Lens mount                                                           | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Lens%20Mount.stl)                                                                                                                                                                                                                                                                                                  | | Camera mount                                                         | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/picamera2/3D%20Models/Compact%20OWL/Camera%20Mount.stl)                                                                                                                                                                                                                                                                                           | | **Computing**                                                        |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | Raspberry Pi 5 4GB (or Pi 4 or 3B+)                                  | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-5-model-b-4gb.html)                                                                                                                                                                                                                                                                                                                                           | | *Green-on-Green ONLY - Google Coral USB Accelerator                  | 1                 | [Link](https://coral.ai/products/accelerator)                                                                                                                                                                                                                                                                                                                                                                     | | 64GB SD Card (min. 16 GB)                                            | 1                 | [Link](https://core-electronics.com.au/extreme-sd-microsd-memory-card-64gb-class-10-adapter-included.html)                                                                                                                                                                                                                                                                                                        | | **Camera** (choose one)                                              |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | RECOMMENDED: Raspberry Pi Global Shutter Camera                      | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-global-shutter-camera.html)                                                                                                                                                                                                                                                                                                                                   | | CCTV 6mm Wide Angle Lens                                             | 1 (GS or HQ only) | [Link](https://core-electronics.com.au/raspberry-pi-6mm-wide-angle-lens.html)                                                                                                                                                                                                                                                                                                                                     | | SUPPORTED: Raspberry Pi 12MP HQ Camera                               | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-hq-camera.html)                                                                                                                                                                                                                                                                                                                                               | | SUPPORTED: Raspberry Pi Camera Module 3                              | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-3.html)                                                                                                                                                                                                                                                                                                                                                | | SUPPORTED: Raspberry Pi V2 Camera (NOT RECOMMENDED)                  | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-board-v2-8-megapixels-38552.html)                                                                                                                                                                                                                                                                                                                      | | âš ï¸NOTEâš ï¸ If you use the RPi 5, make sure you have the right camera cable | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-fpc-adapter-cable-200mm.html)                                                                                                                                                                                                                                                                                                                          | | **Power Management** * items only needed in place of OWL driver board | 1                 |                                                                                                                                                                                                                                                                                                                                                                                                                   | | **OFFICIAL OWL DRIVER BOARD** (incl. power mgmt, relay control)      | 1                 | TBD                                                                                                                                                                                                                                                                                                                                                                                                               | | * 5V 5A Step Down Voltage Regulator                                  | 1                 | [Link](https://core-electronics.com.au/pololu-5v-5a-step-down-voltage-regulator-d24v50f5.html)                                                                                                                                                                                                                                                                                                                    | | * 4 Channel, Relay Control Board HAT                                 | 1                 | [Link](https://core-electronics.com.au/pirelay-v2-relay-board-for-raspberry-pi-1.html?",OpenWeedLocator,PROJECT
"The parts list is substantially reduced:  | **Component**                                                        | **Quantity**      | **Link**                                                                                                                                                                                                                                                                                                                                                                                                          | |----------------------------------------------------------------------|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | **Enclosure**                                                        |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | **OFFICIAL OWL ENCLOSURE** - aluminium                               | 1                 | TBD                                                                                                                                                                                                                                                                                                                                                                                                               | | Extrusion - 3D printed                                               | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Main%20Body.stl)                                                                                                                                                                                                                                                                                                   | | Front plate                                                          | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Frontplate.stl)                                                                                                                                                                                                                                                                                                    | | Tray                                                                 | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/picamera2/3D%20Models/Compact%20OWL/Tray.stl)                                                                                                                                                                                                                                                                                                     | | Back plate - Amphenol, Adafruit RJ45                                 | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Receptacle%20and%20Ethernet.stl)                                                                                                                                                                                                                                                                   | | Back plate - Amphenol only                                           | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Receptacle%20Only.stl)                                                                                                                                                                                                                                                                             | | Back plate - 16 mm cable gland                                       | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Gland.stl)                                                                                                                                                                                                                                                                                         | | Lens mount                                                           | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Lens%20Mount.stl)                                                                                                                                                                                                                                                                                                  | | Camera mount                                                         | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/picamera2/3D%20Models/Compact%20OWL/Camera%20Mount.stl)                                                                                                                                                                                                                                                                                           | | **Computing**                                                        |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | Raspberry Pi 5 4GB (or Pi 4 or 3B+)                                  | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-5-model-b-4gb.html)                                                                                                                                                                                                                                                                                                                                           | | *Green-on-Green ONLY - Google Coral USB Accelerator                  | 1                 | [Link](https://coral.ai/products/accelerator)                                                                                                                                                                                                                                                                                                                                                                     | | 64GB SD Card (min. 16 GB)                                            | 1                 | [Link](https://core-electronics.com.au/extreme-sd-microsd-memory-card-64gb-class-10-adapter-included.html)                                                                                                                                                                                                                                                                                                        | | **Camera** (choose one)                                              |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | RECOMMENDED: Raspberry Pi Global Shutter Camera                      | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-global-shutter-camera.html)                                                                                                                                                                                                                                                                                                                                   | | CCTV 6mm Wide Angle Lens                                             | 1 (GS or HQ only) | [Link](https://core-electronics.com.au/raspberry-pi-6mm-wide-angle-lens.html)                                                                                                                                                                                                                                                                                                                                     | | SUPPORTED: Raspberry Pi 12MP HQ Camera                               | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-hq-camera.html)                                                                                                                                                                                                                                                                                                                                               | | SUPPORTED: Raspberry Pi Camera Module 3                              | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-3.html)                                                                                                                                                                                                                                                                                                                                                | | SUPPORTED: Raspberry Pi V2 Camera (NOT RECOMMENDED)                  | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-board-v2-8-megapixels-38552.html)                                                                                                                                                                                                                                                                                                                      | | âš ï¸NOTEâš ï¸ If you use the RPi 5, make sure you have the right camera cable | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-fpc-adapter-cable-200mm.html)                                                                                                                                                                                                                                                                                                                          | | **Power Management** * items only needed in place of OWL driver board | 1                 |                                                                                                                                                                                                                                                                                                                                                                                                                   | | **OFFICIAL OWL DRIVER BOARD** (incl. power mgmt, relay control)      | 1                 | TBD                                                                                                                                                                                                                                                                                                                                                                                                               | | * 5V 5A Step Down Voltage Regulator                                  | 1                 | [Link](https://core-electronics.com.au/pololu-5v-5a-step-down-voltage-regulator-d24v50f5.html)                                                                                                                                                                                                                                                                                                                    | | * 4 Channel, Relay Control Board HAT                                 | 1                 | [Link](https://core-electronics.com.au/pirelay-v2-relay-board-for-raspberry-pi-1.html?",OpenWeedLocator,PROJECT
"dchild=1&keywords=20+awg+wire&qid=1623697639&sr=8-1-spons&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUEyMUNVM1BBQUNKSFNBJmVuY3J5cHRlZElkPUEwNjQ4MTQ5M0dRTE9ZR0MzUFE5VyZlbmNyeXB0ZWRBZElkPUExMDMwNTIwODM5OVVBOTFNRjdSJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==) |  </details>  ## Hardware Assembly Separate guides are provided for the Original OWL assembly and the Compact OWL.  ### Required tools * Wire strippers * Wire cutters * Pliers * Soldering iron/solder (only for Original OWL)  <details> <summary> Original OWL - Hardware Assembly</summary> <br>  >âš ï¸**NOTE**âš ï¸ All components listed above are relatively ""plug and play"" with minimal soldering or complex electronics required.",OWL,PROJECT
"dchild=1&keywords=20+awg+wire&qid=1623697639&sr=8-1-spons&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUEyMUNVM1BBQUNKSFNBJmVuY3J5cHRlZElkPUEwNjQ4MTQ5M0dRTE9ZR0MzUFE5VyZlbmNyeXB0ZWRBZElkPUExMDMwNTIwODM5OVVBOTFNRjdSJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==) |  </details>  ## Hardware Assembly Separate guides are provided for the Original OWL assembly and the Compact OWL.  ### Required tools * Wire strippers * Wire cutters * Pliers * Soldering iron/solder (only for Original OWL)  <details> <summary> Original OWL - Hardware Assembly</summary> <br>  >âš ï¸**NOTE**âš ï¸ All components listed above are relatively ""plug and play"" with minimal soldering or complex electronics required.",OWL,PROJECT
"dchild=1&keywords=20+awg+wire&qid=1623697639&sr=8-1-spons&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUEyMUNVM1BBQUNKSFNBJmVuY3J5cHRlZElkPUEwNjQ4MTQ5M0dRTE9ZR0MzUFE5VyZlbmNyeXB0ZWRBZElkPUExMDMwNTIwODM5OVVBOTFNRjdSJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==) |  </details>  ## Hardware Assembly Separate guides are provided for the Original OWL assembly and the Compact OWL.  ### Required tools * Wire strippers * Wire cutters * Pliers * Soldering iron/solder (only for Original OWL)  <details> <summary> Original OWL - Hardware Assembly</summary> <br>  >âš ï¸**NOTE**âš ï¸ All components listed above are relatively ""plug and play"" with minimal soldering or complex electronics required.",OWL,PROJECT
"dchild=1&keywords=20+awg+wire&qid=1623697639&sr=8-1-spons&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUEyMUNVM1BBQUNKSFNBJmVuY3J5cHRlZElkPUEwNjQ4MTQ5M0dRTE9ZR0MzUFE5VyZlbmNyeXB0ZWRBZElkPUExMDMwNTIwODM5OVVBOTFNRjdSJndpZGdldE5hbWU9c3BfYXRmJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ==) |  </details>  ## Hardware Assembly Separate guides are provided for the Original OWL assembly and the Compact OWL.  ### Required tools * Wire strippers * Wire cutters * Pliers * Soldering iron/solder (only for Original OWL)  <details> <summary> Original OWL - Hardware Assembly</summary> <br>  >âš ï¸**NOTE**âš ï¸ All components listed above are relatively ""plug and play"" with minimal soldering or complex electronics required.",OWL,PROJECT
Never make changes to the wiring on the detection unit while it is connected to 12V and always remain within the safe operating voltages of any component.  ## Original OWL - Hardware Assembly  A [video guide](https://www.youtube.com/watch?,OWL,PROJECT
v=vZqNKogzz8k) is available for the Original OWL assembly.,OWL,PROJECT
[wiring diagram-01](https://user-images.githubusercontent.com/40649348/156698009-a58ed01b-258f-462a-9524-ba3f8d7ec246.png)  ### Step 1 - enclosure and mounts  Assembling the components for an OWL unit requires the enclosure and mounts as a minimum.,OWL,PROJECT
"[Fuse](https://media.github.sydney.edu.au/user/5402/files/240e3e80-ce9e-11eb-8c0f-25e296720072)  Once the two red wires are soldered to the fuse, the fuse can be mounted on the rear panel of the OWL base.",OWL,PROJECT
Once all the wires have been connected you can now mount the Bulgin connector to the OWL base.  ### Step 4 - mounting the relay control board and voltage regulator  Attach the relay control board to the 3D printed relay control board mount using 2.5 mm standoffs.,OWL,PROJECT
The relay board and voltage regulator can then be installed in the raised slots in the OWL base.,OWL,PROJECT
[OWL - relay board diagram](https://media.github.sydney.edu.au/user/3859/files/431ed600-cf5b-11eb-94df-87f01e0a41a4) | !,OWL,PROJECT
Install in the raised slots in the OWL base.,OWL,PROJECT
|                                          Raspberry Pi mount                                          |                                              Raspberry Pi in OWL base                                               | |:----------------------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------:| | !,OWL,PROJECT
[adapter1](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/3ab35c57-77c1-4ce0-b627-410a3598db93)|!,OpenWeedLocator,PROJECT
[adapter2](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/7f50e5c8-9dd7-4e29-94d7-8bf8e907e011)|!,OpenWeedLocator,PROJECT
[adapter3](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/322d6409-8509-4895-8896-5959a5f529bc)  Mounting the HQ camera to the 3D printed mount: HQ camera and mount | HQ camera mounted in case :-------------: | :-------------: !,OpenWeedLocator,PROJECT
"[Cameracable](https://media.github.sydney.edu.au/user/5402/files/7ed2a200-d026-11eb-93cb-26a91d727094)  The HQ lens will need to be focused, details below, once the software is correctly set up.  ### Step 9 - adding buzzer and LEDs  Mount the buzzer inside the OWL base using double sided mounting tape and connect the 5 V and ground wires to Raspberry Pi GPIO pins 7 and 9, respectively.",OWL,PROJECT
"Install the 5 V LED inside the OWL base and connect the 5V and ground wire to GPIO pins 8 (TX pin) and 20 (GND pin), respectively.",OWL,PROJECT
Install the 12 V LED inside the OWL base and connect the 12 V and GND wires to their respective WAGO terminal blocks.,OWL,PROJECT
Buzzer location | LEDs in OWL base | GPIO pins :-------------: | :-------------: | :-------------: !,OWL,PROJECT
"[GPIOpins](https://media.github.sydney.edu.au/user/5402/files/e0474080-d027-11eb-93fd-8c7b7c783eea)  ### OPTIONAL STEP - adding real time clock module  Although optional, we recommend that you use a real time clock (RTC) module with the OWL system.",OWL,PROJECT
[1T7A9550](https://user-images.githubusercontent.com/40649348/156696142-8dd8aaa7-756a-4ff2-a638-7ebfb68165e6.jpeg)  ### Step 10 - connecting mounting hardware and OWL cover  There are four 6.5 mm holes on the OWL base for mounting to a boom.,OWL,PROJECT
[1T7A9550](https://user-images.githubusercontent.com/40649348/156696142-8dd8aaa7-756a-4ff2-a638-7ebfb68165e6.jpeg)  ### Step 10 - connecting mounting hardware and OWL cover  There are four 6.5 mm holes on the OWL base for mounting to a boom.,OWL,PROJECT
"Prior to installing the OWL cover, decide on a mounting solution suitable to your needs.",OWL,PROJECT
The cover of the OWL unit is secured with 4 x M3 nuts and bolts.,OWL,PROJECT
Place M3 nuts into the slots in the OWL base.,OWL,PROJECT
Mounting hardware | Cover nuts | Completed OWL unit :-------------: | :-------------: | :-------------: !,OWL,PROJECT
[1T7A9554](https://user-images.githubusercontent.com/40649348/156698342-4b3ebba5-337e-469c-bd0e-c34985d57b83.jpeg)  </details>  <details> <summary> Compact OWL - Hardware Assembly</summary> <br>  One major benefit of the Compact OWL (with the OWL driver board) is that no soldering is required.,OWL,PROJECT
[1T7A9554](https://user-images.githubusercontent.com/40649348/156698342-4b3ebba5-337e-469c-bd0e-c34985d57b83.jpeg)  </details>  <details> <summary> Compact OWL - Hardware Assembly</summary> <br>  One major benefit of the Compact OWL (with the OWL driver board) is that no soldering is required.,OWL,PROJECT
[1T7A9554](https://user-images.githubusercontent.com/40649348/156698342-4b3ebba5-337e-469c-bd0e-c34985d57b83.jpeg)  </details>  <details> <summary> Compact OWL - Hardware Assembly</summary> <br>  One major benefit of the Compact OWL (with the OWL driver board) is that no soldering is required.,OWL,PROJECT
"If you choose a Raspberry Pi Relay HAT instead of the  OWL driver board, you'll just need to add a voltage regulator and solder that in to provide the 5V @ 5A required for  the Raspberry Pi 5 or up to 3A required for the older models.",OWL,PROJECT
The 3D printed enclosure and the official OWL extruded aluminium enclosure.,OWL,PROJECT
The Official OWL Enclosure will be available for purchase through the OWL store soon,OWL,PROJECT
The Official OWL Enclosure will be available for purchase through the OWL store soon,OWL,PROJECT
| Compact OWL - Extruded Aluminium Enclosure                                                                            | Compact OWL - 3D Printed Enclosure                                                                                    | |-----------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------| | !,OWL,PROJECT
| Compact OWL - Extruded Aluminium Enclosure                                                                            | Compact OWL - 3D Printed Enclosure                                                                                    | |-----------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------| | !,OWL,PROJECT
[3D Printed OWL](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/a9771aa2-355d-40db-ac15-f6da037b63ed) | !,OWL,PROJECT
[3D Printed OWL](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/a9771aa2-355d-40db-ac15-f6da037b63ed) | !,OpenWeedLocator,PROJECT
[Extruded OWL](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/d153db2d-624b-427e-832e-599a3a841623) |  >âš ï¸**NOTE**âš ï¸ The 3D printed version requires the additional purchase of: 1. [1.6mm](https://au.rs-online.com/web/p/o-ring-cords/1591478) and [3mm](https://au.rs-online.com/web/p/o-ring-cords/1591490) nitrile rubber o-ring cord 2.,OWL,PROJECT
[Extruded OWL](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/d153db2d-624b-427e-832e-599a3a841623) |  >âš ï¸**NOTE**âš ï¸ The 3D printed version requires the additional purchase of: 1. [1.6mm](https://au.rs-online.com/web/p/o-ring-cords/1591478) and [3mm](https://au.rs-online.com/web/p/o-ring-cords/1591490) nitrile rubber o-ring cord 2.,OpenWeedLocator,PROJECT
[camera_adapter_removal](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/8bd43460-e3de-4978-9eb7-641602024ab9) | !,OpenWeedLocator,PROJECT
[camera_install](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/f3972dbc-c41a-4d86-83a4-ea237f0fa267) |   Mount the camera to the front of the tray using M2.5 standoffs.,OpenWeedLocator,PROJECT
[20240502_125849_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/ba347b54-3e51-402a-a9c6-91eb178ac642) | !,OpenWeedLocator,PROJECT
[20240502_125728_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/7298fc98-96b9-4bf0-8a27-7d74a925298f) | !,OpenWeedLocator,PROJECT
"[20240502_125716_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/730ef3dc-40b8-403a-8045-7b8600478413) |  Once the camera is secured, mount the Raspberry Pi using 4 x M2.5 x 5mm long standoffs.",OpenWeedLocator,PROJECT
[20240502_125635_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/6605aeaf-174b-4120-beeb-ad40ec189f3f) |  #### 3D printed enclosure The 37 mm UV lens filter is installed on the faceplate with 8 M2 heat-set threaded inserts and M2 hex head screws.,OpenWeedLocator,PROJECT
[faceplate_clean_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/43e4d33f-5649-48d7-bdcd-cb4fe766fbe0) | !,OpenWeedLocator,PROJECT
[lens_cover_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/e32c8b5a-5c4f-4b3d-8206-9665de723f81) | !,OpenWeedLocator,PROJECT
[faceplate_lens](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/e355db9b-1bdb-4009-afe2-2a6f7ce35461) |  The 3D printed enclosure also supports the mounting of the Camera Module 3 with an extra 3D printed part.,OpenWeedLocator,PROJECT
[20240507_095647_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/13deaca2-0e49-4296-a52f-d3de9e280630) | !,OpenWeedLocator,PROJECT
[20240507_095748_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/828289af-f351-4be2-90f9-46fa3341ffbb) | !,OpenWeedLocator,PROJECT
[20240507_100004_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/d1852882-f98a-4a28-8431-22f9bcb09bd1) |   ### Step 2 - connecting the Official OWL HAT The OWL Hat simply fits over the GPIO pins and is mounted using the 4 x 15 mm standoffs installed in the previous step.,OpenWeedLocator,PROJECT
[20240507_100004_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/d1852882-f98a-4a28-8431-22f9bcb09bd1) |   ### Step 2 - connecting the Official OWL HAT The OWL Hat simply fits over the GPIO pins and is mounted using the 4 x 15 mm standoffs installed in the previous step.,OWL,PROJECT
[20240507_100004_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/d1852882-f98a-4a28-8431-22f9bcb09bd1) |   ### Step 2 - connecting the Official OWL HAT The OWL Hat simply fits over the GPIO pins and is mounted using the 4 x 15 mm standoffs installed in the previous step.,OWL,PROJECT
| Fitted Official OWL HAT                                                                                                  | OWL Hat jumpers                                                                                                               | GPIO pin assignment                                                                                                           | |--------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------| | !,OWL,PROJECT
| Fitted Official OWL HAT                                                                                                  | OWL Hat jumpers                                                                                                               | GPIO pin assignment                                                                                                           | |--------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------| | !,OWL,PROJECT
[mounted owl hat](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/d080805a-507f-4d9a-a363-25124c52c101) | !,OpenWeedLocator,PROJECT
[20240502_123112_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/ad1e522f-316b-4bc3-90c7-4415e5608205) | !,OpenWeedLocator,PROJECT
[20240502_123121_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/8a567873-0c5e-4c46-ba97-c6dbc0a2d352) |  The default option is for GPIO control as pictured.,OpenWeedLocator,PROJECT
By default the OWL HAT is wired as shown above:  ```ini [Relays] # defines the relay ID (left) that matches to a boardpin (right) on the Pi. # Only change if you rewire/change the relay connections. 0 = 13 1 = 15 2 = 16 3 = 18 ``` The final jumper pin `Pi Power Supply Enable` connects the Raspberry Pi to the 5V provided by the HAT.,OWL,PROJECT
"Only connect this jumper once you want the  Pi to start.  #### Step 2a - connecting a generic relay HAT  Instead of the Official OWL HAT, a relay control HAT (such as [this from PiHut](https://core-electronics.com.au/pirelay-v2-relay-board-for-raspberry-pi-1.html?))",OWL,PROJECT
"can be used, with some minor changes to the OWL software.",OWL,PROJECT
[image](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/bf16cf89-0955-4822-8a8a-c3bd7c295c7f)   |    !,OpenWeedLocator,PROJECT
"[image](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/4c0987a7-936f-4338-ac72-6ee352bc2636)           |  In its default configuration, this specific relay HAT assigns GPIO boardpins 29, 31, 33, and 35 to relays 4 - 1 respectively.",OpenWeedLocator,PROJECT
"This differs to the default OWL configuration, so the `[Relays]` section of the config file would need to be updated.",OWL,PROJECT
"<p align=""center""> <img src=""https://user-images.githubusercontent.com/51358498/152514046-37d5bcf5-348b-4e39-8810-c877acfed852.png"" width=""400""> </p>  #### Step 2b - connecting the voltage regulator Without the OWL HAT, 5V power to Pi needs to be supplied separately.",OWL,PROJECT
"<p align=""center""> <img src=""https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/b939e26f-1785-4d4e-8476-ec5721526724"" width=""400""> </p>  ##### Raspberry Pi 3B+ or 4B The earlier models of the Raspberry Pi consumer less power (3A @ 5V) than the Raspberry Pi 5 and can be powered over single 5V and GND pins on the GPIO.",OpenWeedLocator,PROJECT
"<p align=""center""> <img src=""https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/c8eeacde-0e6c-4de2-8120-a343cdcbc756"" width=""400""> </p>  ##### Raspberry Pi 5 The Raspberry Pi 5 consumes up to 5A @ 5V, so it's suggested to use 2 x 5V and 2 x GND pins on the Raspberry Pi.",OpenWeedLocator,PROJECT
[relay_hat_1_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/dc678435-e59a-44ae-a2d8-4eab43c7797c) | !,OpenWeedLocator,PROJECT
[relay_hat_2_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/a87a34c6-148e-41c7-b092-e429aa5cb3fd) | !,OpenWeedLocator,PROJECT
[relay_hat_3_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/e3bb13fa-26f3-42d6-ac21-3862ccfcd701) |  ### Step 3 - wiring the connector and HAT Begin by wiring the [Amphenol EcoMate Aquarius receptacle](https://au.mouser.com/ProductDetail/Amphenol-SINE-Systems/FLS710N3W3S03?,OpenWeedLocator,PROJECT
"<p align=""center""> <img src=""https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/e6ae15b0-cec5-4295-aa66-dbe5f731cc4b"" height=""300""> </p>  Connections are labeled A - F on the receptacle and should be made in the following order:  1. +12V (red) - A 2.",OpenWeedLocator,PROJECT
"GND (black) - E 3. relay 1 (blue) - B 4. relay 2 (green) - C 5. relay 3 (orange) - D 6. relay 4 (white) - F  Unlike the relay HATs and relay board in the Original OWL, the common ground for the OWL driver board is routed through  the board itself, reducing the wiring required.",OWL,PROJECT
"GND (black) - E 3. relay 1 (blue) - B 4. relay 2 (green) - C 5. relay 3 (orange) - D 6. relay 4 (white) - F  Unlike the relay HATs and relay board in the Original OWL, the common ground for the OWL driver board is routed through  the board itself, reducing the wiring required.",OWL,PROJECT
[20240507_160726_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/e225496d-f08b-4304-8239-bcee196a5524) | !,OpenWeedLocator,PROJECT
[owl_driver_board_install](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/8735a833-f157-456a-a6cd-3dd037ed6e5d) | !,OpenWeedLocator,PROJECT
[finished_tray](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/50ea79c5-ee7c-4e95-9a46-5f57d773de1b) |  >**OPTIONAL** Add a 5V buzzer inside the OWL by mounting it to the corner of the HAT with a screw.,OpenWeedLocator,PROJECT
[finished_tray](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/50ea79c5-ee7c-4e95-9a46-5f57d773de1b) |  >**OPTIONAL** Add a 5V buzzer inside the OWL by mounting it to the corner of the HAT with a screw.,OWL,PROJECT
The buzzer is useful for identifying when the OWL has started successfully.,OWL,PROJECT
"[finished_tray_kapton](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/3c206482-1d8a-4a78-9963-11f1e4401bf7) |   ### Step 4 - inserting the tray and closing the device >âš ï¸**NOTE**âš ï¸ For software installation, you'll need access to the Raspberry Pi display, and USB ports.",OpenWeedLocator,PROJECT
[frontplate_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/512e020e-0482-4542-a3a4-25ea811a988d) | !,OpenWeedLocator,PROJECT
[backplate_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/879c68b3-d379-44b0-8254-3137922bfb5c) |  Fix the faceplate to the enclosure with the 4 x M4 screws.,OpenWeedLocator,PROJECT
"<p align=""center""> <img src=""https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/ddf1d660-feac-4e7c-b4d5-f3a36d464192"" width=""400""> </p>  For the single Amphenol EcoMate Aquarius receptacle, push it through the backplate and tighten down.",OpenWeedLocator,PROJECT
[fron_enclosure](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/c5d8c4d7-21d0-4987-9691-cd9a8615b65a) | !,OpenWeedLocator,PROJECT
"[enclosure_back](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/4ddf0538-265e-4e6b-aebd-040336d1562b) |   There is a choice of three different backplates, depending on your hardware requirements.",OpenWeedLocator,PROJECT
[20240502_122757_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/07ff7316-0af2-413c-b7ff-a94f678fe8e6) | !,OpenWeedLocator,PROJECT
[20240503_123813_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/189d9ad1-7734-429e-9ce5-f1c4fdb74ef5) |  And you're all done!,OpenWeedLocator,PROJECT
Congratulations on building your OWL.,OWL,PROJECT
[Groundharness](https://media.github.sydney.edu.au/user/5402/files/7e440680-d03a-11eb-9af1-67132f4cc36f)  </details>  ### SBC Options  A single board computer or SBC is the brains behind the OWL.,OWL,PROJECT
Each has their strengths and weaknesses and may or may not be good fits with the OWL.,OWL,PROJECT
"Currently, only Raspberry Pi 5, 4 and 3B+ work with the OWL and have been tested in full.",OWL,PROJECT
We will update the 'Works with OWL' column as more boards are tested in the community.,OWL,PROJECT
<details> <summary> A summary of possible single board computers (SBCs) to use with the OWL</summary> <br>  | Name                                                                                                      | CPU                                                            | RAM   | MIPI | USB          | GPU               | Pros                                                                                                         | Cons                                         | Dimensions        | OWL?,OWL,PROJECT
<details> <summary> A summary of possible single board computers (SBCs) to use with the OWL</summary> <br>  | Name                                                                                                      | CPU                                                            | RAM   | MIPI | USB          | GPU               | Pros                                                                                                         | Cons                                         | Dimensions        | OWL?,OWL,PROJECT
"| Image                                                                    | |-----------------------------------------------------------------------------------------------------------|----------------------------------------------------------------|-------|------|--------------|-------------------|--------------------------------------------------------------------------------------------------------------|----------------------------------------------|-------------------|------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------| | [Raspberry Pi 5](https://datasheets.raspberrypi.com/rpi5/raspberry-pi-5-product-brief.pdf)                | Broadcom BCM2712, quad-core 64-bit ARM Cortex-A76              | 1-8GB | 2    | 2x3.0, 2x2.0 | VideoCore VII     | Large community, affordable, PCIe 2.0 lane for externals (including Google Coral and solid state harddrives) | Limited GPU performance, no eMMC storage     | 88 x 58 x 19.5mm  | :heavy_check_mark:                                                                                   | Manual install only (disk image coming soon)                             | | [Raspberry Pi 4B](https://www.raspberrypi.com/products/raspberry-pi-4-model-b/specifications/)            | Broadcom BCM2711, quad-core Cortex-A72                         | 1-8GB | 2    | 2x3.0, 2x2.0 | VideoCore VI      | Large community, affordable                                                                                  | Limited GPU performance, no eMMC storage     | 88 x 58 x 19.5mm  | :heavy_check_mark:                                                                                   | [OWL v1.0.0](https://www.dropbox.com/s/ad6uieyk3awav9k/owl.img.zip?",OWL,PROJECT
"dl=0) | | [Raspberry Pi 3B+](https://www.raspberrypi.com/products/raspberry-pi-3-model-b-plus/)                     | Broadcom BCM2837B0, quad-core Cortex-A53                       | 1GB   | 1    | 4x2.0        | VideoCore IV      | Large community, affordable                                                                                  | Limited GPU performance, no eMMC storage     | 85 x 56 x 17mm    | :heavy_check_mark:                                                                                   | [OWL v1.0.0](https://www.dropbox.com/s/ad6uieyk3awav9k/owl.img.zip?",OWL,PROJECT
"variant=raspberry-pi-cm4001000) | Broadcom BCM2711 quad-core Cortex-A72                          | 1-8GB | 0    | 0            | VideoCore IV      | Integration into custom carrier boards, EMMC                                                                 | Needs carrier board                          | 55 x 40 x 4.7mm   | -                                                                                                    | -                                                                        | | [Libre Computer LePotato](https://libre.computer/products/aml-s905x-cc/)                                  | Amlogic S905X                                                  | 1/2GB | 0    | 4x2.0        | Mali-450 @ 750MHz | Affordable                                                                                                   | Limited community support, no onboard Wi-Fi  | 85 x 56mm         | :warning: alpha ([full report here](https://github.com/geezacoleman/OpenWeedLocator/discussions/70)) | TBA                                                                      | | [Libre Computer Renegade](https://libre.computer/products/roc-rk3328-cc/)                                 | Rockchip RK3328, 4 Core Cortex-A53                             | 1-4GB | 0    | 1x3.0, 2x2.0 | Mali-450 @ 500MHz | 4K HDR support                                                                                               | Limited community support, no onboard Wi-Fi  | 85 x 56mm         | -                                                                                                    | -                                                                        | | [Libre Computer Renegade Elite](https://libre.computer/products/roc-rk3399-pc/)                           | Rockchip RK3399, 2 Core Cortex-A72 + 4 Core Cortex-A53         | 4GB   | 2    | 4x3.0        | 4 Core Mali-T860  | PCIe, highest performance Libre Computer                                                                     | Higher cost compared to other options        | 128 x 64mm        | -                                                                                                    | -                                                                        | | [Rock Pi 4B](https://rockpi.org/rockpi4)                                                                  | Rockchip RK3399, 2 Core Cortex-A72 + 4 Core Cortex-A53         | 4GB   | 1    | 2x2.0 2x3.0  | 4 Core Mali-T860  | PCIe, M.2 slot                                                                                               | Limited community support, no onboard Wi-Fi  | 85 x 54mm         | -                                                                                                    | -                                                                        | | [ODROID-XU4](https://wiki.odroid.com/odroid-xu4/odroid-xu4)                                               | Samsung Exynos5422 ARM Cortex-A15 Quad 2Ghz and Cortex-A7 Octa | 2GB   | 0    | 2x3.0, 1x2.0 | Mali-T628 MP6     | eMMC module support                                                                                          | Higher cost compared to Raspberry Pi options | 83 x 58 x 20mm    | -                                                                                                    | -                                                                        | | [NVIDIA Jetson Nano](https://developer.nvidia.com/embedded/jetson-nano-developer-kit)                     | 4 Core ARM Cortex-A57                                          | 2/4GB | 2    | 4x3.0        | 128-core Maxwell  | Powerful GPU, CSI camera                                                                                     | Higher cost compared to Raspberry Pi options | 100 x 79 x 30.2mm | -                                                                                                    | -                                                                        |  Only the Raspberry Pi 5 is currently capable of operating on larger image sizes.",OpenWeedLocator,PROJECT
Find one of the untested platforms and give the OWL a go!,OWL,PROJECT
"These would likely be good options for the OWL, but are substantially more expensive than the options listed above.",OWL,PROJECT
>âš ï¸**NOTE**âš ï¸ 08/05/2024 - OWL transitioned from `picamera` to `picamera2` support.,OWL,PROJECT
>âš ï¸**NOTE**âš ï¸ 17/03/2023 - running of the OWL changed from using `greenonbrown.py` to `owl.py`.,OWL,PROJECT
"For this method you'll need access to:  * Desktop/laptop computer * Micro SD card reader * Internet with large data capacity and high speed (WARNING: the image file is large, and downloading will take time and   use up a substantial quantity of your data allowance if you have are on a limited plan)  <details> <summary><b>Quick method for software installation</b></summary> <br>  ### Step 1 - download the disk image file  Download the entire disk image file (v1.0.0-owl.img) here: [OWL disk image](https://www.dropbox.com/s/ad6uieyk3awav9k/owl.img.zip?",OWL,PROJECT
[OWL - etcher](https://media.github.sydney.edu.au/user/3859/files/e184ea00-d5a0-11eb-9560-4842758686d0)  * Insert the SD card using your SD card reader. * Select `Flash from file` on the Balena Etcher window and navigate to where you downloaded the vXX-XX-XX-owl.dmg file.,OWL,PROJECT
"So to update the OWL software, just run the follow these steps.  1.",OWL,PROJECT
"Have the OWL powered on with screen, keyboard and mouse connected.",OWL,PROJECT
You should see a desktop with the OWL logo. 2.,OWL,PROJECT
"Once you are in the `owl` environment, enter these commands on each new line:  ``` (owl) owl@raspberrypi:~ $ cd ~ (owl) owl@raspberrypi:~ $ mv owl owl-old      # this renames the old 'owl' folder to 'owl-old' (owl) owl@raspberrypi:~ $ git clone https://github.com/geezacoleman/OpenWeedLocator        # download the new software (owl) owl@raspberrypi:~ $ mv OpenWeedLocator owl      # rename the download to 'owl' (owl) owl@raspberrypi:~ $ cd ~/owl (owl) owl@raspberrypi:~/owl $ pip install -r requirements.txt                # installs the necessary software into the (owl) environment  (owl) owl@raspberrypi:~/owl $ chmod a+x owl.py                  # changes owl.py to be executable (owl) owl@raspberrypi:~/owl $ chmod a+x owl_boot.sh                     # changes owl_boot.sh to be executable ```  Once this is complete your software will be up to date and you can move on to focusing the camera.  ### Step 5 - focusing the camera  The final step in the process is to make sure the camera is correctly focused for the mounting height.",OpenWeedLocator,PROJECT
"Once you are in the `owl` environment, enter these commands on each new line:  ``` (owl) owl@raspberrypi:~ $ cd ~ (owl) owl@raspberrypi:~ $ mv owl owl-old      # this renames the old 'owl' folder to 'owl-old' (owl) owl@raspberrypi:~ $ git clone https://github.com/geezacoleman/OpenWeedLocator        # download the new software (owl) owl@raspberrypi:~ $ mv OpenWeedLocator owl      # rename the download to 'owl' (owl) owl@raspberrypi:~ $ cd ~/owl (owl) owl@raspberrypi:~/owl $ pip install -r requirements.txt                # installs the necessary software into the (owl) environment  (owl) owl@raspberrypi:~/owl $ chmod a+x owl.py                  # changes owl.py to be executable (owl) owl@raspberrypi:~/owl $ chmod a+x owl_boot.sh                     # changes owl_boot.sh to be executable ```  Once this is complete your software will be up to date and you can move on to focusing the camera.  ### Step 5 - focusing the camera  The final step in the process is to make sure the camera is correctly focused for the mounting height.",OpenWeedLocator,PROJECT
"If you would like to focus the OWL again, you can always run `.",OWL,PROJECT
[blurry owl](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/34ae71f2-8507-4892-b49a-195e515e56dd) | !,OpenWeedLocator,PROJECT
"[clear owl](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/20db536b-edaf-4085-a613-6ea786747998) |  #### Manual focusing  With the older versions of the software, you need to stop all `owl.py` or `greenonbrown.py` background processes before you can restart the software with the video feed viewable on the screen.",OpenWeedLocator,PROJECT
"In this case it is `515`, but it is likely to be different on your OWL.",OWL,PROJECT
/owl.py --show-display ```  This will bring up a video feed you can use to visualise the OWL detector and also use it to focus the camera.,OWL,PROJECT
"Once you're happy with the focus, press Esc to exit.  ### OPTIONAL Step 6 - enabling UART for status LED  This is just the cherry on top and non-essential to correct operation of the OWL but to make sure the status LED you connected earlier blinks correctly the GPIO UART needs to be enabled.",OWL,PROJECT
"### OPTIONAL Step 7 - running original `greenonbrown.py`  If you are using the v1.0.0-owl.img file and don't update the OWL software as above, you will be using the original `greenonbrown.py` Python script.",OWL,PROJECT
If a screen is not attached but `headless=False` the Raspberry Pi will not boot correctly and the OWL software will not run.,OWL,PROJECT
"</details>    ## Detailed Method **IMPORTANT**: *Suitable for the Raspberry Pi 5 and Bookworm Raspberry Pi OS.*  This setup approach may take a little longer (aproximately 1 hour total) than the quick method, but you'll be much better trained in the ways of OWL and more prepared for any problem solving, upgrades or changes in the future.",OWL,PROJECT
"To get this working you'll need access to:  * Raspberry Pi * Empty SD Card (SanDisk 32GB SDXC ideally) * Your own computer with SD card reader * Power supply (if not using the OWL unit) * Screen and keyboard * WiFi/Ethernet cable  <details> <summary><b>Detailed OWL installation procedure</b></summary> <br>  ### Step 1 - Raspberry Pi setup  #### Step 1a - Rasperry Pi OS Before powering up the Raspberry Pi, you'll need to install the Raspian operating system (just like Windows/MacOSX for laptops) on the new SD card.",OWL,PROJECT
"To get this working you'll need access to:  * Raspberry Pi * Empty SD Card (SanDisk 32GB SDXC ideally) * Your own computer with SD card reader * Power supply (if not using the OWL unit) * Screen and keyboard * WiFi/Ethernet cable  <details> <summary><b>Detailed OWL installation procedure</b></summary> <br>  ### Step 1 - Raspberry Pi setup  #### Step 1a - Rasperry Pi OS Before powering up the Raspberry Pi, you'll need to install the Raspian operating system (just like Windows/MacOSX for laptops) on the new SD card.",OWL,PROJECT
| Raspberry Pi Imager                                                                                                    | Configuring the OWL                                                                                             | |-----------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------| | !,OWL,PROJECT
[Raspberry Pi Imager](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/a86a6358-3a8c-4f40-94df-9eeba9c17e4d) | !,OpenWeedLocator,PROJECT
"[Imager](https://github.com/user-attachments/assets/10a74429-fc12-49a6-a9df-cc67a55dab0c) |  #### Step 1b - Setting up the OWL Once the Raspian OS has been flashed to the SD Card (may take 5 - 10 minutes), remove the SD card and insert it into the Raspberry Pi.",OWL,PROJECT
"Alternatively, you can SSH into your OWL from a separate device and install it remotely.",OWL,PROJECT
"To start, clone the Github repository: ```commandline git clone https://github.com/geezacoleman/OpenWeedLocator owl ``` With the repository cloned into the 'owl' folder, we can now run the bash script `owl_setup.sh`.",OpenWeedLocator,PROJECT
"This will take some time to complete  ```commandline bash owl/owl_setup.sh ``` Once completed successfully, your OWL is ready to go.",OWL,PROJECT
"We recommend the step-by-step approach if you want to become more familiar > with how the OWL works.  #### Step-by-step install Instead of the two-line installation, the following procedure details all steps required.  ##### Free up space The Raspberry Pi comes pre-installed with a range of software.",OWL,PROJECT
To free up space it can be removed from the OWL.,OWL,PROJECT
We're installing packages specific to the OWL.  ### Step 2 - Installing packages We now need to install the Python libraries that let the OWL work.,OWL,PROJECT
We're installing packages specific to the OWL.  ### Step 2 - Installing packages We now need to install the Python libraries that let the OWL work.,OWL,PROJECT
"The most import is OpenCV, which we'll do first before downloading the OWL repository and installing the remainder from the `requirements.txt` file.",OWL,PROJECT
"If that was successful, you can  now move on to Step 4.  ### Step 4 - Installing the OWL Python dependencies Dependencies are Python packages on which the code relies to function correctly.",OWL,PROJECT
"If you would like to focus the OWL again, you can always run `.",OWL,PROJECT
[blurry owl](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/34ae71f2-8507-4892-b49a-195e515e56dd) | !,OpenWeedLocator,PROJECT
"[clear owl](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/20db536b-edaf-4085-a613-6ea786747998) |  #### Manual focusing  With the older versions of the software, you need to stop all `owl.py` or `greenonbrown.py` background processes before you can restart the software with the video feed viewable on the screen.",OpenWeedLocator,PROJECT
"In this case it is `515`, but it is likely to be different on your OWL.",OWL,PROJECT
"If the relays start clicking (the Official OWL  HAT uses transistors and will not click - look for the lights) and lights come on, congratulations, you've successfully  set the OWL up!",OWL,PROJECT
Minimum detection size has been increased. | | DAY_SENSITIVITY_2.ini | OWL detection parameters were tuned to this file,OWL,PROJECT
OWL now supports the use of flags for some parameters.,OWL,PROJECT
"```ini [System] # select your algorithm algorithm = exhsv # operate on a video, image or directory of media input_file_or_directory = # choose how many relays are connected to the OWL relay_num = 4 actuation_duration = 0.15 delay = 0  [Controller] # choose between 'None', 'ute' or 'advanced' controller_type = None  # for advanced controller detection_mode_pin_up = 35 detection_mode_pin_down = 36 recording_pin = 38 sensitivity_pin = 40 low_sensitivity_config = config/DAY_SENSITIVITY_2.ini high_sensitivity_config = config/DAY_SENSITIVITY_3.ini  # for UteController switch_purpose = recording switch_pin = 37  [Visualisation] image_loop_time = 5  [Camera] resolution_width = 640 resolution_height = 480 exp_compensation = -2  [GreenOnGreen] # parameters related to green-on-green detection model_path = models confidence = 0.5 class_filter_id = None  [GreenOnBrown] # parameters related to green-on-brown detection exg_min = 25 exg_max = 200 hue_min = 39 hue_max = 83 saturation_min = 50 saturation_max = 220 brightness_min = 60 brightness_max = 190 min_detection_area = 10 invert_hue = False  [DataCollection] # all data collection related parameters # set sample_images True/False to enable/disable image collection sample_images = False # image collection, sample method include: 'bbox' | 'square' | 'whole' sample_method = whole sample_frequency = 30 save_directory = /media/owl/SanDisk # set to True to disable weed detection for data collection only disable_detection = False # log fps log_fps = False camera_name = cam1  [Relays] # defines the relay ID (left) that matches to a boardpin (right) on the Pi. # Only change if you rewire/change the relay connections. 0 = 13 1 = 15 2 = 16 3 = 18 ```  ### Parameter definitions |       **Parameter**       |                      **Options**                       |                                                                                                       **Description**                                                                                                       | |:-------------------------:|:------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:| |        **System**         |                                                        |                                                                                                                                                                                                                             | |        `algorithm`        | Any of: `gog`,`exg`,`exgr`,`exgs`,`exhu`,`hsv`,`exhsv` |                                        Changes the selected algorithm.",OWL,PROJECT
[image](https://github.com/user-attachments/assets/d8ef7d9a-3832-46e0-b59e-0499ae439311)  | [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Ute%20Controller%20-%20Base.stl)                                                                                                                                                                                                                                                                                                                                      | | Ute Controller Top       |  !,OpenWeedLocator,PROJECT
[image](https://github.com/user-attachments/assets/2fe56f86-46ed-47ac-9765-461fa061802a) | [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Ute%20Controller%20-%20Top.stl)                                                                                                                                                                                                                                                                                                                                                        | | Advanced Controller Base | !,OpenWeedLocator,PROJECT
[image](https://github.com/user-attachments/assets/600ed3c3-28f4-4dd0-a64d-c67924f8aa41)  | Single: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Base%20-%20Single%20OWL.stl)<br/>Double: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Base%20-%20Double%20OWL.stl)<br/>4 OWL: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Base%20-%204%20OWL.stl) | | Advanced Controller Top  | !,OpenWeedLocator,PROJECT
[image](https://github.com/user-attachments/assets/600ed3c3-28f4-4dd0-a64d-c67924f8aa41)  | Single: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Base%20-%20Single%20OWL.stl)<br/>Double: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Base%20-%20Double%20OWL.stl)<br/>4 OWL: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Base%20-%204%20OWL.stl) | | Advanced Controller Top  | !,OpenWeedLocator,PROJECT
[image](https://github.com/user-attachments/assets/600ed3c3-28f4-4dd0-a64d-c67924f8aa41)  | Single: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Base%20-%20Single%20OWL.stl)<br/>Double: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Base%20-%20Double%20OWL.stl)<br/>4 OWL: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Base%20-%204%20OWL.stl) | | Advanced Controller Top  | !,OWL,PROJECT
[image](https://github.com/user-attachments/assets/600ed3c3-28f4-4dd0-a64d-c67924f8aa41)  | Single: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Base%20-%20Single%20OWL.stl)<br/>Double: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Base%20-%20Double%20OWL.stl)<br/>4 OWL: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Base%20-%204%20OWL.stl) | | Advanced Controller Top  | !,OpenWeedLocator,PROJECT
[image](https://github.com/user-attachments/assets/56b64d4f-27a8-40ee-9052-79cc86fe0e90)  | Single: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Top%20-%20Single%20OWL.stl)<br/>Double: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Top%20-%20Double%20OWL.stl)<br/>4 OWL: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Top%20-%204%20OWL.stl)    |   ### Step 3 - Make connections Connections to the switches can be either soldered and covered with heat shrink or connected with automotive-style blade fittings (e.g.,OpenWeedLocator,PROJECT
[image](https://github.com/user-attachments/assets/56b64d4f-27a8-40ee-9052-79cc86fe0e90)  | Single: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Top%20-%20Single%20OWL.stl)<br/>Double: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Top%20-%20Double%20OWL.stl)<br/>4 OWL: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Top%20-%204%20OWL.stl)    |   ### Step 3 - Make connections Connections to the switches can be either soldered and covered with heat shrink or connected with automotive-style blade fittings (e.g.,OpenWeedLocator,PROJECT
[image](https://github.com/user-attachments/assets/56b64d4f-27a8-40ee-9052-79cc86fe0e90)  | Single: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Top%20-%20Single%20OWL.stl)<br/>Double: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Top%20-%20Double%20OWL.stl)<br/>4 OWL: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Top%20-%204%20OWL.stl)    |   ### Step 3 - Make connections Connections to the switches can be either soldered and covered with heat shrink or connected with automotive-style blade fittings (e.g.,OWL,PROJECT
[image](https://github.com/user-attachments/assets/56b64d4f-27a8-40ee-9052-79cc86fe0e90)  | Single: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Top%20-%20Single%20OWL.stl)<br/>Double: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Top%20-%20Double%20OWL.stl)<br/>4 OWL: [Link](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Controllers/Advanced%20Controller%20-%20Top%20-%204%20OWL.stl)    |   ### Step 3 - Make connections Connections to the switches can be either soldered and covered with heat shrink or connected with automotive-style blade fittings (e.g.,OpenWeedLocator,PROJECT
On the other leg of the switch solder a 10A rated wire (for the OWL) and a smaller gauge wire for the 12V LED pin.,OWL,PROJECT
"This method has been successfully tested on PyCharm with Anaconda environments.  ``` > git clone https://github.com/geezacoleman/OpenWeedLocator > cd OpenWeedLocator ```  For the next part, make sure you are in the virtual environment you will be working from.",OpenWeedLocator,PROJECT
[FreeCodeCamp](https://www.freecodecamp.org/news/how-to-setup-virtual-environments-in-python/) has a great blog describing them too.,FreeCodeCamp,PROJECT
[OWL - workflow](https://user-images.githubusercontent.com/51358498/152990264-ddce7eb4-0e2e-4f98-ac77-bc2e535c5c54.png)  ## Results  The performance of each algorithm on 7 different day/night fields is outlined below.,OWL,PROJECT
[detection results resized](https://user-images.githubusercontent.com/51358498/152989906-bcc47ad5-360a-414c-8e25-d9b99875f361.png)  </details>  # 3D Printing  <details> <summary>3D printing instructions and files</summary> <br>  ## Original OWL There are seven total items that need printing for the Original OWL unit.,OWL,PROJECT
[detection results resized](https://user-images.githubusercontent.com/51358498/152989906-bcc47ad5-360a-414c-8e25-d9b99875f361.png)  </details>  # 3D Printing  <details> <summary>3D printing instructions and files</summary> <br>  ## Original OWL There are seven total items that need printing for the Original OWL unit.,OWL,PROJECT
There are two options for Original OWL base:  1.,OWL,PROJECT
[screenshot](https://user-images.githubusercontent.com/51358498/166176068-989cc69b-43c1-48ef-942d-b273fc2f4d98.png)](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Original%20OWL/Enclosure%20-%20single%20connector.stl) | |                      OPTIONAL: OWL base with cable glands instead of single Bulgin connector,OpenWeedLocator,PROJECT
"[screenshot](https://user-images.githubusercontent.com/51358498/166175980-e1fcc526-c835-4ea1-88b5-d28a1ab747b7.png)](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Original%20OWL/Enclosure%20-%20cable%20gland.stl)    | |       OWL cover, slides over the base and is fitted with 4 x M3 bolts/nuts.",OpenWeedLocator,PROJECT
"[OWL Cover](https://user-images.githubusercontent.com/51358498/132754464-8bfe62aa-4487-42ea-a507-71e0b4a4d1a2.png)](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Original%20OWL/Tall%20enclosure%20cover.stl)       | |                            OWL base port cover, covers the cable port on the rear panel",OpenWeedLocator,PROJECT
"[OWL base port cover](https://media.github.sydney.edu.au/user/3859/files/12b7f000-cb87-11eb-980b-564e7b4324f6)](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Original%20OWL/Tall%20enclosure%20plug.stl)         | |                   Raspberry Pi mount, fixes to the Raspberry Pi for easy attachment to OWL base",OpenWeedLocator,PROJECT
"[Raspberry Pi mount](https://media.github.sydney.edu.au/user/3859/files/5d396c80-cb87-11eb-948c-d60efe433ac8)](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Original%20OWL/Raspberry%20Pi%20mount.stl)          | |             Raspberry Pi Camera mount, fixes to the HQ or V2 Camera for simple attachment to the base",OpenWeedLocator,PROJECT
"[Screenshot 2022-03-04 180036](https://user-images.githubusercontent.com/40649348/156715282-bea91301-ac6d-4421-b071-4a4304eb02b0.png)](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Original%20OWL/Camera%20mount.stl)  | |                   Relay board mount, fixes to the relay board for simple attachment to the base",OpenWeedLocator,PROJECT
"[Relay board mount](https://media.github.sydney.edu.au/user/5402/files/d421aa00-d04c-11eb-9191-bcad7b51c1a4)](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Original%20OWL/Relay%20control%20board%20mount.stl)      | | Voltage regulator mount, fixes to the voltage regulator and onto the relay board for simple attachment to the base. |     [!",OpenWeedLocator,PROJECT
"[Voltage regulator mount](https://media.github.sydney.edu.au/user/5402/files/8147f280-d04c-11eb-89ec-4af125a8f232)](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Original%20OWL/Voltage%20regulator%20mount.stl)     |  Ideally supports should be used for the base, and were tested at 0.2mm layer heights with 25% infill on a Prusa MK3S.",OpenWeedLocator,PROJECT
[image](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/dc5d4a38-3a76-42b6-bb2b-96e6cae29a8e)]()                                                                  | |        Frontplate: covers the front of the enclosure.,OpenWeedLocator,PROJECT
[image](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/54113656-8688-4c58-a6b6-dfd3b35736b9)]()                                                                                                                         | |                       Lens mount: Securely mounts the 37 mm UV lens filter to the frontplate,OpenWeedLocator,PROJECT
[image](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/b5c9e4a7-5bd9-4c22-8d42-8134f0f96f6f)]()                                                                                                                         | |                                 Backplate: 1 x Amphenol EcoMate Aquarius Receptacle                                 |                                                                 [!,OpenWeedLocator,PROJECT
"[image](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/a80eb501-f681-4298-a6e1-2fb1595a859a)]()                                                                  | |                Backplate: 1 x Amphenol EcoMate Aquarius Receptacle, 1 x Adafruit Ethernet Connector                 |                                                                 [!",OpenWeedLocator,PROJECT
[image](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/4424e47e-f92c-4db2-b20f-26daec9babe0)]()                                                                  | |                Backplate: blank                 |                                                                 [!,OpenWeedLocator,PROJECT
[image](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/f724822e-be85-47ee-9ab8-968efd161124)]()                                                                  | |                                           Backplate: 1 x 16mm Cable Gland                                           |                                                                 [!,OpenWeedLocator,PROJECT
[image](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/1f318499-fe28-4b97-9e7f-57c53fac8173)]()                                                                  | |               Tray: Mounts all required hardware and fits into the enclosure body on the second rail.,OpenWeedLocator,PROJECT
[image](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/83c7d31d-1b93-4d04-8cff-349998394b2a)]()                                                                  | |               Lens holder: Secures the lens with two M2 x 6mm screws,OpenWeedLocator,PROJECT
[image](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/38baf7e4-b05a-48e7-958e-9e8374bc8990)]()                                                                  |  All .stl files for the 3D printed components of this build are available in the 3D Models directory.,OpenWeedLocator,PROJECT
"Follow the instructions presented here, to use software already installed on the Pi.  ## Version History  All versions of OWL can be found here.",OWL,PROJECT
dl=0) |  Buster (picamera)   | | v2.0.0-owl.img  |                              [Download]()                              | Bookworm (picamera2) |  </details>  # Troubleshooting  <details> <summary>Troubleshooting OWL issues</summary> <br>  ## Software  The table below includes a summary of all current error classes and their hierarchy.,OWL,PROJECT
"Please consider citing the published article using the details below.  ``` @article{Coleman2022, author = {Coleman, Guy and Salter, William and Walsh, Michael}, doi = {10.1038/s41598-021-03858-9}, issn = {2045-2322}, journal = {Scientific Reports}, number = {1}, pages = {170}, title = {{OpenWeedLocator (OWL): an open-source, low-cost device for fallow weed detection}}, url = {https://doi.org/10.1038/s41598-021-03858-9}, volume = {12}, year = {2022} }  ```  </details>  # Acknowledgements  <details> <summary>Acknowledgements</summary> <br>  This project has been developed by Guy Coleman and William Salter at the University of Sydney, Precision Weed Control Lab.",OpenWeedLocator,PROJECT
"Assembly and use of OWL is entirely at your own risk and the license expressly states there is no warranty.  ``` MIT License  Copyright (c) 2020 Guy Coleman  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",OWL,PROJECT
repos=geezacoleman/OpenWeedLocator&type=Timeline)](https://star-history.com/#geezacoleman/OpenWeedLocator&Timeline),OpenWeedLocator,PROJECT
repos=geezacoleman/OpenWeedLocator&type=Timeline)](https://star-history.com/#geezacoleman/OpenWeedLocator&Timeline),OpenWeedLocator,PROJECT
# [PAV-SOD: A New Task Towards Panoramic Audiovisual Saliency Detection (TOMM 2022)](https://drive.google.com/file/d/1-1RcARcbz4pACFzkjXcp6MP8R9CGScqI/view?,PAV-SOD,PROJECT
"To this end, we propose a new task, panoramic audiovisual salient object detection (PAV-SOD), which aims to segment the objects grasping most of the human attention in 360Â° panoramic videos reflecting real-life daily scenes.",panoramic audiovisual salient object detection,PROJECT
"To this end, we propose a new task, panoramic audiovisual salient object detection (PAV-SOD), which aims to segment the objects grasping most of the human attention in 360Â° panoramic videos reflecting real-life daily scenes.",PAV-SOD,PROJECT
"To support the task, we collect PAVS10K, the first panoramic video dataset for audiovisual salient object detection, which consists of 67 4K-resolution equirectangular videos with per-video labels including hierarchical scene categories and associated attributes depicting specific challenges for conducting PAV-SOD, and 10,465 uniformly sampled video frames with manually annotated object-level and instance-level pixel-wise masks.",PAV-SOD,PROJECT
"With extensive experimental results, we gain several findings about PAV-SOD challenges and insights towards PAV-SOD model interpretability.",PAV-SOD,PROJECT
"</em> </p>  ------  # Benchmark Models  **No.** | **Year** | **Pub.** | **Title** | **Links**  :-: | :-:| :-: | :-  | :-:  01 | **2019**| **CVPR** | Cascaded Partial Decoder for Fast and Accurate Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Cascaded_Partial_Decoder_for_Fast_and_Accurate_Salient_Object_Detection_CVPR_2019_paper.pdf)/[Code](https://github.com/wuzhe71/CPD)  02 | **2019**| **CVPR** | See More, Know More: Unsupervised Video Object Segmentation with Co-Attention Siamese Networks | [Paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Lu_See_More_Know_More_Unsupervised_Video_Object_Segmentation_With_Co-Attention_CVPR_2019_paper.pdf)/[Code](https://github.com/carrierlxk/COSNet)  03 | **2019**| **ICCV** | Stacked Cross Refinement Network for Edge-Aware Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Stacked_Cross_Refinement_Network_for_Edge-Aware_Salient_Object_Detection_ICCV_2019_paper.pdf)/[Code](https://github.com/wuzhe71/SCRN) 04 | **2019**| **ICCV** | Semi-Supervised Video Salient Object Detection Using Pseudo-Labels | [Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Yan_Semi-Supervised_Video_Salient_Object_Detection_Using_Pseudo-Labels_ICCV_2019_paper.pdf)/[Code](https://github.com/Kinpzz/RCRNet-Pytorch) 05 | **2020**| **AAAI** | FÂ³Net: Fusion, Feedback and Focus for Salient Object Detection | [Paper](https://ojs.aaai.org/index.php/AAAI/article/download/6916/6770)/[Code](https://github.com/weijun88/F3Net) 06 | **2020**| **AAAI** | Pyramid Constrained Self-Attention Network for Fast Video Salient Object Detection | [Paper](https://ojs.aaai.org/index.php/AAAI/article/view/6718/6572)/[Code](https://github.com/guyuchao/PyramidCSA) 07 | **2020**| **CVPR** | Multi-scale Interactive Network for Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Pang_Multi-Scale_Interactive_Network_for_Salient_Object_Detection_CVPR_2020_paper.pdf)/[Code](https://github.com/lartpang/MINet) 08 | **2020**| **CVPR** | Label Decoupling Framework for Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Label_Decoupling_Framework_for_Salient_Object_Detection_CVPR_2020_paper.pdf)/[Code](https://github.com/weijun88/LDF) 09 | **2020**| **ECCV** | Highly Efficient Salient Object Detection with 100K Parameters | [Paper](http://mftp.mmcheng.net/Papers/20EccvSal100k.pdf)/[Code](https://github.com/ShangHua-Gao/SOD100K) 10 | **2020**| **ECCV** | Suppress and Balance: A Simple Gated Network for Salient Object Detection | [Paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123470035.pdf)/[Code](https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency) 11 | **2020**| **BMVC** | Making a Case for 3D Convolutions for Object Segmentation in Videos | [Paper](https://www.bmvc2020-conference.com/assets/papers/0233.pdf)/[Code](https://github.com/sabarim/3DC-Seg) 12 | **2020**| **SPL** | FANet: Features Adaptation Network for 360Â° Omnidirectional Salient Object Detection | [Paper](https://ieeexplore.ieee.org/document/9211754)/[Code](https://github.com/DreaMKHuang/FANet) 13 | **2021**| **CVPR** | Reciprocal Transformations for Unsupervised Video Object Segmentation | [Paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Ren_Reciprocal_Transformations_for_Unsupervised_Video_Object_Segmentation_CVPR_2021_paper.pdf)/[Code](https://github.com/OliverRensu/RTNet)  ------  # CAV-Net  The codes are available at [src](https://github.com/PanoAsh/PAV-SOD/tree/main/src).",PAV-SOD,PROJECT
"Only researchers and educators who wish to use the videos for non-commercial researches and/or educational purposes, have access to PAVS10K.  ------  # Citation          @article{zhang2023pav,       title={PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection},       author={Zhang, Yi and Chao, Fang-Yi and Hamidouche, Wassim and Deforges, Olivier},       journal={ACM Transactions on Multimedia Computing, Communications and Applications},       volume={19},       number={3},       pages={1--26},       year={2023},       publisher={ACM New York, NY}     }  ------  # Contact  yi23zhang.2022@gmail.com or  fangyichao428@gmail.com (for details of head movement and eye fixation data).",PAV-SOD,PROJECT
# Awesome Active Learning for Medical Image Analysis  [!,Awesome Active Learning for Medical Image Analysis,PROJECT
url=https://paperswithcode.com/badge/dc-shadownet-single-image-hard-and-soft-1/shadow-removal-on-srd)](https://paperswithcode.com/sota/shadow-removal-on-srd?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/dc-shadownet-single-image-hard-and-soft-1/shadow-removal-on-srd)](https://paperswithcode.com/sota/shadow-removal-on-srd?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/dc-shadownet-single-image-hard-and-soft-1/shadow-removal-on-istd)](https://paperswithcode.com/sota/shadow-removal-on-istd?,paperswithcode,PROJECT
url=https://paperswithcode.com/badge/dc-shadownet-single-image-hard-and-soft-1/shadow-removal-on-istd)](https://paperswithcode.com/sota/shadow-removal-on-istd?,paperswithcode,PROJECT
[python](https://img.shields.io/pypi/pyversions/pfhedge.svg)](https://pypi.org/project/pfhedge) [!,pfhedge,PROJECT
[pypi](https://img.shields.io/pypi/v/pfhedge.svg)](https://pypi.org/project/pfhedge) [!,pfhedge,PROJECT
[CI](https://github.com/pfnet-research/pfhedge/workflows/CI/badge.svg)](https://github.com/pfnet-research/pfhedge/actions?,pfhedge,PROJECT
[CI](https://github.com/pfnet-research/pfhedge/workflows/CI/badge.svg)](https://github.com/pfnet-research/pfhedge/actions?,pfhedge,PROJECT
[downloads](https://img.shields.io/pypi/dm/pfhedge)](https://pypi.org/project/pfhedge) [!,pfhedge,PROJECT
[downloads](https://img.shields.io/pypi/dm/pfhedge)](https://pypi.org/project/pfhedge) [!,pfhedge,PROJECT
"[Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)][example-readme-colab]  ### Prepare a Derivative to Hedge  Financial instruments are provided in [`pfhedge.instruments`](https://pfnet-research.github.io/pfhedge/instruments.html) and classified into two types:  * **`Primary` instruments**: A primary instrument is a basic financial instrument that is traded on a market, and therefore their prices are accessible as the market prices.",pfhedge,PROJECT
"Examples include [`EuropeanOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.EuropeanOption.html), [`LookbackOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.LookbackOption.html), [`VarianceSwap`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.VarianceSwap.html), and so forth.",pfhedge,PROJECT
"Examples include [`EuropeanOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.EuropeanOption.html), [`LookbackOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.LookbackOption.html), [`VarianceSwap`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.VarianceSwap.html), and so forth.",pfhedge,PROJECT
"Examples include [`EuropeanOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.EuropeanOption.html), [`LookbackOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.LookbackOption.html), [`VarianceSwap`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.VarianceSwap.html), and so forth.",pfhedge,PROJECT
"We consider a [`BrownianStock`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.BrownianStock.html), which is a stock following the [geometric Brownian motion](https://en.wikipedia.org/wiki/Geometric_Brownian_motion), and a [`EuropeanOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.EuropeanOption.html) which is contingent on it.",pfhedge,PROJECT
"We consider a [`BrownianStock`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.BrownianStock.html), which is a stock following the [geometric Brownian motion](https://en.wikipedia.org/wiki/Geometric_Brownian_motion), and a [`EuropeanOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.EuropeanOption.html) which is contingent on it.",pfhedge,PROJECT
"```py from pfhedge.instruments import BrownianStock from pfhedge.instruments import EuropeanOption  stock = BrownianStock(cost=1e-4) derivative = EuropeanOption(stock)  derivative # EuropeanOption( #   strike=1., maturity=0.0800 #   (underlier): BrownianStock(sigma=0.2000, cost=1.0000e-04, dt=0.0040) # ) ```  ### Create Your Hedger  A [`Hedger`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.nn.Hedger.html) in Deep Hedging is basically characterized by three elements:  * **Inputs**: A hedger uses any market information as input features",pfhedge,PROJECT
- [`MultiLayerPerceptron`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.nn.MultiLayerPerceptron.html): [Multi-layer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron),pfhedge,PROJECT
- [`BlackScholes`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.nn.BlackScholes.html): [Black-Scholes](https://en.wikipedia.org/wiki/Delta_neutral)' delta-hedging strategy,pfhedge,PROJECT
- [`WhalleyWilmott`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.nn.WhalleyWilmott.html): [Whalley-Wilmott][whalley-wilmott]'s asymptotically optimal strategy for small costs,pfhedge,PROJECT
"- [`EntropicRiskMeasure`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.nn.EntropicRiskMeasure.html): [Entropic Risk Measure](https://en.wikipedia.org/wiki/Entropic_risk_measure), a risk measure derived from [exponential utility](https://en.wikipedia.org/wiki/Exponential_utility)",pfhedge,PROJECT
"- [`ExpectedShortFall`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.nn.ExpectedShortfall.html): [Expected Shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) or CVaR, a common measure to assess portfolio risk.",pfhedge,PROJECT
"__init__()          self.delta = BlackScholes(derivative)         self.mlp = MultiLayerPerceptron(out_features=2)         self.clamp = Clamp()      def inputs(self):         return self.delta.inputs() + [""prev_hedge""]      def forward(self, input: Tensor) -> Tensor:         prev_hedge = input[..., [-1]]          delta = self.delta(input[..., :-1])         width = self.mlp(input[..., :-1])          min = delta - fn.leaky_relu(width[..., [0]])         max = delta + fn.leaky_relu(width[..., [1]])          return self.clamp(prev_hedge, min=min, max=max)   model = NoTransactionBandNet(derivative) hedger = Hedger(model, inputs=model.inputs()) ```  ### Autogreek  A module [`pfhedge.autogreek`](https://pfnet-research.github.io/pfhedge/autogreek.html) provides functions implementing automatic evaluation of greeks using automatic differentiation.",pfhedge,PROJECT
abstract_id=3355706 [ntb-network-arxiv]: https://arxiv.org/abs/2103.01775 [whalley-wilmott]: https://doi.org/10.1111/1467-9965.00034 [NoTransactionBandNetwork]: https://github.com/pfnet-research/NoTransactionBandNetwork [example-readme-colab]: https://colab.research.google.com/github/pfnet-research/pfhedge/blob/main/examples/example_readme.ipynb,pfhedge,PROJECT
Data on thousands of AI benchmark results have been imported from [Papers With Code](https://paperswithcode.com/) and are further curated.,Papers With Code,PROJECT
Data on thousands of AI benchmark results have been imported from [Papers With Code](https://paperswithcode.com/) and are further curated.,paperswithcode,PROJECT
ITO includes data from the Papers With Code project (https://paperswithcode.com/).,Papers With Code,PROJECT
ITO includes data from the Papers With Code project (https://paperswithcode.com/).,paperswithcode,PROJECT
Papers With Code is licensed under the CC-BY-SA license.,Papers With Code,PROJECT
Data from Papers With Code are partially altered (manual curation to improve ontological structure and data quality).,Papers With Code,PROJECT
"## Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set ##  <p align=""center"">  <img src=""/images/example.gif""> </p>  ### **_\*\*\*07/20/2021: A [PyTorch implementation](https://github.com/sicxu/Deep3DFaceRecon_pytorch) which has much better performance and is much easier to use is available now.",Deep3DFaceRecon,PROJECT
(https://github.com/Juyong/3DFace).,3DFace,PROJECT
Clone the repository ``` git clone https://github.com/Microsoft/Deep3DFaceReconstruction --recursive cd Deep3DFaceReconstruction ```  #### 2.,Deep3DFaceReconstruction,PROJECT
Clone the repository ``` git clone https://github.com/Microsoft/Deep3DFaceReconstruction --recursive cd Deep3DFaceReconstruction ```  #### 2.,Deep3DFaceReconstruction,PROJECT
"(https://github.com/Juyong/3DFace) You can find a link named ""CoarseData"" in the first row of Introduction part in their repository.",3DFace,PROJECT
"# EvPSNet: Evidential Panoptic Segmentation Network for Uncertainy-aware Panoptic Segmentation  EvPSNet is the first approach to tackle the task of uncertainty-aware panoptic segmentation, with an aim to provide per-pixel semantic and instance label together with per-pixel panoptic uncertainty estimation.  !",EvPSNet,PROJECT
"# EvPSNet: Evidential Panoptic Segmentation Network for Uncertainy-aware Panoptic Segmentation  EvPSNet is the first approach to tackle the task of uncertainty-aware panoptic segmentation, with an aim to provide per-pixel semantic and instance label together with per-pixel panoptic uncertainty estimation.  !",EvPSNet,PROJECT
[Illustration of EvPSNet](.,EvPSNet,PROJECT
"The repository builds on [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS), [mmdetection](https://github.com/open-mmlab/mmdetection) and [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) codebases.",EfficientPS,PROJECT
"The repository builds on [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS), [mmdetection](https://github.com/open-mmlab/mmdetection) and [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) codebases.",EfficientPS,PROJECT
"The repository builds on [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS), [mmdetection](https://github.com/open-mmlab/mmdetection) and [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) codebases.",mmdetection,PROJECT
"The repository builds on [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS), [mmdetection](https://github.com/open-mmlab/mmdetection) and [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) codebases.",mmdetection,PROJECT
```shell git clone https://github.com/kshitij3112/EvPSNet.git cd EvPSNet conda env create -n EvPSnet_env --file=environment.yml conda activate EvPSnet_env ``` b.,EvPSNet,PROJECT
```shell git clone https://github.com/kshitij3112/EvPSNet.git cd EvPSNet conda env create -n EvPSnet_env --file=environment.yml conda activate EvPSnet_env ``` b.,EvPSNet,PROJECT
```shell git clone https://github.com/kshitij3112/EvPSNet.git cd EvPSNet conda env create -n EvPSnet_env --file=environment.yml conda activate EvPSnet_env ``` b.,EvPSnet,PROJECT
```shell git clone https://github.com/kshitij3112/EvPSNet.git cd EvPSNet conda env create -n EvPSnet_env --file=environment.yml conda activate EvPSnet_env ``` b.,EvPSnet,PROJECT
Install EfficientNet implementation ```bash cd efficientNet python setup.py develop ``` d.,EfficientNet,PROJECT
Install EfficientNet implementation ```bash cd efficientNet python setup.py develop ``` d.,efficientNet,PROJECT
Install EvPSNet implementation ```bash cd .. python setup.py develop ``` ## Prepare datasets It is recommended to symlink the dataset root to `$EvPSNet/data`.,EvPSNet,PROJECT
"If your folder structure is different, you may need to change the corresponding paths in config files.  ``` EvPSNet â”œâ”€â”€ mmdet â”œâ”€â”€ tools â”œâ”€â”€ configs â””â”€â”€ data     â””â”€â”€ cityscapes         â”œâ”€â”€ annotations         â”œâ”€â”€ train         â”œâ”€â”€ val         â”œâ”€â”€ stuffthingmaps         â”œâ”€â”€ cityscapes_panoptic_val.json         â””â”€â”€ cityscapes_panoptic_val ``` The cityscapes annotations have to be converted into the aforementioned format using `tools/convert_datasets/cityscapes.py`: ```shell python tools/convert_cityscapes.py ROOT_DIRECTORY_OF_CITYSCAPES .",EvPSNet,PROJECT
Train with a single GPU: ``` python tools/train.py configs/EvPSNet_unc_singlegpu.py --work_dir work_dirs/checkpoints --validate  ``` Train with multiple GPUS: ``` .,EvPSNet,PROJECT
/tools/dist_train.sh configs/EvPSNet_unc_mutigpu.py ${GPU_NUM} --work_dir work_dirs/checkpoints --validate  ``` * --resume_from ${CHECKPOINT_FILE}: Resume from a previous checkpoint file. ### Evaluation Procedure Test with a single GPU: ``` python tools/test.py configs/EvPSNet_unc_singlegpu.py ${CHECKPOINT_FILE} --eval panoptic ``` Test with multiple GPUS: ``` .,EvPSNet,PROJECT
/tools/dist_train.sh configs/EvPSNet_unc_mutigpu.py ${GPU_NUM} --work_dir work_dirs/checkpoints --validate  ``` * --resume_from ${CHECKPOINT_FILE}: Resume from a previous checkpoint file. ### Evaluation Procedure Test with a single GPU: ``` python tools/test.py configs/EvPSNet_unc_singlegpu.py ${CHECKPOINT_FILE} --eval panoptic ``` Test with multiple GPUS: ``` .,EvPSNet,PROJECT
/tools/dist_test.sh configs/EvPSNet_unc_mutigpu.py ${CHECKPOINT_FILE} ${GPU_NUM} --eval panoptic ```  ## Additional Notes:    * tool/cityscapes_inference.py: saves predictions in the official cityscapes panoptic format,EvPSNet,PROJECT
* This is an impmentation of EvPSNet in PyTorch.,EvPSNet,PROJECT
"We especially thank the authors of: - [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS) - [mmdetection](https://github.com/open-mmlab/mmdetection) - [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) - [seamseg](https://github.com/mapillary/seamseg.git)  ## Contacts * [Kshitij Sirohi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Sajad Marvi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Daniel BÃ¼scher](http://www2.informatik.uni-freiburg.de/~buescher/)  ## License For academic usage, the code is released under the [GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html) license.",EfficientPS,PROJECT
"We especially thank the authors of: - [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS) - [mmdetection](https://github.com/open-mmlab/mmdetection) - [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) - [seamseg](https://github.com/mapillary/seamseg.git)  ## Contacts * [Kshitij Sirohi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Sajad Marvi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Daniel BÃ¼scher](http://www2.informatik.uni-freiburg.de/~buescher/)  ## License For academic usage, the code is released under the [GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html) license.",EfficientPS,PROJECT
"We especially thank the authors of: - [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS) - [mmdetection](https://github.com/open-mmlab/mmdetection) - [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) - [seamseg](https://github.com/mapillary/seamseg.git)  ## Contacts * [Kshitij Sirohi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Sajad Marvi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Daniel BÃ¼scher](http://www2.informatik.uni-freiburg.de/~buescher/)  ## License For academic usage, the code is released under the [GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html) license.",mmdetection,PROJECT
"We especially thank the authors of: - [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS) - [mmdetection](https://github.com/open-mmlab/mmdetection) - [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) - [seamseg](https://github.com/mapillary/seamseg.git)  ## Contacts * [Kshitij Sirohi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Sajad Marvi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Daniel BÃ¼scher](http://www2.informatik.uni-freiburg.de/~buescher/)  ## License For academic usage, the code is released under the [GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html) license.",mmdetection,PROJECT
"We especially thank the authors of: - [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS) - [mmdetection](https://github.com/open-mmlab/mmdetection) - [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) - [seamseg](https://github.com/mapillary/seamseg.git)  ## Contacts * [Kshitij Sirohi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Sajad Marvi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Daniel BÃ¼scher](http://www2.informatik.uni-freiburg.de/~buescher/)  ## License For academic usage, the code is released under the [GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html) license.",gen-efficientnet-pytorch,PROJECT
"We especially thank the authors of: - [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS) - [mmdetection](https://github.com/open-mmlab/mmdetection) - [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) - [seamseg](https://github.com/mapillary/seamseg.git)  ## Contacts * [Kshitij Sirohi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Sajad Marvi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Daniel BÃ¼scher](http://www2.informatik.uni-freiburg.de/~buescher/)  ## License For academic usage, the code is released under the [GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html) license.",gen-efficientnet-pytorch,PROJECT
"We especially thank the authors of: - [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS) - [mmdetection](https://github.com/open-mmlab/mmdetection) - [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) - [seamseg](https://github.com/mapillary/seamseg.git)  ## Contacts * [Kshitij Sirohi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Sajad Marvi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Daniel BÃ¼scher](http://www2.informatik.uni-freiburg.de/~buescher/)  ## License For academic usage, the code is released under the [GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html) license.",seamseg,PROJECT
"We especially thank the authors of: - [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS) - [mmdetection](https://github.com/open-mmlab/mmdetection) - [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) - [seamseg](https://github.com/mapillary/seamseg.git)  ## Contacts * [Kshitij Sirohi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Sajad Marvi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Daniel BÃ¼scher](http://www2.informatik.uni-freiburg.de/~buescher/)  ## License For academic usage, the code is released under the [GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html) license.",seamseg,PROJECT
"# ImGAGN: Imbalanced Networks Embedding via Generative Adversarial Graph Networks  This is our Pytorch implementation for the [paper](https://arxiv.org/abs/2106.02817):  > Liang Qu, Huaisheng Zhu, Ruiqi Zheng, Yuhui Shi, and Hongzhi Yin. 2021.",ImGAGN,PROJECT
ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks.,ImGAGN,PROJECT
"ACM, New York, NY, USA, 9 pages. https://doi.org/10.11453447548.3467334  ## Introduction  This work presents a generative adversarial graph network model, called ImGAGN to address the imbalanced classification problem on graphs.",ImGAGN,PROJECT
"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```",ImGAGN,PROJECT
"The documentation of the ontology, together with other more specific information is hosted on [The HaMSE Project Website](https://andreapoltronieri.org/HaMSE_project/).",HaMSE,PROJECT
"The documentation of the ontology, together with other more specific information is hosted on [The HaMSE Project Website](https://andreapoltronieri.org/HaMSE_project/).",HaMSE,PROJECT
Note: The Para-DPMM project depend heavily on the open source Dirichlet Process Mixtures package(http://people.csail.mit.edu/jchang7/code.php) written by Jason Chang.  ###############################################################################,Para-DPMM,PROJECT
# 3D CMR-Domain-Adaptation  This repo contains code to train a deep learning model for **Unsupervised Domain Adaptation (UDA)** of 3D cardiac magnetic resonance (CMR) cine images to **transform from axial to short-axis orientation**.,3D CMR-Domain-Adaptation,PROJECT
- The Deep Learning models/layers are build with TF 2.X. - Setup instruction for the repository are given here: [Install requirements](https://github.com/Cardio-AI/3d-mri-domain-adaptation#setup-instructions-tested-with-osx-and-ubuntu) - An overview of all files and there usage is given here: [Repository Structure](https://github.com/Cardio-AI/3d-mri-domain-adaptation#repository-structure)  # Paper:  Please cite the following paper if you use/modify or adapt part of the code from this repository:  S.,3d-mri-domain-adaptation,PROJECT
"The transformation layer is built on the neuron project, which is also part of the current Voxelmorph approach (https://github.com/voxelmorph/voxelmorph).",neuron,PROJECT
"We thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).",AI-MOTION,PROJECT
"We thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).",DeepFlood,PROJECT
"We thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).",Medi- cal Cognitive Computing Center,PROJECT
"We thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).",MC3,PROJECT
"We thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).",INCONTROL- RL,PROJECT
"We thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).",PRIMAL,PROJECT
"We thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).",S3AI,PROJECT
"We thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).",DL for GranularFlow,PROJECT
"We thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).",EPILEP- SIA,PROJECT
"We thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).",AIRI FG 9-N,PROJECT
"We thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).",AI4GreenHeatingGrids,PROJECT
"We thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).",INTE- GRATE,PROJECT
"We thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).",ELISE,PROJECT
"We thank the projects AI-MOTION (LIT-2018- 6-YOU-212), DeepFlood (LIT-2019-8-YOU-213), Medi- cal Cognitive Computing Center (MC3), INCONTROL- RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG- 872172), DL for GranularFlow (FFG-871302), EPILEP- SIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF- 36235), AI4GreenHeatingGrids(FFG- 899943), INTE- GRATE (FFG-892418), ELISE (H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE- 01-01).",Stars4Waters,PROJECT
# MLLM-Bench MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria <center>  !,MLLM-Bench,PROJECT
# Code search  This project contains the code to reproduce the experiments in the paper [Neural Code Search Revisited: Enhancing Code Snippet Retrieval through Natural Language Intent](https://arxiv.org/abs/2008.12193).,Code search,PROJECT
# DejaVu ## Table of Contents =================    * [Code](#code)     * [Install Requirements](#install-requirements)     * [Usage](#usage)     * [Example](#example)   * [Datasets](#datasets)   * [Deployment and Failure Injection Scripts of Train-Ticket](#deployment-and-failure-injection-scripts-of-train-ticket)   * [Citation](#citation)   * [Supplementary details](#supplementary-details)    ## Paper A preprint version: https://arxiv.org/abs/2207.09021 ## Code ### Install 1.,DejaVu,PROJECT
Pull the code from GitHub    ```bash    git pull https://github.com/NetManAIOps/DejaVu.git DejaVu    ``` 3.,DejaVu,PROJECT
Pull the code from GitHub    ```bash    git pull https://github.com/NetManAIOps/DejaVu.git DejaVu    ``` 3.,DejaVu,PROJECT
Start a Docker container with our image and enter its shell    ```bash    docker run -it --rm -v $(realpath DejaVu):/workspace lizytalk/dejavu bash    ``` 6.,DejaVu,PROJECT
"Totally, the main experiment commands of DejaVu should output as follows: - FDG message, including the data paths, edge types, the number of nodes (failure units), the number of metrics, the metrics of each failure class. - Traning setup message: the faults used for training, validation and testing. - Model architecture: model parameters in each part, total params - Training process: the training/validation/testing loss and accuracy - Time Report. - command output one-line summary.  ### Example See https://github.com/NetManAIOps/DejaVu/issues/4  ## Datasets  The datasets A, B, C, D are public at : - https://www.dropbox.com/sh/ist4ojr03e2oeuw/AAD5NkpAFg1nOI2Ttug3h2qja?",DejaVu,PROJECT
"However, computing failure similarity is not trivial due to the generalizability of DejaVu.",DejaVu,PROJECT
We also exploit training techniques from [DeiT](https://github.com/facebookresearch/deit).,DeiT,PROJECT
We also exploit training techniques from [DeiT](https://github.com/facebookresearch/deit).,deit,PROJECT
