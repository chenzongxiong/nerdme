entity_text,zenodo_gold_name,entity_type,gold_name,sentence
MNIST,mnist,DATASET,mnist,"The MNIST dataset is included in the package but it must be unziped, on a Unix machine run the following command:  ``` unzip ."
mnist,mnist,DATASET,mnist,/datasets/mnist.zip -d .
mnist,mnist,DATASET,mnist,/datasets/mnist/train` and `.
mnist,mnist,DATASET,mnist,/datasets/mnist/test` needed for the experiments.
HAIM-MIMIC-MM,NIL,DATASET,NIL,"We evaluate our HAIM framework by training and characterizing 14,324 independent models based on HAIM-MIMIC-MM, a multimodal clinical database (N=34,537 samples) containing 7,279 unique hospitalizations and 6,485 patients, spanning all possible input combinations of 4 data modalities (i.e., tabular, time-series, text, and images), 11 unique data sources and 12 predictive tasks."
MIMIC-IV,mimic-iv,DATASET,mimic-iv,"Noteevents.csv are public and available for download at Physionet.org; however, other ""NOTES"" data requires pre-release direct permission from Physionet.org for download as ""discharge notes"", ""radiology notes"", ""ECG notes"" and ""ECHO notes"" are not yet publicly released for MIMIC-IV as of Sep 2022, these files are: ds_icustay.csv, ecg_icustay.csv, echo_icustay.csv, rad_icustay.csv)."
MIMIC-IV,mimic-iv,DATASET,mimic-iv,"Please be advised that sufficient RAM or cluster access to parallel processing is needed to run these experiments.  ### UPDATE (Jan. 6, 2023) The radiology and the discharge notes for MIMIC-IV have been officially released on: https://physionet.org/content/mimic-iv-note/2.2/note/#files-panel  ### UPDATE (Jun. 12, 2023) For the publication, our team generated the file 'mimic-cxr-2.0.0-jpeg-txt.csv' by compiling an early-release version of participant notes and text from the images in CXR corresponding to MIMIC-IV."
mimic-iv-note,mimic-iv-note,DATASET,mimic-iv-note,"Please be advised that sufficient RAM or cluster access to parallel processing is needed to run these experiments.  ### UPDATE (Jan. 6, 2023) The radiology and the discharge notes for MIMIC-IV have been officially released on: https://physionet.org/content/mimic-iv-note/2.2/note/#files-panel  ### UPDATE (Jun. 12, 2023) For the publication, our team generated the file 'mimic-cxr-2.0.0-jpeg-txt.csv' by compiling an early-release version of participant notes and text from the images in CXR corresponding to MIMIC-IV."
MIMIC-IV,mimic-iv,DATASET,mimic-iv,"Please be advised that sufficient RAM or cluster access to parallel processing is needed to run these experiments.  ### UPDATE (Jan. 6, 2023) The radiology and the discharge notes for MIMIC-IV have been officially released on: https://physionet.org/content/mimic-iv-note/2.2/note/#files-panel  ### UPDATE (Jun. 12, 2023) For the publication, our team generated the file 'mimic-cxr-2.0.0-jpeg-txt.csv' by compiling an early-release version of participant notes and text from the images in CXR corresponding to MIMIC-IV."
MIMIC-IV,mimic-iv,DATASET,mimic-iv,"As of June 12, 2023, Physionet has not fully released these notes, but it is likely they are planning to do so as part of their full release of MIMIC-IV."
CoNLL04,conll04,DATASET,conll04,The CoNLL04 and ADE datasets (joint entity and relation extraction) in the correct format can be downloaded using https://github.com/markus-eberts/spert/blob/master/scripts/fetch_datasets.sh.
ADE,NIL,DATASET,NIL,The CoNLL04 and ADE datasets (joint entity and relation extraction) in the correct format can be downloaded using https://github.com/markus-eberts/spert/blob/master/scripts/fetch_datasets.sh.
CoNLL04,conll04,DATASET,conll04,"For example, to replicate the paper's results on CoNLL04, have the following section in the config file: ``` [conll04_final] datasets = conll04 model_name_or_path = t5-base num_train_epochs = 200 max_seq_length = 256 max_seq_length_eval = 512 train_split = train,dev per_device_train_batch_size = 8 per_device_eval_batch_size = 16 do_train = True do_eval = False do_predict = True episodes = 1-10 num_beams = 8 ``` Then run `python run.py conll04_final`."
conll04,conll04,DATASET,conll04,"For example, to replicate the paper's results on CoNLL04, have the following section in the config file: ``` [conll04_final] datasets = conll04 model_name_or_path = t5-base num_train_epochs = 200 max_seq_length = 256 max_seq_length_eval = 512 train_split = train,dev per_device_train_batch_size = 8 per_device_eval_batch_size = 16 do_train = True do_eval = False do_predict = True episodes = 1-10 num_beams = 8 ``` Then run `python run.py conll04_final`."
conll04,conll04,DATASET,conll04,"For example, to replicate the paper's results on CoNLL04, have the following section in the config file: ``` [conll04_final] datasets = conll04 model_name_or_path = t5-base num_train_epochs = 200 max_seq_length = 256 max_seq_length_eval = 512 train_split = train,dev per_device_train_batch_size = 8 per_device_eval_batch_size = 16 do_train = True do_eval = False do_predict = True episodes = 1-10 num_beams = 8 ``` Then run `python run.py conll04_final`."
conll04,conll04,DATASET,conll04,"For example, to replicate the paper's results on CoNLL04, have the following section in the config file: ``` [conll04_final] datasets = conll04 model_name_or_path = t5-base num_train_epochs = 200 max_seq_length = 256 max_seq_length_eval = 512 train_split = train,dev per_device_train_batch_size = 8 per_device_eval_batch_size = 16 do_train = True do_eval = False do_predict = True episodes = 1-10 num_beams = 8 ``` Then run `python run.py conll04_final`."
conll04,conll04,DATASET,conll04,"The final weights and intermediate checkpoints are written in a directory such as `experiments/conll04_final-t5-base-ep200-len256-b8-train`, with one subdirectory per episode."
conll04,conll04,DATASET,conll04,"For this, set `do_train = False` or (more easily) provide the `-e` command-line argument: `python run.py conll04_final -e`."
CoNLL04,conll04,DATASET,conll04,"For example, to test the multi-task model on the CoNLL04 dataset, run `python run.py multitask -e --eval_datasets conll04`."
conll04,conll04,DATASET,conll04,"For example, to test the multi-task model on the CoNLL04 dataset, run `python run.py multitask -e --eval_datasets conll04`."
BabelDomains,NIL,DATASET,NIL,"/a2t/topic_classification/) evaluated on BabelDomains (Camacho- Collados and Navigli, 2017)  dataset. - [Relation classification](."
TACRED,tacred,DATASET,tacred,"/a2t/relation_classification/) evaluated on TACRED (Zhang et al., 2017) dataset. -->  To get started with the repository consider reading the **new** [documentation](https://osainz59.github.io/Ask2Transformers)!"
NLI,NIL,DATASET,NIL,"-- $$\text{HiTZ/A2T\_[pretrained\_model]\_[NLI\_datasets]\_[finetune\_datasets]}$$ -->  <h3 align=""center"">HiTZ/A2T_[pretrained_model]_[NLI_datasets]_[finetune_datasets]</h3>   - `pretrained_model`: The checkpoint used for initialization."
NLI,NIL,DATASET,NIL,For example: RoBERTa<sub>large</sub>. - `NLI_datasets`: The NLI datasets used for pivot training
Standford Natural Language Inference,snil,DATASET,snil,- `S`: Standford Natural Language Inference (SNLI) dataset
SNLI,snli,DATASET,snli,- `S`: Standford Natural Language Inference (SNLI) dataset
Multi Natural Language Inference,NIL,DATASET,NIL,- `M`: Multi Natural Language Inference (MNLI) dataset
MNLI,NIL,DATASET,NIL,- `M`: Multi Natural Language Inference (MNLI) dataset
Fever-nli,fever,DATASET,fever,- `F`: Fever-nli dataset
Adversarial Natural Language Inference,anli,DATASET,anli,- `A`: Adversarial Natural Language Inference (ANLI) dataset. - `finetune_datasets`: The datasets used for fine tuning the entailment model.
ANLI,anli,DATASET,anli,- `A`: Adversarial Natural Language Inference (ANLI) dataset. - `finetune_datasets`: The datasets used for fine tuning the entailment model.
ACE-arg,NIL,DATASET,NIL,For example: ACE-arg.
ACE-arg,NIL,DATASET,NIL,Some models like `HiTZ/A2T_RoBERTa_SMFA_ACE-arg` have been trained marking some information between square brackets (`'[['` and `']]'`) like the event trigger span.
NLI,NIL,DATASET,NIL,"To train your own model, first, you will need to convert your actual dataset in some sort of NLI data, we recommend you to have a look to [tacred2mnli.py](https://github.com/osainz59/Ask2Transformers/blob/master/scripts/tacred2mnli.py) script that serves as an example"
babeldomains,NIL,DATASET,NIL,"""Royalty and nobility"",         ""Sport and recreation"",         ""Textile and clothing"",         ""Transport and travel"",         ""Warfare and defense""     ],     ""preprocess_labels"": true,     ""dataset"": ""babeldomains"",     ""test_path"": ""data/babeldomains.domain.gloss.tsv"",     ""use_cuda"": true,     ""half"": true } ```  Consider reading the papers to access the results"
TACRED,tacred,DATASET,tacred,"In our experiments on TACRED we attain 63{\%} F1 zero-shot, 69{\%} with 16 examples per relation (17{\%} points better than the best supervised system on the same conditions), and only 4 points short to the state-of-the-art (which uses 20 times more training data)."
TACRED,tacred,DATASET,tacred,"We also show that the performance can be improved significantly with larger entailment models, up to 12 points in zero-shot, allowing to report the best results to date on TACRED when fully trained."
FinGPT/fingpt-forecaster-dow30-202305-202405,NIL,DATASET,NIL,"RLHF enables an LLM model to learn individual preferences (risk-aversion level, investing habits, personalized robo-advisor, etc.), which is the ""secret"" ingredient of ChatGPT and GPT4.   ### Milestone of AI Robo-Advisor: FinGPT-Forecaster  Try the latest released FinGPT-Forecaster demo at our [HuggingFace Space](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster)  The dataset for FinGPT-Forecaster: https://huggingface.co/datasets/FinGPT/fingpt-forecaster-dow30-202305-202405  !"
TFNS,NIL,DATASET,NIL,"* Benchmark Results:      * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |     | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |     | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 × RTX 3090    | 17.25 hours |     $17.25     |     | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 × A100      |  5.5 hours  |    $ 22.55     |     | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 × A100      |  5.5 hours  |    $ 22.55     |     | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 × RTX 3090    | 6.47 hours  |     $ 6.47     |     | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 × RTX 3090    | 4.15 hours  |     $ 4.15     |     | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |     | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |     | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 × NVIDIA K80 GPU |      -      |       -        |     | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 × A100     |   21 days   | $ 4.23 million |     | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 × A100     |   53 days   | $ 2.67 million |        **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs."
NWGI,NIL,DATASET,NIL,"* Benchmark Results:      * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |     | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |     | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 × RTX 3090    | 17.25 hours |     $17.25     |     | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 × A100      |  5.5 hours  |    $ 22.55     |     | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 × A100      |  5.5 hours  |    $ 22.55     |     | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 × RTX 3090    | 6.47 hours  |     $ 6.47     |     | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 × RTX 3090    | 4.15 hours  |     $ 4.15     |     | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |     | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |     | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 × NVIDIA K80 GPU |      -      |       -        |     | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 × A100     |   21 days   | $ 4.23 million |     | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 × A100     |   53 days   | $ 2.67 million |        **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs."
fingpt-sentiment-train,NIL,DATASET,NIL,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc."
FinGPT/fingpt-sentiment-train,NIL,DATASET,NIL,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc."
fingpt-finred,finred,DATASET,finred,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc."
FinGPT/fingpt-finred,finred,DATASET,finred,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc."
FinGPT/fingpt-headline,NIL,DATASET,NIL,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc."
fingpt-ner,NIL,DATASET,NIL,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc."
FinGPT/fingpt-ner,NIL,DATASET,NIL,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc."
fingpt-fiqa_qa,NIL,DATASET,NIL,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc."
FinGPT/fingpt-fiqa_qa,NIL,DATASET,NIL,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc."
fingpt-fineval,NIL,DATASET,NIL,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc."
FinGPT/fingpt-fineval,NIL,DATASET,NIL,"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc."
FPB,NIL,DATASET,NIL,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?"
financial_phrasebank,NIL,DATASET,NIL,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?"
FiQA-SA,NIL,DATASET,NIL,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?"
pauri32/fiqa-2018,NIL,DATASET,NIL,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?"
TFNS,NIL,DATASET,NIL,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?"
zeroshot/twitter-financial-news-sentiment,NIL,DATASET,NIL,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?"
NWGI,NIL,DATASET,NIL,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?"
oliverwang15/news_with_gpt_instructions,NIL,DATASET,NIL,"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World’s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?"
PromptNet,NIL,DATASET,NIL,"model=text-davinci-003)    Financial News + [Algorithmic Trading using Sentiment Analysis on News Articles](https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704) + [Accessing Historical Financial News Headlines with Python](https://python.plainenglish.io/access-historical-financial-news-headlines-with-python-be1b8faaea9f)  **PromptNet** Analogy to ImageNet and WordNet, it is critical to build a PromptNet"
ImageNet,imagenet,DATASET,imagenet,"model=text-davinci-003)    Financial News + [Algorithmic Trading using Sentiment Analysis on News Articles](https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704) + [Accessing Historical Financial News Headlines with Python](https://python.plainenglish.io/access-historical-financial-news-headlines-with-python-be1b8faaea9f)  **PromptNet** Analogy to ImageNet and WordNet, it is critical to build a PromptNet"
WordNet,languagenet,DATASET,languagenet,"model=text-davinci-003)    Financial News + [Algorithmic Trading using Sentiment Analysis on News Articles](https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704) + [Accessing Historical Financial News Headlines with Python](https://python.plainenglish.io/access-historical-financial-news-headlines-with-python-be1b8faaea9f)  **PromptNet** Analogy to ImageNet and WordNet, it is critical to build a PromptNet"
PromptNet,NIL,DATASET,NIL,"model=text-davinci-003)    Financial News + [Algorithmic Trading using Sentiment Analysis on News Articles](https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704) + [Accessing Historical Financial News Headlines with Python](https://python.plainenglish.io/access-historical-financial-news-headlines-with-python-be1b8faaea9f)  **PromptNet** Analogy to ImageNet and WordNet, it is critical to build a PromptNet"
Ketton Limestone,NIL,DATASET,NIL,"Blunt](https://www.imperial.ac.uk/people/m.blunt)   *Department of Earth Science and Engineering, Imperial College London*    This is the code repository accompanying the publication:    *Conditioning of three-dimensional generative adversarial networks for pore and reservoir-scale models*  [[ArXiv](http://arxiv.org/abs/1802.05622)]  ## Datasets and pre-trained models  ### Ketton Limestone Dataset We provide two pre-trained GAN models."
Maules Creek,NIL,DATASET,NIL,Due to the stochastic nature of the optimization procedure the resulting images have distinctly different features away from the conditioning data.   ### Maules Creek Dataset  We have trained a generative adversarial network on the Maules Creek alluvial aquifer training image.
Maules Creek,NIL,DATASET,NIL,Due to the stochastic nature of the optimization procedure the resulting images have distinctly different features away from the conditioning data.   ### Maules Creek Dataset  We have trained a generative adversarial network on the Maules Creek alluvial aquifer training image.
Maules Creek,NIL,DATASET,NIL,"If you choose to use the Maules Creek training image, please consider citing their originators at [trainingimages.org](www.trainingimages.org)    #### Results  !"
Maules Creek,NIL,DATASET,NIL,[Maules Creek](figures/fig_2.png)  We have conditioned 1024 realizations of the Maules Creek alluvial aquifer model and present mean and standard deviation maps of the resulting ensemble.
Maules Creek,NIL,DATASET,NIL,[Maules Creek](figures/fig_2.png)  We have conditioned 1024 realizations of the Maules Creek alluvial aquifer model and present mean and standard deviation maps of the resulting ensemble.
Caltech UCSD Birds,NIL,DATASET,NIL,"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1."
Butterfly200,NIL,DATASET,NIL,"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1."
Vegfru,vegfru,DATASET,vegfru,"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1."
Butterfly200,NIL,DATASET,NIL,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2."
CUB_200_2011,cub-200-2011,DATASET,cub-200-2011,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2."
Vegfru,vegfru,DATASET,vegfru,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2."
Butterfly200,NIL,DATASET,NIL,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2."
CUB_200_2011,cub-200-2011,DATASET,cub-200-2011,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2."
Vegfru,vegfru,DATASET,vegfru,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2."
Butterfly200,NIL,DATASET,NIL,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2."
CUB_200_2011,cub-200-2011,DATASET,cub-200-2011,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2."
Vegfru,vegfru,DATASET,vegfru,"**Note that**, the correct structure of `$HSE_ROOT` is like:  ``` hse-mm2018 . ├── code │   ├── Butterfly200 │   │   ├── baseline │   │   └── HSE │   ├── CUB_200_2011 │   │   ├── baseline │   │   └── HSE │   └── Vegfru │       ├── baseline │       └── HSE ├── data │   ├── Butterfly200 │   │   └── images │   ├── CUB_200_2011 │   │   └── images │   └── Vegfru │       └── images ├── models │   ├── Butterfly200 │   ├── CUB_200_2011 │   └── Vegfru └── scripts  ```  ## 2."
Caltech UCSD Birds,NIL,DATASET,NIL,"Download datasets  [Caltech UCSD Birds](http://www.vision.caltech.edu/visipedia/CUB-200.html) originally covers 200 classes of birds, and we  extend this dataset with a [four-level category hierarchy](https://www.dropbox.com/sh/kugj7vogy2no795/AABJWUxM6rXWOeNbCUPj269ua?"
CUB-200,cub-200-2011,DATASET,cub-200-2011,"Download datasets  [Caltech UCSD Birds](http://www.vision.caltech.edu/visipedia/CUB-200.html) originally covers 200 classes of birds, and we  extend this dataset with a [four-level category hierarchy](https://www.dropbox.com/sh/kugj7vogy2no795/AABJWUxM6rXWOeNbCUPj269ua?"
Butterfly 200,NIL,DATASET,NIL,[Butterfly 200](https://www.dropbox.com/sh/3p4x1oc5efknd69/AABwnyoH2EKi6H9Emcyd0pXCa?
Caltech UCSD birds,NIL,DATASET,NIL,"Download trained models The trained models of our HSE framework and the baseline methods on the extended Caltech UCSD birds, Butterfly-200, and VegFru datasets can be downloaded from [OneDrive](https://1drv.ms/f/s!"
Butterfly-200,NIL,DATASET,NIL,"Download trained models The trained models of our HSE framework and the baseline methods on the extended Caltech UCSD birds, Butterfly-200, and VegFru datasets can be downloaded from [OneDrive](https://1drv.ms/f/s!"
VegFru,vegfru,DATASET,vegfru,"Download trained models The trained models of our HSE framework and the baseline methods on the extended Caltech UCSD birds, Butterfly-200, and VegFru datasets can be downloaded from [OneDrive](https://1drv.ms/f/s!"
CUB_200_2011,cub-200-2011,DATASET,cub-200-2011,"/scripts/deploy_hse.sh [GPU_ID] [DATASET] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' ``` ## deploy baseline ``` ."
Butterfly200,NIL,DATASET,NIL,"/scripts/deploy_hse.sh [GPU_ID] [DATASET] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' ``` ## deploy baseline ``` ."
Vegfru,vegfru,DATASET,vegfru,"/scripts/deploy_hse.sh [GPU_ID] [DATASET] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' ``` ## deploy baseline ``` ."
CUB_200_2011,cub-200-2011,DATASET,cub-200-2011,"/scripts/deploy_baseline.sh [GPU_ID] [DATASET] [LEVEL] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' LEVEL: require,      CUB_200_2011: LEVEL is chosen in ['order', 'family', 'genus', 'class']     Butterfly200: LEVEL is chosen in ['family', 'subfamily', 'genus', 'species']     Vegfru: LEVEL is chosen in ['sup', 'sub'] ```  # License The code is released under the SYSU License (refer to the LICENSE file for details)."
Butterfly200,NIL,DATASET,NIL,"/scripts/deploy_baseline.sh [GPU_ID] [DATASET] [LEVEL] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' LEVEL: require,      CUB_200_2011: LEVEL is chosen in ['order', 'family', 'genus', 'class']     Butterfly200: LEVEL is chosen in ['family', 'subfamily', 'genus', 'species']     Vegfru: LEVEL is chosen in ['sup', 'sub'] ```  # License The code is released under the SYSU License (refer to the LICENSE file for details)."
Vegfru,vegfru,DATASET,vegfru,"/scripts/deploy_baseline.sh [GPU_ID] [DATASET] [LEVEL] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' LEVEL: require,      CUB_200_2011: LEVEL is chosen in ['order', 'family', 'genus', 'class']     Butterfly200: LEVEL is chosen in ['family', 'subfamily', 'genus', 'species']     Vegfru: LEVEL is chosen in ['sup', 'sub'] ```  # License The code is released under the SYSU License (refer to the LICENSE file for details)."
CUB_200_2011,cub-200-2011,DATASET,cub-200-2011,"/scripts/deploy_baseline.sh [GPU_ID] [DATASET] [LEVEL] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' LEVEL: require,      CUB_200_2011: LEVEL is chosen in ['order', 'family', 'genus', 'class']     Butterfly200: LEVEL is chosen in ['family', 'subfamily', 'genus', 'species']     Vegfru: LEVEL is chosen in ['sup', 'sub'] ```  # License The code is released under the SYSU License (refer to the LICENSE file for details)."
Butterfly200,NIL,DATASET,NIL,"/scripts/deploy_baseline.sh [GPU_ID] [DATASET] [LEVEL] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' LEVEL: require,      CUB_200_2011: LEVEL is chosen in ['order', 'family', 'genus', 'class']     Butterfly200: LEVEL is chosen in ['family', 'subfamily', 'genus', 'species']     Vegfru: LEVEL is chosen in ['sup', 'sub'] ```  # License The code is released under the SYSU License (refer to the LICENSE file for details)."
Vegfru,vegfru,DATASET,vegfru,"/scripts/deploy_baseline.sh [GPU_ID] [DATASET] [LEVEL] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' LEVEL: require,      CUB_200_2011: LEVEL is chosen in ['order', 'family', 'genus', 'class']     Butterfly200: LEVEL is chosen in ['family', 'subfamily', 'genus', 'species']     Vegfru: LEVEL is chosen in ['sup', 'sub'] ```  # License The code is released under the SYSU License (refer to the LICENSE file for details)."
greek_legal_code,NIL,DATASET,NIL,"Dataset is available at HuggingFace 🤗 : https://huggingface.co/datasets/greek_legal_code  *** ### Abstract  In this work, we study the task of classifying legal texts written in the Greek language."
ISBI 2012 EM segmentation benchmark,NIL,DATASET,NIL,"We evaluate our method on a diverse range of data domains, where it defines the new state of the art on four benchmarks, namely the ISBI 2012 EM segmentation benchmark, the BBBC010 C. elegans dataset, and 2d as well as 3d fluorescence microscopy datasets of cell nuclei."
BBBC010 C. elegans,NIL,DATASET,NIL,"We evaluate our method on a diverse range of data domains, where it defines the new state of the art on four benchmarks, namely the ISBI 2012 EM segmentation benchmark, the BBBC010 C. elegans dataset, and 2d as well as 3d fluorescence microscopy datasets of cell nuclei."
3d fluorescence microscopy,NIL,DATASET,NIL,"We evaluate our method on a diverse range of data domains, where it defines the new state of the art on four benchmarks, namely the ISBI 2012 EM segmentation benchmark, the BBBC010 C. elegans dataset, and 2d as well as 3d fluorescence microscopy datasets of cell nuclei."
3d light microscopy data of drosophila neurons,NIL,DATASET,NIL,"We show furthermore that our method also applies to 3d light microscopy data of drosophila neurons, which exhibit extreme cases of complex shape clusters.  ## Installation  This package requires Python 3 and PyTorch."
FlyLight Instances Segmentation Benchmark,NIL,DATASET,NIL,"The following instructions were tested on linux/ubuntu 20.04.   ``` conda create --name ppp --yes conda activate ppp conda install python=3.9 pytorch-cuda torchvision torchaudio cudatoolkit -c pytorch -c nvidia --yes git clone https://github.com/Kainmueller-Lab/PatchPerPix.git cd PatchPerPix PATH=/usr/local/cuda/bin:$PATH CUDA_ROOT=/usr/local/cuda pip install -e . ```  ## Organization - PatchPerPix: contains the code for our instance assembly pipeline to go from predictions to instances - experiments: contains the training and prediction code to generate predictions and the main script; contains one sub-folder per application/dataset   - `run_ppp.py`:  - main script to run the experiments  - command line arguments are used to select the experiment and the sub-task to be executed (training, inference etc, see below for an example)  - the parameters for the network training and the postprocessing have to be defined in a config file ([example config file](https://github.com/Kainmueller-Lab/PatchPerPix/blob/master/experiments/flylight/setups/setup01/default.toml))   - flylight: an example experiment for the FlyLight Instances Segmentation Benchmark Dataset  - setups: here the different experiment setups are placed, the python scripts should not be called manually, but will be called by the main script      - `train.py`: trains the network      - `predict_no_gp.py`: prediction after training      - `decode.py`: if ppp+dec is used, decode the predicted patch encodings to the full patches      - `default.toml`: example configuration file   - `default_train_code.toml`: example configuration file that uses ppp+dec   - `torch_loss.py`: auxiliary file for the loss computation   - `torch_model.py`: auxiliary file for the torch model definition   ## Data preparation  The code expects the data to be in the *zarr* format ([https://zarr.readthedocs.io/en/stable/](https://zarr.readthedocs.io/en/stable/))."
nuclei3d,NIL,DATASET,NIL,"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository."
The FlyLight Instance Segmentation Datset,NIL,DATASET,NIL,"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository."
CaLiGraph,NIL,DATASET,NIL,# CaLiGraph for Semantic Reasoning Evaluation Challenge  ## SemREC'21  This repository contains code and data to use [CaLiGraph](http://caligraph.org) as a benchmark dataset in the [Semantic Reasoning Evaluation Challange](https://semrec.github.io) at the [International Semantic Web Conference 2021 (ISWC'21)](https://iswc2021.semanticweb.org).
CaLiGraph,NIL,DATASET,NIL,# CaLiGraph for Semantic Reasoning Evaluation Challenge  ## SemREC'21  This repository contains code and data to use [CaLiGraph](http://caligraph.org) as a benchmark dataset in the [Semantic Reasoning Evaluation Challange](https://semrec.github.io) at the [International Semantic Web Conference 2021 (ISWC'21)](https://iswc2021.semanticweb.org).
CaLiGraph version 2.1.0,NIL,DATASET,NIL,The paper describing the dataset characteristics and results for well-known reasoners can be found [here](https://arxiv.org/pdf/2110.05028.pdf).  ### Datasets We use [CaLiGraph version 2.1.0](https://zenodo.org/record/5509912) as foundation for the challenge dataset.
caligraph,NIL,DATASET,NIL,"In particular, we use the files `caligraph-ontology.nt.bz2` and `caligraph-instances_types.nt.bz2` to generate our sample data."
CaLiGraph,NIL,DATASET,NIL,The datasets and all potentially inferrable assertions can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/).
CaLiGraph,NIL,DATASET,NIL,The datasets and all potentially inferrable assertions can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/).
CaLiGraph,NIL,DATASET,NIL,The precomputed results for all the sample datasets can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/).   ## SemREC'22  For [SemREC'22](https://semrec.github.io) at the [International Semantic Web Conference 2022 (ISWC'22)](https://iswc2022.semanticweb.org) we provide [CaLiGraph](http://caligraph.org) as an additional challenge dataset for the participants of Task 2 (Systems).
CaLiGraph,NIL,DATASET,NIL,The precomputed results for all the sample datasets can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/).   ## SemREC'22  For [SemREC'22](https://semrec.github.io) at the [International Semantic Web Conference 2022 (ISWC'22)](https://iswc2022.semanticweb.org) we provide [CaLiGraph](http://caligraph.org) as an additional challenge dataset for the participants of Task 2 (Systems).
CaLiGraph,NIL,DATASET,NIL,The precomputed results for all the sample datasets can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/).   ## SemREC'22  For [SemREC'22](https://semrec.github.io) at the [International Semantic Web Conference 2022 (ISWC'22)](https://iswc2022.semanticweb.org) we provide [CaLiGraph](http://caligraph.org) as an additional challenge dataset for the participants of Task 2 (Systems).
caligraph,NIL,DATASET,NIL,The precomputed results for all the sample datasets can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/).   ## SemREC'22  For [SemREC'22](https://semrec.github.io) at the [International Semantic Web Conference 2022 (ISWC'22)](https://iswc2022.semanticweb.org) we provide [CaLiGraph](http://caligraph.org) as an additional challenge dataset for the participants of Task 2 (Systems).
clg_10e4,NIL,DATASET,NIL,"For this purpose, we use the subsets `clg_10e4`, `clg_10e5`, and `clg_full` and split their inferrable assertions into training, validation, and test files."
clg_10e5,NIL,DATASET,NIL,"For this purpose, we use the subsets `clg_10e4`, `clg_10e5`, and `clg_full` and split their inferrable assertions into training, validation, and test files."
clg_ful,NIL,DATASET,NIL,"For this purpose, we use the subsets `clg_10e4`, `clg_10e5`, and `clg_full` and split their inferrable assertions into training, validation, and test files."
CaLiGraph,NIL,DATASET,NIL,The split datasets can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/SemREC-2022-Datasets/).
CaLiGraph,NIL,DATASET,NIL,The split datasets can be found [here](http://data.dws.informatik.uni-mannheim.de/CaLiGraph/CaLiGraph-for-SemREC/SemREC-2022-Datasets/).
ImageNet-1K,imagenet,DATASET,imagenet,The image and text tensors will also have to be transferred accordingly.  ## Available Models  ```python import diht   print(diht.available_models()) ```  ## Example ImageNet-1K zero-shot evaluation  A simple image classification zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download ImageNet-1K dataset from the original website.
ImageNet-1K,imagenet,DATASET,imagenet,The image and text tensors will also have to be transferred accordingly.  ## Available Models  ```python import diht   print(diht.available_models()) ```  ## Example ImageNet-1K zero-shot evaluation  A simple image classification zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download ImageNet-1K dataset from the original website.
imagenet,imagenet,DATASET,imagenet,Edit `IMAGENET_ROOT` in `example_imagenet_eval.py` to match the location on your machine. ``` python example_imagenet_eval.py ``` For DiHT-L/14@336 the output should look like: ``` ImageNet1K acc@1 for diht_vitl14_336px: 77.9 ```  ## Example retrieval zero-shot evaluation  A simple retrieval zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download COCO and Flickr30K datasets from the original websites.
imagenet,imagenet,DATASET,imagenet,Edit `IMAGENET_ROOT` in `example_imagenet_eval.py` to match the location on your machine. ``` python example_imagenet_eval.py ``` For DiHT-L/14@336 the output should look like: ``` ImageNet1K acc@1 for diht_vitl14_336px: 77.9 ```  ## Example retrieval zero-shot evaluation  A simple retrieval zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download COCO and Flickr30K datasets from the original websites.
ImageNet1K,imagenet,DATASET,imagenet,Edit `IMAGENET_ROOT` in `example_imagenet_eval.py` to match the location on your machine. ``` python example_imagenet_eval.py ``` For DiHT-L/14@336 the output should look like: ``` ImageNet1K acc@1 for diht_vitl14_336px: 77.9 ```  ## Example retrieval zero-shot evaluation  A simple retrieval zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download COCO and Flickr30K datasets from the original websites.
COCO,ms coco,DATASET,ms coco,Edit `IMAGENET_ROOT` in `example_imagenet_eval.py` to match the location on your machine. ``` python example_imagenet_eval.py ``` For DiHT-L/14@336 the output should look like: ``` ImageNet1K acc@1 for diht_vitl14_336px: 77.9 ```  ## Example retrieval zero-shot evaluation  A simple retrieval zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download COCO and Flickr30K datasets from the original websites.
Flickr30K,flickr30k,DATASET,flickr30k,Edit `IMAGENET_ROOT` in `example_imagenet_eval.py` to match the location on your machine. ``` python example_imagenet_eval.py ``` For DiHT-L/14@336 the output should look like: ``` ImageNet1K acc@1 for diht_vitl14_336px: 77.9 ```  ## Example retrieval zero-shot evaluation  A simple retrieval zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download COCO and Flickr30K datasets from the original websites.
coco,ms coco,DATASET,ms coco,Json files (`coco_test.json` and `flickr30k_test.json`) can be downloaded from https://github.com/salesforce/ALBEF#download.
flickr30k,flickr30k,DATASET,flickr30k,Json files (`coco_test.json` and `flickr30k_test.json`) can be downloaded from https://github.com/salesforce/ALBEF#download.
COCO,ms coco,DATASET,ms coco,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates."
FLICKR30K,flickr30k,DATASET,flickr30k,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates."
COCO,ms coco,DATASET,ms coco,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates."
COCO,ms coco,DATASET,ms coco,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates."
Flickr30K,flickr30k,DATASET,flickr30k,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates."
Flickr30K,flickr30k,DATASET,flickr30k,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates."
ImageNet-1K,imagenet-p,DATASET,imagenet-p,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates."
COCO,ms coco,DATASET,ms coco,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates."
COCO,ms coco,DATASET,ms coco,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates."
Flickr30K,flickr30k,DATASET,flickr30k,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates."
Flickr30K,flickr30k,DATASET,flickr30k,"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates."
BorealTC,borealtc,DATASET,borealtc,"# Run the Docker image docker run --gpus all -e CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES --rm --ipc host \    --mount type=bind,source=.,target=/code/ \    --mount type=bind,source=/dev/shm,target=/dev/shm \    borealtc-gpu python3 main.py ```  ## Dataset  The `data` directory contains two different datasets:  * the `BorealTC` dataset, our publicly available dataset * the `Vulpi` dataset, from the 2021 [paper](https://doi.org/10.1016/j.jterra.2020.12.002) of Vulpi et al."
Vulpi,NIL,DATASET,NIL,"# Run the Docker image docker run --gpus all -e CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES --rm --ipc host \    --mount type=bind,source=.,target=/code/ \    --mount type=bind,source=/dev/shm,target=/dev/shm \    borealtc-gpu python3 main.py ```  ## Dataset  The `data` directory contains two different datasets:  * the `BorealTC` dataset, our publicly available dataset * the `Vulpi` dataset, from the 2021 [paper](https://doi.org/10.1016/j.jterra.2020.12.002) of Vulpi et al."
Vulpi,NIL,DATASET,NIL,"│   ├── pro_1.csv     │   ├── pro_2.csv     │   └── ...     └── CLASS2         ├── imu_1.csv         ├── imu_2.csv         ├── ...         ├── pro_1.csv         ├── pro_2.csv         └── ... ```  ## Model hyperparameters and checkpoints  ### Mamba  |  Dataset   |          Data Type           | `d_state`   | `d_conv`  | `expand`  | `d_model`   | Norm epsilon | Initial LR | LR&nbsp;drop factor | Reduce&nbsp;LR patience | Max epochs | Minibatch size | Valid patience | Gradient threshold | Focal&nbsp;loss alpha | Focal&nbsp;loss gamma |                       Checkpoint                       | | :--------: | :--------------------------: | ----------- | --------- | --------- | ----------- | ------------ | ---------- | ------------------- | ----------------------- | ---------- | -------------- | -------------- | ------------------ | --------------------- | --------------------- | :----------------------------------------------------: | |  `Vulpi`   | IMU <br/> Wheel&nbsp;service | 56 <br/> 56 | 3 <br/> 3 | 4 <br/> 4 | 56 <br/> 56 | 5e-3         | 5e-3       | 0.33                | 4                       | 60         | 64             | 8              | 2                  | 0.75                  | 2                     |    [mamba_vulpi.ckpt](checkpoints/mamba_vulpi.ckpt)    | | `BorealTC` | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_borealtc.ckpt](checkpoints/mamba_borealtc.ckpt) | |  Combined  | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_combined.ckpt](checkpoints/mamba_combined.ckpt) |  In the above table, `d_state`, `d_conv`, `expand` and `d_model` are parameters specific to Mamba."
vulpi,NIL,DATASET,NIL,"│   ├── pro_1.csv     │   ├── pro_2.csv     │   └── ...     └── CLASS2         ├── imu_1.csv         ├── imu_2.csv         ├── ...         ├── pro_1.csv         ├── pro_2.csv         └── ... ```  ## Model hyperparameters and checkpoints  ### Mamba  |  Dataset   |          Data Type           | `d_state`   | `d_conv`  | `expand`  | `d_model`   | Norm epsilon | Initial LR | LR&nbsp;drop factor | Reduce&nbsp;LR patience | Max epochs | Minibatch size | Valid patience | Gradient threshold | Focal&nbsp;loss alpha | Focal&nbsp;loss gamma |                       Checkpoint                       | | :--------: | :--------------------------: | ----------- | --------- | --------- | ----------- | ------------ | ---------- | ------------------- | ----------------------- | ---------- | -------------- | -------------- | ------------------ | --------------------- | --------------------- | :----------------------------------------------------: | |  `Vulpi`   | IMU <br/> Wheel&nbsp;service | 56 <br/> 56 | 3 <br/> 3 | 4 <br/> 4 | 56 <br/> 56 | 5e-3         | 5e-3       | 0.33                | 4                       | 60         | 64             | 8              | 2                  | 0.75                  | 2                     |    [mamba_vulpi.ckpt](checkpoints/mamba_vulpi.ckpt)    | | `BorealTC` | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_borealtc.ckpt](checkpoints/mamba_borealtc.ckpt) | |  Combined  | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_combined.ckpt](checkpoints/mamba_combined.ckpt) |  In the above table, `d_state`, `d_conv`, `expand` and `d_model` are parameters specific to Mamba."
vulpi,NIL,DATASET,NIL,"│   ├── pro_1.csv     │   ├── pro_2.csv     │   └── ...     └── CLASS2         ├── imu_1.csv         ├── imu_2.csv         ├── ...         ├── pro_1.csv         ├── pro_2.csv         └── ... ```  ## Model hyperparameters and checkpoints  ### Mamba  |  Dataset   |          Data Type           | `d_state`   | `d_conv`  | `expand`  | `d_model`   | Norm epsilon | Initial LR | LR&nbsp;drop factor | Reduce&nbsp;LR patience | Max epochs | Minibatch size | Valid patience | Gradient threshold | Focal&nbsp;loss alpha | Focal&nbsp;loss gamma |                       Checkpoint                       | | :--------: | :--------------------------: | ----------- | --------- | --------- | ----------- | ------------ | ---------- | ------------------- | ----------------------- | ---------- | -------------- | -------------- | ------------------ | --------------------- | --------------------- | :----------------------------------------------------: | |  `Vulpi`   | IMU <br/> Wheel&nbsp;service | 56 <br/> 56 | 3 <br/> 3 | 4 <br/> 4 | 56 <br/> 56 | 5e-3         | 5e-3       | 0.33                | 4                       | 60         | 64             | 8              | 2                  | 0.75                  | 2                     |    [mamba_vulpi.ckpt](checkpoints/mamba_vulpi.ckpt)    | | `BorealTC` | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_borealtc.ckpt](checkpoints/mamba_borealtc.ckpt) | |  Combined  | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_combined.ckpt](checkpoints/mamba_combined.ckpt) |  In the above table, `d_state`, `d_conv`, `expand` and `d_model` are parameters specific to Mamba."
BorealTC,borealtc,DATASET,borealtc,"│   ├── pro_1.csv     │   ├── pro_2.csv     │   └── ...     └── CLASS2         ├── imu_1.csv         ├── imu_2.csv         ├── ...         ├── pro_1.csv         ├── pro_2.csv         └── ... ```  ## Model hyperparameters and checkpoints  ### Mamba  |  Dataset   |          Data Type           | `d_state`   | `d_conv`  | `expand`  | `d_model`   | Norm epsilon | Initial LR | LR&nbsp;drop factor | Reduce&nbsp;LR patience | Max epochs | Minibatch size | Valid patience | Gradient threshold | Focal&nbsp;loss alpha | Focal&nbsp;loss gamma |                       Checkpoint                       | | :--------: | :--------------------------: | ----------- | --------- | --------- | ----------- | ------------ | ---------- | ------------------- | ----------------------- | ---------- | -------------- | -------------- | ------------------ | --------------------- | --------------------- | :----------------------------------------------------: | |  `Vulpi`   | IMU <br/> Wheel&nbsp;service | 56 <br/> 56 | 3 <br/> 3 | 4 <br/> 4 | 56 <br/> 56 | 5e-3         | 5e-3       | 0.33                | 4                       | 60         | 64             | 8              | 2                  | 0.75                  | 2                     |    [mamba_vulpi.ckpt](checkpoints/mamba_vulpi.ckpt)    | | `BorealTC` | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_borealtc.ckpt](checkpoints/mamba_borealtc.ckpt) | |  Combined  | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_combined.ckpt](checkpoints/mamba_combined.ckpt) |  In the above table, `d_state`, `d_conv`, `expand` and `d_model` are parameters specific to Mamba."
borealtc,borealtc,DATASET,borealtc,"│   ├── pro_1.csv     │   ├── pro_2.csv     │   └── ...     └── CLASS2         ├── imu_1.csv         ├── imu_2.csv         ├── ...         ├── pro_1.csv         ├── pro_2.csv         └── ... ```  ## Model hyperparameters and checkpoints  ### Mamba  |  Dataset   |          Data Type           | `d_state`   | `d_conv`  | `expand`  | `d_model`   | Norm epsilon | Initial LR | LR&nbsp;drop factor | Reduce&nbsp;LR patience | Max epochs | Minibatch size | Valid patience | Gradient threshold | Focal&nbsp;loss alpha | Focal&nbsp;loss gamma |                       Checkpoint                       | | :--------: | :--------------------------: | ----------- | --------- | --------- | ----------- | ------------ | ---------- | ------------------- | ----------------------- | ---------- | -------------- | -------------- | ------------------ | --------------------- | --------------------- | :----------------------------------------------------: | |  `Vulpi`   | IMU <br/> Wheel&nbsp;service | 56 <br/> 56 | 3 <br/> 3 | 4 <br/> 4 | 56 <br/> 56 | 5e-3         | 5e-3       | 0.33                | 4                       | 60         | 64             | 8              | 2                  | 0.75                  | 2                     |    [mamba_vulpi.ckpt](checkpoints/mamba_vulpi.ckpt)    | | `BorealTC` | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_borealtc.ckpt](checkpoints/mamba_borealtc.ckpt) | |  Combined  | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_combined.ckpt](checkpoints/mamba_combined.ckpt) |  In the above table, `d_state`, `d_conv`, `expand` and `d_model` are parameters specific to Mamba."
borealtc,borealtc,DATASET,borealtc,"│   ├── pro_1.csv     │   ├── pro_2.csv     │   └── ...     └── CLASS2         ├── imu_1.csv         ├── imu_2.csv         ├── ...         ├── pro_1.csv         ├── pro_2.csv         └── ... ```  ## Model hyperparameters and checkpoints  ### Mamba  |  Dataset   |          Data Type           | `d_state`   | `d_conv`  | `expand`  | `d_model`   | Norm epsilon | Initial LR | LR&nbsp;drop factor | Reduce&nbsp;LR patience | Max epochs | Minibatch size | Valid patience | Gradient threshold | Focal&nbsp;loss alpha | Focal&nbsp;loss gamma |                       Checkpoint                       | | :--------: | :--------------------------: | ----------- | --------- | --------- | ----------- | ------------ | ---------- | ------------------- | ----------------------- | ---------- | -------------- | -------------- | ------------------ | --------------------- | --------------------- | :----------------------------------------------------: | |  `Vulpi`   | IMU <br/> Wheel&nbsp;service | 56 <br/> 56 | 3 <br/> 3 | 4 <br/> 4 | 56 <br/> 56 | 5e-3         | 5e-3       | 0.33                | 4                       | 60         | 64             | 8              | 2                  | 0.75                  | 2                     |    [mamba_vulpi.ckpt](checkpoints/mamba_vulpi.ckpt)    | | `BorealTC` | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_borealtc.ckpt](checkpoints/mamba_borealtc.ckpt) | |  Combined  | IMU <br/> Wheel&nbsp;service | 16 <br/> 16 | 4 <br/> 3 | 4 <br/> 6 | 32 <br/> 8  | 6.3e-6       | 1.5e-3     | 0.25                | 4                       | 60         | 16             | 8              | None               | 0.75                  | 2.25                  | [mamba_combined.ckpt](checkpoints/mamba_combined.ckpt) |  In the above table, `d_state`, `d_conv`, `expand` and `d_model` are parameters specific to Mamba."
Public Event,NIL,DATASET,NIL,Download an event-based dataset:     - [The Public Event dataset](https://rpg.ifi.uzh.ch/davis_data.html)     - [The MVSEC dataset](https://daniilidis-group.github.io/mvsec/) 2.
MVSEC,mvsec,DATASET,mvsec,Download an event-based dataset:     - [The Public Event dataset](https://rpg.ifi.uzh.ch/davis_data.html)     - [The MVSEC dataset](https://daniilidis-group.github.io/mvsec/) 2.
mvsec,mvsec,DATASET,mvsec,Download an event-based dataset:     - [The Public Event dataset](https://rpg.ifi.uzh.ch/davis_data.html)     - [The MVSEC dataset](https://daniilidis-group.github.io/mvsec/) 2.
EuRoC,euroc mav,DATASET,euroc mav,You can also use a non-event-based dataset to explore the visual and inertial modes:     - [The EuRoC dataset](https://projects.asl.ethz.ch/datasets/doku.php?
EuRoC,euroc mav,DATASET,euroc mav,The original algorithm is designed for larger images of datasets like `EuRoC`.
Public Event,NIL,DATASET,NIL,If you try to run it on small images of an event-based dataset (especially the hdr_boxes of The Public Event dataset) the backend (optimizer) throws unhandled exceptions.
S3DIS,s3dis,DATASET,s3dis,"We reach SOTA results for S3DIS (74.7 mIoU  6-Fold) and on KITTI- 360 (58.3 mIoU) without requiring point colorization,  meshing, or the use of depth cameras: our full pipeline **only requires raw,  large-scale 3D point clouds and a set of images and poses**."
KITTI- 360,kitti-360,DATASET,kitti-360,"We reach SOTA results for S3DIS (74.7 mIoU  6-Fold) and on KITTI- 360 (58.3 mIoU) without requiring point colorization,  meshing, or the use of depth cameras: our full pipeline **only requires raw,  large-scale 3D point clouds and a set of images and poses**."
s3dis,s3dis,DATASET,s3dis,url=https://paperswithcode.com/badge/learning-multi-view-aggregation-in-the-wild/semantic-segmentation-on-s3dis)](https://paperswithcode.com/sota/semantic-segmentation-on-s3dis?
s3dis,s3dis,DATASET,s3dis,url=https://paperswithcode.com/badge/learning-multi-view-aggregation-in-the-wild/semantic-segmentation-on-s3dis)](https://paperswithcode.com/sota/semantic-segmentation-on-s3dis?
kitti-360,kitti-360,DATASET,kitti-360,url=https://paperswithcode.com/badge/learning-multi-view-aggregation-in-the-wild/3d-semantic-segmentation-on-kitti-360)](https://paperswithcode.com/sota/3d-semantic-segmentation-on-kitti-360?
kitti-360,kitti-360,DATASET,kitti-360,url=https://paperswithcode.com/badge/learning-multi-view-aggregation-in-the-wild/3d-semantic-segmentation-on-kitti-360)](https://paperswithcode.com/sota/3d-semantic-segmentation-on-kitti-360?
S3DIS,s3dis,DATASET,s3dis,"The most important ones can be found in the following: - `conf/data/segmentation/multimodal`: configs for the 3D+2D datasets. - `conf/models/segmentation/multimodal`: configs for the 3D+2D models. - `torch_points3d/core/data_transform/multimodal`: transforms for 3D+2D data. - `torch_points3d/core/multimodal`: multimodal data and mapping objects. - `torch_points3d/datasets/segmentation/multimodal`: 3D+2D datasets (eg S3DIS, ScanNet, KITTI360). - `torch_points3d/models/segmentation/multimodal`: 3D+2D architectures. - `torch_points3d/modules/multimodal`: 3D+2D modules."
ScanNet,scannet,DATASET,scannet,"The most important ones can be found in the following: - `conf/data/segmentation/multimodal`: configs for the 3D+2D datasets. - `conf/models/segmentation/multimodal`: configs for the 3D+2D models. - `torch_points3d/core/data_transform/multimodal`: transforms for 3D+2D data. - `torch_points3d/core/multimodal`: multimodal data and mapping objects. - `torch_points3d/datasets/segmentation/multimodal`: 3D+2D datasets (eg S3DIS, ScanNet, KITTI360). - `torch_points3d/models/segmentation/multimodal`: 3D+2D architectures. - `torch_points3d/modules/multimodal`: 3D+2D modules."
KITTI360,kitti-360,DATASET,kitti-360,"The most important ones can be found in the following: - `conf/data/segmentation/multimodal`: configs for the 3D+2D datasets. - `conf/models/segmentation/multimodal`: configs for the 3D+2D models. - `torch_points3d/core/data_transform/multimodal`: transforms for 3D+2D data. - `torch_points3d/core/multimodal`: multimodal data and mapping objects. - `torch_points3d/datasets/segmentation/multimodal`: 3D+2D datasets (eg S3DIS, ScanNet, KITTI360). - `torch_points3d/models/segmentation/multimodal`: 3D+2D architectures. - `torch_points3d/modules/multimodal`: 3D+2D modules."
kitti360,kitti-360,DATASET,kitti-360,"You can also run inference from a checkpoint and visualize predictions: - `notebooks/kitti360_visualization.ipynb` (at least **350G** of memory 💾 ) - `notebooks/s3dis_visualization.ipynb` (at least **400G** of memory 💾 ) - `notebooks/scannet_visualization.ipynb` (at least **1.3T** of memory 💾 )  Notebooks to create multimodal models, get familiar with model configuration and run forward and backward passes for debugging: - `notebooks/multimodal_model.ipynb`  Notebooks to run full inference on multimodal datasets, from a model checkpoint."
s3dis,s3dis,DATASET,s3dis,"You can also run inference from a checkpoint and visualize predictions: - `notebooks/kitti360_visualization.ipynb` (at least **350G** of memory 💾 ) - `notebooks/s3dis_visualization.ipynb` (at least **400G** of memory 💾 ) - `notebooks/scannet_visualization.ipynb` (at least **1.3T** of memory 💾 )  Notebooks to create multimodal models, get familiar with model configuration and run forward and backward passes for debugging: - `notebooks/multimodal_model.ipynb`  Notebooks to run full inference on multimodal datasets, from a model checkpoint."
scannet,scannet,DATASET,scannet,"You can also run inference from a checkpoint and visualize predictions: - `notebooks/kitti360_visualization.ipynb` (at least **350G** of memory 💾 ) - `notebooks/s3dis_visualization.ipynb` (at least **400G** of memory 💾 ) - `notebooks/scannet_visualization.ipynb` (at least **1.3T** of memory 💾 )  Notebooks to create multimodal models, get familiar with model configuration and run forward and backward passes for debugging: - `notebooks/multimodal_model.ipynb`  Notebooks to run full inference on multimodal datasets, from a model checkpoint."
kitti360,kitti-360,DATASET,kitti-360,"Those should allow you to reproduce our results by using the pretrained models in [Models](#models): - `notebooks/kitti360_inference.ipynb` - `notebooks/s3dis_inference.ipynb` - `notebooks/scannet_inference.ipynb`  Scripts to replicate our paper's best experiments 📈  for each dataset: - `scripts/train_kitti360.sh` - `scripts/train_s3dis.sh` - `scripts/train_scannet.sh`  If you need to go deeper into this project, see the [Documentation](#documentation-books) section."
s3dis,s3dis,DATASET,s3dis,"Those should allow you to reproduce our results by using the pretrained models in [Models](#models): - `notebooks/kitti360_inference.ipynb` - `notebooks/s3dis_inference.ipynb` - `notebooks/scannet_inference.ipynb`  Scripts to replicate our paper's best experiments 📈  for each dataset: - `scripts/train_kitti360.sh` - `scripts/train_s3dis.sh` - `scripts/train_scannet.sh`  If you need to go deeper into this project, see the [Documentation](#documentation-books) section."
scannet,scannet,DATASET,scannet,"Those should allow you to reproduce our results by using the pretrained models in [Models](#models): - `notebooks/kitti360_inference.ipynb` - `notebooks/s3dis_inference.ipynb` - `notebooks/scannet_inference.ipynb`  Scripts to replicate our paper's best experiments 📈  for each dataset: - `scripts/train_kitti360.sh` - `scripts/train_s3dis.sh` - `scripts/train_scannet.sh`  If you need to go deeper into this project, see the [Documentation](#documentation-books) section."
kitti360,kitti-360,DATASET,kitti-360,"Those should allow you to reproduce our results by using the pretrained models in [Models](#models): - `notebooks/kitti360_inference.ipynb` - `notebooks/s3dis_inference.ipynb` - `notebooks/scannet_inference.ipynb`  Scripts to replicate our paper's best experiments 📈  for each dataset: - `scripts/train_kitti360.sh` - `scripts/train_s3dis.sh` - `scripts/train_scannet.sh`  If you need to go deeper into this project, see the [Documentation](#documentation-books) section."
s3dis,s3dis,DATASET,s3dis,"Those should allow you to reproduce our results by using the pretrained models in [Models](#models): - `notebooks/kitti360_inference.ipynb` - `notebooks/s3dis_inference.ipynb` - `notebooks/scannet_inference.ipynb`  Scripts to replicate our paper's best experiments 📈  for each dataset: - `scripts/train_kitti360.sh` - `scripts/train_s3dis.sh` - `scripts/train_scannet.sh`  If you need to go deeper into this project, see the [Documentation](#documentation-books) section."
scannet,scannet,DATASET,scannet,"Those should allow you to reproduce our results by using the pretrained models in [Models](#models): - `notebooks/kitti360_inference.ipynb` - `notebooks/s3dis_inference.ipynb` - `notebooks/scannet_inference.ipynb`  Scripts to replicate our paper's best experiments 📈  for each dataset: - `scripts/train_kitti360.sh` - `scripts/train_s3dis.sh` - `scripts/train_scannet.sh`  If you need to go deeper into this project, see the [Documentation](#documentation-books) section."
S3DIS 6-Fold,s3dis,DATASET,s3dis,<br>  ## 🤖  Models | Model name                                            |   Dataset    |         mIoU          |  💾    |                                             👇                                              | |-------------------------------------------------------|:------------:|:---------------------:|:-----:|:------------------------------------------------------------------------------------------:| | Res16UNet34-L4-early                                  | S3DIS 6-Fold |         74.7          | 2.0G  | [link](https://drive.google.com/file/d/19SgU1f2Ny1du5fRL0d9L1721Gqi1AnsY/view?
KITTI-360,kitti-360,DATASET,kitti-360,usp=sharing) | | Res16UNet34-PointPyramid-early-cityscapes-interpolate |  KITTI-360   | 61.7 Val / 58.3 Test  | 339M  | [link](https://drive.google.com/file/d/1ucQVJ1cdzwpW6HzthaOqTR1BwTp95vrl/view?
ScanNet,scannet,DATASET,scannet,usp=sharing) | | Res16UNet34-L4-early                                  |   ScanNet    |       71.0 Val        | 341M  | [link](https://drive.google.com/file/d/1H03540psSjturqerEBJkX5B7R8s6fEba/view?
S3DIS Fold 5,s3dis,DATASET,s3dis,"/illustrations/interactive_visualization_snapshot.png""> </p>   Examples of such HTML produced on S3DIS Fold 5 are zipped [here](."
KITTI-360,kitti-360,DATASET,kitti-360,"- For datasets, some code from the official [KITTI-360](https://github.com/autonomousvision/kitti360Scripts) and [ScanNet](https://github.com/ScanNet/ScanNet)  repositories was used."
kitti360,kitti-360,DATASET,kitti-360,"- For datasets, some code from the official [KITTI-360](https://github.com/autonomousvision/kitti360Scripts) and [ScanNet](https://github.com/ScanNet/ScanNet)  repositories was used."
ScanNet,scannet,DATASET,scannet,"- For datasets, some code from the official [KITTI-360](https://github.com/autonomousvision/kitti360Scripts) and [ScanNet](https://github.com/ScanNet/ScanNet)  repositories was used."
ScanNet,scannet,DATASET,scannet,"- For datasets, some code from the official [KITTI-360](https://github.com/autonomousvision/kitti360Scripts) and [ScanNet](https://github.com/ScanNet/ScanNet)  repositories was used."
ScanNet,scannet,DATASET,scannet,"- For datasets, some code from the official [KITTI-360](https://github.com/autonomousvision/kitti360Scripts) and [ScanNet](https://github.com/ScanNet/ScanNet)  repositories was used."
RumexLeaves,NIL,DATASET,NIL,The underlying dataset \textit{RumexLeaves} is made publicly available and is the first of its kind with keypoint-guided polyline annotations leading along the line from the lowest stem point along the leaf basal to the leaf apex.
RumexLeaves,NIL,DATASET,NIL,__Sources__: * [RumexLeaves Website](https://dtu-pas.github.io/RumexLeaves/) * [Publication](https://ieeexplore.ieee.org/document/10373101) * [Arxiv](https://arxiv.org/abs/2312.08805) * [Dataset](https://data.dtu.dk/articles/dataset/_strong_RumexLeaves_Dataset_introduced_by_Paper_Fine-grained_Leaf_Analysis_for_Efficient_Weeding_Robots_strong_/23659524)  ## Getting started locally 1.
RumexLeaves,NIL,DATASET,NIL,__Sources__: * [RumexLeaves Website](https://dtu-pas.github.io/RumexLeaves/) * [Publication](https://ieeexplore.ieee.org/document/10373101) * [Arxiv](https://arxiv.org/abs/2312.08805) * [Dataset](https://data.dtu.dk/articles/dataset/_strong_RumexLeaves_Dataset_introduced_by_Paper_Fine-grained_Leaf_Analysis_for_Efficient_Weeding_Robots_strong_/23659524)  ## Getting started locally 1.
RumexLeaves,NIL,DATASET,NIL,__Sources__: * [RumexLeaves Website](https://dtu-pas.github.io/RumexLeaves/) * [Publication](https://ieeexplore.ieee.org/document/10373101) * [Arxiv](https://arxiv.org/abs/2312.08805) * [Dataset](https://data.dtu.dk/articles/dataset/_strong_RumexLeaves_Dataset_introduced_by_Paper_Fine-grained_Leaf_Analysis_for_Efficient_Weeding_Robots_strong_/23659524)  ## Getting started locally 1.
rumexleaves,NIL,DATASET,NIL,Visualize example images with annotations     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/visualizations/visualize_data.py     ``` 4.
rumexleaves,NIL,DATASET,NIL,Visualize example images with annotations     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/visualizations/visualize_data.py     ``` 4.
rumexleaves,NIL,DATASET,NIL,Run example inference     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/inference.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth -img data/processed/RumexLeaves/iNaturalist/4150.jpg     ``` 6.
rumexleaves,NIL,DATASET,NIL,Run example inference     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/inference.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth -img data/processed/RumexLeaves/iNaturalist/4150.jpg     ``` 6.
RumexLeaves,NIL,DATASET,NIL,Run example inference     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/inference.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth -img data/processed/RumexLeaves/iNaturalist/4150.jpg     ``` 6.
iNaturalist,inaturalist,DATASET,inaturalist,Run validation of final model on iNaturalist data     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/evaluate.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth     ``` 7.
rumexleaves,NIL,DATASET,NIL,Run validation of final model on iNaturalist data     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/evaluate.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth     ``` 7.
rumexleaves,NIL,DATASET,NIL,Run validation of final model on iNaturalist data     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/evaluate.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth     ``` 7.
rumexleaves,NIL,DATASET,NIL,Training final model from scratch     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/train.py --exp_file exp_files/train_final_model.py     ```  ## Getting started with Docker 1.
rumexleaves,NIL,DATASET,NIL,Training final model from scratch     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/train.py --exp_file exp_files/train_final_model.py     ```  ## Getting started with Docker 1.
football,NIL,DATASET,NIL,"The `util/` directory contains some codes to parse specific datasets and generate queries.        ### Dataset Structure ---------------- The graphs and query/answer sets can be in any format but in order to use our train script and dataset utilities these should be in following format.        ### File Structure ---------------- Our training script assume that the data stored in the following format: ``` data/     dataset_name/         graph_{snapshot_id}.txt         graph_{snapshot_id}.txt         queries_{snapshot_id}.txt         queries_{snapshot_id}.txt ```  So for example for a a dataset named `foo` with 3 snapshots it would be like:  ``` data/     foo/         graph_1.txt         graph_2.txt         graph_3.txt         queries_1.txt         queries_2.txt         queries_3.txt         test_queries.txt         valid_queries.txt ```        ### Graph file and QueryFiles ---------------- The graph files must have the following format.  ``` node_id1 node_id2 .. .. ```  The query files must have the following format.   ``` q1_node1,q1_node2,... answer1_node1,answer1_node2,... q2_node1,q2_node2,... answer2_node1,answer2_node2 ```         ### Training the Models & Execution ---------------- Finally the models could be trained and evaluated with the following command.   ``` python3 train.py <DATASET_NAME> <MAX_VERTICES> <START_SNAPSHOT_ID> <END_SNAPSHOT_ID> <THRESHOLD> <HIDDEN_DIM_SIZE> <EPOCHS> <LEARNING_RATE> <REGULARIZATION> ```  For example: ``` python3 train.py football 200 1 4 0.4 64 100 0.001 0.00001 ```   ### Reference ----------------   ``` @inproceedings{CS-TGN, author = {Hashemi, Farnoosh and Behrouz, Ali and Rezaei Hajidehi, Milad}, title = {CS-TGN: Community Search via Temporal Graph Neural Networks}, year = {2023}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3543873.3587654}, doi = {10.1145/3543873.3587654}, booktitle = {Companion Proceedings of the Web Conference 2023}, numpages = {8}, location = {AUSTIN, TEXAS, USA}, series = {WWW '23} } ```"
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,[PyPI](https://img.shields.io/pypi/v/sc2-datasets?
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,style=flat-square)](https://pypi.org/project/sc2-datasets/) [!
StarCraft II,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,[Python](https://img.shields.io/badge/python-3.10%5E-blue)](https://www.python.org/)  # StarCraft II Datasets  Library can be used to interface with datasets that were pre-processed with our pipeline as described in: - [SC2DatasetPreparator](https://github.com/Kaszanas/SC2DatasetPreparator)  Currently we have exposed PyTorch and PyTorch Lightning API.
SC2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,[Python](https://img.shields.io/badge/python-3.10%5E-blue)](https://www.python.org/)  # StarCraft II Datasets  Library can be used to interface with datasets that were pre-processed with our pipeline as described in: - [SC2DatasetPreparator](https://github.com/Kaszanas/SC2DatasetPreparator)  Currently we have exposed PyTorch and PyTorch Lightning API.
SC2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,[Python](https://img.shields.io/badge/python-3.10%5E-blue)](https://www.python.org/)  # StarCraft II Datasets  Library can be used to interface with datasets that were pre-processed with our pipeline as described in: - [SC2DatasetPreparator](https://github.com/Kaszanas/SC2DatasetPreparator)  Currently we have exposed PyTorch and PyTorch Lightning API.
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"Please refer to the [**official documentation**](https://sc2-datasets.readthedocs.io/), or contact contributors directly for all of the details.  ## Supported Datasets  ### SC2EGSet: StarCraft II Esport Game State Dataset  This project contains official API implementation for the [SC2EGSet: StarCraft II Esport Game State Dataset](https://doi.org/10.5281/zenodo.5503997), which is built based on [SC2ReSet: StarCraft II Esport Replaypack Set](https://doi.org/10.5281/zenodo.5575796)."
SC2EGSet,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"Please refer to the [**official documentation**](https://sc2-datasets.readthedocs.io/), or contact contributors directly for all of the details.  ## Supported Datasets  ### SC2EGSet: StarCraft II Esport Game State Dataset  This project contains official API implementation for the [SC2EGSet: StarCraft II Esport Game State Dataset](https://doi.org/10.5281/zenodo.5503997), which is built based on [SC2ReSet: StarCraft II Esport Replaypack Set](https://doi.org/10.5281/zenodo.5575796)."
StarCraft II Esport Game State Dataset,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"Please refer to the [**official documentation**](https://sc2-datasets.readthedocs.io/), or contact contributors directly for all of the details.  ## Supported Datasets  ### SC2EGSet: StarCraft II Esport Game State Dataset  This project contains official API implementation for the [SC2EGSet: StarCraft II Esport Game State Dataset](https://doi.org/10.5281/zenodo.5503997), which is built based on [SC2ReSet: StarCraft II Esport Replaypack Set](https://doi.org/10.5281/zenodo.5575796)."
SC2EGSet,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"Please refer to the [**official documentation**](https://sc2-datasets.readthedocs.io/), or contact contributors directly for all of the details.  ## Supported Datasets  ### SC2EGSet: StarCraft II Esport Game State Dataset  This project contains official API implementation for the [SC2EGSet: StarCraft II Esport Game State Dataset](https://doi.org/10.5281/zenodo.5503997), which is built based on [SC2ReSet: StarCraft II Esport Replaypack Set](https://doi.org/10.5281/zenodo.5575796)."
StarCraft II Esport Game State Dataset,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"Please refer to the [**official documentation**](https://sc2-datasets.readthedocs.io/), or contact contributors directly for all of the details.  ## Supported Datasets  ### SC2EGSet: StarCraft II Esport Game State Dataset  This project contains official API implementation for the [SC2EGSet: StarCraft II Esport Game State Dataset](https://doi.org/10.5281/zenodo.5503997), which is built based on [SC2ReSet: StarCraft II Esport Replaypack Set](https://doi.org/10.5281/zenodo.5575796)."
StarCraft II,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,Contents of this library provide PyTorch and PyTorch Lightning API for pre-processed StarCraft II dataset.  ## Installation  1.
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,Perform the following command:  ```bash $ pip install sc2_datasets ```  ## Usage  Basic example usage can be seen below.
SC2EGSet,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir=""."
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir=""."
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir=""."
SC2EGSet,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir=""."
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir=""."
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir=""."
SC2EGSet,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir=""."
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir=""."
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir=""."
SC2EGSet,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir=""."
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir=""."
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir=""."
SC2EGSet,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir=""."
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir=""."
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir=""."
SC2EGSet,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir=""."
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"download=True,                 replaypacks=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.             )     sc2_egset_datamodule.prepare_data()     sc2_egset_datamodule.setup() ```  ## Contributing  Interested in contributing?"
sc2,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"download=True,                 replaypacks=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.             )     sc2_egset_datamodule.prepare_data()     sc2_egset_datamodule.setup() ```  ## Contributing  Interested in contributing?"
sc2egset,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"By contributing to this project, you agree to abide by its terms.  ## License  `sc2egset_dataset` was created by Andrzej Białecki."
SC2ReSet,sc2reset: starcraft ii esport replaypack set,DATASET,sc2reset: starcraft ii esport replaypack set,"It is licensed under the terms of the GNU General Public License v3.0 license.  ## Cite  ### [Dataset Description Article](https://www.researchgate.net/publication/373767449_SC2EGSet_StarCraft_II_Esport_Replay_and_Game-state_Dataset)  To cite the article that introduces [SC2ReSet](https://doi.org/10.5281/zenodo.5575796) and [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) use this:  ```bibtex @article{Białecki2023,   author   = {Bia{\l}ecki, Andrzej               and Jakubowska, Natalia               and Dobrowolski, Pawe{\l}               and Bia{\l}ecki, Piotr               and Krupi{\'{n}}ski, Leszek               and Szczap, Andrzej               and Bia{\l}ecki, Robert               and Gajewski, Jan},   title    = {SC2EGSet: StarCraft II Esport Replay and Game-state Dataset},   journal  = {Scientific Data},   year     = {2023},   month    = {Sep},   day      = {08},   volume   = {10},   number   = {1},   pages    = {600},   issn     = {2052-4463},   doi      = {10.1038/s41597-023-02510-7},   url      = {https://doi.org/10.1038/s41597-023-02510-7} } ```"
SC2EGSet,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"It is licensed under the terms of the GNU General Public License v3.0 license.  ## Cite  ### [Dataset Description Article](https://www.researchgate.net/publication/373767449_SC2EGSet_StarCraft_II_Esport_Replay_and_Game-state_Dataset)  To cite the article that introduces [SC2ReSet](https://doi.org/10.5281/zenodo.5575796) and [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) use this:  ```bibtex @article{Białecki2023,   author   = {Bia{\l}ecki, Andrzej               and Jakubowska, Natalia               and Dobrowolski, Pawe{\l}               and Bia{\l}ecki, Piotr               and Krupi{\'{n}}ski, Leszek               and Szczap, Andrzej               and Bia{\l}ecki, Robert               and Gajewski, Jan},   title    = {SC2EGSet: StarCraft II Esport Replay and Game-state Dataset},   journal  = {Scientific Data},   year     = {2023},   month    = {Sep},   day      = {08},   volume   = {10},   number   = {1},   pages    = {600},   issn     = {2052-4463},   doi      = {10.1038/s41597-023-02510-7},   url      = {https://doi.org/10.1038/s41597-023-02510-7} } ```"
SC2EGSet,sc2egset: starcraft ii esport game state dataset,DATASET,sc2egset: starcraft ii esport game state dataset,"It is licensed under the terms of the GNU General Public License v3.0 license.  ## Cite  ### [Dataset Description Article](https://www.researchgate.net/publication/373767449_SC2EGSet_StarCraft_II_Esport_Replay_and_Game-state_Dataset)  To cite the article that introduces [SC2ReSet](https://doi.org/10.5281/zenodo.5575796) and [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) use this:  ```bibtex @article{Białecki2023,   author   = {Bia{\l}ecki, Andrzej               and Jakubowska, Natalia               and Dobrowolski, Pawe{\l}               and Bia{\l}ecki, Piotr               and Krupi{\'{n}}ski, Leszek               and Szczap, Andrzej               and Bia{\l}ecki, Robert               and Gajewski, Jan},   title    = {SC2EGSet: StarCraft II Esport Replay and Game-state Dataset},   journal  = {Scientific Data},   year     = {2023},   month    = {Sep},   day      = {08},   volume   = {10},   number   = {1},   pages    = {600},   issn     = {2052-4463},   doi      = {10.1038/s41597-023-02510-7},   url      = {https://doi.org/10.1038/s41597-023-02510-7} } ```"
EuRoC,euroc mav,DATASET,euroc mav,We provide examples to run the SLAM system in the [EuRoC dataset](https://projects.asl.ethz.ch/datasets/doku.php?
EuRoC,euroc mav,DATASET,euroc mav,"If no distortion coefficients are provided, they are assumed to be zero.  ### 3.2 EuRoC Example  1."
EuRoC,euroc mav,DATASET,euroc mav,"The specific `<TIMESTAMPS_FILE>`, `<CALIB_FILE>` and `<SETTINGS_FILE>` for the EuRoC dataset are provided in `Examples/EurocData`.  ### 3.3 Video Example It is also possible to run your own custom videos with known camera calibration."
Euroc,euroc mav,DATASET,euroc mav,"The specific `<TIMESTAMPS_FILE>`, `<CALIB_FILE>` and `<SETTINGS_FILE>` for the EuRoC dataset are provided in `Examples/EurocData`.  ### 3.3 Video Example It is also possible to run your own custom videos with known camera calibration."
RuleAlign,NIL,DATASET,NIL,"<div align=""center""> <h2 align=""center"">    <b>RuleAlign Dataset</b> </h2> <div> <a target=""_blank"" href=""https://scholar.google.com.sg/citations?"
RuleEval,NIL,DATASET,NIL,"user=GqZfs_IAAAAJ&hl=en"">Fangyuan&#160;Yu</a><sup>1 2</sup> </div> <sup>1</sup>Temus&#160&#160&#160</span> <sup>2</sup>Stanford University</span> <br /> <sup>&#9993&#160;</sup>Corresponding author&#160;&#160;</span> <br/> <br/> <div align=""center"">     <a href=""https://arxiv.org/abs/2408.16667"" target=""_blank""> </div> </div> <h3 align=""center""> <b>:fire: Code will be released soon</b> </h3>  ## :books: RuleEval Dataset  RuleAlign is a dataset designed to evaluate rule-based alignment of language models."
RuleAlign,NIL,DATASET,NIL,"user=GqZfs_IAAAAJ&hl=en"">Fangyuan&#160;Yu</a><sup>1 2</sup> </div> <sup>1</sup>Temus&#160&#160&#160</span> <sup>2</sup>Stanford University</span> <br /> <sup>&#9993&#160;</sup>Corresponding author&#160;&#160;</span> <br/> <br/> <div align=""center"">     <a href=""https://arxiv.org/abs/2408.16667"" target=""_blank""> </div> </div> <h3 align=""center""> <b>:fire: Code will be released soon</b> </h3>  ## :books: RuleEval Dataset  RuleAlign is a dataset designed to evaluate rule-based alignment of language models."
RuleAlign,NIL,DATASET,NIL,"It accompanies the paper ""Iterative Graph Reasoning"" and contains:  - 5 distinct rule-based scenarios - 200 training queries per scenario (low-data learning scenario) - 100 test queries per scenario - Weak annotations provided for each query  ## :new: Updates - [08/2024] [Arxiv paper](https://arxiv.org/abs/2408.16667) released. - [08/2024] RuleAlign dataset announced.  ## :gear: Evaluation  To run the evaluation on the RuleAlign dataset, use the following command:  ```shell python -m script.eval --model_name xxx ```  Replace `xxx` with the name of the model you want to evaluate.  ## :hugs: Citation If you find this dataset useful for your research, please kindly cite our paper:  ``` @misc{yu2024iterativegraphreasoning,       title={Iterative Graph Reasoning},        author={Fangyuan Yu},       year={2024},       eprint={2408.03615},       archivePrefix={arXiv},       primaryClass={cs.AI} } ```  ## :mailbox: Contact  For any questions or issues regarding the RuleEval dataset, please contact the corresponding author."
RuleAlign,NIL,DATASET,NIL,"It accompanies the paper ""Iterative Graph Reasoning"" and contains:  - 5 distinct rule-based scenarios - 200 training queries per scenario (low-data learning scenario) - 100 test queries per scenario - Weak annotations provided for each query  ## :new: Updates - [08/2024] [Arxiv paper](https://arxiv.org/abs/2408.16667) released. - [08/2024] RuleAlign dataset announced.  ## :gear: Evaluation  To run the evaluation on the RuleAlign dataset, use the following command:  ```shell python -m script.eval --model_name xxx ```  Replace `xxx` with the name of the model you want to evaluate.  ## :hugs: Citation If you find this dataset useful for your research, please kindly cite our paper:  ``` @misc{yu2024iterativegraphreasoning,       title={Iterative Graph Reasoning},        author={Fangyuan Yu},       year={2024},       eprint={2408.03615},       archivePrefix={arXiv},       primaryClass={cs.AI} } ```  ## :mailbox: Contact  For any questions or issues regarding the RuleEval dataset, please contact the corresponding author."
RuleEval,NIL,DATASET,NIL,"It accompanies the paper ""Iterative Graph Reasoning"" and contains:  - 5 distinct rule-based scenarios - 200 training queries per scenario (low-data learning scenario) - 100 test queries per scenario - Weak annotations provided for each query  ## :new: Updates - [08/2024] [Arxiv paper](https://arxiv.org/abs/2408.16667) released. - [08/2024] RuleAlign dataset announced.  ## :gear: Evaluation  To run the evaluation on the RuleAlign dataset, use the following command:  ```shell python -m script.eval --model_name xxx ```  Replace `xxx` with the name of the model you want to evaluate.  ## :hugs: Citation If you find this dataset useful for your research, please kindly cite our paper:  ``` @misc{yu2024iterativegraphreasoning,       title={Iterative Graph Reasoning},        author={Fangyuan Yu},       year={2024},       eprint={2408.03615},       archivePrefix={arXiv},       primaryClass={cs.AI} } ```  ## :mailbox: Contact  For any questions or issues regarding the RuleEval dataset, please contact the corresponding author."
BD Ortho,NIL,DATASET,NIL,"The following datasets have been used to extract the different layers: * BD Ortho for the satellite images * BD Foret v2 for vegetation data * BD Topo for buildings and roads  Important: note that the *data precision is 50cm per pixel.*  Initially, lots of classes were present in the dataset."
BD Foret v2,NIL,DATASET,NIL,"The following datasets have been used to extract the different layers: * BD Ortho for the satellite images * BD Foret v2 for vegetation data * BD Topo for buildings and roads  Important: note that the *data precision is 50cm per pixel.*  Initially, lots of classes were present in the dataset."
BD Topo,NIL,DATASET,NIL,"The following datasets have been used to extract the different layers: * BD Ortho for the satellite images * BD Foret v2 for vegetation data * BD Topo for buildings and roads  Important: note that the *data precision is 50cm per pixel.*  Initially, lots of classes were present in the dataset."
ImageNet-22K,NIL,DATASET,NIL,"usp=sharing).  ``` # single-gpu testing python tools/test.py <CONFIG_FILE> <SEG_CHECKPOINT_FILE> --eval mIoU  # multi-gpu testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --eval mIoU  # multi-gpu, multi-scale testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --aug-test --eval mIoU ```  Example on the Ampli ANR project:   ``` # Evaluate checkpoint on a single GPU python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --eval mIoU  # Display segmentation results python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --show ```  ### Training  To train with pre-trained models, run: ``` # single-gpu training python tools/train.py <CONFIG_FILE> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  # multi-gpu training tools/dist_train.sh <CONFIG_FILE> <GPU_NUM> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  ```  Example on the Ampli ANR project with the ImageNet-22K pretrained model (available [here](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth)) :   ``` python tools/train.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py --options model.pretrained=""."
BouroujeniEtAl,NIL,DATASET,NIL,"Extract relevant features sets (`BouroujeniEtAl`, `MarrasEtAl`, `LalleConati`, and `ChenCui`) through our lab's EDM 2021 contribution on [benchmarks for feature predictive power](https://github.com/epfl-ml4ed/flipped-classroom)."
MarrasEtAl,NIL,DATASET,NIL,"Extract relevant features sets (`BouroujeniEtAl`, `MarrasEtAl`, `LalleConati`, and `ChenCui`) through our lab's EDM 2021 contribution on [benchmarks for feature predictive power](https://github.com/epfl-ml4ed/flipped-classroom)."
LalleConati,NIL,DATASET,NIL,"Extract relevant features sets (`BouroujeniEtAl`, `MarrasEtAl`, `LalleConati`, and `ChenCui`) through our lab's EDM 2021 contribution on [benchmarks for feature predictive power](https://github.com/epfl-ml4ed/flipped-classroom)."
ChenCui,NIL,DATASET,NIL,"Extract relevant features sets (`BouroujeniEtAl`, `MarrasEtAl`, `LalleConati`, and `ChenCui`) through our lab's EDM 2021 contribution on [benchmarks for feature predictive power](https://github.com/epfl-ml4ed/flipped-classroom)."
PRW,prw,DATASET,prw,"# or ""python setup.py develop"" pip install sklearn ```  - If you have the problem ```local variable 'beta1' referenced before assignment``` with PyTorch1.8, add one table space in L110 of [```optim/adamw.py```](https://github.com/pytorch/pytorch/issues/55740)  ## Train and Inference   #####  Datasets and Annotations  - Download [PRW](https://github.com/liangzheng06/PRW-baseline) and [CUHK-SYSU](https://github.com/ShuangLI59/person_search) datasets. - Download the [json annotations](https://drive.google.com/file/d/1J2YAU7n954TiSwqopJCWdK25IaF6Mb9_/view?"
PRW,prw,DATASET,prw,"# or ""python setup.py develop"" pip install sklearn ```  - If you have the problem ```local variable 'beta1' referenced before assignment``` with PyTorch1.8, add one table space in L110 of [```optim/adamw.py```](https://github.com/pytorch/pytorch/issues/55740)  ## Train and Inference   #####  Datasets and Annotations  - Download [PRW](https://github.com/liangzheng06/PRW-baseline) and [CUHK-SYSU](https://github.com/ShuangLI59/person_search) datasets. - Download the [json annotations](https://drive.google.com/file/d/1J2YAU7n954TiSwqopJCWdK25IaF6Mb9_/view?"
CUHK-SYSU,cuhk-sysu,DATASET,cuhk-sysu,"# or ""python setup.py develop"" pip install sklearn ```  - If you have the problem ```local variable 'beta1' referenced before assignment``` with PyTorch1.8, add one table space in L110 of [```optim/adamw.py```](https://github.com/pytorch/pytorch/issues/55740)  ## Train and Inference   #####  Datasets and Annotations  - Download [PRW](https://github.com/liangzheng06/PRW-baseline) and [CUHK-SYSU](https://github.com/ShuangLI59/person_search) datasets. - Download the [json annotations](https://drive.google.com/file/d/1J2YAU7n954TiSwqopJCWdK25IaF6Mb9_/view?"
json annotations,NIL,DATASET,NIL,"# or ""python setup.py develop"" pip install sklearn ```  - If you have the problem ```local variable 'beta1' referenced before assignment``` with PyTorch1.8, add one table space in L110 of [```optim/adamw.py```](https://github.com/pytorch/pytorch/issues/55740)  ## Train and Inference   #####  Datasets and Annotations  - Download [PRW](https://github.com/liangzheng06/PRW-baseline) and [CUHK-SYSU](https://github.com/ShuangLI59/person_search) datasets. - Download the [json annotations](https://drive.google.com/file/d/1J2YAU7n954TiSwqopJCWdK25IaF6Mb9_/view?"
PRW,prw,DATASET,prw,"usp=sharing) provided by [AlignPS](https://github.com/daodaofr/AlignPS).   #####  Train with a single GPU ```shell python tools/train.py ${CONFIG_FILE} --no-validate ``` - CONFIG_FILE about PSTR is in [configs/PSTR](configs/pstr)  #####  Test with a single GPU  ```shell PRW: sh run_test_prw.sh  CUHK: sh run_test_cuhk.sh   ```  - If you want to output the results of different models, please  change CONFIGPATH, MODELPATH, OUTPATH for diffferent models   ## Results  We provide some models with different backbones and results on PRW and CUHK-SYSU datasets, which have a little difference to CVPR version due to jitter"
prw,prw,DATASET,prw,"usp=sharing) provided by [AlignPS](https://github.com/daodaofr/AlignPS).   #####  Train with a single GPU ```shell python tools/train.py ${CONFIG_FILE} --no-validate ``` - CONFIG_FILE about PSTR is in [configs/PSTR](configs/pstr)  #####  Test with a single GPU  ```shell PRW: sh run_test_prw.sh  CUHK: sh run_test_cuhk.sh   ```  - If you want to output the results of different models, please  change CONFIGPATH, MODELPATH, OUTPATH for diffferent models   ## Results  We provide some models with different backbones and results on PRW and CUHK-SYSU datasets, which have a little difference to CVPR version due to jitter"
PRW,prw,DATASET,prw,"usp=sharing) provided by [AlignPS](https://github.com/daodaofr/AlignPS).   #####  Train with a single GPU ```shell python tools/train.py ${CONFIG_FILE} --no-validate ``` - CONFIG_FILE about PSTR is in [configs/PSTR](configs/pstr)  #####  Test with a single GPU  ```shell PRW: sh run_test_prw.sh  CUHK: sh run_test_cuhk.sh   ```  - If you want to output the results of different models, please  change CONFIGPATH, MODELPATH, OUTPATH for diffferent models   ## Results  We provide some models with different backbones and results on PRW and CUHK-SYSU datasets, which have a little difference to CVPR version due to jitter"
CUHK-SYSU,cuhk-sysu,DATASET,cuhk-sysu,"usp=sharing) provided by [AlignPS](https://github.com/daodaofr/AlignPS).   #####  Train with a single GPU ```shell python tools/train.py ${CONFIG_FILE} --no-validate ``` - CONFIG_FILE about PSTR is in [configs/PSTR](configs/pstr)  #####  Test with a single GPU  ```shell PRW: sh run_test_prw.sh  CUHK: sh run_test_cuhk.sh   ```  - If you want to output the results of different models, please  change CONFIGPATH, MODELPATH, OUTPATH for diffferent models   ## Results  We provide some models with different backbones and results on PRW and CUHK-SYSU datasets, which have a little difference to CVPR version due to jitter"
PRW,prw,DATASET,prw,|    name  | dataset  | backbone |  mAP  | top-1 |  mAP+ | top-1+  | download| | :-------------: | :-----: | :-----: | :-------------------: | :-----: | :-----: | :------: | :-----------------: | |     PSTR | PRW    | PVTv2-B2  |   57.46  |   90.57   |58.07   |    92.03     |          [model](https://drive.google.com/file/d/1hrmyvS9f8fzflpoIlEhWQ-XDyNp_qCGq/view?
PRW,prw,DATASET,prw,usp=sharing)         | |     PSTR |  PRW   | ResNet50  |   50.03   | 88.04   | 50.64   |    89.94   |        [model](https://drive.google.com/file/d/12j71smXyc3QAyCvIPRlQCbyhSXZENBIX/view?
PRW,prw,DATASET,prw,usp=sharing)         | |     PSTR |  PRW   | ResNet50-DCN  |   51.09   | 88.33   | 51.62   |    90.13   |        [model](https://drive.google.com/file/d/111f_efZOYMFkz9i76TgqcO7a88npoJV5/view?
CUHK-SYSU,cuhk-sysu,DATASET,cuhk-sysu,usp=sharing)         | |     PSTR | CUHK-SYSU     | PVTv2-B2    |   95.31  |   96.28   |95.78   |    96.83      |       [model](https://drive.google.com/file/d/1vrQdZTVgJ2D6ty_XJAYmsJgziW9TZHHW/view?
CUHK-SYSU,cuhk-sysu,DATASET,cuhk-sysu,usp=sharing)         | |     PSTR | CUHK-SYSU    | ResNet50|   93.55   | 94.93   | 94.16   | 95.48   |          [model](https://drive.google.com/file/d/1U4r_WaTfODmuhslL_15u5bXdFwLLBC5m/view?
CUHK-SYSU,cuhk-sysu,DATASET,cuhk-sysu,usp=sharing)         | |     PSTR | CUHK-SYSU    | ResNet50-DCN|   94.22   | 95.28   | 94.90   | 95.97   |          [model](https://drive.google.com/file/d/1cCbpAGrldxQaRrF7FCZXqx4VaNP-C278/view?
BLEBeacon,blebeacon,DATASET,blebeacon,# BLEBeacon Dataset The BLEBeacon dataset is a collection of Bluetooth Low Energy (BLE)  advertisement packets/traces generated from BLE beacons carried by people following their daily routine inside a university building for a whole month.
BLEBeacon,blebeacon,DATASET,blebeacon,# BLEBeacon Dataset The BLEBeacon dataset is a collection of Bluetooth Low Energy (BLE)  advertisement packets/traces generated from BLE beacons carried by people following their daily routine inside a university building for a whole month.
BLEBeacon,blebeacon,DATASET,blebeacon,"Since 2019, the BLEBeacon dataset is part of the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD): https://crawdad.org/unm/blebeacon/20190312/  To cite the dataset:  > Dimitrios Sikeridis, Ioannis Papapanagiotou, Michael Devetsikiotis, CRAWDAD dataset unm/blebeacon (v. 2019‑03‑12), downloaded from https://crawdad.org/unm/blebeacon/20190312, Mar 2019"
CRAWDAD,NIL,DATASET,NIL,"Since 2019, the BLEBeacon dataset is part of the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD): https://crawdad.org/unm/blebeacon/20190312/  To cite the dataset:  > Dimitrios Sikeridis, Ioannis Papapanagiotou, Michael Devetsikiotis, CRAWDAD dataset unm/blebeacon (v. 2019‑03‑12), downloaded from https://crawdad.org/unm/blebeacon/20190312, Mar 2019"
blebeacon,blebeacon,DATASET,blebeacon,"Since 2019, the BLEBeacon dataset is part of the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD): https://crawdad.org/unm/blebeacon/20190312/  To cite the dataset:  > Dimitrios Sikeridis, Ioannis Papapanagiotou, Michael Devetsikiotis, CRAWDAD dataset unm/blebeacon (v. 2019‑03‑12), downloaded from https://crawdad.org/unm/blebeacon/20190312, Mar 2019"
blebeacon,blebeacon,DATASET,blebeacon,"Since 2019, the BLEBeacon dataset is part of the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD): https://crawdad.org/unm/blebeacon/20190312/  To cite the dataset:  > Dimitrios Sikeridis, Ioannis Papapanagiotou, Michael Devetsikiotis, CRAWDAD dataset unm/blebeacon (v. 2019‑03‑12), downloaded from https://crawdad.org/unm/blebeacon/20190312, Mar 2019"
blebeacon,blebeacon,DATASET,blebeacon,"Since 2019, the BLEBeacon dataset is part of the Community Resource for Archiving Wireless Data At Dartmouth (CRAWDAD): https://crawdad.org/unm/blebeacon/20190312/  To cite the dataset:  > Dimitrios Sikeridis, Ioannis Papapanagiotou, Michael Devetsikiotis, CRAWDAD dataset unm/blebeacon (v. 2019‑03‑12), downloaded from https://crawdad.org/unm/blebeacon/20190312, Mar 2019"
blebeacon,blebeacon,DATASET,blebeacon,> [Cite using BibTeX](https://crawdad.org/unm/blebeacon/20190312/)    # Technical Summary  ## Operation     Users carried off-the-shelf Gimbal Series 10 iBeacons that continuously transmit BLE advertisement packets.
BLEBeacon,blebeacon,DATASET,blebeacon,"[RSSI](https://github.com/dimisik/BLEBeacon-Dataset/blob/master/images/ARCH.png) * RSSI Report: all advertisement packet receptions from beacon devices are directly reported to a server with a message that contains the beacon/user ID, the packet's Received Signal Strength Indicator (RSSI), a reception timestamp, and finally the ID of the RPi that received the advertisement (Fig. 1).   !"
BLEBeacon,blebeacon,DATASET,blebeacon,[Check](https://github.com/dimisik/BLEBeacon-Dataset/blob/master/images/check.png) * Check-In/Check-Out Report: each RPi scanner continuously manages a list of current occupants/users in its proximity.
BLEBeacon,blebeacon,DATASET,blebeacon,"A thirty-second period is used to ensure that the occupant exited the RPi proximity.   ## Dataset  The BLEBeacon dataset contains two files, one with the trial readings from the RSSI report operation (RSSI Report file) and the other from the Check-In/Check-Out report operation (Check-In Check-Out Report file)."
BLEBeacon,blebeacon,DATASET,blebeacon,"# Readme  Further information and a detailed description of the sensing infrastructure setup, the real-subject trial, and the BLEBeacon dataset can be found in: D."
BLEBeacon,blebeacon,DATASET,blebeacon,"Devetsikiotis,  [""BLEBeacon: A Real-Subject Trial Dataset from Mobile Bluetooth Low Energy Beacons""](https://arxiv.org/abs/1802.08782), arXiv preprint arXiv:1802.08782, 2018.   ### Research Papers   Publications related to the BLEBeacon dataset:  * D."
BLEBeacon,blebeacon,DATASET,blebeacon,"Devetsikiotis,  [""BLEBeacon: A Real-Subject Trial Dataset from Mobile Bluetooth Low Energy Beacons""](https://arxiv.org/abs/1802.08782), arXiv preprint arXiv:1802.08782, 2018.   ### Research Papers   Publications related to the BLEBeacon dataset:  * D."
Flickr30K,flickr30k,DATASET,flickr30k,"For image-text retrieval on Flickr30K and MSCOCO, we compute IR@1 and TR@1 for the Recall@1 on image-retrieval (IR) and text-retrieval (TR)."
MSCOCO,ms coco,DATASET,ms coco,"For image-text retrieval on Flickr30K and MSCOCO, we compute IR@1 and TR@1 for the Recall@1 on image-retrieval (IR) and text-retrieval (TR)."
AlpacaEval,alpacaeval,DATASET,alpacaeval,"We present the results on three different model sizes in the table below, including the training times of TempNet on Nvidia A100-80G GPUs and their win rates on AlpacaEval data."
AlpacaEval,alpacaeval,DATASET,alpacaeval,"<div align=""center"">   <img src=""images/exp4.jpg"" width=""40%""/> </div>  Here, we reveal why TempNet enhances performance by comparing the performances of LLaMA2 7B Chat (with the default $\tau=0.7$) and LLaMA2 7B Chat + TempNet on the AlpacaEval dataset."
sst2,sst-2,DATASET,sst-2,The results are saved at `data/sst2/filter_train_nonewline.json` and `data/sst2/filter_val_nonewline.json`.  ``` python build_classifier_data_for_sst2.py ```  3.
sst2,sst-2,DATASET,sst-2,The results are saved at `data/sst2/filter_train_nonewline.json` and `data/sst2/filter_val_nonewline.json`.  ``` python build_classifier_data_for_sst2.py ```  3.
sst2,sst-2,DATASET,sst-2,The results are saved at `data/sst2/filter_train_nonewline.json` and `data/sst2/filter_val_nonewline.json`.  ``` python build_classifier_data_for_sst2.py ```  3.
sst2,sst-2,DATASET,sst-2,The logits are saved at `dp_finetuning/sst2/domain_classifier_output`.
SST-2,sst-2,DATASET,sst-2,"**In this simple implementation, we directly select natural sentences instead of fixed-length sequences.** We found that this simple implementation is enough to achieve good performance on SST-2.   ``` cd .."
sst-2,sst-2,DATASET,sst-2,Private fine-tuning on sst-2.
Meta-Reasoning-MATH,NIL,DATASET,NIL,"/codes/examples.py --model_name_or_path GAIR/ReasonEval-7B # Specify the model name or path here --model_size 7B # Indicate the model size of ReasonEval (7B or 34B) ```  ## Meta Evaluation ### Datasets The datasets for meta-evaluations are composed of three parts:  - **Meta-Reasoning-MATH**: This dataset is constructed as follows:    - To collect the first type of errors affecting the correctness of steps, we recruit undergraduates who have a solid mathematical background to label solutions generated by Abel and WizardMath."
mr-math_invalid_errors,NIL,DATASET,NIL,/dataset/mr-math_invalid_errors.json     {       // Instance ID.
PRM800K,prm800k,DATASET,prm800k,"""model_output_solution_first_error_step"": ""N/A""     }     ```    - For the second type of errors affecting the efficiency of problem solving process, as they are more rarer than the first one, we sample solutions from the test set of [PRM800K](https://github.com/openai/prm800k) directly, containing 150 samples with redundant steps and 150 samples without."
prm800k,prm800k,DATASET,prm800k,"""model_output_solution_first_error_step"": ""N/A""     }     ```    - For the second type of errors affecting the efficiency of problem solving process, as they are more rarer than the first one, we sample solutions from the test set of [PRM800K](https://github.com/openai/prm800k) directly, containing 150 samples with redundant steps and 150 samples without."
mr-math_redundant_errors,NIL,DATASET,NIL,/dataset/mr-math_redundant_errors.json     {       // Instance ID.
PRM800K,prm800k,DATASET,prm800k,"""generator"": ""GPT-4 (PRM800K)"",        // The solution in a step-by-step format."
Meta-Reasoning-GSM8K,NIL,DATASET,NIL,"""rating"": [1, 1, 1, 1, 1, 1, 1, 1, 1]     }     ``` - **Meta-Reasoning-GSM8K**: This dataset consists of 3000 solutions from problems in GSM8K, including variations of code solutions and backward reasoning."
GSM8K,gsm8k,DATASET,gsm8k,"""rating"": [1, 1, 1, 1, 1, 1, 1, 1, 1]     }     ``` - **Meta-Reasoning-GSM8K**: This dataset consists of 3000 solutions from problems in GSM8K, including variations of code solutions and backward reasoning."
MR-GSM8K,NIL,DATASET,NIL,"For more details, see the official repository for [MR-GSM8K](https://github.com/dvlab-research/MR-GSM8K). - **Perturbation**: We introduce 6 types of errors to one of the intermediate steps in the solutions from the test set of [PRM800K](https://github.com/openai/prm800k)."
MR-GSM8K,NIL,DATASET,NIL,"For more details, see the official repository for [MR-GSM8K](https://github.com/dvlab-research/MR-GSM8K). - **Perturbation**: We introduce 6 types of errors to one of the intermediate steps in the solutions from the test set of [PRM800K](https://github.com/openai/prm800k)."
Perturbation,NIL,DATASET,NIL,"For more details, see the official repository for [MR-GSM8K](https://github.com/dvlab-research/MR-GSM8K). - **Perturbation**: We introduce 6 types of errors to one of the intermediate steps in the solutions from the test set of [PRM800K](https://github.com/openai/prm800k)."
PRM800K,prm800k,DATASET,prm800k,"For more details, see the official repository for [MR-GSM8K](https://github.com/dvlab-research/MR-GSM8K). - **Perturbation**: We introduce 6 types of errors to one of the intermediate steps in the solutions from the test set of [PRM800K](https://github.com/openai/prm800k)."
prm800k,prm800k,DATASET,prm800k,"For more details, see the official repository for [MR-GSM8K](https://github.com/dvlab-research/MR-GSM8K). - **Perturbation**: We introduce 6 types of errors to one of the intermediate steps in the solutions from the test set of [PRM800K](https://github.com/openai/prm800k)."
perturbatio,NIL,DATASET,NIL,/dataset/perturbation.json   {     // Instance ID.
mr-gsm8k,NIL,DATASET,NIL,/codes/mr-gsm8k_eval.py python .
mr-math,NIL,DATASET,NIL,/codes/mr-math_eval.py --error_type invalid python .
mr-math,NIL,DATASET,NIL,"/codes/mr-math_eval.py --error_type redundant ```  ### Citation Please cite the paper if the resource in this repo or the paper is helpful to you. ``` @article{xia2024evaluating,         title={Evaluating Mathematical Reasoning Beyond Accuracy},          author={Xia, Shijie and Li, Xuefeng and Liu, Yixin and Wu, Tongshuang and Liu, Pengfei},         journal={arXiv preprint arXiv:2404.05692},         year={2024}, } ```"
MNIST,mnist,DATASET,mnist,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
MNIST,mnist,DATASET,mnist,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
Connect-4,NIL,DATASET,NIL,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
Connect-4,NIL,DATASET,NIL,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
Letter,letter,DATASET,letter,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
Letter,letter,DATASET,letter,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
Optical recognition,NIL,DATASET,NIL,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
Optical recognition,NIL,DATASET,NIL,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
Pendigits,NIL,DATASET,NIL,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
Pendigits,NIL,DATASET,NIL,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
Protein,proteins,DATASET,proteins,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
Protein,proteins,DATASET,proteins,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
SenseIT,NIL,DATASET,NIL,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
SenseIT,NIL,DATASET,NIL,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
USPS,usps,DATASET,usps,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
USPS,usps,DATASET,usps,"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87±0.31       | | CART  |     MNIST         |      1.1k       |    88.59±0.14      | | TnT  |    Connect-4        |      864      |     78.85±0.46       | | CART  |     Connect-4         |      931       |    77.23±0.01      | | TnT  |    Letter      |      1.2k     |     86.62±0.02       | | CART  |     Letter         |      1.3k       |    86.26±0.15      | | TnT  |    Optical recognition     |      174     |     86.32±0.24       | | CART  |     Optical recognition         |      193       |    85.56±0.46      | | TnT  |    Pendigits       |      125     |     92.61±0.53       | | CART  |     Pendigits         |    166       |    91.74±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76±1.36       | | CART  |     USPS         |      109       |    87.35±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }"
SemanticKITTI,semantickitti,DATASET,semantickitti,:car:  ## Updates  - \[2024.04\] - We have further improved our manuscript and code. - \[2023.12\] - We provide trained weights on SemanticKITTI and nuScenes.
nuScenes,nuscenes,DATASET,nuscenes,:car:  ## Updates  - \[2024.04\] - We have further improved our manuscript and code. - \[2023.12\] - We provide trained weights on SemanticKITTI and nuScenes.
SemanticKITTI,semantickitti,DATASET,semantickitti,/docs/DATA_PREPARE.md) for the details to prepare the <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org) and <sup>2</sup>[nuScenes](https://www.nuscenes.org) datasets.  ## :rocket: Getting Started  Please refer to [GET_STARTED.md](.
kitti,kitti,DATASET,kitti,/docs/DATA_PREPARE.md) for the details to prepare the <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org) and <sup>2</sup>[nuScenes](https://www.nuscenes.org) datasets.  ## :rocket: Getting Started  Please refer to [GET_STARTED.md](.
nuScenes,nuscenes,DATASET,nuscenes,/docs/DATA_PREPARE.md) for the details to prepare the <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org) and <sup>2</sup>[nuScenes](https://www.nuscenes.org) datasets.  ## :rocket: Getting Started  Please refer to [GET_STARTED.md](.
SemanticKITTI,semantickitti,DATASET,semantickitti,"/docs/GET_STARTED.md) to learn more usage of this codebase.  ## :bar_chart: Main Results  ### Framework Overview  | <img src=""docs/figs/framework.png"" align=""center"" width=""99%""> | | :------------------------------------------------------------: |  ### Comparisons to State of the Arts  <table>     <tr>         <th rowspan=""2"">Method</th>         <th rowspan=""2"">Param</th>         <th colspan=""3"">SemanticKITTI</th>         <th colspan=""2"">nuScenes</th>         <th colspan=""2"">ScribbleKITTI</th>         <th colspan=""2"">SemanticPOSS</th>     </tr>     <tr>         <td>FPS</td> <td>Val</td> <td>Test</td>         <td>Val</td> <td>Test</td>         <td>mIoU</td> <td>mAcc</td>         <td>mIoU</td> <td>mAcc</td>     </tr>     <tr>         <td>CENet</td>         <td>6.8 M</td>         <td>33.4</td> <td>62.6</td> <td>64.7</td>         <td>-</td> <td>-</td>         <td>55.7</td> <td>66.8</td>         <td>50.3</td> <td>-</td>     </tr>     <tr>         <td>RangeViT</td>         <td>23.7 M</td>         <td>10.0</td> <td>60.7</td> <td>64.0</td>         <td>75.2</td> <td>-</td>         <td>53.6</td> <td>66.5</td>         <td>-</td> <td>-</td>     </tr>     <tr>         <td><strong>Fast-FRNet</strong></td>         <td>7.5 M</td>         <td>33.8</td> <td>67.1</td> <td>72.5</td>         <td>78.8</td> <td>82.1</td>         <td>62.4</td> <td>71.2</td>         <td>52.5</td> <td>67.1</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>10.0 M</td>         <td>29.1</td> <td>68.7</td> <td>73.3</td>         <td>79.0</td> <td>82.5</td>         <td>63.1</td> <td>72.3</td>         <td>53.5</td> <td>68.1</td>     </tr> </table>  ### Label-Efficient LiDAR Segmentation  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""4"">SemanticKITTI</th>         <th colspan=""4"">nuScenes</th>         <th colspan=""4"">ScribbleKITTI</th>     </tr>     <tr>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>     </tr>     <tr>         <td>Sup."
nuScenes,nuscenes,DATASET,nuscenes,"/docs/GET_STARTED.md) to learn more usage of this codebase.  ## :bar_chart: Main Results  ### Framework Overview  | <img src=""docs/figs/framework.png"" align=""center"" width=""99%""> | | :------------------------------------------------------------: |  ### Comparisons to State of the Arts  <table>     <tr>         <th rowspan=""2"">Method</th>         <th rowspan=""2"">Param</th>         <th colspan=""3"">SemanticKITTI</th>         <th colspan=""2"">nuScenes</th>         <th colspan=""2"">ScribbleKITTI</th>         <th colspan=""2"">SemanticPOSS</th>     </tr>     <tr>         <td>FPS</td> <td>Val</td> <td>Test</td>         <td>Val</td> <td>Test</td>         <td>mIoU</td> <td>mAcc</td>         <td>mIoU</td> <td>mAcc</td>     </tr>     <tr>         <td>CENet</td>         <td>6.8 M</td>         <td>33.4</td> <td>62.6</td> <td>64.7</td>         <td>-</td> <td>-</td>         <td>55.7</td> <td>66.8</td>         <td>50.3</td> <td>-</td>     </tr>     <tr>         <td>RangeViT</td>         <td>23.7 M</td>         <td>10.0</td> <td>60.7</td> <td>64.0</td>         <td>75.2</td> <td>-</td>         <td>53.6</td> <td>66.5</td>         <td>-</td> <td>-</td>     </tr>     <tr>         <td><strong>Fast-FRNet</strong></td>         <td>7.5 M</td>         <td>33.8</td> <td>67.1</td> <td>72.5</td>         <td>78.8</td> <td>82.1</td>         <td>62.4</td> <td>71.2</td>         <td>52.5</td> <td>67.1</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>10.0 M</td>         <td>29.1</td> <td>68.7</td> <td>73.3</td>         <td>79.0</td> <td>82.5</td>         <td>63.1</td> <td>72.3</td>         <td>53.5</td> <td>68.1</td>     </tr> </table>  ### Label-Efficient LiDAR Segmentation  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""4"">SemanticKITTI</th>         <th colspan=""4"">nuScenes</th>         <th colspan=""4"">ScribbleKITTI</th>     </tr>     <tr>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>     </tr>     <tr>         <td>Sup."
ScribbleKITTI,scribblekitti,DATASET,scribblekitti,"/docs/GET_STARTED.md) to learn more usage of this codebase.  ## :bar_chart: Main Results  ### Framework Overview  | <img src=""docs/figs/framework.png"" align=""center"" width=""99%""> | | :------------------------------------------------------------: |  ### Comparisons to State of the Arts  <table>     <tr>         <th rowspan=""2"">Method</th>         <th rowspan=""2"">Param</th>         <th colspan=""3"">SemanticKITTI</th>         <th colspan=""2"">nuScenes</th>         <th colspan=""2"">ScribbleKITTI</th>         <th colspan=""2"">SemanticPOSS</th>     </tr>     <tr>         <td>FPS</td> <td>Val</td> <td>Test</td>         <td>Val</td> <td>Test</td>         <td>mIoU</td> <td>mAcc</td>         <td>mIoU</td> <td>mAcc</td>     </tr>     <tr>         <td>CENet</td>         <td>6.8 M</td>         <td>33.4</td> <td>62.6</td> <td>64.7</td>         <td>-</td> <td>-</td>         <td>55.7</td> <td>66.8</td>         <td>50.3</td> <td>-</td>     </tr>     <tr>         <td>RangeViT</td>         <td>23.7 M</td>         <td>10.0</td> <td>60.7</td> <td>64.0</td>         <td>75.2</td> <td>-</td>         <td>53.6</td> <td>66.5</td>         <td>-</td> <td>-</td>     </tr>     <tr>         <td><strong>Fast-FRNet</strong></td>         <td>7.5 M</td>         <td>33.8</td> <td>67.1</td> <td>72.5</td>         <td>78.8</td> <td>82.1</td>         <td>62.4</td> <td>71.2</td>         <td>52.5</td> <td>67.1</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>10.0 M</td>         <td>29.1</td> <td>68.7</td> <td>73.3</td>         <td>79.0</td> <td>82.5</td>         <td>63.1</td> <td>72.3</td>         <td>53.5</td> <td>68.1</td>     </tr> </table>  ### Label-Efficient LiDAR Segmentation  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""4"">SemanticKITTI</th>         <th colspan=""4"">nuScenes</th>         <th colspan=""4"">ScribbleKITTI</th>     </tr>     <tr>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>     </tr>     <tr>         <td>Sup."
SemanticPOSS,semanticposs,DATASET,semanticposs,"/docs/GET_STARTED.md) to learn more usage of this codebase.  ## :bar_chart: Main Results  ### Framework Overview  | <img src=""docs/figs/framework.png"" align=""center"" width=""99%""> | | :------------------------------------------------------------: |  ### Comparisons to State of the Arts  <table>     <tr>         <th rowspan=""2"">Method</th>         <th rowspan=""2"">Param</th>         <th colspan=""3"">SemanticKITTI</th>         <th colspan=""2"">nuScenes</th>         <th colspan=""2"">ScribbleKITTI</th>         <th colspan=""2"">SemanticPOSS</th>     </tr>     <tr>         <td>FPS</td> <td>Val</td> <td>Test</td>         <td>Val</td> <td>Test</td>         <td>mIoU</td> <td>mAcc</td>         <td>mIoU</td> <td>mAcc</td>     </tr>     <tr>         <td>CENet</td>         <td>6.8 M</td>         <td>33.4</td> <td>62.6</td> <td>64.7</td>         <td>-</td> <td>-</td>         <td>55.7</td> <td>66.8</td>         <td>50.3</td> <td>-</td>     </tr>     <tr>         <td>RangeViT</td>         <td>23.7 M</td>         <td>10.0</td> <td>60.7</td> <td>64.0</td>         <td>75.2</td> <td>-</td>         <td>53.6</td> <td>66.5</td>         <td>-</td> <td>-</td>     </tr>     <tr>         <td><strong>Fast-FRNet</strong></td>         <td>7.5 M</td>         <td>33.8</td> <td>67.1</td> <td>72.5</td>         <td>78.8</td> <td>82.1</td>         <td>62.4</td> <td>71.2</td>         <td>52.5</td> <td>67.1</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>10.0 M</td>         <td>29.1</td> <td>68.7</td> <td>73.3</td>         <td>79.0</td> <td>82.5</td>         <td>63.1</td> <td>72.3</td>         <td>53.5</td> <td>68.1</td>     </tr> </table>  ### Label-Efficient LiDAR Segmentation  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""4"">SemanticKITTI</th>         <th colspan=""4"">nuScenes</th>         <th colspan=""4"">ScribbleKITTI</th>     </tr>     <tr>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>     </tr>     <tr>         <td>Sup."
SemanticKITTI,semantickitti,DATASET,semantickitti,"/docs/GET_STARTED.md) to learn more usage of this codebase.  ## :bar_chart: Main Results  ### Framework Overview  | <img src=""docs/figs/framework.png"" align=""center"" width=""99%""> | | :------------------------------------------------------------: |  ### Comparisons to State of the Arts  <table>     <tr>         <th rowspan=""2"">Method</th>         <th rowspan=""2"">Param</th>         <th colspan=""3"">SemanticKITTI</th>         <th colspan=""2"">nuScenes</th>         <th colspan=""2"">ScribbleKITTI</th>         <th colspan=""2"">SemanticPOSS</th>     </tr>     <tr>         <td>FPS</td> <td>Val</td> <td>Test</td>         <td>Val</td> <td>Test</td>         <td>mIoU</td> <td>mAcc</td>         <td>mIoU</td> <td>mAcc</td>     </tr>     <tr>         <td>CENet</td>         <td>6.8 M</td>         <td>33.4</td> <td>62.6</td> <td>64.7</td>         <td>-</td> <td>-</td>         <td>55.7</td> <td>66.8</td>         <td>50.3</td> <td>-</td>     </tr>     <tr>         <td>RangeViT</td>         <td>23.7 M</td>         <td>10.0</td> <td>60.7</td> <td>64.0</td>         <td>75.2</td> <td>-</td>         <td>53.6</td> <td>66.5</td>         <td>-</td> <td>-</td>     </tr>     <tr>         <td><strong>Fast-FRNet</strong></td>         <td>7.5 M</td>         <td>33.8</td> <td>67.1</td> <td>72.5</td>         <td>78.8</td> <td>82.1</td>         <td>62.4</td> <td>71.2</td>         <td>52.5</td> <td>67.1</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>10.0 M</td>         <td>29.1</td> <td>68.7</td> <td>73.3</td>         <td>79.0</td> <td>82.5</td>         <td>63.1</td> <td>72.3</td>         <td>53.5</td> <td>68.1</td>     </tr> </table>  ### Label-Efficient LiDAR Segmentation  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""4"">SemanticKITTI</th>         <th colspan=""4"">nuScenes</th>         <th colspan=""4"">ScribbleKITTI</th>     </tr>     <tr>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>     </tr>     <tr>         <td>Sup."
nuScenes,nuscenes,DATASET,nuscenes,"/docs/GET_STARTED.md) to learn more usage of this codebase.  ## :bar_chart: Main Results  ### Framework Overview  | <img src=""docs/figs/framework.png"" align=""center"" width=""99%""> | | :------------------------------------------------------------: |  ### Comparisons to State of the Arts  <table>     <tr>         <th rowspan=""2"">Method</th>         <th rowspan=""2"">Param</th>         <th colspan=""3"">SemanticKITTI</th>         <th colspan=""2"">nuScenes</th>         <th colspan=""2"">ScribbleKITTI</th>         <th colspan=""2"">SemanticPOSS</th>     </tr>     <tr>         <td>FPS</td> <td>Val</td> <td>Test</td>         <td>Val</td> <td>Test</td>         <td>mIoU</td> <td>mAcc</td>         <td>mIoU</td> <td>mAcc</td>     </tr>     <tr>         <td>CENet</td>         <td>6.8 M</td>         <td>33.4</td> <td>62.6</td> <td>64.7</td>         <td>-</td> <td>-</td>         <td>55.7</td> <td>66.8</td>         <td>50.3</td> <td>-</td>     </tr>     <tr>         <td>RangeViT</td>         <td>23.7 M</td>         <td>10.0</td> <td>60.7</td> <td>64.0</td>         <td>75.2</td> <td>-</td>         <td>53.6</td> <td>66.5</td>         <td>-</td> <td>-</td>     </tr>     <tr>         <td><strong>Fast-FRNet</strong></td>         <td>7.5 M</td>         <td>33.8</td> <td>67.1</td> <td>72.5</td>         <td>78.8</td> <td>82.1</td>         <td>62.4</td> <td>71.2</td>         <td>52.5</td> <td>67.1</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>10.0 M</td>         <td>29.1</td> <td>68.7</td> <td>73.3</td>         <td>79.0</td> <td>82.5</td>         <td>63.1</td> <td>72.3</td>         <td>53.5</td> <td>68.1</td>     </tr> </table>  ### Label-Efficient LiDAR Segmentation  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""4"">SemanticKITTI</th>         <th colspan=""4"">nuScenes</th>         <th colspan=""4"">ScribbleKITTI</th>     </tr>     <tr>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>     </tr>     <tr>         <td>Sup."
ScribbleKITTI,scribblekitti,DATASET,scribblekitti,"/docs/GET_STARTED.md) to learn more usage of this codebase.  ## :bar_chart: Main Results  ### Framework Overview  | <img src=""docs/figs/framework.png"" align=""center"" width=""99%""> | | :------------------------------------------------------------: |  ### Comparisons to State of the Arts  <table>     <tr>         <th rowspan=""2"">Method</th>         <th rowspan=""2"">Param</th>         <th colspan=""3"">SemanticKITTI</th>         <th colspan=""2"">nuScenes</th>         <th colspan=""2"">ScribbleKITTI</th>         <th colspan=""2"">SemanticPOSS</th>     </tr>     <tr>         <td>FPS</td> <td>Val</td> <td>Test</td>         <td>Val</td> <td>Test</td>         <td>mIoU</td> <td>mAcc</td>         <td>mIoU</td> <td>mAcc</td>     </tr>     <tr>         <td>CENet</td>         <td>6.8 M</td>         <td>33.4</td> <td>62.6</td> <td>64.7</td>         <td>-</td> <td>-</td>         <td>55.7</td> <td>66.8</td>         <td>50.3</td> <td>-</td>     </tr>     <tr>         <td>RangeViT</td>         <td>23.7 M</td>         <td>10.0</td> <td>60.7</td> <td>64.0</td>         <td>75.2</td> <td>-</td>         <td>53.6</td> <td>66.5</td>         <td>-</td> <td>-</td>     </tr>     <tr>         <td><strong>Fast-FRNet</strong></td>         <td>7.5 M</td>         <td>33.8</td> <td>67.1</td> <td>72.5</td>         <td>78.8</td> <td>82.1</td>         <td>62.4</td> <td>71.2</td>         <td>52.5</td> <td>67.1</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>10.0 M</td>         <td>29.1</td> <td>68.7</td> <td>73.3</td>         <td>79.0</td> <td>82.5</td>         <td>63.1</td> <td>72.3</td>         <td>53.5</td> <td>68.1</td>     </tr> </table>  ### Label-Efficient LiDAR Segmentation  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""4"">SemanticKITTI</th>         <th colspan=""4"">nuScenes</th>         <th colspan=""4"">ScribbleKITTI</th>     </tr>     <tr>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>         <td>1%</td> <td>10%</td> <td>20%</td> <td>50%</td>     </tr>     <tr>         <td>Sup."
SemKITTI,semkitti-dvps,DATASET,semkitti-dvps,"-only</td>         <td>44.9</td> <td>60.4</td> <td>61.8</td> <td>63.1</td>         <td>51.9</td> <td>68.1</td> <td>70.9</td> <td>74.6</td>         <td>42.4</td> <td>53.5</td> <td>55.1</td> <td>57.0</td>     </tr>     <tr>         <td>LaserMix</td>         <td>52.9</td> <td>62.9</td> <td>63.2</td> <td>65.0</td>         <td>58.7</td> <td>71.5</td> <td>72.3</td> <td>75.0</td>         <td>45.8</td> <td>56.8</td> <td>57.7</td> <td>59.0</td>     </tr>     <tr>         <td><strong>FrustumMix</strong></td>         <td>55.8</td> <td>64.8</td> <td>65.2</td> <td>65.4</td>         <td>61.2</td> <td>72.2</td> <td>74.6</td> <td>75.4</td>         <td>46.6</td> <td>57.0</td> <td>59.5</td> <td>61.2</td>     </tr> </table>  ### Robustness  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""2"">SemKITTI-C</th>         <th colspan=""2"">nuScenes-C</th>     </tr>     <tr>         <td>mCE</td> <td>mRR</td>         <td>mCE</td> <td>mRR</td>     </tr>     <tr>         <td>CENet</td>         <td>103.4</td> <td>81.3</td>         <td>112.8</td> <td>76.0</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>96.8</td> <td>80.0</td>         <td>98.6</td> <td>77.5</td>     </tr> </table>  **:memo: Note**:  - **mCE (the lower the better)**: The *average corruption error* (in percentage) of a candidate model compared to the baseline model, which is calculated among all corruption types across three severity levels. - **mRR (the higher the better)**: The *average resilience rate* (in percentage) of a candidate model compared to its ""clean"" performance, which is calculated among all corruption types across three severity levels.  ### :round_pushpin: Pre-Trained Checkpoints  We provide the trained models for SemanticKITTI and nuScenes."
nuScenes,nuscenes,DATASET,nuscenes,"-only</td>         <td>44.9</td> <td>60.4</td> <td>61.8</td> <td>63.1</td>         <td>51.9</td> <td>68.1</td> <td>70.9</td> <td>74.6</td>         <td>42.4</td> <td>53.5</td> <td>55.1</td> <td>57.0</td>     </tr>     <tr>         <td>LaserMix</td>         <td>52.9</td> <td>62.9</td> <td>63.2</td> <td>65.0</td>         <td>58.7</td> <td>71.5</td> <td>72.3</td> <td>75.0</td>         <td>45.8</td> <td>56.8</td> <td>57.7</td> <td>59.0</td>     </tr>     <tr>         <td><strong>FrustumMix</strong></td>         <td>55.8</td> <td>64.8</td> <td>65.2</td> <td>65.4</td>         <td>61.2</td> <td>72.2</td> <td>74.6</td> <td>75.4</td>         <td>46.6</td> <td>57.0</td> <td>59.5</td> <td>61.2</td>     </tr> </table>  ### Robustness  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""2"">SemKITTI-C</th>         <th colspan=""2"">nuScenes-C</th>     </tr>     <tr>         <td>mCE</td> <td>mRR</td>         <td>mCE</td> <td>mRR</td>     </tr>     <tr>         <td>CENet</td>         <td>103.4</td> <td>81.3</td>         <td>112.8</td> <td>76.0</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>96.8</td> <td>80.0</td>         <td>98.6</td> <td>77.5</td>     </tr> </table>  **:memo: Note**:  - **mCE (the lower the better)**: The *average corruption error* (in percentage) of a candidate model compared to the baseline model, which is calculated among all corruption types across three severity levels. - **mRR (the higher the better)**: The *average resilience rate* (in percentage) of a candidate model compared to its ""clean"" performance, which is calculated among all corruption types across three severity levels.  ### :round_pushpin: Pre-Trained Checkpoints  We provide the trained models for SemanticKITTI and nuScenes."
SemanticKITTI,semantickitti,DATASET,semantickitti,"-only</td>         <td>44.9</td> <td>60.4</td> <td>61.8</td> <td>63.1</td>         <td>51.9</td> <td>68.1</td> <td>70.9</td> <td>74.6</td>         <td>42.4</td> <td>53.5</td> <td>55.1</td> <td>57.0</td>     </tr>     <tr>         <td>LaserMix</td>         <td>52.9</td> <td>62.9</td> <td>63.2</td> <td>65.0</td>         <td>58.7</td> <td>71.5</td> <td>72.3</td> <td>75.0</td>         <td>45.8</td> <td>56.8</td> <td>57.7</td> <td>59.0</td>     </tr>     <tr>         <td><strong>FrustumMix</strong></td>         <td>55.8</td> <td>64.8</td> <td>65.2</td> <td>65.4</td>         <td>61.2</td> <td>72.2</td> <td>74.6</td> <td>75.4</td>         <td>46.6</td> <td>57.0</td> <td>59.5</td> <td>61.2</td>     </tr> </table>  ### Robustness  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""2"">SemKITTI-C</th>         <th colspan=""2"">nuScenes-C</th>     </tr>     <tr>         <td>mCE</td> <td>mRR</td>         <td>mCE</td> <td>mRR</td>     </tr>     <tr>         <td>CENet</td>         <td>103.4</td> <td>81.3</td>         <td>112.8</td> <td>76.0</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>96.8</td> <td>80.0</td>         <td>98.6</td> <td>77.5</td>     </tr> </table>  **:memo: Note**:  - **mCE (the lower the better)**: The *average corruption error* (in percentage) of a candidate model compared to the baseline model, which is calculated among all corruption types across three severity levels. - **mRR (the higher the better)**: The *average resilience rate* (in percentage) of a candidate model compared to its ""clean"" performance, which is calculated among all corruption types across three severity levels.  ### :round_pushpin: Pre-Trained Checkpoints  We provide the trained models for SemanticKITTI and nuScenes."
nuScenes,nuscenes,DATASET,nuscenes,"-only</td>         <td>44.9</td> <td>60.4</td> <td>61.8</td> <td>63.1</td>         <td>51.9</td> <td>68.1</td> <td>70.9</td> <td>74.6</td>         <td>42.4</td> <td>53.5</td> <td>55.1</td> <td>57.0</td>     </tr>     <tr>         <td>LaserMix</td>         <td>52.9</td> <td>62.9</td> <td>63.2</td> <td>65.0</td>         <td>58.7</td> <td>71.5</td> <td>72.3</td> <td>75.0</td>         <td>45.8</td> <td>56.8</td> <td>57.7</td> <td>59.0</td>     </tr>     <tr>         <td><strong>FrustumMix</strong></td>         <td>55.8</td> <td>64.8</td> <td>65.2</td> <td>65.4</td>         <td>61.2</td> <td>72.2</td> <td>74.6</td> <td>75.4</td>         <td>46.6</td> <td>57.0</td> <td>59.5</td> <td>61.2</td>     </tr> </table>  ### Robustness  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""2"">SemKITTI-C</th>         <th colspan=""2"">nuScenes-C</th>     </tr>     <tr>         <td>mCE</td> <td>mRR</td>         <td>mCE</td> <td>mRR</td>     </tr>     <tr>         <td>CENet</td>         <td>103.4</td> <td>81.3</td>         <td>112.8</td> <td>76.0</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>96.8</td> <td>80.0</td>         <td>98.6</td> <td>77.5</td>     </tr> </table>  **:memo: Note**:  - **mCE (the lower the better)**: The *average corruption error* (in percentage) of a candidate model compared to the baseline model, which is calculated among all corruption types across three severity levels. - **mRR (the higher the better)**: The *average resilience rate* (in percentage) of a candidate model compared to its ""clean"" performance, which is calculated among all corruption types across three severity levels.  ### :round_pushpin: Pre-Trained Checkpoints  We provide the trained models for SemanticKITTI and nuScenes."
SemanticKITTI,semantickitti,DATASET,semantickitti,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix)."
kitti,kitti,DATASET,kitti,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix)."
SemanticKITTI,semantickitti,DATASET,semantickitti,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix)."
kitti,kitti,DATASET,kitti,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix)."
nuScenes,nuscenes,DATASET,nuscenes,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix)."
nuscenes,nuscenes,DATASET,nuscenes,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix)."
nuScenes,nuscenes,DATASET,nuscenes,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix)."
nuscenes,nuscenes,DATASET,nuscenes,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix)."
ScribbleKITTI,scribblekitti,DATASET,scribblekitti,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix)."
scribblekitti,scribblekitti,DATASET,scribblekitti,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix)."
SemanticPOSS,semanticposs,DATASET,semanticposs,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix)."
semanticposs,semanticposs,DATASET,semanticposs,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix)."
SemanticPOSS,semanticposs,DATASET,semanticposs,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix)."
SemanticKITTI,semantickitti,DATASET,semantickitti,"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix)."
ProteinKG25,proteinkg25,DATASET,proteinkg25,"And we introduce **ProteinKG25**, a new large-scale KG dataset, promting the research on protein language pre-training."
ProteinKG25,proteinkg25,DATASET,proteinkg25,"BLAST, DeepGraphGO, we provide related links to configurate as follows:  [BLAST](https://www.ncbi.nlm.nih.gov/books/NBK569861/) / [Interproscan](https://github.com/ebi-pf-team/interproscan) / [DeepGraphGO](https://github.com/yourh/DeepGraphGO) / [GNN-PPI](https://github.com/lvguofeng/GNN_PPI)  ## Data preparation <span id=""data-preparation""></span> For pretraining OntoProtein, fine-tuning on protein-related tasks and inference, we provide acquirement approach of related data.  ### Pre-training data <span id=""pre-training-data""></span> To incorporate Gene Ontology knowledge into language models and train OntoProtein, we construct [ProteinKG25](https://zjunlp.github.io/project/ProteinKG25/), a large-scale KG dataset with aligned descriptions and protein sequences respectively to GO terms and protein entities."
ProteinKG25,proteinkg25,DATASET,proteinkg25,"BLAST, DeepGraphGO, we provide related links to configurate as follows:  [BLAST](https://www.ncbi.nlm.nih.gov/books/NBK569861/) / [Interproscan](https://github.com/ebi-pf-team/interproscan) / [DeepGraphGO](https://github.com/yourh/DeepGraphGO) / [GNN-PPI](https://github.com/lvguofeng/GNN_PPI)  ## Data preparation <span id=""data-preparation""></span> For pretraining OntoProtein, fine-tuning on protein-related tasks and inference, we provide acquirement approach of related data.  ### Pre-training data <span id=""pre-training-data""></span> To incorporate Gene Ontology knowledge into language models and train OntoProtein, we construct [ProteinKG25](https://zjunlp.github.io/project/ProteinKG25/), a large-scale KG dataset with aligned descriptions and protein sequences respectively to GO terms and protein entities."
ProteinKG25,proteinkg25,DATASET,proteinkg25,"There have two approach to acquire the pre-training data: 1) download our prepared data **ProteinKG25**, 2) generate your own pre-training data."
ProteinKG25,proteinkg25,DATASET,proteinkg25,"<div align=center><img src=""resources/img/times.png"" width=""50%"" height=""50%"" /></div>  #### Download released data  We have released our prepared data **ProteinKG25** in [Google Drive](https://drive.google.com/file/d/1iTC2-zbvYZCDhWM_wxRufCvV6vvPk8HR/view)."
CoNLL-2003,conll 2003,DATASET,conll 2003,/cleanlab`  #### Download Datasets  --- CoNLL-2003:  - Original paper: [Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition](https://arxiv.org/pdf/cs/0306050v1.pdf)  - Original dataset: [Papers with Code](https://paperswithcode.com/dataset/conll-2003)
conll-2003,conll 2003,DATASET,conll 2003,/cleanlab`  #### Download Datasets  --- CoNLL-2003:  - Original paper: [Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition](https://arxiv.org/pdf/cs/0306050v1.pdf)  - Original dataset: [Papers with Code](https://paperswithcode.com/dataset/conll-2003)
CoNLL-2003,conll 2003,DATASET,conll 2003,"We consider the named entity recognition dataset CoNLL-2003, and use CoNLL++ as the ground truth."
CoNLL++,conll++,DATASET,conll++,"We consider the named entity recognition dataset CoNLL-2003, and use CoNLL++ as the ground truth."
lasot-ext,NIL,DATASET,NIL,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-lasot-ext)](https://paperswithcode.com/sota/visual-object-tracking-on-lasot-ext?
lasot-ext,NIL,DATASET,NIL,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-lasot-ext)](https://paperswithcode.com/sota/visual-object-tracking-on-lasot-ext?
lasot,lasot,DATASET,lasot,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-lasot)](https://paperswithcode.com/sota/visual-object-tracking-on-lasot?
lasot,lasot,DATASET,lasot,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-lasot)](https://paperswithcode.com/sota/visual-object-tracking-on-lasot?
tnl2k,tnl2k,DATASET,tnl2k,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-tracking-on-tnl2k)](https://paperswithcode.com/sota/visual-tracking-on-tnl2k?
tnl2k,tnl2k,DATASET,tnl2k,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-tracking-on-tnl2k)](https://paperswithcode.com/sota/visual-tracking-on-tnl2k?
trackingnet,trackingnet,DATASET,trackingnet,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-trackingnet)](https://paperswithcode.com/sota/visual-object-tracking-on-trackingnet?
trackingnet,trackingnet,DATASET,trackingnet,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/visual-object-tracking-on-trackingnet)](https://paperswithcode.com/sota/visual-object-tracking-on-trackingnet?
youtube-vis,youtube-vis 2019,DATASET,youtube-vis 2019,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/video-instance-segmentation-on-youtube-vis-1)](https://paperswithcode.com/sota/video-instance-segmentation-on-youtube-vis-1?
youtube-vis,youtube-vis 2019,DATASET,youtube-vis 2019,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/video-instance-segmentation-on-youtube-vis-1)](https://paperswithcode.com/sota/video-instance-segmentation-on-youtube-vis-1?
ovis,ovis,DATASET,ovis,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/video-instance-segmentation-on-ovis-1)](https://paperswithcode.com/sota/video-instance-segmentation-on-ovis-1?
ovis,ovis,DATASET,ovis,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/video-instance-segmentation-on-ovis-1)](https://paperswithcode.com/sota/video-instance-segmentation-on-ovis-1?
davis,davis,DATASET,davis,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-davis)](https://paperswithcode.com/sota/referring-expression-segmentation-on-davis?
davis,davis,DATASET,davis,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-davis)](https://paperswithcode.com/sota/referring-expression-segmentation-on-davis?
refcoco,refcoco,DATASET,refcoco,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refcoco)](https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco?
refcoco,refcoco,DATASET,refcoco,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refcoco)](https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco?
refcoco,refcoco,DATASET,refcoco,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refcoco-3)](https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco-3?
refcoco,refcoco,DATASET,refcoco,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-segmentation-on-refcoco-3)](https://paperswithcode.com/sota/referring-expression-segmentation-on-refcoco-3?
refcoco,refcoco,DATASET,refcoco,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on-refcoco)](https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco?
refcoco,refcoco,DATASET,refcoco,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on-refcoco)](https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco?
refcoco,refcoco,DATASET,refcoco,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on-refcoco-1)](https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco-1?
refcoco,refcoco,DATASET,refcoco,url=https://paperswithcode.com/badge/universal-instance-perception-as-object/referring-expression-comprehension-on-refcoco-1)](https://paperswithcode.com/sota/referring-expression-comprehension-on-refcoco-1?
MBES,NIL,DATASET,NIL,"user=wPea4DkAAAAJ&hl=en&oi=ao)<sup>3</sup>, [John Folkesson](https://www.kth.se/profile/johnf)<sup>1</sup>, [Anna Wåhlin](https://www.gu.se/en/about/find-staff/annawahlin)<sup>4</sup>  |<sup>1</sup>KTH Royal Institute of Technology|<sup>2</sup>TU Graz|<sup>3</sup>Ocean Infinity|<sup>4</sup>University of Gothenburg|  For more information, please check out the [project website](https://luxiya01.github.io/mbes-registration-project-page/)  ### Contacts If you have any questions, feel free to contact us at: - Li Ling (liling@kth.se)  # Instructions This repository contains the implementation for the MBES Dataset class and data loader, the classical methods GICP and FPFH, as well as the code for metrics computation and evaluations."
MNIST,mnist,DATASET,mnist,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
mnist,mnist,DATASET,mnist,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
CIFAR,cifar-10,DATASET,cifar-10,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
cifar,cifar-10,DATASET,cifar-10,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
SVHN,svhn,DATASET,svhn,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
Food101,food-101,DATASET,food-101,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
food-101,food-101,DATASET,food-101,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
Flowers102,NIL,DATASET,NIL,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
Cervical,NIL,DATASET,NIL,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
CheXpert,chexpert,DATASET,chexpert,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
chexpert,chexpert,DATASET,chexpert,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
Facial Expression,NIL,DATASET,NIL,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
fer2013,fer2013,DATASET,fer2013,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
Celeb,celeba,DATASET,celeba,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
CelebA,celeba,DATASET,celeba,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
Adult,adult,DATASET,adult,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
COMPAS,compas,DATASET,compas,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
FICO,NIL,DATASET,NIL,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
Boston Housing,the boston housing dataset,DATASET,the boston housing dataset,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
German Credit,german credit dataset,DATASET,german credit dataset,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
Student Admission,NIL,DATASET,NIL,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
Student Performance,NIL,DATASET,NIL,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
GMSC,gmsc,DATASET,gmsc,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
GiveMeSomeCredit,NIL,DATASET,NIL,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
Diabetes,diabetes,DATASET,diabetes,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
Breast Cancer,NIL,DATASET,NIL,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
Cora,cora,DATASET,cora,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
CORA,cora,DATASET,cora,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
Bitcoin,NIL,DATASET,NIL,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
CIC-IDS2017,cic-andmal2017,DATASET,cic-andmal2017,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
ids-2017,NIL,DATASET,NIL,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
IMDB Review,imdb movie reviews,DATASET,imdb movie reviews,"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | ≤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | ≤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | ≤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | ≤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | ≤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances."
VisE-D,visevent,DATASET,visevent,"The paper is available on: - *Computer Vision Foundation (CVF)*: https://openaccess.thecvf.com/content/WACV2021/html/Muller-Budack_Ontology-Driven_Event_Type_Classification_in_Images_WACV_2021_paper.html - *arXiv*: https://arxiv.org/pdf/2011.04714.pdf  Further information can be found on the **EventKG** website: http://eventkg.l3s.uni-hannover.de/VisE   ## Content  - [Ontology-driven Event Type Classification in Images](#ontology-driven-event-type-classification-in-images)   - [Content](#content)   - [Setup](#setup)     - [Setup with Singularity (for Reproducibility)](#setup-with-singularity-for-reproducibility)     - [Setup with Virtual Environment](#setup-with-virtual-environment)     - [Setup with Docker](#setup-with-docker)   - [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models)   - [Models](#models)   - [Inference](#inference)   - [Test](#test)   - [VisE-D: Visual Event Classification Dataset](#vise-d-visual-event-classification-dataset)   - [VisE-O: Visual Event Ontology](#vise-o-visual-event-ontology)   - [Benchmark Ontologies](#benchmark-ontologies)   - [Supplemental Material](#supplemental-material)   - [LICENSE](#license)   ## Setup  We provide three different ways to setup the project."
Visual Event Classification,visevent,DATASET,visevent,"The paper is available on: - *Computer Vision Foundation (CVF)*: https://openaccess.thecvf.com/content/WACV2021/html/Muller-Budack_Ontology-Driven_Event_Type_Classification_in_Images_WACV_2021_paper.html - *arXiv*: https://arxiv.org/pdf/2011.04714.pdf  Further information can be found on the **EventKG** website: http://eventkg.l3s.uni-hannover.de/VisE   ## Content  - [Ontology-driven Event Type Classification in Images](#ontology-driven-event-type-classification-in-images)   - [Content](#content)   - [Setup](#setup)     - [Setup with Singularity (for Reproducibility)](#setup-with-singularity-for-reproducibility)     - [Setup with Virtual Environment](#setup-with-virtual-environment)     - [Setup with Docker](#setup-with-docker)   - [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models)   - [Models](#models)   - [Inference](#inference)   - [Test](#test)   - [VisE-D: Visual Event Classification Dataset](#vise-d-visual-event-classification-dataset)   - [VisE-O: Visual Event Ontology](#vise-o-visual-event-ontology)   - [Benchmark Ontologies](#benchmark-ontologies)   - [Supplemental Material](#supplemental-material)   - [LICENSE](#license)   ## Setup  We provide three different ways to setup the project."
VisE-Bing,visevent,DATASET,visevent,"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path."
VisE-Wiki,visevent,DATASET,visevent,"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path."
VisE-Bing,visevent,DATASET,visevent,"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path."
VisE-Wiki,visevent,DATASET,visevent,"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path."
VisE-Bing,visevent,DATASET,visevent,"```--batch_size <int>``` specifies the batch size (default ```16```)  ```--num_predictions <int>``` sets the number of top predictions printed on the console (default ```3```)   ```--s2l_strategy [leafprob, cossim, leafprob*cossim]``` specifies the strategies to retrieve the leaf node vector from a subgraph vector (default ```leafprob*cossim```)    ## Test  This step requires to download the test images in the *VisE-Bing* or *VisE-Wiki* dataset."
VisE-Wiki,visevent,DATASET,visevent,"```--batch_size <int>``` specifies the batch size (default ```16```)  ```--num_predictions <int>``` sets the number of top predictions printed on the console (default ```3```)   ```--s2l_strategy [leafprob, cossim, leafprob*cossim]``` specifies the strategies to retrieve the leaf node vector from a subgraph vector (default ```leafprob*cossim```)    ## Test  This step requires to download the test images in the *VisE-Bing* or *VisE-Wiki* dataset."
VisE-D,visevent,DATASET,visevent,"```--batch_size <int>``` specifies the batch size (default ```16```)  ```--s2l_strategy [leafprob, cossim, leafprob*cossim]``` specifies the strategies to retrieve the leaf node vector from a subgraph vector (default ```leafprob*cossim```)    ## VisE-D: Visual Event Classification Dataset  The *Visual Event Classification Dataset (VisE-D)* is available on: https://data.uni-hannover.de/de/dataset/vise  You can automatically download the dataset by following the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models)."
Visual Event Classification Dataset,visevent,DATASET,visevent,"```--batch_size <int>``` specifies the batch size (default ```16```)  ```--s2l_strategy [leafprob, cossim, leafprob*cossim]``` specifies the strategies to retrieve the leaf node vector from a subgraph vector (default ```leafprob*cossim```)    ## VisE-D: Visual Event Classification Dataset  The *Visual Event Classification Dataset (VisE-D)* is available on: https://data.uni-hannover.de/de/dataset/vise  You can automatically download the dataset by following the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models)."
Visual Event Classification Dataset,visevent,DATASET,visevent,"```--batch_size <int>``` specifies the batch size (default ```16```)  ```--s2l_strategy [leafprob, cossim, leafprob*cossim]``` specifies the strategies to retrieve the leaf node vector from a subgraph vector (default ```leafprob*cossim```)    ## VisE-D: Visual Event Classification Dataset  The *Visual Event Classification Dataset (VisE-D)* is available on: https://data.uni-hannover.de/de/dataset/vise  You can automatically download the dataset by following the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models)."
VisE-D,visevent,DATASET,visevent,"```--batch_size <int>``` specifies the batch size (default ```16```)  ```--s2l_strategy [leafprob, cossim, leafprob*cossim]``` specifies the strategies to retrieve the leaf node vector from a subgraph vector (default ```leafprob*cossim```)    ## VisE-D: Visual Event Classification Dataset  The *Visual Event Classification Dataset (VisE-D)* is available on: https://data.uni-hannover.de/de/dataset/vise  You can automatically download the dataset by following the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models)."
vise-d,visevent,DATASET,visevent,"Different versions of the *Visual Event Ontology (VisE-O)* can be downloaded here: [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/99ce7e4d-df5b-40f6-afb4-16085dbf697d/download/vise-d.tar.gz)  Furthermore you can explore the *Ontologies* using the following links:  - **Initial Ontology** (result of Section 3.2.2): [explore](https://tibhannover.github.io/VisE/VisE-O_initial/index.html) - **Disambiguated Ontology** (result of Section 3.2.3): [explore](https://tibhannover.github.io/VisE/VisE-O_disambiguated/index.html) - **Refined Ontology** (result of Section 3.2.4): [explore](https://tibhannover.github.io/VisE/VisE-O_refined/index.html)  **USAGE:** After opening an *Ontology*, the *Leaf Event Nodes* (blue), *Branch Event Nodes* (orange), and *Root Node* (yellow) as well as their *Relations* are displayed."
Web Images for Event Recognition,wider,DATASET,wider,"In addition, the search bar can be used to directly access a specific *Event Node*.   ## Benchmark Ontologies  In order to evaluate the presented ontology-diven approach on other benchmark datasets, we have manually linked classes of the *Web Images for Event Recognition (WIDER)*, *Social Event Dataset (SocEID)*, and the *Rare Event Dataset (RED)* to the *Wikidata* knowledge base according to Section 5.3.3."
WIDER,wider,DATASET,wider,"In addition, the search bar can be used to directly access a specific *Event Node*.   ## Benchmark Ontologies  In order to evaluate the presented ontology-diven approach on other benchmark datasets, we have manually linked classes of the *Web Images for Event Recognition (WIDER)*, *Social Event Dataset (SocEID)*, and the *Rare Event Dataset (RED)* to the *Wikidata* knowledge base according to Section 5.3.3."
Social Event Dataset,NIL,DATASET,NIL,"In addition, the search bar can be used to directly access a specific *Event Node*.   ## Benchmark Ontologies  In order to evaluate the presented ontology-diven approach on other benchmark datasets, we have manually linked classes of the *Web Images for Event Recognition (WIDER)*, *Social Event Dataset (SocEID)*, and the *Rare Event Dataset (RED)* to the *Wikidata* knowledge base according to Section 5.3.3."
SocEID,NIL,DATASET,NIL,"In addition, the search bar can be used to directly access a specific *Event Node*.   ## Benchmark Ontologies  In order to evaluate the presented ontology-diven approach on other benchmark datasets, we have manually linked classes of the *Web Images for Event Recognition (WIDER)*, *Social Event Dataset (SocEID)*, and the *Rare Event Dataset (RED)* to the *Wikidata* knowledge base according to Section 5.3.3."
Rare Event Dataset,NIL,DATASET,NIL,"In addition, the search bar can be used to directly access a specific *Event Node*.   ## Benchmark Ontologies  In order to evaluate the presented ontology-diven approach on other benchmark datasets, we have manually linked classes of the *Web Images for Event Recognition (WIDER)*, *Social Event Dataset (SocEID)*, and the *Rare Event Dataset (RED)* to the *Wikidata* knowledge base according to Section 5.3.3."
RED,red,DATASET,red,"In addition, the search bar can be used to directly access a specific *Event Node*.   ## Benchmark Ontologies  In order to evaluate the presented ontology-diven approach on other benchmark datasets, we have manually linked classes of the *Web Images for Event Recognition (WIDER)*, *Social Event Dataset (SocEID)*, and the *Rare Event Dataset (RED)* to the *Wikidata* knowledge base according to Section 5.3.3."
wider,wider,DATASET,wider,"The resulting *Ontologies* for these datasets can be downloaded and explored here: - **WIDER Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b1c2f92b-4b69-46fc-9282-16acc7a1c9aa/download/wider.tar.gz) | [explore](https://tibhannover.github.io/VisE/WIDER/index.html) - **SocEID Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/a8373c98-32a8-408c-b8e9-51e6b1e01777/download/soceid.tar.gz) | [explore](https://tibhannover.github.io/VisE/SocEID/index.html) - **RED Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/d0f5cd8b-7c3e-4055-9810-f9cba2b69a33/download/red.tar.gz) | [explore](https://tibhannover.github.io/VisE/RED/index.html)  ## Supplemental Material  Detailed information on the sampling strategy to gather event images, statistics for the training and testing datasets presented in Section 3.3, and results using different inference strategies (Section 4.2.3) are available in the [vise_supplemental.pdf](vise_supplemental.pdf).    ## LICENSE  This work is published under the GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007."
soceid,NIL,DATASET,NIL,"The resulting *Ontologies* for these datasets can be downloaded and explored here: - **WIDER Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b1c2f92b-4b69-46fc-9282-16acc7a1c9aa/download/wider.tar.gz) | [explore](https://tibhannover.github.io/VisE/WIDER/index.html) - **SocEID Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/a8373c98-32a8-408c-b8e9-51e6b1e01777/download/soceid.tar.gz) | [explore](https://tibhannover.github.io/VisE/SocEID/index.html) - **RED Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/d0f5cd8b-7c3e-4055-9810-f9cba2b69a33/download/red.tar.gz) | [explore](https://tibhannover.github.io/VisE/RED/index.html)  ## Supplemental Material  Detailed information on the sampling strategy to gather event images, statistics for the training and testing datasets presented in Section 3.3, and results using different inference strategies (Section 4.2.3) are available in the [vise_supplemental.pdf](vise_supplemental.pdf).    ## LICENSE  This work is published under the GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007."
red,red,DATASET,red,"The resulting *Ontologies* for these datasets can be downloaded and explored here: - **WIDER Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b1c2f92b-4b69-46fc-9282-16acc7a1c9aa/download/wider.tar.gz) | [explore](https://tibhannover.github.io/VisE/WIDER/index.html) - **SocEID Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/a8373c98-32a8-408c-b8e9-51e6b1e01777/download/soceid.tar.gz) | [explore](https://tibhannover.github.io/VisE/SocEID/index.html) - **RED Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/d0f5cd8b-7c3e-4055-9810-f9cba2b69a33/download/red.tar.gz) | [explore](https://tibhannover.github.io/VisE/RED/index.html)  ## Supplemental Material  Detailed information on the sampling strategy to gather event images, statistics for the training and testing datasets presented in Section 3.3, and results using different inference strategies (Section 4.2.3) are available in the [vise_supplemental.pdf](vise_supplemental.pdf).    ## LICENSE  This work is published under the GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007."
MNIST,mnist,DATASET,mnist,"# Sparse Coding with Multi-Layer Decoders using Variance Regularization This is a PyTorch implementation for the setup described in  [Sparse Coding with Multi-Layer Decoders using Variance Regularization](https://arxiv.org/abs/2112.09214).   ### Requirements  - Python 3.7 - [PyTorch](https://pytorch.org/get-started/previous-versions/) 1.6.0 with torchvision 0.7.0 - Other dependencies: numpy, tensorboardX  ### Datasets  In our experiments, we use: - the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset."
mnist,mnist,DATASET,mnist,"# Sparse Coding with Multi-Layer Decoders using Variance Regularization This is a PyTorch implementation for the setup described in  [Sparse Coding with Multi-Layer Decoders using Variance Regularization](https://arxiv.org/abs/2112.09214).   ### Requirements  - Python 3.7 - [PyTorch](https://pytorch.org/get-started/previous-versions/) 1.6.0 with torchvision 0.7.0 - Other dependencies: numpy, tensorboardX  ### Datasets  In our experiments, we use: - the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset."
MNIST,mnist,DATASET,mnist,"We provide the train and validation splits in  ```data/MNIST_train.npy``` and ```data/MNIST_val.npy```. - a custom dataset with 200,000 gray-scale natural image patches of size 28x28 extracted from  [ImageNet](https://www.image-net.org/index.php)."
MNIST,mnist,DATASET,mnist,"We provide the train and validation splits in  ```data/MNIST_train.npy``` and ```data/MNIST_val.npy```. - a custom dataset with 200,000 gray-scale natural image patches of size 28x28 extracted from  [ImageNet](https://www.image-net.org/index.php)."
ImageNet,imagenet,DATASET,imagenet,"We provide the train and validation splits in  ```data/MNIST_train.npy``` and ```data/MNIST_val.npy```. - a custom dataset with 200,000 gray-scale natural image patches of size 28x28 extracted from  [ImageNet](https://www.image-net.org/index.php)."
image-net,imagenet,DATASET,imagenet,"We provide the train and validation splits in  ```data/MNIST_train.npy``` and ```data/MNIST_val.npy```. - a custom dataset with 200,000 gray-scale natural image patches of size 28x28 extracted from  [ImageNet](https://www.image-net.org/index.php)."
imagenet,imagenet,DATASET,imagenet,The script to generate it is  [build_imagenet_LCN.sh](https://github.com/kevtimova/deep-sparse/blob/main/scripts/build_ImageNet_LCN.sh).  ### Training  The scripts below can be used to train sparse autoencoders with our different setups
ImageNet,imagenet,DATASET,imagenet,The script to generate it is  [build_imagenet_LCN.sh](https://github.com/kevtimova/deep-sparse/blob/main/scripts/build_ImageNet_LCN.sh).  ### Training  The scripts below can be used to train sparse autoencoders with our different setups
MNIST,mnist,DATASET,mnist,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
MNIST,mnist,DATASET,mnist,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
MNIST,mnist,DATASET,mnist,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
MNIST,mnist,DATASET,mnist,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
MNIST,mnist,DATASET,mnist,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
MNIST,mnist,DATASET,mnist,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
MNIST,mnist,DATASET,mnist,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
ImageNet_patches,imagenet-patch,DATASET,imagenet-patch,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
ImageNet,imagenet,DATASET,imagenet,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
ImageNet_patches,imagenet-patch,DATASET,imagenet-patch,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
ImageNet,imagenet,DATASET,imagenet,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
ImageNet_patches,imagenet-patch,DATASET,imagenet-patch,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
ImageNet,imagenet,DATASET,imagenet,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
Imagenet_patches,imagenet-patch,DATASET,imagenet-patch,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
ImageNet,imagenet,DATASET,imagenet,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
MNIST,mnist,DATASET,mnist,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
ImageNet,imagenet,DATASET,imagenet,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
MNIST,mnist,DATASET,mnist,| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.
MNIST,mnist,DATASET,mnist,"Step 1: Given a pre-trained encoder, ```compute_codes.py``` can be used to create a dataset containing the codes  for each MNIST image."
DAMP,NIL,DATASET,NIL,The corpus consists of lyrics of the data from DAMP and DALI datasets and artists from Billboard (2015-2018) [4]   **Pronunciation Model**: A predefined lexicon that provides a mapping between words and their phonemic representations.
DALI,NIL,DATASET,NIL,The corpus consists of lyrics of the data from DAMP and DALI datasets and artists from Billboard (2015-2018) [4]   **Pronunciation Model**: A predefined lexicon that provides a mapping between words and their phonemic representations.
DAMP,NIL,DATASET,NIL,"We use the singing adapted version presented here[5].   ## Setup    #### 1) Kaldi  This framework is built as a [Kaldi](http://kaldi-asr.org/)[1] recipe  For instructions on Kaldi installation, please visit https://github.com/kaldi-asr/kaldi  #### 2) Dependencies  ``` pip install -r requirements.txt ```   ## How to run  * Modify ```KALDI_ROOT``` in  ```path.sh``` according to where your Kaldi installation is.  ### A) Data preparation: #### 1) Retrieve the data:  * DAMP:  We use the Sing!"
Sing!300x30x2,NIL,DATASET,NIL,"We use the singing adapted version presented here[5].   ## Setup    #### 1) Kaldi  This framework is built as a [Kaldi](http://kaldi-asr.org/)[1] recipe  For instructions on Kaldi installation, please visit https://github.com/kaldi-asr/kaldi  #### 2) Dependencies  ``` pip install -r requirements.txt ```   ## How to run  * Modify ```KALDI_ROOT``` in  ```path.sh``` according to where your Kaldi installation is.  ### A) Data preparation: #### 1) Retrieve the data:  * DAMP:  We use the Sing!"
Sing!300x30x2,NIL,DATASET,NIL,300x30x2 data within the DAMP repository[6].
DAMP,NIL,DATASET,NIL,300x30x2 data within the DAMP repository[6].
damp,NIL,DATASET,NIL,"To retrieve the data, you need to apply for authorization from https://ccrma.stanford.edu/damp/ ."
damp,NIL,DATASET,NIL,Please define the directory where you downloaded the dataset (```path-to-damp```) as follows: ``` datadir_damp='path-to-damp' ```  * DALI:  DALI_v2.0 is used to train the polyphonic and cross-domain lyrics transcription models.
damp,NIL,DATASET,NIL,Please define the directory where you downloaded the dataset (```path-to-damp```) as follows: ``` datadir_damp='path-to-damp' ```  * DALI:  DALI_v2.0 is used to train the polyphonic and cross-domain lyrics transcription models.
damp,NIL,DATASET,NIL,Please define the directory where you downloaded the dataset (```path-to-damp```) as follows: ``` datadir_damp='path-to-damp' ```  * DALI:  DALI_v2.0 is used to train the polyphonic and cross-domain lyrics transcription models.
DALI,NIL,DATASET,NIL,Please define the directory where you downloaded the dataset (```path-to-damp```) as follows: ``` datadir_damp='path-to-damp' ```  * DALI:  DALI_v2.0 is used to train the polyphonic and cross-domain lyrics transcription models.
DALI_v2.0,NIL,DATASET,NIL,Please define the directory where you downloaded the dataset (```path-to-damp```) as follows: ``` datadir_damp='path-to-damp' ```  * DALI:  DALI_v2.0 is used to train the polyphonic and cross-domain lyrics transcription models.
DALI,NIL,DATASET,NIL,"To retrieve the data, please refer to the relevant Github repository at:  https://github.com/gabolsgabs/DALI  According to the repository, you can download the audio files under 'Getting the audio' section."
DALI-TALT,NIL,DATASET,NIL,"Refer this as: ``` datadir_dali='path-to-dali' ```  * DALI-TALT:  This dataset is a subset of DALI, presented in [7] It is the largest test set used for evaluating polyphonic ALT models."
DALI,NIL,DATASET,NIL,"Refer this as: ``` datadir_dali='path-to-dali' ```  * DALI-TALT:  This dataset is a subset of DALI, presented in [7] It is the largest test set used for evaluating polyphonic ALT models."
DALI-TestSet4ALT,NIL,DATASET,NIL,The data can be retrieved via the tutorial at: https://github.com/emirdemirel/DALI-TestSet4ALT . ``` datadir_dali_talt='path-to-dali-talt' ```  * Jamendo:  Jamendo(lyrics) is a benchmark evaluation set for both lyrics transcription and audio-to-lyrics alignment tasks[8].
dali_talt,NIL,DATASET,NIL,The data can be retrieved via the tutorial at: https://github.com/emirdemirel/DALI-TestSet4ALT . ``` datadir_dali_talt='path-to-dali-talt' ```  * Jamendo:  Jamendo(lyrics) is a benchmark evaluation set for both lyrics transcription and audio-to-lyrics alignment tasks[8].
dali-talt,NIL,DATASET,NIL,The data can be retrieved via the tutorial at: https://github.com/emirdemirel/DALI-TestSet4ALT . ``` datadir_dali_talt='path-to-dali-talt' ```  * Jamendo:  Jamendo(lyrics) is a benchmark evaluation set for both lyrics transcription and audio-to-lyrics alignment tasks[8].
Jamendo,jamendo lyrics,DATASET,jamendo lyrics,The data can be retrieved via the tutorial at: https://github.com/emirdemirel/DALI-TestSet4ALT . ``` datadir_dali_talt='path-to-dali-talt' ```  * Jamendo:  Jamendo(lyrics) is a benchmark evaluation set for both lyrics transcription and audio-to-lyrics alignment tasks[8].
Jamendo(lyrics),jamendo lyrics,DATASET,jamendo lyrics,The data can be retrieved via the tutorial at: https://github.com/emirdemirel/DALI-TestSet4ALT . ``` datadir_dali_talt='path-to-dali-talt' ```  * Jamendo:  Jamendo(lyrics) is a benchmark evaluation set for both lyrics transcription and audio-to-lyrics alignment tasks[8].
jamendolyrics,jamendo lyrics,DATASET,jamendo lyrics,Data can be retrieved at https://github.com/f90/jamendolyrics .  ``` datadir_jamendo='path-to-jamendo' ```   ### B) Running the training pipeline  There are two recipes included in this repository.
jamendo,jamendo lyrics,DATASET,jamendo lyrics,Data can be retrieved at https://github.com/f90/jamendolyrics .  ``` datadir_jamendo='path-to-jamendo' ```   ### B) Running the training pipeline  There are two recipes included in this repository.
jamendo,jamendo lyrics,DATASET,jamendo lyrics,Data can be retrieved at https://github.com/f90/jamendolyrics .  ``` datadir_jamendo='path-to-jamendo' ```   ### B) Running the training pipeline  There are two recipes included in this repository.
DAMP,NIL,DATASET,NIL,It is written to work on DAMP.
DAMP,NIL,DATASET,NIL,Define the absolute path to the DAMP - Sing!
DAMP,NIL,DATASET,NIL,"300x30x2  repository.    ```  audiopath = path-to-DAMP-dataset ``` Then, simply pass the ```$audiopath``` variable to the main recipe: ``` ."
damp,NIL,DATASET,NIL,"/run_mstrenet.sh --datadir_damp ${datadir_damp} --datadir_dali ${datadir_dali} \     --datadir_dali_talt ${datadir_dali_talt} --datadir_jamendo ${datadir_jamendo} \ ``` If you'd like to see the help menu, simply type: ``` ."
damp,NIL,DATASET,NIL,"/run_mstrenet.sh --datadir_damp ${datadir_damp} --datadir_dali ${datadir_dali} \     --datadir_dali_talt ${datadir_dali_talt} --datadir_jamendo ${datadir_jamendo} \ ``` If you'd like to see the help menu, simply type: ``` ."
dali,NIL,DATASET,NIL,"/run_mstrenet.sh --datadir_damp ${datadir_damp} --datadir_dali ${datadir_dali} \     --datadir_dali_talt ${datadir_dali_talt} --datadir_jamendo ${datadir_jamendo} \ ``` If you'd like to see the help menu, simply type: ``` ."
dali_talt,NIL,DATASET,NIL,"/run_mstrenet.sh --datadir_damp ${datadir_damp} --datadir_dali ${datadir_dali} \     --datadir_dali_talt ${datadir_dali_talt} --datadir_jamendo ${datadir_jamendo} \ ``` If you'd like to see the help menu, simply type: ``` ."
dali_talt,NIL,DATASET,NIL,"/run_mstrenet.sh --datadir_damp ${datadir_damp} --datadir_dali ${datadir_dali} \     --datadir_dali_talt ${datadir_dali_talt} --datadir_jamendo ${datadir_jamendo} \ ``` If you'd like to see the help menu, simply type: ``` ."
jamendo,jamendo lyrics,DATASET,jamendo lyrics,"/run_mstrenet.sh --datadir_damp ${datadir_damp} --datadir_dali ${datadir_dali} \     --datadir_dali_talt ${datadir_dali_talt} --datadir_jamendo ${datadir_jamendo} \ ``` If you'd like to see the help menu, simply type: ``` ."
jamendo,jamendo lyrics,DATASET,jamendo lyrics,"/run_mstrenet.sh --datadir_damp ${datadir_damp} --datadir_dali ${datadir_dali} \     --datadir_dali_talt ${datadir_dali_talt} --datadir_jamendo ${datadir_jamendo} \ ``` If you'd like to see the help menu, simply type: ``` ."
damp,NIL,DATASET,NIL,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)"
DAMP,NIL,DATASET,NIL,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)"
dali,NIL,DATASET,NIL,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)"
DALI,NIL,DATASET,NIL,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)"
dali_talt,NIL,DATASET,NIL,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)"
DALI-TALT,NIL,DATASET,NIL,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)"
jamendo,jamendo lyrics,DATASET,jamendo lyrics,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)"
jamendo,jamendo lyrics,DATASET,jamendo lyrics,"Should be -10 to initialize the training""  --datadir_damp                                   # path to DAMP dataset  --datadir_dali                                   # path to DALI dataset  --datadir_dali_talt                              # path to DALI-TALT dataset  --datadir_jamendo                                # path to jamendo dataset  --pretrained_model <model>                       # directory to a pretrained model (if specificed, i.e. models/ijcnn)"
CIFAR-10,cifar-10,DATASET,cifar-10,/data/.  ## Run HOC + Vote Based and Rank Based method  On CIFAR-10 .  ``` sh .
CIFAR-100,cifar-100,DATASET,cifar-100,/test_c10_instance.sh   ```  On CIFAR-100  ``` sh .
Ontonotes 5.0,ontonotes 5.0,DATASET,ontonotes 5.0,"If you just want the data, it's a few steps:  Let `ldc_ontonotes_path` be the path to your LDC download of `Ontonotes 5.0`, that is, `LDC2013T19`."
ontonotes-release-5.0,ontonotes 5.0,DATASET,ontonotes 5.0,Mine looks like `/scr/corpora/ldc/2013/LDC2013T19/ontonotes-release-5.0/data/files/data/`.
conll-2012-train.v4,conll-2012,DATASET,conll-2012,"Next, due to some regrettable firewalling, our script to download the train/dev/test split information fails, so you have to navigate via a browser to:        https://cemantix.org/conll/2012/download/  and manually download `conll-2012-train.v4.tar.gz`, `conll-2012-development.v4.tar.gz`, and then navigate to the `test` folder and download `conll-2012-test-key.tar.gz`."
conll-2012-development.v4,conll-2012,DATASET,conll-2012,"Next, due to some regrettable firewalling, our script to download the train/dev/test split information fails, so you have to navigate via a browser to:        https://cemantix.org/conll/2012/download/  and manually download `conll-2012-train.v4.tar.gz`, `conll-2012-development.v4.tar.gz`, and then navigate to the `test` folder and download `conll-2012-test-key.tar.gz`."
conll-2012-test,conll-2012,DATASET,conll-2012,"Next, due to some regrettable firewalling, our script to download the train/dev/test split information fails, so you have to navigate via a browser to:        https://cemantix.org/conll/2012/download/  and manually download `conll-2012-train.v4.tar.gz`, `conll-2012-development.v4.tar.gz`, and then navigate to the `test` folder and download `conll-2012-test-key.tar.gz`."
ontonotes,ontonotes 5.0,DATASET,ontonotes 5.0,Place these files in the `scripts/ontonotes_scripts/` directory of this repository.
ontonotes,ontonotes 5.0,DATASET,ontonotes 5.0,"Now, run  ``` cd scripts/ontonotes_scripts ldc_ontonotes_path=/scr/corpora/ldc/2013/LDC2013T19/ontonotes-release-5.0/data/files/data/ bash prep_ontonotes_v4.sh $ldc_onotonotes_path ```  Nice."
ontonotes-release-5.0,ontonotes 5.0,DATASET,ontonotes 5.0,"Now, run  ``` cd scripts/ontonotes_scripts ldc_ontonotes_path=/scr/corpora/ldc/2013/LDC2013T19/ontonotes-release-5.0/data/files/data/ bash prep_ontonotes_v4.sh $ldc_onotonotes_path ```  Nice."
ontonotes,ontonotes 5.0,DATASET,ontonotes 5.0,"Now, run  ``` cd scripts/ontonotes_scripts ldc_ontonotes_path=/scr/corpora/ldc/2013/LDC2013T19/ontonotes-release-5.0/data/files/data/ bash prep_ontonotes_v4.sh $ldc_onotonotes_path ```  Nice."
onotonotes,ontonotes 5.0,DATASET,ontonotes 5.0,"Now, run  ``` cd scripts/ontonotes_scripts ldc_ontonotes_path=/scr/corpora/ldc/2013/LDC2013T19/ontonotes-release-5.0/data/files/data/ bash prep_ontonotes_v4.sh $ldc_onotonotes_path ```  Nice."
QuAC,quac,DATASET,quac,"In our evaluation, human annotators chat with conversational QA models about passages from the [QuAC](https://quac.ai) development set, and after that the annotators judge the correctness of model answers."
quac,quac,DATASET,quac,"In our evaluation, human annotators chat with conversational QA models about passages from the [QuAC](https://quac.ai) development set, and after that the annotators judge the correctness of model answers."
QuAC,quac,DATASET,quac,"""context"": ""Azaria wrote and directed the 2004 short film Nobody's Perfect, ..."",         # The ID from the original QuAC dataset."
QuAC,quac,DATASET,quac,"[Auto-rewrite](figs/autorewrite.png)  ## Setup  ### Install dependencies  Please install all dependency packages using the following command: ```bash pip install -r requirements.txt ```  ### Download the datasets  Our experiments use [QuAC dataset](https://quac.ai) for passages and conversations, and the test set of [CANARD dataset](https://sites.google.com/view/qanta/projects/canard) for context-independent questions in `Auto-Replace`.  ## Evaluating existing models  We provide our implementations for the four models that we used in our paper: BERT, [GraphFlow](https://www.ijcai.org/Proceedings/2020/171), [HAM](https://dl.acm.org/doi/abs/10.1145/3357384.3357905), [ExCorD](https://aclanthology.org/2021.acl-long.478/)."
quac,quac,DATASET,quac,"[Auto-rewrite](figs/autorewrite.png)  ## Setup  ### Install dependencies  Please install all dependency packages using the following command: ```bash pip install -r requirements.txt ```  ### Download the datasets  Our experiments use [QuAC dataset](https://quac.ai) for passages and conversations, and the test set of [CANARD dataset](https://sites.google.com/view/qanta/projects/canard) for context-independent questions in `Auto-Replace`.  ## Evaluating existing models  We provide our implementations for the four models that we used in our paper: BERT, [GraphFlow](https://www.ijcai.org/Proceedings/2020/171), [HAM](https://dl.acm.org/doi/abs/10.1145/3357384.3357905), [ExCorD](https://aclanthology.org/2021.acl-long.478/)."
CANARD,canard,DATASET,canard,"[Auto-rewrite](figs/autorewrite.png)  ## Setup  ### Install dependencies  Please install all dependency packages using the following command: ```bash pip install -r requirements.txt ```  ### Download the datasets  Our experiments use [QuAC dataset](https://quac.ai) for passages and conversations, and the test set of [CANARD dataset](https://sites.google.com/view/qanta/projects/canard) for context-independent questions in `Auto-Replace`.  ## Evaluating existing models  We provide our implementations for the four models that we used in our paper: BERT, [GraphFlow](https://www.ijcai.org/Proceedings/2020/171), [HAM](https://dl.acm.org/doi/abs/10.1145/3357384.3357905), [ExCorD](https://aclanthology.org/2021.acl-long.478/)."
coco,ms coco,DATASET,ms coco,"- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [ECCV Caption](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.  ``` pip install cython && pip install -r requirements.txt python -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```  ### Dockerfile  You can use my docker image as well ``` docker pull sanghyukchun/pcme:torch1.2-apex-dali ```  Please Add `--model__cache_dir /vector_cache` when you run the code  ## Configuration  All experiments are based on configuration files (see [config/coco](config/coco) and [config/cub](config/cub))."
coco,ms coco,DATASET,ms coco,"- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [ECCV Caption](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.  ``` pip install cython && pip install -r requirements.txt python -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```  ### Dockerfile  You can use my docker image as well ``` docker pull sanghyukchun/pcme:torch1.2-apex-dali ```  Please Add `--model__cache_dir /vector_cache` when you run the code  ## Configuration  All experiments are based on configuration files (see [config/coco](config/coco) and [config/cub](config/cub))."
COCO Caption,coco captions,DATASET,coco captions,"If you want to change only a few options, instead of re-writing a new configuration file, you can override the configuration as the follows:  ``` python <train | eval>.py --dataloader__batch_size 32 --dataloader__eval_batch_size 8 --model__eval_method matching_prob ```  See [config/parser.py](config/parser.py) for details  ## Dataset preparation  ### COCO Caption  We followed the same split provided by [VSE++](http://www.cs.toronto.edu/~faghri/vsepp/data.tar)."
CUB Caption,NIL,DATASET,NIL,"Note that we also need `instances_<train | val>2014.json` for computing PMRP score.  ### CUB Caption  Download images (CUB-200-2011) from [this link](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html), and download caption from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016)."
CUB-200-2011,cub-200-2011,DATASET,cub-200-2011,"Note that we also need `instances_<train | val>2014.json` for computing PMRP score.  ### CUB Caption  Download images (CUB-200-2011) from [this link](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html), and download caption from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016)."
CUB-200-2011,cub-200-2011,DATASET,cub-200-2011,"Note that we also need `instances_<train | val>2014.json` for computing PMRP score.  ### CUB Caption  Download images (CUB-200-2011) from [this link](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html), and download caption from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016)."
COCO Caption,coco captions,DATASET,coco captions,<br> We are planning to re-implement efficient PMRP as soon as possible.  ### COCO Caption  ``` # Compute recall metrics python evaluate_recall_coco.py .
coco,ms coco,DATASET,ms coco,<br> We are planning to re-implement efficient PMRP as soon as possible.  ### COCO Caption  ``` # Compute recall metrics python evaluate_recall_coco.py .
coco,ms coco,DATASET,ms coco,/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image ```  ``` # Compute plausible match R-Precision (PMRP) metric python extract_rankings_coco.py .
coco,ms coco,DATASET,ms coco,/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image ```  ``` # Compute plausible match R-Precision (PMRP) metric python extract_rankings_coco.py .
coco,ms coco,DATASET,ms coco,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py ."
coco,ms coco,DATASET,ms coco,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py ."
coco,ms coco,DATASET,ms coco,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py ."
coco,ms coco,DATASET,ms coco,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py ."
ECCV Caption,NIL,DATASET,NIL,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py ."
eccv-caption,NIL,DATASET,NIL,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py ."
ECCV Caption,NIL,DATASET,NIL,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py ."
eccv-caption,NIL,DATASET,NIL,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py ."
eccv-caption,NIL,DATASET,NIL,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py ."
CUB Caption,NIL,DATASET,NIL,"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py ."
COCO Caption,coco captions,DATASET,coco captions,"<br> Please note that, hence, the results couldn't be reproduced if you use smaller hardware than V100.  ### COCO Caption  ``` python train_coco.py ."
coco,ms coco,DATASET,ms coco,"<br> Please note that, hence, the results couldn't be reproduced if you use smaller hardware than V100.  ### COCO Caption  ``` python train_coco.py ."
coco,ms coco,DATASET,ms coco,"/config/coco/pcme_coco.yaml --dataset_root <your_dataset_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 46 hours in a single V100 with mixed precision training.  ### CUB Caption  We use CUB Caption dataset [(Reed, et al. 2016)](https://openaccess.thecvf.com/content_cvpr_2016/papers/Reed_Learning_Deep_Representations_CVPR_2016_paper.pdf) as a new cross-modal retrieval benchmark."
coco,ms coco,DATASET,ms coco,"/config/coco/pcme_coco.yaml --dataset_root <your_dataset_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 46 hours in a single V100 with mixed precision training.  ### CUB Caption  We use CUB Caption dataset [(Reed, et al. 2016)](https://openaccess.thecvf.com/content_cvpr_2016/papers/Reed_Learning_Deep_Representations_CVPR_2016_paper.pdf) as a new cross-modal retrieval benchmark."
CUB Caption,NIL,DATASET,NIL,"/config/coco/pcme_coco.yaml --dataset_root <your_dataset_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 46 hours in a single V100 with mixed precision training.  ### CUB Caption  We use CUB Caption dataset [(Reed, et al. 2016)](https://openaccess.thecvf.com/content_cvpr_2016/papers/Reed_Learning_Deep_Representations_CVPR_2016_paper.pdf) as a new cross-modal retrieval benchmark."
CUB Caption,NIL,DATASET,NIL,"/config/coco/pcme_coco.yaml --dataset_root <your_dataset_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 46 hours in a single V100 with mixed precision training.  ### CUB Caption  We use CUB Caption dataset [(Reed, et al. 2016)](https://openaccess.thecvf.com/content_cvpr_2016/papers/Reed_Learning_Deep_Representations_CVPR_2016_paper.pdf) as a new cross-modal retrieval benchmark."
cub,cub-200-2011,DATASET,cub-200-2011,"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.  #### hyperparameter search  We additionally use cross-validation splits by (Xian, et el. 2017), namely using 100 classes for training and 50 classes for validation.   ``` python train_cub.py ."
cub,cub-200-2011,DATASET,cub-200-2011,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name cub_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `cub_trainval2` and `cub_trainval3` as well.  #### training with full training classes  ``` python train_cub.py ."
cub,cub-200-2011,DATASET,cub-200-2011,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name cub_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `cub_trainval2` and `cub_trainval3` as well.  #### training with full training classes  ``` python train_cub.py ."
cub,cub-200-2011,DATASET,cub-200-2011,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name cub_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `cub_trainval2` and `cub_trainval3` as well.  #### training with full training classes  ``` python train_cub.py ."
cub,cub-200-2011,DATASET,cub-200-2011,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name cub_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `cub_trainval2` and `cub_trainval3` as well.  #### training with full training classes  ``` python train_cub.py ."
cub,cub-200-2011,DATASET,cub-200-2011,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name cub_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `cub_trainval2` and `cub_trainval3` as well.  #### training with full training classes  ``` python train_cub.py ."
cub,cub-200-2011,DATASET,cub-200-2011,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name cub_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `cub_trainval2` and `cub_trainval3` as well.  #### training with full training classes  ``` python train_cub.py ."
cub,cub-200-2011,DATASET,cub-200-2011,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 4 hours in a single V100 with mixed precision training.  ## How to cite  ``` @inproceedings{chun2021pcme,     title={Probabilistic Embeddings for Cross-Modal Retrieval},     author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},     year={2021},     booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, } ```  I would like to suggest citing [ECCV Caption](https://github.com/naver-ai/eccv-caption) and [PCME++](https://github.com/naver-ai/pcmepp), too. ``` @inproceedings{chun2022eccv_caption,     title={ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},      author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},     year={2022},     booktitle={European Conference on Computer Vision (ECCV)}, }  @inproceedings{chun2024pcmepp,     title={Improved Probabilistic Image-Text Representations},     author={Chun, Sanghyuk},     year={2024},     booktitle={International Conference on Learning Representations (ICLR)}, } ```  ## License  ``` MIT License  Copyright (c) 2021-present NAVER Corp."
ECCV Caption,NIL,DATASET,NIL,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 4 hours in a single V100 with mixed precision training.  ## How to cite  ``` @inproceedings{chun2021pcme,     title={Probabilistic Embeddings for Cross-Modal Retrieval},     author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},     year={2021},     booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, } ```  I would like to suggest citing [ECCV Caption](https://github.com/naver-ai/eccv-caption) and [PCME++](https://github.com/naver-ai/pcmepp), too. ``` @inproceedings{chun2022eccv_caption,     title={ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},      author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},     year={2022},     booktitle={European Conference on Computer Vision (ECCV)}, }  @inproceedings{chun2024pcmepp,     title={Improved Probabilistic Image-Text Representations},     author={Chun, Sanghyuk},     year={2024},     booktitle={International Conference on Learning Representations (ICLR)}, } ```  ## License  ``` MIT License  Copyright (c) 2021-present NAVER Corp."
eccv-caption,NIL,DATASET,NIL,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 4 hours in a single V100 with mixed precision training.  ## How to cite  ``` @inproceedings{chun2021pcme,     title={Probabilistic Embeddings for Cross-Modal Retrieval},     author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},     year={2021},     booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, } ```  I would like to suggest citing [ECCV Caption](https://github.com/naver-ai/eccv-caption) and [PCME++](https://github.com/naver-ai/pcmepp), too. ``` @inproceedings{chun2022eccv_caption,     title={ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},      author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},     year={2022},     booktitle={European Conference on Computer Vision (ECCV)}, }  @inproceedings{chun2024pcmepp,     title={Improved Probabilistic Image-Text Representations},     author={Chun, Sanghyuk},     year={2024},     booktitle={International Conference on Learning Representations (ICLR)}, } ```  ## License  ``` MIT License  Copyright (c) 2021-present NAVER Corp."
eccv_caption,NIL,DATASET,NIL,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 4 hours in a single V100 with mixed precision training.  ## How to cite  ``` @inproceedings{chun2021pcme,     title={Probabilistic Embeddings for Cross-Modal Retrieval},     author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},     year={2021},     booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, } ```  I would like to suggest citing [ECCV Caption](https://github.com/naver-ai/eccv-caption) and [PCME++](https://github.com/naver-ai/pcmepp), too. ``` @inproceedings{chun2022eccv_caption,     title={ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},      author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},     year={2022},     booktitle={European Conference on Computer Vision (ECCV)}, }  @inproceedings{chun2024pcmepp,     title={Improved Probabilistic Image-Text Representations},     author={Chun, Sanghyuk},     year={2024},     booktitle={International Conference on Learning Representations (ICLR)}, } ```  ## License  ``` MIT License  Copyright (c) 2021-present NAVER Corp."
ECCV Caption,NIL,DATASET,NIL,"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 4 hours in a single V100 with mixed precision training.  ## How to cite  ``` @inproceedings{chun2021pcme,     title={Probabilistic Embeddings for Cross-Modal Retrieval},     author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},     year={2021},     booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, } ```  I would like to suggest citing [ECCV Caption](https://github.com/naver-ai/eccv-caption) and [PCME++](https://github.com/naver-ai/pcmepp), too. ``` @inproceedings{chun2022eccv_caption,     title={ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},      author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},     year={2022},     booktitle={European Conference on Computer Vision (ECCV)}, }  @inproceedings{chun2024pcmepp,     title={Improved Probabilistic Image-Text Representations},     author={Chun, Sanghyuk},     year={2024},     booktitle={International Conference on Learning Representations (ICLR)}, } ```  ## License  ``` MIT License  Copyright (c) 2021-present NAVER Corp."
UW,NIL,DATASET,NIL,/model_recommendation/rank_utility_privacy.csv` - separate ranking for UW and VUMC data under different synthesis paradigms
VUMC,NIL,DATASET,NIL,/model_recommendation/rank_utility_privacy.csv` - separate ranking for UW and VUMC data under different synthesis paradigms
UW,NIL,DATASET,NIL,/model_recommendation/rank_utility_privacy.csv` - comparison rank for UW synthetic data generated under combined and separate synthesis paradigms.
UW,NIL,DATASET,NIL,|Dataset | Number of records (rows)| Number of features (columns) | | ----------- | ----------- | ----------- | |UW training | 13213 | 2670 | |UW test | 13213 | 2670 | |UW population | 46698 | 2670 | |VUMC training | 1434 | 2596 | |VUMC test | 615 | 2596 | |VUMC population | 63303 | 2596  All the names for binary medical features in the datasets shown in the headlines files have been converted to pseudonyms.
UW,NIL,DATASET,NIL,|Dataset | Number of records (rows)| Number of features (columns) | | ----------- | ----------- | ----------- | |UW training | 13213 | 2670 | |UW test | 13213 | 2670 | |UW population | 46698 | 2670 | |VUMC training | 1434 | 2596 | |VUMC test | 615 | 2596 | |VUMC population | 63303 | 2596  All the names for binary medical features in the datasets shown in the headlines files have been converted to pseudonyms.
UW,NIL,DATASET,NIL,|Dataset | Number of records (rows)| Number of features (columns) | | ----------- | ----------- | ----------- | |UW training | 13213 | 2670 | |UW test | 13213 | 2670 | |UW population | 46698 | 2670 | |VUMC training | 1434 | 2596 | |VUMC test | 615 | 2596 | |VUMC population | 63303 | 2596  All the names for binary medical features in the datasets shown in the headlines files have been converted to pseudonyms.
VUMC,NIL,DATASET,NIL,|Dataset | Number of records (rows)| Number of features (columns) | | ----------- | ----------- | ----------- | |UW training | 13213 | 2670 | |UW test | 13213 | 2670 | |UW population | 46698 | 2670 | |VUMC training | 1434 | 2596 | |VUMC test | 615 | 2596 | |VUMC population | 63303 | 2596  All the names for binary medical features in the datasets shown in the headlines files have been converted to pseudonyms.
VUMC,NIL,DATASET,NIL,|Dataset | Number of records (rows)| Number of features (columns) | | ----------- | ----------- | ----------- | |UW training | 13213 | 2670 | |UW test | 13213 | 2670 | |UW population | 46698 | 2670 | |VUMC training | 1434 | 2596 | |VUMC test | 615 | 2596 | |VUMC population | 63303 | 2596  All the names for binary medical features in the datasets shown in the headlines files have been converted to pseudonyms.
VUMC,NIL,DATASET,NIL,|Dataset | Number of records (rows)| Number of features (columns) | | ----------- | ----------- | ----------- | |UW training | 13213 | 2670 | |UW test | 13213 | 2670 | |UW population | 46698 | 2670 | |VUMC training | 1434 | 2596 | |VUMC test | 615 | 2596 | |VUMC population | 63303 | 2596  All the names for binary medical features in the datasets shown in the headlines files have been converted to pseudonyms.
UW,NIL,DATASET,NIL,"Besides binary medical features, the UW datasets have features including race and gender."
VUMC,NIL,DATASET,NIL,"Besides binary medical features, the VUMC datasets have features including race, age, gender, and 7 continuous medical features."
Unhealthy Comments Corpus,NIL,DATASET,NIL,# The Unhealthy Comments Corpus (UCC)   The Unhealthy Comments Corpus (UCC) is corpus of 44355 comments intended to assist in research on identifying subtle attributes which contribute to unhealthy conversations online.
UCC,ucc,DATASET,ucc,# The Unhealthy Comments Corpus (UCC)   The Unhealthy Comments Corpus (UCC) is corpus of 44355 comments intended to assist in research on identifying subtle attributes which contribute to unhealthy conversations online.
Unhealthy Comments Corpus,ucc,DATASET,ucc,# The Unhealthy Comments Corpus (UCC)   The Unhealthy Comments Corpus (UCC) is corpus of 44355 comments intended to assist in research on identifying subtle attributes which contribute to unhealthy conversations online.
UCC,ucc,DATASET,ucc,# The Unhealthy Comments Corpus (UCC)   The Unhealthy Comments Corpus (UCC) is corpus of 44355 comments intended to assist in research on identifying subtle attributes which contribute to unhealthy conversations online.
UCC,ucc,DATASET,ucc,"The UCC contributes further high quality data on  attributes  like  sarcasm,  hostility,  and  condescension, adding to existing datasets on these and related attributes, and provides (to the best of our knowledge) the first dataset of this scale with labels for dismissiveness,  unfair generalisations,  antagonistic behavior, and overall assessments of whether those comments fall within 'healthy' conversation."
SOCC,NIL,DATASET,NIL,The raw comments were taken from the [comments](https://github.com/sfu-discourse-lab/SOCC#comments) corpus within the [SFU Opinion and Comments Corpus](https://github.com/sfu-discourse-lab/SOCC).   ## Baseline classification  We provide notebooks to replicate our baseline classification results on this dataset in `UnhealthyConversations.ipynb`.
SFU Opinion and Comments Corpus,NIL,DATASET,NIL,The raw comments were taken from the [comments](https://github.com/sfu-discourse-lab/SOCC#comments) corpus within the [SFU Opinion and Comments Corpus](https://github.com/sfu-discourse-lab/SOCC).   ## Baseline classification  We provide notebooks to replicate our baseline classification results on this dataset in `UnhealthyConversations.ipynb`.
SOCC,NIL,DATASET,NIL,The raw comments were taken from the [comments](https://github.com/sfu-discourse-lab/SOCC#comments) corpus within the [SFU Opinion and Comments Corpus](https://github.com/sfu-discourse-lab/SOCC).   ## Baseline classification  We provide notebooks to replicate our baseline classification results on this dataset in `UnhealthyConversations.ipynb`.
eICUSubsampleUnobs,NIL,DATASET,NIL,"For example, to train a single model: ``` python -m clinicaldg.scripts.train\        --algorithm ERM\        --dataset eICUSubsampleUnobs\        --es_method val\        --hparams  '{""eicu_architecture"": ""GRU"", ""eicu_subsample_g1_mean"": 0.5, ""eicu_subsample_g2_mean"": 0.05}'\        --output_dir /path/to/output ```  To sweep a range of datasets, algorithms, and hyperparameters: ``` python -m clinicaldg.scripts.sweep launch\        --output_dir=/my/sweep/output/path\        --command_launcher slurm\        --algorithms ERMID ERM IRM VREx RVP IGA CORAL MLDG GroupDRO \        --datasets CXR CXRBinary\        --n_hparams 10\        --n_trials 5\        --es_method train\        --hparams '{""cxr_augment"": 1}' ```  A detailed list of `hparams` available for each dataset can be found [here](hparams.md)."
CXR,NIL,DATASET,NIL,"For example, to train a single model: ``` python -m clinicaldg.scripts.train\        --algorithm ERM\        --dataset eICUSubsampleUnobs\        --es_method val\        --hparams  '{""eicu_architecture"": ""GRU"", ""eicu_subsample_g1_mean"": 0.5, ""eicu_subsample_g2_mean"": 0.05}'\        --output_dir /path/to/output ```  To sweep a range of datasets, algorithms, and hyperparameters: ``` python -m clinicaldg.scripts.sweep launch\        --output_dir=/my/sweep/output/path\        --command_launcher slurm\        --algorithms ERMID ERM IRM VREx RVP IGA CORAL MLDG GroupDRO \        --datasets CXR CXRBinary\        --n_hparams 10\        --n_trials 5\        --es_method train\        --hparams '{""cxr_augment"": 1}' ```  A detailed list of `hparams` available for each dataset can be found [here](hparams.md)."
CXRBinary,NIL,DATASET,NIL,"For example, to train a single model: ``` python -m clinicaldg.scripts.train\        --algorithm ERM\        --dataset eICUSubsampleUnobs\        --es_method val\        --hparams  '{""eicu_architecture"": ""GRU"", ""eicu_subsample_g1_mean"": 0.5, ""eicu_subsample_g2_mean"": 0.05}'\        --output_dir /path/to/output ```  To sweep a range of datasets, algorithms, and hyperparameters: ``` python -m clinicaldg.scripts.sweep launch\        --output_dir=/my/sweep/output/path\        --command_launcher slurm\        --algorithms ERMID ERM IRM VREx RVP IGA CORAL MLDG GroupDRO \        --datasets CXR CXRBinary\        --n_hparams 10\        --n_trials 5\        --es_method train\        --hparams '{""cxr_augment"": 1}' ```  A detailed list of `hparams` available for each dataset can be found [here](hparams.md)."
COCO,ms coco,DATASET,ms coco,Link datasets  COCO  Download the json file([coco_to_ytvis2019.json](https://drive.google.com/file/d/17L33_woQh7eUMemCmnFDOgPUKi-2fTW0/view?
coco,ms coco,DATASET,ms coco,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.
coco,ms coco,DATASET,ms coco,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.
coco,ms coco,DATASET,ms coco,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.
coco,ms coco,DATASET,ms coco,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.
YTVIS 2019,youtube-vis 2019,DATASET,youtube-vis 2019,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.
ytvis_2019,youtube-vis 2019,DATASET,youtube-vis 2019,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.
ytvis_2019,youtube-vis 2019,DATASET,youtube-vis 2019,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.
ytvis_2019,youtube-vis 2019,DATASET,youtube-vis 2019,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.
ytvis_2019,youtube-vis 2019,DATASET,youtube-vis 2019,usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.
COCO,ms coco,DATASET,ms coco,* Training using 4 GPUS(TESLA V100-PCIE-32GB) * Pre-training on COCO dataset ```bash python train_net.py --num-gpus 4 --config-file .
coco,ms coco,DATASET,ms coco,/checkpoint/coco/ ``` * Fine-tuning on YTVIS2019 with pre-trained weights on COCO dataset ([r50_coco.pth](https://drive.google.com/file/d/1rb3i9MBtAjh3SJ2AWdgb3PvuPpF8Swpi/view?
YTVIS2019,youtube-vis 2019,DATASET,youtube-vis 2019,/checkpoint/coco/ ``` * Fine-tuning on YTVIS2019 with pre-trained weights on COCO dataset ([r50_coco.pth](https://drive.google.com/file/d/1rb3i9MBtAjh3SJ2AWdgb3PvuPpF8Swpi/view?
COCO,ms coco,DATASET,ms coco,/checkpoint/coco/ ``` * Fine-tuning on YTVIS2019 with pre-trained weights on COCO dataset ([r50_coco.pth](https://drive.google.com/file/d/1rb3i9MBtAjh3SJ2AWdgb3PvuPpF8Swpi/view?
ytvis_2019,youtube-vis 2019,DATASET,youtube-vis 2019,/checkpoint/ytvis_2019/ MODEL.WEIGHTS path/to/pre-trained-model.pth ```  4.
YTVIS 2019,youtube-vis 2019,DATASET,youtube-vis 2019,Evaluating on YTVIS 2019 ```bash python train_net.py --eval-only --num-gpus 1 --config-file .
ytvis_2019,youtube-vis 2019,DATASET,youtube-vis 2019,"/checkpoint/ytvis_2019/ MODEL.WEIGHTS path/to/model.pth ``` ""results.json"" saved in OUTPUT_DIR/inference/  ## Model Checkpoints (YTVIS 2019) Due to the small size of YTVIS dataset, the scores may fluctuate even if retrained with the same configuration."
YTVIS 2019,youtube-vis 2019,DATASET,youtube-vis 2019,"/checkpoint/ytvis_2019/ MODEL.WEIGHTS path/to/model.pth ``` ""results.json"" saved in OUTPUT_DIR/inference/  ## Model Checkpoints (YTVIS 2019) Due to the small size of YTVIS dataset, the scores may fluctuate even if retrained with the same configuration."
YTVIS,youtube-vis 2019,DATASET,youtube-vis 2019,"/checkpoint/ytvis_2019/ MODEL.WEIGHTS path/to/model.pth ``` ""results.json"" saved in OUTPUT_DIR/inference/  ## Model Checkpoints (YTVIS 2019) Due to the small size of YTVIS dataset, the scores may fluctuate even if retrained with the same configuration."
YouTube-VIS 2019,youtube-vis 2019,DATASET,youtube-vis 2019,usp=sharing) |  ## Video Comparisons The overall flow of our VISOLO and the comparison of different VIS methods on the YouTube-VIS 2019 dataset are provided at https://youtu.be/j33H7vcJ2uU  ## License  VISOLO is released under the [Apache 2.0 license](LICENSE).
KAIST,kaist,DATASET,kaist,"If using the KAIST dataset, we recommend that you place the images in the root directory following the defined list on line 60 of `train.py`.  #### Dataset Selection GUI   !"
KAIST,kaist,DATASET,kaist,/Figures/data_selection.png)  This is highly recommended to do since the KAIST dataset is very large and our current implementation takes every 200th image.
KAIST,kaist,DATASET,kaist,"When using the KAIST dataset for evaluation, it is also recommended to simply add `""KAIST""` to the `saved_dirs.json` file."
KAIST,kaist,DATASET,kaist,"When using the KAIST dataset for evaluation, it is also recommended to simply add `""KAIST""` to the `saved_dirs.json` file."
Thermal,NIL,DATASET,NIL,"/Datasets/Thermal/train/640_flir_hr""     ],     ""test_dirs"": [       ""KAIST"",       ""."
KAIST,kaist,DATASET,kaist,"/Datasets/Thermal/train/640_flir_hr""     ],     ""test_dirs"": [       ""KAIST"",       ""."
Thermal,NIL,DATASET,NIL,"/Datasets/Thermal/test/320_axis_mr"",       ""."
Thermal,NIL,DATASET,NIL,"/Datasets/Thermal/test/640_flir_hr"",       ""."
Thermal,NIL,DATASET,NIL,"/Datasets/Thermal/test/Flir_test"",       ""."
Thermal,NIL,DATASET,NIL,"/Datasets/Thermal/test/160_domo_lr""     ],     ""val_dirs"": [       ""."
Thermal,NIL,DATASET,NIL,"/Datasets/Thermal/test/Flir_test""     ]   },   ""div2k"": {     ""train_dirs"": [       ""."
DIV2K,div2k,DATASET,div2k,"/Datasets/DIV2K/train""     ],     ""test_dirs"": [       ""."
DIV2K,div2k,DATASET,div2k,"/Datasets/DIV2K/test"",       ""."
Urban100,urban100,DATASET,urban100,"/Datasets/Urban100"",       ""."
BSDS100,bsd100,DATASET,bsd100,"/Datasets/BSDS100"",       ""."
Thermal,NIL,DATASET,NIL,"/Datasets/Thermal/test/320_axis_mr"",       ""."
Thermal,NIL,DATASET,NIL,"/Datasets/Thermal/test/640_flir_hr"",       ""."
Thermal,NIL,DATASET,NIL,"/Datasets/Thermal/test/Flir_test"",       ""."
Thermal,NIL,DATASET,NIL,"/Datasets/Thermal/test/160_domo_lr""     ],     ""val_dirs"": [       ""."
DIV2K,div2k,DATASET,div2k,"/Datasets/DIV2K/test""     ]   } } ```  In this JSON file, `Flir_test` refers to the [FLIR Thermal Dataset for Algorithm Training](https://www.flir.com/oem/adas/adas-dataset-form/), `320_axis_mr, 640_flir_hr, 160_domo_lr` refers to the [Thermal Image Super-Resolution Challenge](https://pbvs-workshop.github.io/datasets.html) dataset, and `KAIST` refers to the [KAIST](https://soonminhwang.github.io/rgbt-ped-detection/) dataset."
KAIST,kaist,DATASET,kaist,"/Datasets/DIV2K/test""     ]   } } ```  In this JSON file, `Flir_test` refers to the [FLIR Thermal Dataset for Algorithm Training](https://www.flir.com/oem/adas/adas-dataset-form/), `320_axis_mr, 640_flir_hr, 160_domo_lr` refers to the [Thermal Image Super-Resolution Challenge](https://pbvs-workshop.github.io/datasets.html) dataset, and `KAIST` refers to the [KAIST](https://soonminhwang.github.io/rgbt-ped-detection/) dataset."
KAIST,kaist,DATASET,kaist,"/Datasets/DIV2K/test""     ]   } } ```  In this JSON file, `Flir_test` refers to the [FLIR Thermal Dataset for Algorithm Training](https://www.flir.com/oem/adas/adas-dataset-form/), `320_axis_mr, 640_flir_hr, 160_domo_lr` refers to the [Thermal Image Super-Resolution Challenge](https://pbvs-workshop.github.io/datasets.html) dataset, and `KAIST` refers to the [KAIST](https://soonminhwang.github.io/rgbt-ped-detection/) dataset."
SciNLI,NIL,DATASET,NIL,"# [SciNLI: A Corpus for Natural Language Inference on Scientific Text](https://aclanthology.org/2022.acl-long.511.pdf) This repository contains the dataset and code for the ACL 2022 paper ""SciNLI: A Corpus for Natural Language Inference on Scientific Text."""
SciNLI,NIL,DATASET,NIL,"# [SciNLI: A Corpus for Natural Language Inference on Scientific Text](https://aclanthology.org/2022.acl-long.511.pdf) This repository contains the dataset and code for the ACL 2022 paper ""SciNLI: A Corpus for Natural Language Inference on Scientific Text."""
SciNLI,NIL,DATASET,NIL,"In this paper, we introduce SciNLI, a large dataset for NLI that captures the formality in scientific text and contains 107,412 sentence pairs extracted from scholarly papers on NLP and computational linguistics."
SciNLI,NIL,DATASET,NIL,Our experiments show that SciNLI is harder to classify than the existing NLI datasets.
SciNLI,NIL,DATASET,NIL,Our best performing model with XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23% showing that there is substantial room for improvement.  ## Dataset Description We derive [SciNLI](https://drive.google.com/drive/folders/1kjBTVBV1HlMWW5xK8V096LahsU3pULHU?
SNLI,snil,DATASET,snil,"* 'label': corresponding label representing the semantic relation between the premise and hypothesis.      => train.jsonl, test.jsonl and dev.jsonl contain the same data as the CSV files but they are formatted in a json formal similar to SNLI and MNLI."
MNLI,NIL,DATASET,NIL,"* 'label': corresponding label representing the semantic relation between the premise and hypothesis.      => train.jsonl, test.jsonl and dev.jsonl contain the same data as the CSV files but they are formatted in a json formal similar to SNLI and MNLI."
SciNLI,NIL,DATASET,NIL,"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions."
PASCAL VOC,pascal voc,DATASET,pascal voc,"Fast R-CNN  - trains state-of-the-art models, like VGG16, 9x faster than traditional R-CNN and 3x faster than SPPnet,  - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time,  - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet,  - and is written in Python and C++/Caffe."
PASCAL VOC,pascal voc,DATASET,pascal voc,[optional] MATLAB (required for PASCAL VOC evaluation only)  ### Requirements: hardware  1.
PASCAL VOC 2007,pascal voc 2007,DATASET,pascal voc 2007,/tools/demo.py ``` The demo performs detection using a VGG16 network trained for detection on PASCAL VOC 2007.
voc2007,vot2020,DATASET,vot2020,"Download the training, validation, test data and VOCdevkit   ```Shell  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtrainval_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtest_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCdevkit_08-Jun-2007.tar  ```   2."
voc2007,vot2020,DATASET,vot2020,"Download the training, validation, test data and VOCdevkit   ```Shell  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtrainval_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtest_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCdevkit_08-Jun-2007.tar  ```   2."
voc2007,vot2020,DATASET,vot2020,"Download the training, validation, test data and VOCdevkit   ```Shell  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtrainval_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtest_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCdevkit_08-Jun-2007.tar  ```   2."
PASCAL VOC,pascal voc,DATASET,pascal voc,Create symlinks for the PASCAL VOC dataset   ```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5.
PASCAL,pascal voc,DATASET,pascal voc,Create symlinks for the PASCAL VOC dataset   ```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5.
PASCAL VOC 2010,pascal voc 2011,DATASET,pascal voc 2011,[Optional] follow similar steps to get PASCAL VOC 2010 and 2012 6.
VOC2007,pascal voc 2007,DATASET,pascal voc 2007,Follow the next sections to download pre-computed object proposals and pre-trained ImageNet models  ### Download pre-computed Selective Search object proposals  Pre-computed selective search boxes can also be downloaded for VOC2007 and VOC2012.
VOC2012,pascal voc,DATASET,pascal voc,Follow the next sections to download pre-computed object proposals and pre-trained ImageNet models  ### Download pre-computed Selective Search object proposals  Pre-computed selective search boxes can also be downloaded for VOC2007 and VOC2012.
ImageNet,imagenet,DATASET,imagenet,"/data/scripts/fetch_selective_search_data.sh ```  This will populate the `$FRCN_ROOT/data` folder with `selective_selective_data`.  ### Download pre-trained ImageNet models  Pre-trained ImageNet models can be downloaded for the three networks described in the paper: CaffeNet (model **S**), VGG_CNN_M_1024 (model **M**), and VGG16 (model **L**)."
VOC 2007,pascal voc,DATASET,pascal voc,"For example, train a VGG16 network on VOC 2007 trainval:  ```Shell ."
PASCAL VOC,pascal voc,DATASET,pascal voc,MATLAB is currently required for PASCAL VOC evaluation.
VOC 2007,pascal voc 2007,DATASET,pascal voc 2007,"For example, test the VGG 16 network on VOC 2007 test:  ```Shell ."
voc_2007,pascal voc 2007,DATASET,pascal voc 2007,/tools/test_net.py --gpu 1 --def models/VGG16/test.prototxt \  --net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel ```  Test output is written underneath `$FRCN_ROOT/output`.
voc_2007,pascal voc 2007,DATASET,pascal voc 2007,/tools/compress_net.py --def models/VGG16/test.prototxt \  --def-svd models/VGG16/compressed/test.prototxt \     --net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel # Test the model you just compressed .
PASCAL VOC,pascal voc,DATASET,pascal voc,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val"
voc_2007,pascal voc 2007,DATASET,pascal voc 2007,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val"
voc_2007,pascal voc 2007,DATASET,pascal voc 2007,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val"
fast_rcnn,NIL,DATASET,NIL,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val"
voc_2007,pascal voc 2007,DATASET,pascal voc 2007,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val"
voc_2007,pascal voc 2007,DATASET,pascal voc 2007,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val"
voc_2012,pascal voc 2007,DATASET,pascal voc 2007,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val"
voc_2012,pascal voc 2007,DATASET,pascal voc 2007,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val"
VOC07,pascal voc 2007,DATASET,pascal voc 2007,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val"
VOC12,pascal voc 2007,DATASET,pascal voc 2007,"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val"
MEDI+MTEBcls,NIL,DATASET,NIL,The models are available on the Hugging Face model hub:  - [avsolatorio/GIST-large-Embedding-v0](https://huggingface.co/avsolatorio/GIST-large-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.
MEDI+MTEBcls,NIL,DATASET,NIL,The base model used is the [`BAAI/bge-large-en-v1.5`](https://huggingface.co/BAAI/bge-large-en-v1.5). - [avsolatorio/GIST-Embedding-v0](https://huggingface.co/avsolatorio/GIST-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.
MEDI+MTEBcls,NIL,DATASET,NIL,The base model used is the [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5). - [avsolatorio/GIST-small-Embedding-v0](https://huggingface.co/avsolatorio/GIST-small-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.
MEDI+MTEBcls,NIL,DATASET,NIL,The base model used is the [`BAAI/bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-small-en-v1.5). - [avsolatorio/GIST-all-MiniLM-L6-v2](https://huggingface.co/avsolatorio/GIST-all-MiniLM-L6-v2): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.
MEDI,NIL,DATASET,NIL,# Data  The dataset used is a compilation of the MEDI dataset and the MTEB Classification training dataset.
MTEB Classification,mteb,DATASET,mteb,# Data  The dataset used is a compilation of the MEDI dataset and the MTEB Classification training dataset.
avsolatorio/medi-data-mteb_avs_triplets,NIL,DATASET,NIL,"A HuggingFace Dataset version of the compiled dataset, and the specific revision used to train the model, is available:  - Dataset: [avsolatorio/medi-data-mteb_avs_triplets](https://huggingface.co/datasets/avsolatorio/medi-data-mteb_avs_triplets) - Revision: 238a0499b6e6b690cc64ea56fde8461daa8341bb  The dataset contains a `task_type` key which can be used to select only the mteb classification tasks (prefixed with `mteb_`)."
avsolatorio/medi-data-mteb_avs_triplets,NIL,DATASET,NIL,"A HuggingFace Dataset version of the compiled dataset, and the specific revision used to train the model, is available:  - Dataset: [avsolatorio/medi-data-mteb_avs_triplets](https://huggingface.co/datasets/avsolatorio/medi-data-mteb_avs_triplets) - Revision: 238a0499b6e6b690cc64ea56fde8461daa8341bb  The dataset contains a `task_type` key which can be used to select only the mteb classification tasks (prefixed with `mteb_`)."
mteb,mteb,DATASET,mteb,"A HuggingFace Dataset version of the compiled dataset, and the specific revision used to train the model, is available:  - Dataset: [avsolatorio/medi-data-mteb_avs_triplets](https://huggingface.co/datasets/avsolatorio/medi-data-mteb_avs_triplets) - Revision: 238a0499b6e6b690cc64ea56fde8461daa8341bb  The dataset contains a `task_type` key which can be used to select only the mteb classification tasks (prefixed with `mteb_`)."
MEDI,NIL,DATASET,NIL,"The **MEDI Dataset** is published in the following paper: [One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://arxiv.org/abs/2212.09741)."
MTEB,mteb,DATASET,mteb,"The MTEB Benchmark results of the GIST embedding model, compared with the base model, suggest that the fine-tuning dataset has perturbed the model considerably, which resulted in significant improvements in certain tasks while adversely degrading performance in some."
MTEB,mteb,DATASET,mteb,# Evaluation  The model was evaluated using the [MTEB Evaluation](https://huggingface.co/mteb) suite
mteb,mteb,DATASET,mteb,# Evaluation  The model was evaluated using the [MTEB Evaluation](https://huggingface.co/mteb) suite
USPTO,uspto-50k,DATASET,uspto-50k,"**Correction**: In our paper, we denote the dataset 'USPTO' incorrectly as 'USTPO'."
ZINC,zinc,DATASET,zinc,"It is also easy to build your own library, you only need to create a pickled file of a dictionary as:     {         'reactant' :[List of smiles]         'reagent' :[List of smiles]     } You can run this to generate the representation for this library: ``` python generation/build_react_lib.py --input_file $YOUR_LIB_PATH --model_dir ckpt/uni_rxn_gen.ckpt ``` Run this script with the default configuration will create a representation library using provided reactants (from ZINC subset) and reagents (from USPTO)."
USPTO,uspto-50k,DATASET,uspto-50k,"It is also easy to build your own library, you only need to create a pickled file of a dictionary as:     {         'reactant' :[List of smiles]         'reagent' :[List of smiles]     } You can run this to generate the representation for this library: ``` python generation/build_react_lib.py --input_file $YOUR_LIB_PATH --model_dir ckpt/uni_rxn_gen.ckpt ``` Run this script with the default configuration will create a representation library using provided reactants (from ZINC subset) and reagents (from USPTO)."
USPTO,uspto-50k,DATASET,uspto-50k,"Then you are able to run the generation process based on your library ```  #generate structure analogues  INPUT=/place/of/your/input/seed #support sdf file and raw smiles file OUTPUT=/place/to_store/your/output  python generation/generate_paths_reb.py --model_dir ckpt/uni_rxn_gen.ckpt --input_file $INPUT --react_lib_file dataset/data/react_lib_smi_rep.pkl --output_dir $OUTPUT ```  The output file is a list of chemical reaction paths, each path is formulated as: input seed --> reactants, reagents --> intermediate product --> reactants, reagents --> intermediate product --> ... --> final product  See more configuration guide by running `python generation/generate_paths_reb.py -h`    ## Train From Scratch (Optional) ### Dataset Construction  First download the USTPO_MIT dataset from (https://github.com/wengong-jin/nips17-rexgen/blob/master/USPTO/data.zip) and save it to `dataset/raw/`, also unzip it."
LIDC-IDRI,lidc-idri,DATASET,lidc-idri,"The project is built upon two prominent datasets: **LIDC-IDRI** and **LUNA16**, both of which are publicly available and widely used in lung nodule research."
LUNA16,luna16,DATASET,luna16,"The project is built upon two prominent datasets: **LIDC-IDRI** and **LUNA16**, both of which are publicly available and widely used in lung nodule research."
LIDC-IDRI,lidc-idri,DATASET,lidc-idri,"By utilizing these datasets, the project aims to achieve a more comprehensive analysis of the performance of deep learning models in the medical imaging field.  ## Datasets The project utilizes several key datasets that have been essential in driving advancements in lung nodule detection and diagnosis:  - **LIDC-IDRI**: A large-scale dataset containing over 1,000 lung CT cases with multi-radiologist annotations."
LUNA16,luna16,DATASET,luna16,"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification."
LIDC-IDRI,lidc-idri,DATASET,lidc-idri,"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification."
LUNA16,luna16,DATASET,luna16,"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification."
ELCAP,NIL,DATASET,NIL,"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification."
NSCLC,NIL,DATASET,NIL,"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification."
ANODE09,NIL,DATASET,NIL,"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification."
THUMOS14,thumos14,DATASET,thumos14,"Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points and crossing the 60% mAP for the first time."
ActivityNet 1.3,activitynet,DATASET,activitynet,"Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.56% average mAP) and the more challenging EPIC-Kitchens 100 (+13.5% average mAP over prior works)."
EPIC-Kitchens 100,epic-kitchens-100,DATASET,epic-kitchens-100,"Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.56% average mAP) and the more challenging EPIC-Kitchens 100 (+13.5% average mAP over prior works)."
Ego4D Moment Queries,NIL,DATASET,NIL,"In addition, ActionFormer is the backbone for many winning solutions in the Ego4D Moment Queries Challenge 2022."
Ego4D Moment Queries,NIL,DATASET,NIL,logo=arXiv)](https://arxiv.org/abs/2404.02257) <br>  ## Changelog * 11/18/2022: We have released the [tech report](https://arxiv.org/abs/2211.09074) for our submission to the [Ego4D Moment Queries (MQ) Challenge](https://eval.ai/web/challenges/challenge-page/1626/overview).
Ego4D,ego4d,DATASET,ego4d,"The code repo now includes config files, pre-trained models and results on the Ego4D MQ benchmark"
ActivityNet,activitynet,DATASET,activitynet,* 08/01/2022: Updated code repo with latest results on ActivityNet
THUMOS14,thumos14,DATASET,thumos14,"* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average mAP on THUMOS14 (>66.0%) and slightly improved results on ActivityNet and EPIC-Kitchens 100.   ## Code Overview The structure of this code repo is heavily inspired by Detectron2."
ActivityNet,activitynet,DATASET,activitynet,"* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average mAP on THUMOS14 (>66.0%) and slightly improved results on ActivityNet and EPIC-Kitchens 100.   ## Code Overview The structure of this code repo is heavily inspired by Detectron2."
EPIC-Kitchens 100,epic-kitchens-100,DATASET,epic-kitchens-100,"* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average mAP on THUMOS14 (>66.0%) and slightly improved results on ActivityNet and EPIC-Kitchens 100.   ## Code Overview The structure of this code repo is heavily inspired by Detectron2."
THUMOS14,thumos14,DATASET,thumos14,"/libs/utils: Utility functions for training, inference, and postprocessing.  ## Installation * Follow INSTALL.md for installing necessary dependencies and compiling the code.  ## Frequently Asked Questions * See FAQ.md.   ## To Reproduce Our Results on THUMOS14 **Download Features and Annotations** * Download *thumos.tar.gz* (`md5sum 375f76ffbf7447af1035e694971ec9b2`) from [this Box link](https://uwmadison.box.com/s/glpuxadymf3gd01m1cj6g5c3bn39qbgr) or [this Google Drive link](https://drive.google.com/file/d/1zt2eoldshf99vJMDuu8jqxda55dCyhZP/view?"
thumos,thumos14,DATASET,thumos14,"/libs/utils: Utility functions for training, inference, and postprocessing.  ## Installation * Follow INSTALL.md for installing necessary dependencies and compiling the code.  ## Frequently Asked Questions * See FAQ.md.   ## To Reproduce Our Results on THUMOS14 **Download Features and Annotations** * Download *thumos.tar.gz* (`md5sum 375f76ffbf7447af1035e694971ec9b2`) from [this Box link](https://uwmadison.box.com/s/glpuxadymf3gd01m1cj6g5c3bn39qbgr) or [this Google Drive link](https://drive.google.com/file/d/1zt2eoldshf99vJMDuu8jqxda55dCyhZP/view?"
ActivityNet,activitynet,DATASET,activitynet,"pwd=74eh). * The file includes I3D features, action annotations in json format (similar to ActivityNet annotation format), and external classification scores."
thumos,thumos14,DATASET,thumos14,/configs/thumos_i3d.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.
thumos,thumos14,DATASET,thumos14,/ckpt/thumos_i3d_reproduce/logs ``` * Evaluate the trained model.
thumos,thumos14,DATASET,thumos14,/configs/thumos_i3d.yaml .
thumos,thumos14,DATASET,thumos14,"/ckpt/thumos_i3d_reproduce ``` * Training our model on THUMOS requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory."
THUMOS,thumos14,DATASET,thumos14,"/ckpt/thumos_i3d_reproduce ``` * Training our model on THUMOS requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory."
THUMOS 14,thumos14,DATASET,thumos14,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for THUMOS 14.
thumos,thumos14,DATASET,thumos14,│ └───pretrained/ │    └───thumos_i3d_reproduce/ │    │  └───thumos_reproduce_log.txt │    │  └───thumos_reproduce_results.txt │    │   └───
thumos,thumos14,DATASET,thumos14,│ └───pretrained/ │    └───thumos_i3d_reproduce/ │    │  └───thumos_reproduce_log.txt │    │  └───thumos_reproduce_results.txt │    │   └───
thumos,thumos14,DATASET,thumos14,│ └───pretrained/ │    └───thumos_i3d_reproduce/ │    │  └───thumos_reproduce_log.txt │    │  └───thumos_reproduce_results.txt │    │   └───
thumos,thumos14,DATASET,thumos14,/pretrained/thumos_i3d_reproduce/config.txt*. * The training log is located at *.
thumos,thumos14,DATASET,thumos14,/pretrained/thumos_i3d_reproduce/thumos_reproduce_log.txt* and also *.
thumos,thumos14,DATASET,thumos14,/pretrained/thumos_i3d_reproduce/logs*. * The pre-trained model is *.
thumos,thumos14,DATASET,thumos14,/pretrained/thumos_i3d_reproduce/epoch_034.pth.tar*. * Evaluate the pre-trained model.
thumos,thumos14,DATASET,thumos14,/configs/thumos_i3d.yaml .
thumos,thumos14,DATASET,thumos14,/pretrained/thumos_i3d_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method            |  0.3  |  0.4  |  0.5  |  0.6  |  0.7  |  Avg  | |-------------------|-------|-------|-------|-------|-------|-------| | ActionFormer      | 82.13 | 77.80 | 70.95 | 59.40 | 43.87 | 66.83 |   ## To Reproduce Our Results on ActivityNet 1.3 **Download Features and Annotations** * Download *anet_1.3.tar.gz* (`md5sum c415f50120b9425ee1ede9ac3ce11203`) from [this Box link](https://uwmadison.box.com/s/aisdoymowukc99zoc7gpqegxbb4whikx) or [this Google Drive Link](https://drive.google.com/file/d/1VW8px1Nz9A17i0wMVUfxh6YsPCLVqL-S/view?
ActivityNet 1.3,activitynet,DATASET,activitynet,/pretrained/thumos_i3d_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method            |  0.3  |  0.4  |  0.5  |  0.6  |  0.7  |  Avg  | |-------------------|-------|-------|-------|-------|-------|-------| | ActionFormer      | 82.13 | 77.80 | 70.95 | 59.40 | 43.87 | 66.83 |   ## To Reproduce Our Results on ActivityNet 1.3 **Download Features and Annotations** * Download *anet_1.3.tar.gz* (`md5sum c415f50120b9425ee1ede9ac3ce11203`) from [this Box link](https://uwmadison.box.com/s/aisdoymowukc99zoc7gpqegxbb4whikx) or [this Google Drive Link](https://drive.google.com/file/d/1VW8px1Nz9A17i0wMVUfxh6YsPCLVqL-S/view?
ActivityNet,activitynet,DATASET,activitynet,"pwd=xuit). * The file includes TSP features, action annotations in json format (similar to ActivityNet annotation format), and external classification scores."
ActivityNet,activitynet,DATASET,activitynet,"**Details**: The features are extracted from the R(2+1)D-34 model pretrained with TSP on ActivityNet using clips of `16 frames` at a frame rate of `15 fps` and a stride of `16 frames` (*i.e.,* **non-overlapping** clips)."
ActivityNet,activitynet,DATASET,activitynet,"/ckpt/anet_tsp_reproduce ``` * Training our model on ActivityNet requires ~4.6GB GPU memory, yet the inference might require over 10GB GPU memory."
ActivityNet 1.3,activitynet,DATASET,activitynet,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for ActivityNet 1.3.
EPIC Kitchens 100,epic-kitchens-100,DATASET,epic-kitchens-100,/pretrained/anet_i3d_reproduce/ ```  * The results (mAP at tIoUs) with I3D features should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.29 | 36.71 |  8.24 | 36.03 |  ## To Reproduce Our Results on EPIC Kitchens 100 **Download Features and Annotations** * Download *epic_kitchens.tar.gz* (`md5sum add9803756afd9a023bc9a9c547e0229`) from [this Box link](https://uwmadison.box.com/s/vdha47qnce6jhqktz9g4mq1gc40w82yj) or [this Google Drive Link](https://drive.google.com/file/d/1Z4U_dLuu6_cV5NBIrSzsSDOOj2Uar85X/view?
epic_kitchens,epic-kitchens-55,DATASET,epic-kitchens-55,/pretrained/anet_i3d_reproduce/ ```  * The results (mAP at tIoUs) with I3D features should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.29 | 36.71 |  8.24 | 36.03 |  ## To Reproduce Our Results on EPIC Kitchens 100 **Download Features and Annotations** * Download *epic_kitchens.tar.gz* (`md5sum add9803756afd9a023bc9a9c547e0229`) from [this Box link](https://uwmadison.box.com/s/vdha47qnce6jhqktz9g4mq1gc40w82yj) or [this Google Drive Link](https://drive.google.com/file/d/1Z4U_dLuu6_cV5NBIrSzsSDOOj2Uar85X/view?
SlowFast,NIL,DATASET,NIL,pwd=f3tx). * The file includes SlowFast features as well as action annotations in json format (similar to ActivityNet annotation format).
ActivityNet,activitynet,DATASET,activitynet,pwd=f3tx). * The file includes SlowFast features as well as action annotations in json format (similar to ActivityNet annotation format).
EPIC Kitchens 100,epic-kitchens-100,DATASET,epic-kitchens-100,**Details**: The features are extracted from the SlowFast model pretrained on the training set of EPIC Kitchens 100 (action classification) using clips of `32 frames` at a frame rate of `30 fps` and a stride of `16 frames`.
EPIC Kitchens,epic-kitchens-100,DATASET,epic-kitchens-100,"│ └───data/ │    └───epic_kitchens/ │    │  └───annotations │    │  └───features    │    └───... | └───libs │ │   ... ```  **Training and Evaluation** * On EPIC Kitchens, we train separate models for nouns and verbs. * To train our ActionFormer on verbs with SlowFast features, use ```shell python ."
SlowFast,NIL,DATASET,NIL,"│ └───data/ │    └───epic_kitchens/ │    │  └───annotations │    │  └───features    │    └───... | └───libs │ │   ... ```  **Training and Evaluation** * On EPIC Kitchens, we train separate models for nouns and verbs. * To train our ActionFormer on verbs with SlowFast features, use ```shell python ."
SlowFast,NIL,DATASET,NIL,"/configs/epic_slowfast_verb.yaml --output reproduce ``` * To train our ActionFormer on nouns with SlowFast features, use ```shell python ."
EPIC Kitchens,epic-kitchens-100,DATASET,epic-kitchens-100,"/ckpt/epic_slowfast_noun_reproduce ``` * Training our model on EPIC Kitchens requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory."
EPIC-Kitchens 100,epic-kitchens-100,DATASET,epic-kitchens-100,**[Optional] Evaluating Our Pre-trained Model**  We also provide a pre-trained model for EPIC-Kitchens 100.
Ego4D Moment Queries,NIL,DATASET,NIL,/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).
SlowFast,NIL,DATASET,NIL,/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).
Omnivore,NIL,DATASET,NIL,/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).
Ego4D,ego4d,DATASET,ego4d,/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).
EgoVLP,NIL,DATASET,NIL,/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).
Omnivore,NIL,DATASET,NIL,"For example, training on Omnivore and EgoVLP features will create an experiment folder under *."
EgoVLP,NIL,DATASET,NIL,"For example, training on Omnivore and EgoVLP features will create an experiment folder under *."
Ego4D,ego4d,DATASET,ego4d,"/ckpt/ego4d_omnivore_egovlp_reproduce ``` * Training our model on Ego4D with all three features requires ~4.5GB GPU memory, yet the inference might require over 10GB GPU memory."
Ego4D,ego4d,DATASET,ego4d,**[Optional] Evaluating Our Pre-trained Model**  We also provide pre-trained models for Ego4D trained with all feature combinations.
ego4d_omnivore_egovlp,NIL,DATASET,NIL,│ └───pretrained/ │    └───ego4d_omnivore_egovlp_reproduce/ │    │   └───ego4d_omnivore_egovlp_reproduce_log.txt │    │   └───ego4d_omnivore_egovlp_reproduce_results.txt │    │   └───
ego4d_omnivore_egovlp,NIL,DATASET,NIL,│ └───pretrained/ │    └───ego4d_omnivore_egovlp_reproduce/ │    │   └───ego4d_omnivore_egovlp_reproduce_log.txt │    │   └───ego4d_omnivore_egovlp_reproduce_results.txt │    │   └───
ego4d_omnivore_egovlp,NIL,DATASET,NIL,│ └───pretrained/ │    └───ego4d_omnivore_egovlp_reproduce/ │    │   └───ego4d_omnivore_egovlp_reproduce_log.txt │    │   └───ego4d_omnivore_egovlp_reproduce_results.txt │    │   └───
ego4d_omnivore_egovlp,NIL,DATASET,NIL,/pretrained/ego4d_omnivore_egovlp_reproduce/config.txt*. * The training log is located at *.
ego4d_omnivore_egovlp,NIL,DATASET,NIL,/pretrained/ego4d_omnivore_egovlp_reproduce/ego4d_omnivore_egovlp_reproduce_log.txt* and also *.
ego4d_omnivore_egovlp,NIL,DATASET,NIL,/pretrained/ego4d_omnivore_egovlp_reproduce/logs*. * The pre-trained model is *.
ego4d_omnivore_egovlp,NIL,DATASET,NIL,/pretrained/ego4d_omnivore_egovlp_reproduce/epoch_010.pth.tar*. * Evaluate the pre-trained model.
Ego4D,ego4d,DATASET,ego4d,"Stay tuned.  ## Contact Yin Li (yin.li@wisc.edu)  ## References If you are using our code, please consider citing our paper. ``` @inproceedings{zhang2022actionformer,   title={ActionFormer: Localizing Moments of Actions with Transformers},   author={Zhang, Chen-Lin and Wu, Jianxin and Li, Yin},   booktitle={European Conference on Computer Vision},   series={LNCS},   volume={13664},   pages={492-510},   year={2022} } ```  If you cite our results on Ego4D, please consider citing our tech report in addition to the main paper. ``` @article{mu2022actionformerego4d,   title={Where a Strong Backbone Meets Strong Features -- ActionFormer for Ego4D Moment Queries Challenge},   author={Mu, Fangzhou and Mo, Sicheng and Wang, Gillian, and Li, Yin},   journal={arXiv e-prints},   year={2022} } ```  If you are using TSP features, please cite ``` @inproceedings{alwassel2021tsp,   title={{TSP}: Temporally-sensitive pretraining of video encoders for localization tasks},   author={Alwassel, Humam and Giancola, Silvio and Ghanem, Bernard},   booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},   pages={3173--3183},   year={2021} } ```"
MedQA,medqa,DATASET,medqa,More details about the configuration parameters can be found [here](configs).  ### Datasets We include the following medical datasets formatted in the required format:  - MedQA - MedMCQA - PubMedQA - MMLU (medical subsets) - CareQA  Other datasets can be used if they are in the required format.
MedMCQA,medmcqa,DATASET,medmcqa,More details about the configuration parameters can be found [here](configs).  ### Datasets We include the following medical datasets formatted in the required format:  - MedQA - MedMCQA - PubMedQA - MMLU (medical subsets) - CareQA  Other datasets can be used if they are in the required format.
PubMedQA,pubmedqa,DATASET,pubmedqa,More details about the configuration parameters can be found [here](configs).  ### Datasets We include the following medical datasets formatted in the required format:  - MedQA - MedMCQA - PubMedQA - MMLU (medical subsets) - CareQA  Other datasets can be used if they are in the required format.
MMLU (medical subsets),mmlu,DATASET,mmlu,More details about the configuration parameters can be found [here](configs).  ### Datasets We include the following medical datasets formatted in the required format:  - MedQA - MedMCQA - PubMedQA - MMLU (medical subsets) - CareQA  Other datasets can be used if they are in the required format.
CareQA,care,DATASET,care,More details about the configuration parameters can be found [here](configs).  ### Datasets We include the following medical datasets formatted in the required format:  - MedQA - MedMCQA - PubMedQA - MMLU (medical subsets) - CareQA  Other datasets can be used if they are in the required format.
FICO,NIL,DATASET,NIL,"/reachable_db.h5` db.generate(data, overwrite=True)  # Pull reachable set for first point in dataset x = data.iloc[0] reachable_set = db[x] print(reachable_set) # should return the following output: ##    age  marital_status  years_since_last_default  job_type_a  job_type_b  job_type_c ## 0  32.0             1.0                       5.0         0.0         1.0         0.0 ## 1  32.0             1.0                       5.0         0.0         0.0         1.0 ## 2  32.0             1.0                       5.0         1.0         0.0         0.0 ## 3  33.0             1.0                       6.0         0.0         0.0         1.0 ## 4  33.0             1.0                       6.0         0.0         1.0         0.0 ## 5  33.0             1.0                       6.0         1.0         0.0         0.0 ``` Given a classifier `clf` with a predict method, you can test if a point has recourse as `np.any(clf.predict(reachable_set.X))`  For more examples, check out [this script](https://github.com/ustunb/reachml/blob/main/research/iclr2024/scripts/setup_dataset_actionset_fico.py) which sets up the action set for the FICO dataset.  ### Resources and Citation  For more about recourse verification, check out our paper ICLR 2024 spotlight paper: [Prediction without Preclusion](https://openreview.net/forum?"
Xsum,xsum,DATASET,xsum,"Some task categories are synthetically generated through the Alpaca paradigm, others are manually labeled more classic NLP tasks (Xsum, SST2, Squad, CONLL) and are used as target tasks in most of the paper's experiments."
SST2,sst-2,DATASET,sst-2,"Some task categories are synthetically generated through the Alpaca paradigm, others are manually labeled more classic NLP tasks (Xsum, SST2, Squad, CONLL) and are used as target tasks in most of the paper's experiments."
Squad,squad,DATASET,squad,"Some task categories are synthetically generated through the Alpaca paradigm, others are manually labeled more classic NLP tasks (Xsum, SST2, Squad, CONLL) and are used as target tasks in most of the paper's experiments."
CONLL,conll,DATASET,conll,"Some task categories are synthetically generated through the Alpaca paradigm, others are manually labeled more classic NLP tasks (Xsum, SST2, Squad, CONLL) and are used as target tasks in most of the paper's experiments."
CIFAR10 (TinyImagenet),NIL,DATASET,NIL,"Compared to VGG16 baselines, XPert achieves 10.24x (4.7x) lower EDAP, 1.72x (1.62x) higher TOPS/W,1.93x (3x) higher TOPS/mm2 at 92.46% (56.7%) accuracy for CIFAR10 (TinyImagenet) datasets."
SemanticKITTI,semantickitti,DATASET,semantickitti,"We thoroughly evaluate our method on SemanticKITTI and SemanticPOSS datasets, showing that it can significantly outperform the baseline."
SemanticPOSS,semanticposs,DATASET,semanticposs,"We thoroughly evaluate our method on SemanticKITTI and SemanticPOSS datasets, showing that it can significantly outperform the baseline."
SemanticKITTI,semantickitti,DATASET,semantickitti,"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html)."
semantic-kitti,semantickitti,DATASET,semantickitti,"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html)."
SemanticPOSS,semanticposs,DATASET,semanticposs,"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html)."
semanticposs,semanticposs,DATASET,semanticposs,"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html)."
SemanticPOSS,semanticposs,DATASET,semanticposs,"│   └── labels/              |          ├── 000000.label             |          ├── 000001.label             |          └── ...             └── ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No 101058589."
SemanticKITTI,semantickitti,DATASET,semantickitti,"│   └── labels/              |          ├── 000000.label             |          ├── 000001.label             |          └── ...             └── ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No 101058589."
SemanticPOSS,semanticposs,DATASET,semanticposs,"│   └── labels/              |          ├── 000000.label             |          ├── 000001.label             |          └── ...             └── ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No 101058589."
SemanticKITTI,semantickitti,DATASET,semantickitti,"│   └── labels/              |          ├── 000000.label             |          ├── 000001.label             |          └── ...             └── ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No 101058589."
SemanticPOSS,semanticposs,DATASET,semanticposs,"│   └── labels/              |          ├── 000000.label             |          ├── 000001.label             |          └── ...             └── ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No 101058589."
SemanticKITTI,semantickitti,DATASET,semantickitti,"│   └── labels/              |          ├── 000000.label             |          ├── 000001.label             |          └── ...             └── ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union’s Horizon Europe research and innovation programme under grant agreement No 101058589."
SVHN,svhn,DATASET,svhn,EC-GAN is compared to the shared architecture method on SVHN at different dataset sizes.
wordnet31,NIL,DATASET,NIL,"For the following example, assume to work with the dataset provided in the folder `test_data`: `wordnet31.gz`."
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.gz -S -P -O   This script will produce the following files: `wordnet31.subjects_vocab`, `wordnet31.predicates_vocab` and `wordnet31.objects_vocab`.  2."
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.gz -S -P -O   This script will produce the following files: `wordnet31.subjects_vocab`, `wordnet31.predicates_vocab` and `wordnet31.objects_vocab`.  2."
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.gz -S -P -O   This script will produce the following files: `wordnet31.subjects_vocab`, `wordnet31.predicates_vocab` and `wordnet31.objects_vocab`.  2."
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.gz -S -P -O   This script will produce the following files: `wordnet31.subjects_vocab`, `wordnet31.predicates_vocab` and `wordnet31.objects_vocab`.  2."
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.gz   This script will map the dataset to integer triples,  producing the file `wordnet31.mapped.unsorted`.  3."
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.gz   This script will map the dataset to integer triples,  producing the file `wordnet31.mapped.unsorted`.  3."
wordnet31,NIL,DATASET,NIL,Sort the file `wordnet31.mapped.unsorted` materializing the needed permutations.
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.mapped.unsorted wordnet31   This script will produce the four permutations, one per file:  `wordnet31.mapped.sorted.spo`, `wordnet31.mapped.sorted.pos`, `wordnet31.mapped.sorted.osp` and `wordnet31.mapped.sorted.ops`.  4."
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.mapped.unsorted wordnet31   This script will produce the four permutations, one per file:  `wordnet31.mapped.sorted.spo`, `wordnet31.mapped.sorted.pos`, `wordnet31.mapped.sorted.osp` and `wordnet31.mapped.sorted.ops`.  4."
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.mapped.unsorted wordnet31   This script will produce the four permutations, one per file:  `wordnet31.mapped.sorted.spo`, `wordnet31.mapped.sorted.pos`, `wordnet31.mapped.sorted.osp` and `wordnet31.mapped.sorted.ops`.  4."
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.mapped.unsorted wordnet31   This script will produce the four permutations, one per file:  `wordnet31.mapped.sorted.spo`, `wordnet31.mapped.sorted.pos`, `wordnet31.mapped.sorted.osp` and `wordnet31.mapped.sorted.ops`.  4."
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.mapped.unsorted wordnet31   This script will produce the four permutations, one per file:  `wordnet31.mapped.sorted.spo`, `wordnet31.mapped.sorted.pos`, `wordnet31.mapped.sorted.osp` and `wordnet31.mapped.sorted.ops`.  4."
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.mapped.unsorted wordnet31   This script will produce the four permutations, one per file:  `wordnet31.mapped.sorted.spo`, `wordnet31.mapped.sorted.pos`, `wordnet31.mapped.sorted.osp` and `wordnet31.mapped.sorted.ops`.  4."
wordnet31,NIL,DATASET,NIL,python3 build_stats.py wordnet31.mapped.sorted   This script will create the file `wordnet31.mapped.sorted.stats`.
wordnet31,NIL,DATASET,NIL,python3 build_stats.py wordnet31.mapped.sorted   This script will create the file `wordnet31.mapped.sorted.stats`.
wordnet31,NIL,DATASET,NIL,/test_data/wordnet31.gz  to prepare the `wordnet31` collection for indexing.
wordnet31,NIL,DATASET,NIL,/test_data/wordnet31.gz  to prepare the `wordnet31` collection for indexing.
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.mapped.sorted -o wordnet31.pef_3t.bin  will build a 3T index (see Section 3.1 of [1]), compressed with partitioned Elias-Fano (PEF), that is serialized to the binary file `wordnet31.pef_3t.bin`."
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.mapped.sorted -o wordnet31.pef_3t.bin  will build a 3T index (see Section 3.1 of [1]), compressed with partitioned Elias-Fano (PEF), that is serialized to the binary file `wordnet31.pef_3t.bin`."
wordnet31,NIL,DATASET,NIL,"/test_data/wordnet31.mapped.sorted -o wordnet31.pef_3t.bin  will build a 3T index (see Section 3.1 of [1]), compressed with partitioned Elias-Fano (PEF), that is serialized to the binary file `wordnet31.pef_3t.bin`."
wordnet31,NIL,DATASET,NIL,/test_data/wordnet31.mapped.unsorted > ../..
wordnet31,NIL,DATASET,NIL,/test_data/wordnet31.mapped.unsorted.queries.5000  that will create a querylog with 5000 triples selected at random.
wordnet31,NIL,DATASET,NIL,/queries pef_3t 1 wordnet31.pef_3t.bin -q ..
wordnet31,NIL,DATASET,NIL,/test_data/wordnet31.mapped.unsorted.queries.5000 -n 5000 -w 1  will execute 5000 SP?
wordnet31,NIL,DATASET,NIL,"/statistics pef_2tp wordnet31.pef_2tp.bin  Testing <a name=""testing""></a> -------  Run the script `test/check_everything.py` from within the `."
wordnet31,NIL,DATASET,NIL,/test_data/wordnet31.mapped.sorted . wordnet  This script will check every triple selection pattern for all the different types of indexes.
ImageNet-LT,imagenet-lt,DATASET,imagenet-lt,"We revive the balanced undersampling produces a more equitable distribution of accuracy across categories, and devise a straightforward model ensemble strategy, which does not result in any additional overhead and achieves improved harmonic and geometric mean while keeping the average accuracy.* BTM is a simple, and efficient framework for long-tailed recognition.  ## Installation  **Requirements**  * Python 3.8 * torchvision 0.13.0 * Pytorch 1.12.0  **Dataset Preparation** * [ImageNet-LT](http://image-net.org/index) * [iNaturalist 2018](https://github.com/visipedia/inat_comp/tree/master/2018) * [Places-LT](http://places2.csail.mit.edu/download.html)  Change the `data_path` in `config/*/*.yaml` accordingly.  ## Training  **Stage-1**:  To get a model of Stage-1, you can directly download from [MiSLAS](https://github.com/dvlab-research/MiSLAS), or run:  ``` python train_stage1.py --cfg ."
iNaturalist 2018,inaturalist,DATASET,inaturalist,"We revive the balanced undersampling produces a more equitable distribution of accuracy across categories, and devise a straightforward model ensemble strategy, which does not result in any additional overhead and achieves improved harmonic and geometric mean while keeping the average accuracy.* BTM is a simple, and efficient framework for long-tailed recognition.  ## Installation  **Requirements**  * Python 3.8 * torchvision 0.13.0 * Pytorch 1.12.0  **Dataset Preparation** * [ImageNet-LT](http://image-net.org/index) * [iNaturalist 2018](https://github.com/visipedia/inat_comp/tree/master/2018) * [Places-LT](http://places2.csail.mit.edu/download.html)  Change the `data_path` in `config/*/*.yaml` accordingly.  ## Training  **Stage-1**:  To get a model of Stage-1, you can directly download from [MiSLAS](https://github.com/dvlab-research/MiSLAS), or run:  ``` python train_stage1.py --cfg ."
Places-LT,places-lt,DATASET,places-lt,"We revive the balanced undersampling produces a more equitable distribution of accuracy across categories, and devise a straightforward model ensemble strategy, which does not result in any additional overhead and achieves improved harmonic and geometric mean while keeping the average accuracy.* BTM is a simple, and efficient framework for long-tailed recognition.  ## Installation  **Requirements**  * Python 3.8 * torchvision 0.13.0 * Pytorch 1.12.0  **Dataset Preparation** * [ImageNet-LT](http://image-net.org/index) * [iNaturalist 2018](https://github.com/visipedia/inat_comp/tree/master/2018) * [Places-LT](http://places2.csail.mit.edu/download.html)  Change the `data_path` in `config/*/*.yaml` accordingly.  ## Training  **Stage-1**:  To get a model of Stage-1, you can directly download from [MiSLAS](https://github.com/dvlab-research/MiSLAS), or run:  ``` python train_stage1.py --cfg ."
COCO,ms coco,DATASET,ms coco,"Extensive experiments on the COCO dataset show that Team DETR achieves remarkable gains, especially for small and large objects.  ## Framework  !"
COCO2017,ms coco,DATASET,ms coco,We strongly recommend you use `pytorch >= 1.11.0` for its less GPU memory consumption.  ### Dataset  [COCO2017](https://cocodataset.org/) is used to validate our method.
Adult Census,adult census income,DATASET,adult census income,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)"
adult,adult,DATASET,adult,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)"
Bank Marketing,bank marketing,DATASET,bank marketing,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)"
Bank%2BMarketing,bank marketing,DATASET,bank marketing,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)"
Credit Card Default,default of credit card clients data set,DATASET,default of credit card clients data set,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)"
Dutch Census,NIL,DATASET,NIL,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)"
COMPAS,compas,DATASET,compas,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)"
compas,compas,DATASET,compas,"These findings raise questions about the limitations of the current group fairness metrics, as well as the arbitrariness, hence unfairness, of the whole debiasing process.  ## Structure  ``` . ├── README.md ├── fairlearn_int <-- [Fairlearn](https://fairlearn.org/) with slight modifications to allow working with multiple different data structures ├── fairness  |   ├── helpers <-- file containing helper functions |   ├── avd_helpers  <-- file containing helper functions ├── notebooks <-- folder containing experiments for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── name |   |   ├── runs ├── results <-- folder containing experiment results necessary for further analyses for each analysed dataset, structured in the following way: |   ├── name_dataset |   |   ├── results ```  ## Data The following datasets are used for the analysis: * [Adult Census](https://archive.ics.uci.edu/ml/datasets/adult) * [Bank Marketing](https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing) * [Credit Card Default](https://archive.ics.uci.edu/ml/datasets/default%2Bof%2Bcredit%2Bcard%2Bclients) * [Dutch Census](https://microdata.worldbank.org/index.php/catalog/2102/data-dictionary) * [COMPAS](https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis)"
OntoEvent,ontoevent,DATASET,ontoevent,"# OntoED and OntoEvent  <p align=""center"">     <font size=4><strong>OntoED: A Model for Low-resource Event Detection with Ontology Embedding</strong></font> </p>   🍎  The project is an official implementation for [**OntoED**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoED) model and a repository for [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) dataset, which has firstly been proposed in the paper [OntoED: Low-resource Event Detection with Ontology Embedding](https://arxiv.org/pdf/2105.10922.pdf) accepted by ACL 2021"
OntoEvent,ontoevent,DATASET,ontoevent,"# OntoED and OntoEvent  <p align=""center"">     <font size=4><strong>OntoED: A Model for Low-resource Event Detection with Ontology Embedding</strong></font> </p>   🍎  The project is an official implementation for [**OntoED**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoED) model and a repository for [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) dataset, which has firstly been proposed in the paper [OntoED: Low-resource Event Detection with Ontology Embedding](https://arxiv.org/pdf/2105.10922.pdf) accepted by ACL 2021"
OntoEvent,ontoevent,DATASET,ontoevent,"# OntoED and OntoEvent  <p align=""center"">     <font size=4><strong>OntoED: A Model for Low-resource Event Detection with Ontology Embedding</strong></font> </p>   🍎  The project is an official implementation for [**OntoED**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoED) model and a repository for [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) dataset, which has firstly been proposed in the paper [OntoED: Low-resource Event Detection with Ontology Embedding](https://arxiv.org/pdf/2105.10922.pdf) accepted by ACL 2021"
MAVEN,maven,DATASET,maven,🤗  The implementations are based on [Huggingface's Transformers](https://github.com/huggingface/transformers) and remanagement is referred to [MAVEN's baselines](https://github.com/THU-KEG/MAVEN-dataset/) & [DeepKE](https://github.com/zjunlp/DeepKE)
OntoEvent,ontoevent,DATASET,ontoevent,"It models the relationship between event types through ontology embedding: it can transfer knowledge of high-resource event types to low-resource ones, and the unseen event type can establish connection with seen ones via event ontology.   ## Project Structure The structure of data and code is as follows:   ```shell Reasoning_In_EE ├── README.md ├── OntoED   # model │   ├── README.md │   ├── data_utils.py  # for data processing │   ├── ontoed.py   # main model │   ├── run_ontoed.py  # for model running │   └── run_ontoed.sh  # bash file for model running ├── OntoEvent  # data │   ├── README.md │   ├── __init__.py │   ├── event_dict_data_on_doc.json.zip  # raw full ED data │   ├── event_dict_train_data.json   # ED data for training │   ├── event_dict_test_data.json   # ED data for testing │   ├── event_dict_valid_data.json   # ED data for validation │   └── event_relation.json     # event-event relation data └── baselines  # baseline models     ├── DMCNN     │   ├── README.md     │   ├── convert.py   # for data processing     │   ├── data    # data     │   │   └── labels.json     │   ├── dmcnn.config  # configure training & testing     │   ├── eval.sh    # bash file for model evaluation     │   ├── formatter     │   │   ├── DmcnnFormatter.py # runtime data processing     │   │   └── __init__.py     │   ├── main.py    # project entrance     │   ├── model     │   │   ├── Dmcnn.py  # main model     │   │   └── __init__.py     │   ├── raw     │   │   └── 100.utf8  # word vector     │   ├── reader     │   │   ├── MavenReader.py # runtime data reader     │   │   └── __init__.py     │   ├── requirements.txt # requirements     │   ├── train.sh   # bash file for model training     │   └── utils     │       ├── __init__.py     │       ├── configparser_hook.py     │       ├── evaluation.py     │       ├── global_variables.py     │       ├── initializer.py     │       └── runner.py     ├── JMEE     │   ├── README.md     │   ├── data    # to store data file     │   ├── enet     │   │   ├── __init__.py     │   │   ├── consts.py  # configurable parameters     │   │   ├── corpus     │   │   │   ├── Corpus.py # dataset class     │   │   │   ├── Data.py     │   │   │   ├── Sentence.py     │   │   │   └── __init__.py     │   │   ├── models   # modules of JMEE     │   │   │   ├── DynamicLSTM.py     │   │   │   ├── EmbeddingLayer.py     │   │   │   ├── GCN.py     │   │   │   ├── HighWay.py     │   │   │   ├── SelfAttention.py     │   │   │   ├── __init__.py     │   │   │   ├── ee.py     │   │   │   └── model.py # main model     │   │   ├── run     │   │   │   ├── __init__.py     │   │   │   └── ee     │   │   │       ├── __init__.py     │   │   │       └── runner.py # runner class     │   │   ├── testing.py  # evaluation     │   │   ├── training.py  # training     │   │   └── util.py     │   ├── eval.sh    # bash file for model evaluation     │   ├── requirements.txt # requirements     │   └── train.sh   # bash file for model training     ├── README.md     ├── eq1.png     ├── eq2.png     ├── jointEE-NN     │   ├── README.md     │   ├── data     │   │   └── fistDoc.nnData4.txt # data format sample     │   ├── evaluateJEE.py   # model evaluation     │   ├── jeeModels.py   # main model     │   ├── jee_processData.py  # data process     │   └── jointEE.py    # project entrance     └── stanford.zip   # cleaned dataset for baseline models ```  ## Requirements  - python==3.6.9  - torch==1.8.0 (lower may also be OK)  - transformers==2.8.0  - sklearn==0.20.2   ## Usage   **1."
OntoEvent,ontoevent,DATASET,ontoevent,- 'LABEL\_PATH' and 'RELATION\_PATH' means the path for [event\_dict\_train_data.json](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent/event_dict_train_data.json) and [event_relation.json](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent/event_relation.json) respectively.   **3.
OntoEvent,ontoevent,DATASET,ontoevent,- 'LABEL\_PATH' and 'RELATION\_PATH' means the path for [event\_dict\_train_data.json](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent/event_dict_train_data.json) and [event_relation.json](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent/event_relation.json) respectively.   **3.
OntoEvent,ontoevent,DATASET,ontoevent,"/run_ontoed.sh ('--do_train', '--do_eval', '--evaluate_during_training', '--do_test' is necessarily input in 'run_ontoed.sh')  Or you can run run_ontoed.py with manual parameter input (parameters can be copied from 'run_ontoed.sh')  python run_ontoed.py --para...  ```   ## How about the Dataset [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent)  is proposed for ED and also annotated with correlations among events."
OntoEvent,ontoevent,DATASET,ontoevent,"/run_ontoed.sh ('--do_train', '--do_eval', '--evaluate_during_training', '--do_test' is necessarily input in 'run_ontoed.sh')  Or you can run run_ontoed.py with manual parameter input (parameters can be copied from 'run_ontoed.sh')  python run_ontoed.py --para...  ```   ## How about the Dataset [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent)  is proposed for ED and also annotated with correlations among events."
OntoEvent,ontoevent,DATASET,ontoevent,"Please refer to [OntoEvent](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) for details.   ### Statistics The statistics of OntoEvent are shown below, and the detailed data schema can be referred to our paper."
OntoEvent,ontoevent,DATASET,ontoevent,"Please refer to [OntoEvent](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) for details.   ### Statistics The statistics of OntoEvent are shown below, and the detailed data schema can be referred to our paper."
ACE 2005,ace 2005,DATASET,ace 2005,"Dataset         | #Doc | #Instance | #SuperType | #SubType | #EventCorrelation | | :----------------- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- | ACE 2005        | 599 | 4,090 | 8 | 33 | None | TAC KBP 2017    | 167 | 4,839 | 8 | 18  | None | FewEvent              | - | 70,852 | 19 | 100  | None | MAVEN           | 4,480 | 111,611 | 21 | 168  | None | ***OntoEvent***    | 4,115 | 60,546 | 13 | 100 | 3,804 |  ### Data Format The OntoEvent dataset is stored in json format."
TAC KBP 2017,NIL,DATASET,NIL,"Dataset         | #Doc | #Instance | #SuperType | #SubType | #EventCorrelation | | :----------------- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- | ACE 2005        | 599 | 4,090 | 8 | 33 | None | TAC KBP 2017    | 167 | 4,839 | 8 | 18  | None | FewEvent              | - | 70,852 | 19 | 100  | None | MAVEN           | 4,480 | 111,611 | 21 | 168  | None | ***OntoEvent***    | 4,115 | 60,546 | 13 | 100 | 3,804 |  ### Data Format The OntoEvent dataset is stored in json format."
FewEvent,fewevent,DATASET,fewevent,"Dataset         | #Doc | #Instance | #SuperType | #SubType | #EventCorrelation | | :----------------- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- | ACE 2005        | 599 | 4,090 | 8 | 33 | None | TAC KBP 2017    | 167 | 4,839 | 8 | 18  | None | FewEvent              | - | 70,852 | 19 | 100  | None | MAVEN           | 4,480 | 111,611 | 21 | 168  | None | ***OntoEvent***    | 4,115 | 60,546 | 13 | 100 | 3,804 |  ### Data Format The OntoEvent dataset is stored in json format."
MAVEN,maven,DATASET,maven,"Dataset         | #Doc | #Instance | #SuperType | #SubType | #EventCorrelation | | :----------------- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- | ACE 2005        | 599 | 4,090 | 8 | 33 | None | TAC KBP 2017    | 167 | 4,839 | 8 | 18  | None | FewEvent              | - | 70,852 | 19 | 100  | None | MAVEN           | 4,480 | 111,611 | 21 | 168  | None | ***OntoEvent***    | 4,115 | 60,546 | 13 | 100 | 3,804 |  ### Data Format The OntoEvent dataset is stored in json format."
OntoEvent,ontoevent,DATASET,ontoevent,"Dataset         | #Doc | #Instance | #SuperType | #SubType | #EventCorrelation | | :----------------- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- | ACE 2005        | 599 | 4,090 | 8 | 33 | None | TAC KBP 2017    | 167 | 4,839 | 8 | 18  | None | FewEvent              | - | 70,852 | 19 | 100  | None | MAVEN           | 4,480 | 111,611 | 21 | 168  | None | ***OntoEvent***    | 4,115 | 60,546 | 13 | 100 | 3,804 |  ### Data Format The OntoEvent dataset is stored in json format."
OntoEvent,ontoevent,DATASET,ontoevent,"Dataset         | #Doc | #Instance | #SuperType | #SubType | #EventCorrelation | | :----------------- | ---------------- | ---------------- | ---------------- | ---------------- | ---------------- | ACE 2005        | 599 | 4,090 | 8 | 33 | None | TAC KBP 2017    | 167 | 4,839 | 8 | 18  | None | FewEvent              | - | 70,852 | 19 | 100  | None | MAVEN           | 4,480 | 111,611 | 21 | 168  | None | ***OntoEvent***    | 4,115 | 60,546 | 13 | 100 | 3,804 |  ### Data Format The OntoEvent dataset is stored in json format."
OntoEvent,ontoevent,DATASET,ontoevent,"🍒 For each *event instance* in [```event_dict_data_on_doc.json```](https://github.com/231sm/Reasoning_In_EE/blob/main/OntoEvent/event_dict_data_on_doc.json.zip), the data format is as below:  ``` {     'doc_id': '...',      'doc_title': 'XXX',      'sent_id': ,      'event_mention': '......',      'event_mention_tokens': ['.', '.', '.', '.', '.', '.'],      'trigger': '...',      'trigger_pos': [, ],      'event_type': '' } ``` 🍒 For each *event relation* in [```event_relation.json```](https://github.com/231sm/Reasoning_In_EE/blob/main/OntoEvent/event_relation.json), we list the *event instance pair*, and the data format is as below:  ``` 'EVENT_RELATION_1': [      [         {             'doc_id': '...',              'doc_title': 'XXX',              'sent_id': ,              'event_mention': '......',              'event_mention_tokens': ['.', '.', '.', '.', '.', '.'],              'trigger': '...',              'trigger_pos': [, ],              'event_type': ''         },          {             'doc_id': '...',              'doc_title': 'XXX',              'sent_id': ,              'event_mention': '......',              'event_mention_tokens': ['.', '.', '.', '.', '.', '.'],              'trigger': '...',              'trigger_pos': [, ],              'event_type': ''         }     ],      ... ] ``` 🍒 Especially for ""COSUPER"", ""SUBSUPER"" and ""SUPERSUB"", we list the *event type pair*, and the data format is as below:  ``` ""COSUPER"": [     [""Conflict.Attack"", ""Conflict.Protest""],      [""Conflict.Attack"", ""Conflict.Sending""],      ... ] ```   ## How to Cite 📋  Thank you very much for your interest in our work."
OntoEvent,ontoevent,DATASET,ontoevent,"🍒 For each *event instance* in [```event_dict_data_on_doc.json```](https://github.com/231sm/Reasoning_In_EE/blob/main/OntoEvent/event_dict_data_on_doc.json.zip), the data format is as below:  ``` {     'doc_id': '...',      'doc_title': 'XXX',      'sent_id': ,      'event_mention': '......',      'event_mention_tokens': ['.', '.', '.', '.', '.', '.'],      'trigger': '...',      'trigger_pos': [, ],      'event_type': '' } ``` 🍒 For each *event relation* in [```event_relation.json```](https://github.com/231sm/Reasoning_In_EE/blob/main/OntoEvent/event_relation.json), we list the *event instance pair*, and the data format is as below:  ``` 'EVENT_RELATION_1': [      [         {             'doc_id': '...',              'doc_title': 'XXX',              'sent_id': ,              'event_mention': '......',              'event_mention_tokens': ['.', '.', '.', '.', '.', '.'],              'trigger': '...',              'trigger_pos': [, ],              'event_type': ''         },          {             'doc_id': '...',              'doc_title': 'XXX',              'sent_id': ,              'event_mention': '......',              'event_mention_tokens': ['.', '.', '.', '.', '.', '.'],              'trigger': '...',              'trigger_pos': [, ],              'event_type': ''         }     ],      ... ] ``` 🍒 Especially for ""COSUPER"", ""SUBSUPER"" and ""SUPERSUB"", we list the *event type pair*, and the data format is as below:  ``` ""COSUPER"": [     [""Conflict.Attack"", ""Conflict.Protest""],      [""Conflict.Attack"", ""Conflict.Sending""],      ... ] ```   ## How to Cite 📋  Thank you very much for your interest in our work."
VisDial v1.0,visdial,DATASET,visdial,"Download the VisDial v1.0 dialog json files from [here][3] and keep it under `$PROJECT_ROOT/data` directory, for default arguments to work effectively.  2."
VisDial v1.0,visdial,DATASET,visdial,Get the word counts for VisDial v1.0 train split [here][4].
VisDial v1.0,visdial,DATASET,visdial,"[batra-mlp-lab][6] provides pre-extracted image features of VisDial v1.0 images, using a Faster-RCNN pre-trained on Visual Genome."
Visual Genome,visual genome,DATASET,visual genome,"[batra-mlp-lab][6] provides pre-extracted image features of VisDial v1.0 images, using a Faster-RCNN pre-trained on Visual Genome."
VisDial v1.0,visdial,DATASET,visdial,"If you wish to extract your own image features, skip this step and download VisDial v1.0 images from [here][3] instead."
MSCOCO,ms coco,DATASET,ms coco,Prepare the [MSCOCO][11] and [Flickr][3] images.  2.
Flickr,flickr1024,DATASET,flickr1024,Prepare the [MSCOCO][11] and [Flickr][3] images.  2.
MSCOCO,ms coco,DATASET,ms coco,/data/extract_features_detectron.py --image-root /path/to/MSCOCO/train2014/ /path/to/MSCOCO/val2014/ --save-path /path/to/feature --split train # Bottom-up features of 36 proposals from images of train split. python .
MSCOCO,ms coco,DATASET,ms coco,/data/extract_features_detectron.py --image-root /path/to/MSCOCO/train2014/ /path/to/MSCOCO/val2014/ --save-path /path/to/feature --split train # Bottom-up features of 36 proposals from images of train split. python .
Flickr,flickr1024,DATASET,flickr1024,/data/extract_features_detectron.py --image-root /path/to/Flickr/VisualDialog_val2018 --save-path /path/to/feature --split val # Bottom-up features of 36 proposals from images of val split. python .
Flickr,flickr1024,DATASET,flickr1024,/data/extract_features_detectron.py --image-root /path/to/Flickr/VisualDialog_test2018 --save-path /path/to/feature --split test # Bottom-up features of 36 proposals from images of test split. ```  Initializing GloVe Word Embeddings -------------- Simply run  ```shell python data/init_glove.py ```   Training --------  Train the model provided in this repository as:  ```shell python train.py --config-yml configs/rva.yml --gpu-ids 0 # provide more ids for multi-GPU execution other args... ```  ### Saving model checkpoints  This script will save model checkpoints at every epoch as per path specified by `--save-dirpath`.
AFHQ,afhq,DATASET,afhq,"Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data._  ### Requirement  __Library__ ``` pip install -r requirements.txt  * pytorch==1.1.0 or 1.2.0   * tqdm   * opencv-python   * scipy   * sklearn * matplotlib   * pillow   * tensorboardX  ```  __Dataset__   * This repo. utilizes the variant of ""ImageFolder"". * Download : [AFHQ (StarGANv2)](https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?"
afhq,afhq,DATASET,afhq,"Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data._  ### Requirement  __Library__ ``` pip install -r requirements.txt  * pytorch==1.1.0 or 1.2.0   * tqdm   * opencv-python   * scipy   * sklearn * matplotlib   * pillow   * tensorboardX  ```  __Dataset__   * This repo. utilizes the variant of ""ImageFolder"". * Download : [AFHQ (StarGANv2)](https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?"
AnimalFaces,afhq,DATASET,afhq,"dl=0) / [AnimalFaces (FUNIT)](https://github.com/NVlabs/FUNIT) * Example directory hierarchy (AFHQ, AnimalFaces):  ``` Project |--- tunit |          |--- main.py |          |--- train |                 |--- train_unsupervised.py |                 |--- ... | |--- data        |--- afhq              |--- train              |--- test        |--- animal_faces              |--- n02085620              |--- n02085782              |--- ...        |--- ffhq              |--- images                     |--- 000001.jpg                     |--- ...        |--- lsun_car              |--- images                     |--- 000001.jpg                     |--- ..."
AFHQ,afhq,DATASET,afhq,"dl=0) / [AnimalFaces (FUNIT)](https://github.com/NVlabs/FUNIT) * Example directory hierarchy (AFHQ, AnimalFaces):  ``` Project |--- tunit |          |--- main.py |          |--- train |                 |--- train_unsupervised.py |                 |--- ... | |--- data        |--- afhq              |--- train              |--- test        |--- animal_faces              |--- n02085620              |--- n02085782              |--- ...        |--- ffhq              |--- images                     |--- 000001.jpg                     |--- ...        |--- lsun_car              |--- images                     |--- 000001.jpg                     |--- ..."
AnimalFaces,afhq,DATASET,afhq,"dl=0) / [AnimalFaces (FUNIT)](https://github.com/NVlabs/FUNIT) * Example directory hierarchy (AFHQ, AnimalFaces):  ``` Project |--- tunit |          |--- main.py |          |--- train |                 |--- train_unsupervised.py |                 |--- ... | |--- data        |--- afhq              |--- train              |--- test        |--- animal_faces              |--- n02085620              |--- n02085782              |--- ...        |--- ffhq              |--- images                     |--- 000001.jpg                     |--- ...        |--- lsun_car              |--- images                     |--- 000001.jpg                     |--- ..."
afhq,afhq,DATASET,afhq,"dl=0) / [AnimalFaces (FUNIT)](https://github.com/NVlabs/FUNIT) * Example directory hierarchy (AFHQ, AnimalFaces):  ``` Project |--- tunit |          |--- main.py |          |--- train |                 |--- train_unsupervised.py |                 |--- ... | |--- data        |--- afhq              |--- train              |--- test        |--- animal_faces              |--- n02085620              |--- n02085782              |--- ...        |--- ffhq              |--- images                     |--- 000001.jpg                     |--- ...        |--- lsun_car              |--- images                     |--- 000001.jpg                     |--- ..."
animal_faces,afhq,DATASET,afhq,"dl=0) / [AnimalFaces (FUNIT)](https://github.com/NVlabs/FUNIT) * Example directory hierarchy (AFHQ, AnimalFaces):  ``` Project |--- tunit |          |--- main.py |          |--- train |                 |--- train_unsupervised.py |                 |--- ... | |--- data        |--- afhq              |--- train              |--- test        |--- animal_faces              |--- n02085620              |--- n02085782              |--- ...        |--- ffhq              |--- images                     |--- 000001.jpg                     |--- ...        |--- lsun_car              |--- images                     |--- 000001.jpg                     |--- ..."
ffhq,ffhq,DATASET,ffhq,"dl=0) / [AnimalFaces (FUNIT)](https://github.com/NVlabs/FUNIT) * Example directory hierarchy (AFHQ, AnimalFaces):  ``` Project |--- tunit |          |--- main.py |          |--- train |                 |--- train_unsupervised.py |                 |--- ... | |--- data        |--- afhq              |--- train              |--- test        |--- animal_faces              |--- n02085620              |--- n02085782              |--- ...        |--- ffhq              |--- images                     |--- 000001.jpg                     |--- ...        |--- lsun_car              |--- images                     |--- 000001.jpg                     |--- ..."
lsun_car,lsun,DATASET,lsun,"dl=0) / [AnimalFaces (FUNIT)](https://github.com/NVlabs/FUNIT) * Example directory hierarchy (AFHQ, AnimalFaces):  ``` Project |--- tunit |          |--- main.py |          |--- train |                 |--- train_unsupervised.py |                 |--- ... | |--- data        |--- afhq              |--- train              |--- test        |--- animal_faces              |--- n02085620              |--- n02085782              |--- ...        |--- ffhq              |--- images                     |--- 000001.jpg                     |--- ...        |--- lsun_car              |--- images                     |--- 000001.jpg                     |--- ..."
animal_faces,afhq,DATASET,afhq,__Train on local__ ``` Supervised python main.py --gpu $GPU_TO_USE --p_semi 1.0 --dataset animal_faces --data_path='..
animal_faces,afhq,DATASET,afhq,/data'  Semi-supervised python main.py --gpu $GPU_TO_USE --p_semi 0.5 --dataset animal_faces --data_path='..
animal_faces,afhq,DATASET,afhq,/data'  Unsupervised python main.py --gpu $GPU_TO_USE --p_semi 0.0 --dataset animal_faces --data_path='..
animal_faces,afhq,DATASET,afhq,/data' ```  __Test on local__ ``` python main.py --gpu $GPU_TO_USE --validation --load_model $DIR_TO_LOAD --dataset animal_faces ```  __Monitoring__ ``` tensorboard --logdir=$DIR/events --port=$PORT ```  __Actual example__ ``` Train python main.py --gpu 0 --dataset animal_faces --output_k 10 --data_path '..
animal_faces,afhq,DATASET,afhq,/data' ```  __Test on local__ ``` python main.py --gpu $GPU_TO_USE --validation --load_model $DIR_TO_LOAD --dataset animal_faces ```  __Monitoring__ ``` tensorboard --logdir=$DIR/events --port=$PORT ```  __Actual example__ ``` Train python main.py --gpu 0 --dataset animal_faces --output_k 10 --data_path '..
animal_faces,afhq,DATASET,afhq,/data' --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --data_path '..
afhq_cat,afhq,DATASET,afhq,/data' --p_semi 0.2 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --data_path '..
animal_faces,afhq,DATASET,afhq,/data' --p_semi 0.0 python main.py --gpu 1 --dataset animal_faces --data_path '..
summer2winter,NIL,DATASET,NIL,"/data' --p_semi 1.0 python main.py --gpu 0,1 --dataset summer2winter --output_k 2 --data_path '.."
animal_faces,afhq,DATASET,afhq,/data' --p_semi 0.0 --img_size 256 --batch_size 16 --ddp  Test python main.py --gpu 0 --dataset animal_faces --output_k 10 --data_path '..
afhq_cat,afhq,DATASET,afhq,/data' --validation --load_model GAN_20190101_101010 python main.py --gpu 1 --dataset afhq_cat --output_k 10 --data_path '..
summer2winter,NIL,DATASET,NIL,/data' --validation --load_model GAN_20190101_101010 python main.py --gpu 2 --dataset summer2winter --output_k 2 --data_path '..
animal_faces,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
AFHQ,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
afhq_cat,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
afhq_cat,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
AFHQ,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
afhq_dog,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
afhq,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
afhq_dog,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
AFHQ,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
afhq,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
afhq,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
afhq_wild,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
AnimalFaces,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
animal_faces,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
animal_faces,afhq,DATASET,afhq,"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations."
AnimalFaces,afhq,DATASET,afhq,"* For more classes on AnimalFaces, change the list at main.py#L227 then, set args.output_k to len(args.att_to_use)     * ex) args.att_to_use = \[i for i in range(100)\] then, run: python main.py --output_k 100 ...  ### Arguments * batch_size, img_size, data_path and p_semi are frequently speified. * Please refer ""help"" of the arguments in main.py.  ### Code Structure * main.py     * Execute main.py to run the codes"
afhq_cat,afhq,DATASET,afhq,[afhq_cat](.
afhq_dog,afhq,DATASET,afhq,[afhq_dog](.
afhq_wild],afhq,DATASET,afhq,[afhq_wild](.
CIFAR-10,cifar-10,DATASET,cifar-10,"**Evaluation**：We will run the submitted learning method on poisoned CIFAR-10 datasets by 10 backdoor attacks used in our paper, then test the Attack Sucess Rate (ASR) and Clean Accuracy (CA) of the final model."
CIFAR-10,cifar-10,DATASET,cifar-10,"| #     |           Paper            |    Venue     | Poisoned data | Architecture | Attack | ASR (%)| CA (%)| | ----- | :------------------------: | :----------: | :------------: | :----------: | :---------: | :-----------: | :----------: | | **1** | **[ABL]()** | NeurIPS 2021 |  *available* |    WRN-16-1    |   BadNets   |     3.04     |    86.11      | | **2** |                            |              |                |              |             |               |              | | **3** |                            |              |                |              |             |               |              | | **4** |                            |              |                |              |             |               |              | | **5** |                            |              |                |              |             |               |              | | **6** |                            |              |                |              |             |               |              | | **7** |                            |              |                |              |             |               |              | | **8** |                            |              |                |              |             |               |              |  ------  ## Verifying the unlearning effect of ABL with 1% isolated data:  ### An example with a pretrained model WRN-16-1, CIFAR-10, GridTrigger, target label 0, weights: `."
CIFAR-10,cifar-10,DATASET,cifar-10,Run the following command to verify the unlearning effect:  ```bash $ python quick_unlearning_demo.py  ``` The training logs are shown below. 1% isolation = 500 images from poisoned CIFAR-10.
WikiSQL,wikisql,DATASET,wikisql,We present the setup for the WikiSQL experiments
WikiSQL,wikisql,DATASET,wikisql,Download data from [WikiSQL](https://github.com/salesforce/WikiSQL).   ``` $ cd wikisql_data $ wget https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2 $ tar -xvjf data.tar.bz2 ``` 2.
WikiSQL,wikisql,DATASET,wikisql,Download data from [WikiSQL](https://github.com/salesforce/WikiSQL).   ``` $ cd wikisql_data $ wget https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2 $ tar -xvjf data.tar.bz2 ``` 2.
wikisql,wikisql,DATASET,wikisql,Download data from [WikiSQL](https://github.com/salesforce/WikiSQL).   ``` $ cd wikisql_data $ wget https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2 $ tar -xvjf data.tar.bz2 ``` 2.
WikiSQL,wikisql,DATASET,wikisql,Download data from [WikiSQL](https://github.com/salesforce/WikiSQL).   ``` $ cd wikisql_data $ wget https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2 $ tar -xvjf data.tar.bz2 ``` 2.
WikiSQL,wikisql,DATASET,wikisql,Put the [lib directory](https://github.com/salesforce/WikiSQL/tree/master/lib) under `wikisql_data/scripts/` 3.
wikisql,wikisql,DATASET,wikisql,Put the [lib directory](https://github.com/salesforce/WikiSQL/tree/master/lib) under `wikisql_data/scripts/` 3.
wikisql,wikisql,DATASET,wikisql,Run annotation using Stanza and preproces the dataset ``` $ cd wikisql_data/scripts/ $ python annotate.py $ python prepare.py ```  4.
wikisql,wikisql,DATASET,wikisql,"AryzSDJYB5TxnF31OCt_4to7uY2t), where the ``wikisql_train.dat``, ``wikisql_test.dat``, ``wikisql_dev.dat`` are the files that can be directly used in training."
wikisql,wikisql,DATASET,wikisql,"AryzSDJYB5TxnF31OCt_4to7uY2t), where the ``wikisql_train.dat``, ``wikisql_test.dat``, ``wikisql_dev.dat`` are the files that can be directly used in training."
wikisql,wikisql,DATASET,wikisql,"AryzSDJYB5TxnF31OCt_4to7uY2t), where the ``wikisql_train.dat``, ``wikisql_test.dat``, ``wikisql_dev.dat`` are the files that can be directly used in training."
WikiSQL,wikisql,DATASET,wikisql,Note: the version 2 dataset matches the v1.1 release of [WikiSQL](https://github.com/salesforce/WikiSQL).
WikiSQL,wikisql,DATASET,wikisql,Note: the version 2 dataset matches the v1.1 release of [WikiSQL](https://github.com/salesforce/WikiSQL).
wikisql,wikisql,DATASET,wikisql,"The preprocessing script ``wikisql_data/scripts/prepare_v2.py`` (python3 required) processes WikiSQL v1.1 raw data and table files to generate ``wikisql_train.dat``, ``wikisql_test.dat``, ``wikisql_dev.dat``.   ## Training Meta + Sum loss training ``` $ OUTDIR=output/meta_sum $ mkdir $OUTDIR $ python run.py --input-dir ."
WikiSQL v1.1,wikisql,DATASET,wikisql,"The preprocessing script ``wikisql_data/scripts/prepare_v2.py`` (python3 required) processes WikiSQL v1.1 raw data and table files to generate ``wikisql_train.dat``, ``wikisql_test.dat``, ``wikisql_dev.dat``.   ## Training Meta + Sum loss training ``` $ OUTDIR=output/meta_sum $ mkdir $OUTDIR $ python run.py --input-dir ."
wikisql,wikisql,DATASET,wikisql,"The preprocessing script ``wikisql_data/scripts/prepare_v2.py`` (python3 required) processes WikiSQL v1.1 raw data and table files to generate ``wikisql_train.dat``, ``wikisql_test.dat``, ``wikisql_dev.dat``.   ## Training Meta + Sum loss training ``` $ OUTDIR=output/meta_sum $ mkdir $OUTDIR $ python run.py --input-dir ."
wikisql,wikisql,DATASET,wikisql,"The preprocessing script ``wikisql_data/scripts/prepare_v2.py`` (python3 required) processes WikiSQL v1.1 raw data and table files to generate ``wikisql_train.dat``, ``wikisql_test.dat``, ``wikisql_dev.dat``.   ## Training Meta + Sum loss training ``` $ OUTDIR=output/meta_sum $ mkdir $OUTDIR $ python run.py --input-dir ."
SciTweets,NIL,DATASET,NIL,# SciTweets  <!
SciTweets,NIL,DATASET,NIL,"-- Short Introduction what this repo is about --> This repository contains a dataset, annotation framework and code for the work *""SciTweets - A Dataset and Annotation Framework for Detecting Scientific Online Discourse""* published at **CIKM2022**."
SciTweets,NIL,DATASET,NIL,"-- *TODO*: refer to our work before being published (e.g arxiv preprint)  *TODO 2*: give examples of scientific online discourse here, e.g ""The **SciTweets** dataset consists of ..."" -->  __Table of contents:__ - [Contents of the Repository](#contents-of-the-repository)   - [Directory Structure](#directory-structure)   - [Statistics](#statistics) - [Pretrained Model](#pretrained-model) - [Publication](#publication) - [Licensing](#licensing) - [Contact](#credits) - [Acknowledgment](#acknowledgment)  ## Contents of the Repository  ### Directory Structure =======================<br/> This repository contains the following directories and files:  1."
SciTweets,NIL,DATASET,NIL,**annotations.tsv** the annotated SciTweets dataset 2.
SciTweets,NIL,DATASET,NIL,"""SciTweets-A Dataset and Annotation Framework for Detecting Scientific Online Discourse."""
SciTweets,NIL,DATASET,NIL,"Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 2022, [download](https://arxiv.org/abs/2206.07360).*  ```bib @inproceedings{hafid2022scitweets,   title={SciTweets-A Dataset and Annotation Framework for Detecting Scientific Online Discourse},   author={Hafid, Salim and Schellhammer, Sebastian and Bringay, Sandra and Todorov, Konstantin and Dietze, Stefan},   booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},   pages={3988--3992},   year={2022} } ```  ## Licensing This dataset is published under CC BY 4.0 license."
ip_dataset.csv,NIL,DATASET,NIL,"For now, we copy a dummy sample dataset that is available at: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData    For the current example, we should choose the sample dataset [ip_dataset.csv](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv)."
ip_dataset.csv,NIL,DATASET,NIL,"For now, we copy a dummy sample dataset that is available at: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData    For the current example, we should choose the sample dataset [ip_dataset.csv](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv)."
ip_dataset.csv,NIL,DATASET,NIL,"We should copy this Comma-Sparated Values (CSV) file into the correct path, in this case at target/data (the data sub-directory must be created):    ```bash  cd target/  mkdir data  cp /home/user/ML-Quadrat/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv data/  ```  Now, in order to run the generated application / IoT service:    ```bash  java -jar SmartPingPongCfg-1.0.0-jar-with-dependencies.jar  ```  After running the service, you will see the output in the terminal."
Moving MNIST,moving mnist,DATASET,moving mnist,Here is an example of single GPU non-distributed training SimVP+gSTA on Moving MNIST dataset.
BAIR Robot Pushing,bair robot pushing,DATASET,bair robot pushing,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?
Human3.6M,human3.6m,DATASET,human3.6m,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?
KTH Action,kth,DATASET,kth,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?
KittiCaltech Pedestrian,NIL,DATASET,NIL,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?
Kinetics-400,kinetics 400,DATASET,kinetics 400,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?
kinetics,kinetics,DATASET,kinetics,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?
Moving MNIST,moving mnist,DATASET,moving mnist,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?
mmnist,NIL,DATASET,NIL,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?
Moving FMNIST,NIL,DATASET,NIL,<details open>     <summary>Currently supported datasets</summary>      - [x] [BAIR Robot Pushing](https://arxiv.org/abs/1710.05268) (CoRL'2017) [[download](https://sites.google.com/berkeley.edu/robotic-interaction-datasets)] [[config](configs/bair)]     - [x] [Human3.6M](http://vision.imar.ro/human3.6m/pami-h36m.pdf) (TPAMI'2014) [[download](http://vision.imar.ro/human3.6m/description.php)] [[config](configs/human)]     - [x] [KTH Action](https://ieeexplore.ieee.org/document/1334462) (ICPR'2004) [[download](https://www.csc.kth.se/cvap/actions/)] [[config](configs/kth)]     - [x] [KittiCaltech Pedestrian](https://dl.acm.org/doi/10.1177/0278364913491297) (IJRR'2013) [[download](https://www.dropbox.com/s/rpwlnn6j39jjme4/kitti_data.zip)] [[config](configs/kitticaltech)]     - [x] [Kinetics-400](https://arxiv.org/abs/1705.06950) (ArXiv'2017) [[download](https://deepmind.com/research/open-source/kinetics)] [[config](configs/kinetics)]     - [x] [Moving MNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](http://www.cs.toronto.edu/~nitish/unsupervised_video/)] [[config](configs/mmnist)]     - [x] [Moving FMNIST](http://arxiv.org/abs/1502.04681) (ICML'2015) [[download](https://pan.baidu.com/s/1fudsBHyrf3nbt-7d42YWWg?
mfmnist,NIL,DATASET,NIL,"pwd=kjfk)] [[config](configs/mfmnist)]     - [x] [TaxiBJ](https://arxiv.org/abs/1610.00081) (AAAI'2017) [[download](https://github.com/TolicWang/DeepST/tree/master/data/TaxiBJ)] [[config](configs/taxibj)]     - [x] [WeatherBench](https://arxiv.org/abs/2002.00469) (ArXiv'2020) [[download](https://github.com/pangeo-data/WeatherBench)] [[config](configs/weather)]      </details>  <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ## Visualization  We present visualization examples of ConvLSTM below."
TaxiBJ,taxibj,DATASET,taxibj,"pwd=kjfk)] [[config](configs/mfmnist)]     - [x] [TaxiBJ](https://arxiv.org/abs/1610.00081) (AAAI'2017) [[download](https://github.com/TolicWang/DeepST/tree/master/data/TaxiBJ)] [[config](configs/taxibj)]     - [x] [WeatherBench](https://arxiv.org/abs/2002.00469) (ArXiv'2020) [[download](https://github.com/pangeo-data/WeatherBench)] [[config](configs/weather)]      </details>  <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ## Visualization  We present visualization examples of ConvLSTM below."
TaxiBJ,taxibj,DATASET,taxibj,"pwd=kjfk)] [[config](configs/mfmnist)]     - [x] [TaxiBJ](https://arxiv.org/abs/1610.00081) (AAAI'2017) [[download](https://github.com/TolicWang/DeepST/tree/master/data/TaxiBJ)] [[config](configs/taxibj)]     - [x] [WeatherBench](https://arxiv.org/abs/2002.00469) (ArXiv'2020) [[download](https://github.com/pangeo-data/WeatherBench)] [[config](configs/weather)]      </details>  <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ## Visualization  We present visualization examples of ConvLSTM below."
WeatherBench,weatherbench,DATASET,weatherbench,"pwd=kjfk)] [[config](configs/mfmnist)]     - [x] [TaxiBJ](https://arxiv.org/abs/1610.00081) (AAAI'2017) [[download](https://github.com/TolicWang/DeepST/tree/master/data/TaxiBJ)] [[config](configs/taxibj)]     - [x] [WeatherBench](https://arxiv.org/abs/2002.00469) (ArXiv'2020) [[download](https://github.com/pangeo-data/WeatherBench)] [[config](configs/weather)]      </details>  <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ## Visualization  We present visualization examples of ConvLSTM below."
WeatherBench,weatherbench,DATASET,weatherbench,"pwd=kjfk)] [[config](configs/mfmnist)]     - [x] [TaxiBJ](https://arxiv.org/abs/1610.00081) (AAAI'2017) [[download](https://github.com/TolicWang/DeepST/tree/master/data/TaxiBJ)] [[config](configs/taxibj)]     - [x] [WeatherBench](https://arxiv.org/abs/2002.00469) (ArXiv'2020) [[download](https://github.com/pangeo-data/WeatherBench)] [[config](configs/weather)]      </details>  <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ## Visualization  We present visualization examples of ConvLSTM below."
Moving MNIST,moving mnist,DATASET,moving mnist,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
Moving FMNIST,NIL,DATASET,NIL,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
mmnist,NIL,DATASET,NIL,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
Moving MNIST-CIFAR,NIL,DATASET,NIL,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
KittiCaltech,NIL,DATASET,NIL,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
moving_mnist,moving mnist,DATASET,moving mnist,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
kitticaltech,NIL,DATASET,NIL,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
KTH,kth,DATASET,kth,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
Human 3.6M,human3.6m,DATASET,human3.6m,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
Traffic - in flow,traffic,DATASET,traffic,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
Traffic - out flow,traffic,DATASET,traffic,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
Weather - Temperature,weather,DATASET,weather,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
Weather - Humidity,weather,DATASET,weather,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
Weather - Latitude Wind,weather,DATASET,weather,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
Weather - Cloud Cover,weather,DATASET,weather,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
BAIR Robot Pushing,bair robot pushing,DATASET,bair robot pushing,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
Kinetics-400,kinetics 400,DATASET,kinetics 400,"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE)."
NUBes,NIL,DATASET,NIL,# NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts  This repository contains the NUBes corpus and other related material.
NUBes,NIL,DATASET,NIL,# NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts  This repository contains the NUBes corpus and other related material.
NUBes,NIL,DATASET,NIL,[NUBes and IULA+](#nubes-and-iula)     * [Size and Composition](#size-and-composition)     * [Data Protection](#data-protection)     * [Related Publications](#related-publications) 1.
IULA+,NIL,DATASET,NIL,[NUBes and IULA+](#nubes-and-iula)     * [Size and Composition](#size-and-composition)     * [Data Protection](#data-protection)     * [Related Publications](#related-publications) 1.
nubes,NIL,DATASET,NIL,[NUBes and IULA+](#nubes-and-iula)     * [Size and Composition](#size-and-composition)     * [Data Protection](#data-protection)     * [Related Publications](#related-publications) 1.
iula,NIL,DATASET,NIL,[NUBes and IULA+](#nubes-and-iula)     * [Size and Composition](#size-and-composition)     * [Data Protection](#data-protection)     * [Related Publications](#related-publications) 1.
NUBes,NIL,DATASET,NIL,"[Contact](#contact)   ## NUBes and IULA+  The NUBes corpus (from ""Negation and Uncertainty annotations in Biomedical texts in Spanish"") consists of sentences obtained from anonymised health records and annotated with negation and uncertainty phenomena."
IULA+,NIL,DATASET,NIL,"[Contact](#contact)   ## NUBes and IULA+  The NUBes corpus (from ""Negation and Uncertainty annotations in Biomedical texts in Spanish"") consists of sentences obtained from anonymised health records and annotated with negation and uncertainty phenomena."
NUBes,NIL,DATASET,NIL,"[Contact](#contact)   ## NUBes and IULA+  The NUBes corpus (from ""Negation and Uncertainty annotations in Biomedical texts in Spanish"") consists of sentences obtained from anonymised health records and annotated with negation and uncertainty phenomena."
Negation and Uncertainty annotations in Biomedical texts in Spanish,NIL,DATASET,NIL,"[Contact](#contact)   ## NUBes and IULA+  The NUBes corpus (from ""Negation and Uncertainty annotations in Biomedical texts in Spanish"") consists of sentences obtained from anonymised health records and annotated with negation and uncertainty phenomena."
IULA+,NIL,DATASET,NIL,IULA+ is a new version of the IULA-SCRC corpus (accessible [here](http://eines.iula.upf.edu/brat/\#/NegationOnCR_IULA)).
IULA-SCRC,NIL,DATASET,NIL,IULA+ is a new version of the IULA-SCRC corpus (accessible [here](http://eines.iula.upf.edu/brat/\#/NegationOnCR_IULA)).
iula,NIL,DATASET,NIL,IULA+ is a new version of the IULA-SCRC corpus (accessible [here](http://eines.iula.upf.edu/brat/\#/NegationOnCR_IULA)).
IULA,NIL,DATASET,NIL,IULA+ is a new version of the IULA-SCRC corpus (accessible [here](http://eines.iula.upf.edu/brat/\#/NegationOnCR_IULA)).
NUBes,NIL,DATASET,NIL,"More specifically, it consists of the same texts but annotated with NUBes' guidelines."
NUBes,NIL,DATASET,NIL,/NUBes-guias-de-anotacion.pdf) (in Spanish). * NUBes and IULA+ are distributed in [BRAT standoff format](https://brat.nlplab.org/standoff.html).    ### Size and Composition  * NUBes is divided into 10 samples of approximately 3K sentences each. * The first sample ([SAMPLE-001](.
NUBes,NIL,DATASET,NIL,/NUBes-guias-de-anotacion.pdf) (in Spanish). * NUBes and IULA+ are distributed in [BRAT standoff format](https://brat.nlplab.org/standoff.html).    ### Size and Composition  * NUBes is divided into 10 samples of approximately 3K sentences each. * The first sample ([SAMPLE-001](.
IULA+,NIL,DATASET,NIL,/NUBes-guias-de-anotacion.pdf) (in Spanish). * NUBes and IULA+ are distributed in [BRAT standoff format](https://brat.nlplab.org/standoff.html).    ### Size and Composition  * NUBes is divided into 10 samples of approximately 3K sentences each. * The first sample ([SAMPLE-001](.
NUBes,NIL,DATASET,NIL,/NUBes-guias-de-anotacion.pdf) (in Spanish). * NUBes and IULA+ are distributed in [BRAT standoff format](https://brat.nlplab.org/standoff.html).    ### Size and Composition  * NUBes is divided into 10 samples of approximately 3K sentences each. * The first sample ([SAMPLE-001](.
NUBes,NIL,DATASET,NIL,"/NUBes/SAMPLE-001)) has been annotated following a process involving 2 annotators and   a referee, while the rest have been annotated by one person only."
NUBes,NIL,DATASET,NIL,"/NUBes/SAMPLE-001/sample-001.traum.chico.txt)   contains sentences of the specialty ""traumatology"" and the section ""chief complaint""."
NUBes,NIL,DATASET,NIL,"You can consult the sizes of NUBes and IULA+ in the following table: <table>     <thead>     <th></th>     <th>NUBes</th>     <th>IULA+</th>     </thead>     <tbody>     <tr>     <td colspan=""3""><b>overall stats</b></td>     </tr>     <tr>     <td>sentences</td>     <td>29,682</td>     <td>3,363</td>     </tr>     <tr>     <td>tokens</td>     <td>518,068</td>     <td>38,208</td>     </tr>     <tr>     <td>vocabulary size</td>     <td>31,698</td>     <td>8,651</td>     </tr>     <tr>     <td colspan=""3""><b>negation</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>7,567</td>     <td>1,022<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.25 +/- 0.66</td>     <td>1.20 +/- 0.59</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>0</td>     <td>0</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>4.01 +/- 3.59</td>     <td>3.13 +/- 2.67</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>219</td>     <td>24</td>     </tr>     <tr>     <td colspan=""3""><b>uncertainty</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>2,219</td>     <td>178<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.12 +/- 0.38</td>     <td>1.12 +/- 0.38</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>95</td>     <td>20</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>5.27 +/- 4.97</td>     <td>4.75 +/- 3.96</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>123</td>     <td>7</td>     </tr>     </tbody> </table>  <small><sup>*</sup> These numbers do not match those in the paper's Table 1; the correct counts are shown here."
IULA+,NIL,DATASET,NIL,"You can consult the sizes of NUBes and IULA+ in the following table: <table>     <thead>     <th></th>     <th>NUBes</th>     <th>IULA+</th>     </thead>     <tbody>     <tr>     <td colspan=""3""><b>overall stats</b></td>     </tr>     <tr>     <td>sentences</td>     <td>29,682</td>     <td>3,363</td>     </tr>     <tr>     <td>tokens</td>     <td>518,068</td>     <td>38,208</td>     </tr>     <tr>     <td>vocabulary size</td>     <td>31,698</td>     <td>8,651</td>     </tr>     <tr>     <td colspan=""3""><b>negation</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>7,567</td>     <td>1,022<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.25 +/- 0.66</td>     <td>1.20 +/- 0.59</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>0</td>     <td>0</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>4.01 +/- 3.59</td>     <td>3.13 +/- 2.67</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>219</td>     <td>24</td>     </tr>     <tr>     <td colspan=""3""><b>uncertainty</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>2,219</td>     <td>178<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.12 +/- 0.38</td>     <td>1.12 +/- 0.38</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>95</td>     <td>20</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>5.27 +/- 4.97</td>     <td>4.75 +/- 3.96</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>123</td>     <td>7</td>     </tr>     </tbody> </table>  <small><sup>*</sup> These numbers do not match those in the paper's Table 1; the correct counts are shown here."
NUBes,NIL,DATASET,NIL,"You can consult the sizes of NUBes and IULA+ in the following table: <table>     <thead>     <th></th>     <th>NUBes</th>     <th>IULA+</th>     </thead>     <tbody>     <tr>     <td colspan=""3""><b>overall stats</b></td>     </tr>     <tr>     <td>sentences</td>     <td>29,682</td>     <td>3,363</td>     </tr>     <tr>     <td>tokens</td>     <td>518,068</td>     <td>38,208</td>     </tr>     <tr>     <td>vocabulary size</td>     <td>31,698</td>     <td>8,651</td>     </tr>     <tr>     <td colspan=""3""><b>negation</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>7,567</td>     <td>1,022<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.25 +/- 0.66</td>     <td>1.20 +/- 0.59</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>0</td>     <td>0</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>4.01 +/- 3.59</td>     <td>3.13 +/- 2.67</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>219</td>     <td>24</td>     </tr>     <tr>     <td colspan=""3""><b>uncertainty</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>2,219</td>     <td>178<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.12 +/- 0.38</td>     <td>1.12 +/- 0.38</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>95</td>     <td>20</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>5.27 +/- 4.97</td>     <td>4.75 +/- 3.96</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>123</td>     <td>7</td>     </tr>     </tbody> </table>  <small><sup>*</sup> These numbers do not match those in the paper's Table 1; the correct counts are shown here."
IULA+,NIL,DATASET,NIL,"You can consult the sizes of NUBes and IULA+ in the following table: <table>     <thead>     <th></th>     <th>NUBes</th>     <th>IULA+</th>     </thead>     <tbody>     <tr>     <td colspan=""3""><b>overall stats</b></td>     </tr>     <tr>     <td>sentences</td>     <td>29,682</td>     <td>3,363</td>     </tr>     <tr>     <td>tokens</td>     <td>518,068</td>     <td>38,208</td>     </tr>     <tr>     <td>vocabulary size</td>     <td>31,698</td>     <td>8,651</td>     </tr>     <tr>     <td colspan=""3""><b>negation</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>7,567</td>     <td>1,022<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.25 +/- 0.66</td>     <td>1.20 +/- 0.59</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>0</td>     <td>0</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>4.01 +/- 3.59</td>     <td>3.13 +/- 2.67</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>219</td>     <td>24</td>     </tr>     <tr>     <td colspan=""3""><b>uncertainty</b></td>     </tr>     <tr>     <td>sentences affected</td>     <td>2,219</td>     <td>178<small><sup>*</sup></small></td>     </tr>     <tr>     <td>average cues per affected sentence</td>     <td>1.12 +/- 0.38</td>     <td>1.12 +/- 0.38</td>     </tr>     <tr>     <td>discontinuous cues</td>     <td>95</td>     <td>20</td>     </tr>     <tr>     <td>average scope size in tokens</td>     <td>5.27 +/- 4.97</td>     <td>4.75 +/- 3.96</td>     </tr>     <tr>     <td>discontinuous scopes</td>     <td>123</td>     <td>7</td>     </tr>     </tbody> </table>  <small><sup>*</sup> These numbers do not match those in the paper's Table 1; the correct counts are shown here."
NUBes,NIL,DATASET,NIL,"</small>  ### Data Protection  All sensitive information (e.g., people names, healthcare facilities, dates, and so on) in NUBes have been subsituted with fake similar data."
NUBes,NIL,DATASET,NIL,"What is more, sentences that belong to the same health record are scattered across different samples.   ### Related Publications  To know more about NUBes, read our article ""NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts"" [[pdf](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.708.pdf)]."
NUBes,NIL,DATASET,NIL,"What is more, sentences that belong to the same health record are scattered across different samples.   ### Related Publications  To know more about NUBes, read our article ""NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts"" [[pdf](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.708.pdf)]."
IULA-SCRC,NIL,DATASET,NIL,"To know more about IULA-SCRC, read the article ""Annotation of negation in the IULA Spanish Clinical Record Corpus"" by Montserrat Marimon, Jorge Vivaldi and Núria Bel [[pdf](https://www.aclweb.org/anthology/W17-1807.pdf)]."
NUBes,NIL,DATASET,NIL,"positional arguments:   pred                  path to predictions file (accepts multiple paths)  optional arguments:   -h, --help            show this help message and exit   --task {bin,class,full}                         evaluation mode (see above; default: ""cat"")   --true T              path to gold standard file (default: data/test.bio) ```  Once you have trained your own model(s) and decoded the test set, you may evaluate the results simply by doing:  ```shell script python3 eval.py /PATH/TO/PREDICTION-1 /PATH/TO/PREDICTION-2 /PATH/TO/PREDICTION-3 ```  If you do:  ```shell script python3 eval.py data/test.bio ```  you should obtain perfect results (because you will be evaluating the gold labels against themselves).  ## Citation  If you use NUBes, IULA+ or any of the provided material in your publications, please cite us appropriately:  ```bibtex @inproceedings{lima2020nubes,   author      = {Salvador Lima Lopez and Naiara Perez and Montse Cuadros and German Rigau},   title       = ""{NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts}"",   booktitle   = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC2020)},   month       = {May},   year        = {2020},   address     = {Marseille, France},   publisher   = {European Language Resources Association},   pages       = {5772--5781} }  ```  If you use IULA+, please cite also the paper describing the original corpus, IULA-SCRC:  ```bibtex @inproceedings{marimon2017annotation,   author      = {Montserrat Marimon and Jorge Vivaldi and N{\'u}ria Bel Rafecas},   title       = ""{Annotation of negation in the IULA Spanish Clinical Record Corpus}"",   booktitle   = {Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR)},   month       = {Apr},   year        = {2017},   address     = {Valencia, Spain},   publisher   = {Association for Computational Linguistics},   pages       = {43--52} } ```  ## License  The resources [NUBes](."
IULA+,NIL,DATASET,NIL,"positional arguments:   pred                  path to predictions file (accepts multiple paths)  optional arguments:   -h, --help            show this help message and exit   --task {bin,class,full}                         evaluation mode (see above; default: ""cat"")   --true T              path to gold standard file (default: data/test.bio) ```  Once you have trained your own model(s) and decoded the test set, you may evaluate the results simply by doing:  ```shell script python3 eval.py /PATH/TO/PREDICTION-1 /PATH/TO/PREDICTION-2 /PATH/TO/PREDICTION-3 ```  If you do:  ```shell script python3 eval.py data/test.bio ```  you should obtain perfect results (because you will be evaluating the gold labels against themselves).  ## Citation  If you use NUBes, IULA+ or any of the provided material in your publications, please cite us appropriately:  ```bibtex @inproceedings{lima2020nubes,   author      = {Salvador Lima Lopez and Naiara Perez and Montse Cuadros and German Rigau},   title       = ""{NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts}"",   booktitle   = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC2020)},   month       = {May},   year        = {2020},   address     = {Marseille, France},   publisher   = {European Language Resources Association},   pages       = {5772--5781} }  ```  If you use IULA+, please cite also the paper describing the original corpus, IULA-SCRC:  ```bibtex @inproceedings{marimon2017annotation,   author      = {Montserrat Marimon and Jorge Vivaldi and N{\'u}ria Bel Rafecas},   title       = ""{Annotation of negation in the IULA Spanish Clinical Record Corpus}"",   booktitle   = {Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR)},   month       = {Apr},   year        = {2017},   address     = {Valencia, Spain},   publisher   = {Association for Computational Linguistics},   pages       = {43--52} } ```  ## License  The resources [NUBes](."
NUBes,NIL,DATASET,NIL,"positional arguments:   pred                  path to predictions file (accepts multiple paths)  optional arguments:   -h, --help            show this help message and exit   --task {bin,class,full}                         evaluation mode (see above; default: ""cat"")   --true T              path to gold standard file (default: data/test.bio) ```  Once you have trained your own model(s) and decoded the test set, you may evaluate the results simply by doing:  ```shell script python3 eval.py /PATH/TO/PREDICTION-1 /PATH/TO/PREDICTION-2 /PATH/TO/PREDICTION-3 ```  If you do:  ```shell script python3 eval.py data/test.bio ```  you should obtain perfect results (because you will be evaluating the gold labels against themselves).  ## Citation  If you use NUBes, IULA+ or any of the provided material in your publications, please cite us appropriately:  ```bibtex @inproceedings{lima2020nubes,   author      = {Salvador Lima Lopez and Naiara Perez and Montse Cuadros and German Rigau},   title       = ""{NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts}"",   booktitle   = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC2020)},   month       = {May},   year        = {2020},   address     = {Marseille, France},   publisher   = {European Language Resources Association},   pages       = {5772--5781} }  ```  If you use IULA+, please cite also the paper describing the original corpus, IULA-SCRC:  ```bibtex @inproceedings{marimon2017annotation,   author      = {Montserrat Marimon and Jorge Vivaldi and N{\'u}ria Bel Rafecas},   title       = ""{Annotation of negation in the IULA Spanish Clinical Record Corpus}"",   booktitle   = {Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR)},   month       = {Apr},   year        = {2017},   address     = {Valencia, Spain},   publisher   = {Association for Computational Linguistics},   pages       = {43--52} } ```  ## License  The resources [NUBes](."
IULA+,NIL,DATASET,NIL,"positional arguments:   pred                  path to predictions file (accepts multiple paths)  optional arguments:   -h, --help            show this help message and exit   --task {bin,class,full}                         evaluation mode (see above; default: ""cat"")   --true T              path to gold standard file (default: data/test.bio) ```  Once you have trained your own model(s) and decoded the test set, you may evaluate the results simply by doing:  ```shell script python3 eval.py /PATH/TO/PREDICTION-1 /PATH/TO/PREDICTION-2 /PATH/TO/PREDICTION-3 ```  If you do:  ```shell script python3 eval.py data/test.bio ```  you should obtain perfect results (because you will be evaluating the gold labels against themselves).  ## Citation  If you use NUBes, IULA+ or any of the provided material in your publications, please cite us appropriately:  ```bibtex @inproceedings{lima2020nubes,   author      = {Salvador Lima Lopez and Naiara Perez and Montse Cuadros and German Rigau},   title       = ""{NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts}"",   booktitle   = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC2020)},   month       = {May},   year        = {2020},   address     = {Marseille, France},   publisher   = {European Language Resources Association},   pages       = {5772--5781} }  ```  If you use IULA+, please cite also the paper describing the original corpus, IULA-SCRC:  ```bibtex @inproceedings{marimon2017annotation,   author      = {Montserrat Marimon and Jorge Vivaldi and N{\'u}ria Bel Rafecas},   title       = ""{Annotation of negation in the IULA Spanish Clinical Record Corpus}"",   booktitle   = {Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR)},   month       = {Apr},   year        = {2017},   address     = {Valencia, Spain},   publisher   = {Association for Computational Linguistics},   pages       = {43--52} } ```  ## License  The resources [NUBes](."
IULA-SCRC,NIL,DATASET,NIL,"positional arguments:   pred                  path to predictions file (accepts multiple paths)  optional arguments:   -h, --help            show this help message and exit   --task {bin,class,full}                         evaluation mode (see above; default: ""cat"")   --true T              path to gold standard file (default: data/test.bio) ```  Once you have trained your own model(s) and decoded the test set, you may evaluate the results simply by doing:  ```shell script python3 eval.py /PATH/TO/PREDICTION-1 /PATH/TO/PREDICTION-2 /PATH/TO/PREDICTION-3 ```  If you do:  ```shell script python3 eval.py data/test.bio ```  you should obtain perfect results (because you will be evaluating the gold labels against themselves).  ## Citation  If you use NUBes, IULA+ or any of the provided material in your publications, please cite us appropriately:  ```bibtex @inproceedings{lima2020nubes,   author      = {Salvador Lima Lopez and Naiara Perez and Montse Cuadros and German Rigau},   title       = ""{NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts}"",   booktitle   = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC2020)},   month       = {May},   year        = {2020},   address     = {Marseille, France},   publisher   = {European Language Resources Association},   pages       = {5772--5781} }  ```  If you use IULA+, please cite also the paper describing the original corpus, IULA-SCRC:  ```bibtex @inproceedings{marimon2017annotation,   author      = {Montserrat Marimon and Jorge Vivaldi and N{\'u}ria Bel Rafecas},   title       = ""{Annotation of negation in the IULA Spanish Clinical Record Corpus}"",   booktitle   = {Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR)},   month       = {Apr},   year        = {2017},   address     = {Valencia, Spain},   publisher   = {Association for Computational Linguistics},   pages       = {43--52} } ```  ## License  The resources [NUBes](."
NUBes,NIL,DATASET,NIL,"positional arguments:   pred                  path to predictions file (accepts multiple paths)  optional arguments:   -h, --help            show this help message and exit   --task {bin,class,full}                         evaluation mode (see above; default: ""cat"")   --true T              path to gold standard file (default: data/test.bio) ```  Once you have trained your own model(s) and decoded the test set, you may evaluate the results simply by doing:  ```shell script python3 eval.py /PATH/TO/PREDICTION-1 /PATH/TO/PREDICTION-2 /PATH/TO/PREDICTION-3 ```  If you do:  ```shell script python3 eval.py data/test.bio ```  you should obtain perfect results (because you will be evaluating the gold labels against themselves).  ## Citation  If you use NUBes, IULA+ or any of the provided material in your publications, please cite us appropriately:  ```bibtex @inproceedings{lima2020nubes,   author      = {Salvador Lima Lopez and Naiara Perez and Montse Cuadros and German Rigau},   title       = ""{NUBes: A Corpus of Negation and Uncertainty in Spanish Clinical Texts}"",   booktitle   = {Proceedings of The 12th Language Resources and Evaluation Conference (LREC2020)},   month       = {May},   year        = {2020},   address     = {Marseille, France},   publisher   = {European Language Resources Association},   pages       = {5772--5781} }  ```  If you use IULA+, please cite also the paper describing the original corpus, IULA-SCRC:  ```bibtex @inproceedings{marimon2017annotation,   author      = {Montserrat Marimon and Jorge Vivaldi and N{\'u}ria Bel Rafecas},   title       = ""{Annotation of negation in the IULA Spanish Clinical Record Corpus}"",   booktitle   = {Proceedings of the Workshop Computational Semantics Beyond Events and Roles (SemBEaR)},   month       = {Apr},   year        = {2017},   address     = {Valencia, Spain},   publisher   = {Association for Computational Linguistics},   pages       = {43--52} } ```  ## License  The resources [NUBes](."
NUBes,NIL,DATASET,NIL,"/NUBes), [IULA+](."
IULA+,NIL,DATASET,NIL,"/NUBes), [IULA+](."
IULA+,NIL,DATASET,NIL,"/IULA+), [NUBes experiment splits](."
NUBes,NIL,DATASET,NIL,"/IULA+), [NUBes experiment splits](."
NUBes,NIL,DATASET,NIL,/LREC2020/data) and [NUBes annotation guidelines](.
NUBes,NIL,DATASET,NIL,/NUBes-guias-de-anotacion.pdf) are licensed under the Creative Commons Attribution-ShareAlike 3.0 Spain  License.
AppSim,NIL,DATASET,NIL,# UI understanding datasets for UIBert  This directory contains two datasets that are used in the downstream tasks for evaluating [UIBert: Learning Generic Multimodal Representations for UI Understanding](https://arxiv.org/abs/2107.13731): app similar UI component retrieval data (AppSim) and referring expression component retrieval data (RefExp).
RefExp,google refexp,DATASET,google refexp,# UI understanding datasets for UIBert  This directory contains two datasets that are used in the downstream tasks for evaluating [UIBert: Learning Generic Multimodal Representations for UI Understanding](https://arxiv.org/abs/2107.13731): app similar UI component retrieval data (AppSim) and referring expression component retrieval data (RefExp).
Rico,ricosca,DATASET,ricosca,"Both datasets are extended from the public Rico dataset, which contains 72k mobile app UI data."
AppSim,NIL,DATASET,NIL,"In AppSim, each datapoint contains two UIs of similar category and the     annotation of two semantically similar UI elements on them, such as a “Menu”     buttons that appear on two UIs. 2."
RefExp,google refexp,DATASET,google refexp,"In RefExp, each datapoint contains a UI and a referring expression of a UI     element on it, such as “Red button on the top”."
Rico,ricosca,DATASET,ricosca,"We use unique image ids in both datasets to represent the UI and they can be used to retrieve the original UI data in [Rico](https://interactionmining.org/rico).  ## Data format  Both data are saved as TFRecords, which makes it easy to be loaded in Tensorflow.  1."
rico,ricosca,DATASET,ricosca,"We use unique image ids in both datasets to represent the UI and they can be used to retrieve the original UI data in [Rico](https://interactionmining.org/rico).  ## Data format  Both data are saved as TFRecords, which makes it easy to be loaded in Tensorflow.  1."
AppSim,NIL,DATASET,NIL,AppSim: Each TFRecord contains unique image ids of an anchor UI and a search     UI.
RefExp,google refexp,DATASET,google refexp,RefExp: Each TFRecord has the following features.
ref_exp,google refexp,DATASET,google refexp,-   `image/ref_exp/label`: int64_list[1].
ref_exp,google refexp,DATASET,google refexp,-   `image/ref_exp/text`: bytes_list[1].
HaluEval,halueval,DATASET,halueval,# HaluEval: A Hallucination Evaluation Benchmark for LLMs  This is the repo for our paper: [HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2305.11747).
HaluEval,halueval,DATASET,halueval,# HaluEval: A Hallucination Evaluation Benchmark for LLMs  This is the repo for our paper: [HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2305.11747).
HaluEval,halueval,DATASET,halueval,"The repo contains:  - The [35K data](#data-release) used for evaluating the LLM. - The code for [generating the data](#data-generation-process). - The code for [evaluating the model](#evaluation). - The code for [analyzing the model](#analysis).  ## Overview  HaluEval includes 5,000 general user queries with ChatGPT responses and  30,000 task-specific examples from three tasks, i.e., question answering, knowledge-grounded dialogue, and text summarization."
HaluEval,halueval,DATASET,halueval,"Furthermore, for the task-specific examples in HaluEval, we design an automatic approach to generate hallucinated samples."
HotpotQA,hotpotqa,DATASET,hotpotqa,"First, based on existing task datasets (e.g., HotpotQA) as seed data, we design task-specific instructions for ChatGPT to generate hallucinated samples in two methods, i.e., one-pass and conversational."
HaluEval,halueval,DATASET,halueval,"<a href=""https://github.com/RUCAIBox/HaluEval"" target=""_blank""><img src=""assets/pipeline.png"" alt=""HaluEval"" style=""width: 90%; min-width: 300px; display: block; margin: auto;""></a>  ## Data Release  The directory [`data`](."
HaluEval,halueval,DATASET,halueval,"<a href=""https://github.com/RUCAIBox/HaluEval"" target=""_blank""><img src=""assets/pipeline.png"" alt=""HaluEval"" style=""width: 90%; min-width: 300px; display: block; margin: auto;""></a>  ## Data Release  The directory [`data`](."
HotpotQA,hotpotqa,DATASET,hotpotqa,/data/qa_data.json): 10K hallucinated samples for QA based on [HotpotQA](https://hotpotqa.github.io/) as seed data.
hotpotqa,hotpotqa,DATASET,hotpotqa,/data/qa_data.json): 10K hallucinated samples for QA based on [HotpotQA](https://hotpotqa.github.io/) as seed data.
HotpotQA,hotpotqa,DATASET,hotpotqa,"For each sample dictionary, the fields `knowledge`, `question`, and `right_answer` refer to the knowledge from Wikipedia, question text, and ground-truth answer collected from HotpotQA."
OpenDialKG,opendialkg,DATASET,opendialkg,/data/dialogue_data.json): 10K hallucinated samples for dialogue based on [OpenDialKG](https://github.com/facebookresearch/opendialkg) as seed data.
opendialkg,opendialkg,DATASET,opendialkg,/data/dialogue_data.json): 10K hallucinated samples for dialogue based on [OpenDialKG](https://github.com/facebookresearch/opendialkg) as seed data.
OpenDialKG,opendialkg,DATASET,opendialkg,"For each sample dictionary, the fields `knowledge`, `dialogue_history`, and `right_response` refer to the knowledge from Wikipedia, dialogue history, and ground-truth response collected from OpenDialKG."
CNN/Daily Mail,cnn/daily mail,DATASET,cnn/daily mail,/data/summarization_data.json): 10K hallucinated samples for summarization based on [CNN/Daily Mail](https://github.com/abisee/cnn-dailymail) as seed data.
cnn-dailymail,cnn/daily mail,DATASET,cnn/daily mail,/data/summarization_data.json): 10K hallucinated samples for summarization based on [CNN/Daily Mail](https://github.com/abisee/cnn-dailymail) as seed data.
CNN/Daily Mail,cnn/daily mail,DATASET,cnn/daily mail,"For each sample dictionary, the fields `document` and `right_summary` refer to the document and ground-truth summary collected from CNN/Daily Mail."
HotpotQA,hotpotqa,DATASET,hotpotqa,"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy"
OpenDialKG,opendialkg,DATASET,opendialkg,"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy"
CNN/Daily Mail,cnn/daily mail,DATASET,cnn/daily mail,"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy"
hotpot/hotpot_train_v1.1,hotpotqa,DATASET,hotpotqa,"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy"
opendialkg,opendialkg,DATASET,opendialkg,"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy"
opendialkg,opendialkg,DATASET,opendialkg,"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy"
ccdv/cnn_dailymail,cnn/daily mail,DATASET,cnn/daily mail,"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy"
HotpotQA,hotpotqa,DATASET,hotpotqa,"- `seed_data`: the downloaded training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail"
OpenDialKG,opendialkg,DATASET,opendialkg,"- `seed_data`: the downloaded training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail"
CNN/Daily Mail,cnn/daily mail,DATASET,cnn/daily mail,"- `seed_data`: the downloaded training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail"
hotpot_train_v1.1,hotpotqa,DATASET,hotpotqa,"(one-pass and conversational in our paper) ``` python generate.py --seed_data hotpot_train_v1.1.json --task qa --strategy one-turn ```  - Finally, we select the most plausible and difficult hallucinated sample from these two sampling methods."
HaluEval,halueval,DATASET,halueval,/evaluation/qa/qa_gpt-3.5-turbo_result.json --category all ```  ## License  HaluEval uses [MIT License](.
HaluEval,halueval,DATASET,halueval,"/LICENSE).  ## Reference  Please cite the repo if you use the data or code in this repo.  ``` @misc{HaluEval,   author = {Junyi Li and Xiaoxue Cheng and Wayne Xin Zhao and Jian-Yun Nie and Ji-Rong Wen },   title = {HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models},   year = {2023},   journal={arXiv preprint arXiv:2305.11747},   url={https://arxiv.org/abs/2305.11747} } ```"
HaluEval,halueval,DATASET,halueval,"/LICENSE).  ## Reference  Please cite the repo if you use the data or code in this repo.  ``` @misc{HaluEval,   author = {Junyi Li and Xiaoxue Cheng and Wayne Xin Zhao and Jian-Yun Nie and Ji-Rong Wen },   title = {HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models},   year = {2023},   journal={arXiv preprint arXiv:2305.11747},   url={https://arxiv.org/abs/2305.11747} } ```"
Goldfinch,goldfinch,DATASET,goldfinch,# Goldfinch: GOogLe image-search Dataset for FINe grained CHallenges  Goldfinch is a dataset for fine-grained recognition challenges.
Goldfinch,goldfinch,DATASET,goldfinch,# Goldfinch: GOogLe image-search Dataset for FINe grained CHallenges  Goldfinch is a dataset for fine-grained recognition challenges.
Goldfinch,goldfinch,DATASET,goldfinch,Goldfinch was published along with our [ECCV'16](http://www.eccv2016.org/) paper [The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition](https://arxiv.org/abs/1511.06789).
cub,cub-200-2011,DATASET,cub-200-2011,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity."
CUB,cub-200-2011,DATASET,cub-200-2011,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity."
CUB,cub-200-2011,DATASET,cub-200-2011,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity."
birdsnap,birdsnap,DATASET,birdsnap,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity."
Birdsnap,birdsnap,DATASET,birdsnap,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity."
Birdsnap,birdsnap,DATASET,birdsnap,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity."
fgvc,fgvc-aircraft,DATASET,fgvc-aircraft,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity."
FGVC-Aircraft,fgvc-aircraft,DATASET,fgvc-aircraft,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity."
FGVC,fgvc-aircraft,DATASET,fgvc-aircraft,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity."
FGVC,fgvd,DATASET,fgvd,"Questions can be directed to jkrause@google.com and howardzhou@google.com.  ---- ## Class files  ### cub_classes.txt: For each category in the CUB dataset: ``` CUB class name,freebase id ``` E.g. ``` Black-footed Albatross,/m/04njb7 Laysan Albatross,/m/0544gc Sooty Albatross,/m/07yydf ... ```  ### birdsnap_classes.txt: For each category in the Birdsnap dadataset: ``` Birdsnap class name (scientific),freebase id ``` E.g. ``` Accipiter cooperii,/m/01_ryh Accipiter gentilis,/m/01vl7n Accipiter striatus,/m/01_rvy ... ```  ### fgvc_classes.txt: For each category in the FGVC-Aircraft dataset: ``` FGVC class name ``` Note: no freebase id is used for aircraft since not all aircraft in FGVC correspond to a freebase entity."
Stanford Dogs,stanford dogs,DATASET,stanford dogs,"E.g. ``` ATR-42 ATR-72 Airbus A300B4 ... ```  ### sdog_classes.txt: For each category in the Stanford Dogs dataset: ``` Stanford Dogs class name,freebase id ``` E.g. ``` affenpinscher,/m/01p2dp afghan_hound,/m/011l1 african_hunting_dog,/m/02twvh ... ```  ### bird_classes.txt: For each category in L-Bird: ``` scientific name (estimated),freebase id ``` E.g. ``` ""lithoptila abdounensis"",/m/0100hjcf ""kurrartapu johnnguyeni"",/m/010hmtwz ""zosterops kirki"",/m/011l7vvz ... ```  ### lepidoptera_classes.txt: For each category in L-Butterfly: ``` scientific name (estimated),freebase id ``` E.g. ``` ""sibirarctia kindermanni"",/m/0100bkjp ""zamarada metallicata"",/m/0100jnk9 ""secusio strigata"",/m/0100mnkj ... ```  ### dog_classes.txt: For each category in L-Dog: ``` breed name,freebase id ``` E.g. ``` ""greyhound"",/m/03d12 ""boykin spaniel"",/m/08s0ws ""white shepherd"",/m/05912f ... ```  ### aircraft_classes.txt: For each category in L-Aircraft: ``` category name ``` E.g. ``` ""aai rq 7 shadow"" ""aero ae 45"" ""aero boero ab 95"" ... ```  ---- ## Image URLs  Compressed version of URL text files."
Stanford Dogs,stanford dogs,DATASET,stanford dogs,"E.g. ``` ATR-42 ATR-72 Airbus A300B4 ... ```  ### sdog_classes.txt: For each category in the Stanford Dogs dataset: ``` Stanford Dogs class name,freebase id ``` E.g. ``` affenpinscher,/m/01p2dp afghan_hound,/m/011l1 african_hunting_dog,/m/02twvh ... ```  ### bird_classes.txt: For each category in L-Bird: ``` scientific name (estimated),freebase id ``` E.g. ``` ""lithoptila abdounensis"",/m/0100hjcf ""kurrartapu johnnguyeni"",/m/010hmtwz ""zosterops kirki"",/m/011l7vvz ... ```  ### lepidoptera_classes.txt: For each category in L-Butterfly: ``` scientific name (estimated),freebase id ``` E.g. ``` ""sibirarctia kindermanni"",/m/0100bkjp ""zamarada metallicata"",/m/0100jnk9 ""secusio strigata"",/m/0100mnkj ... ```  ### dog_classes.txt: For each category in L-Dog: ``` breed name,freebase id ``` E.g. ``` ""greyhound"",/m/03d12 ""boykin spaniel"",/m/08s0ws ""white shepherd"",/m/05912f ... ```  ### aircraft_classes.txt: For each category in L-Aircraft: ``` category name ``` E.g. ``` ""aai rq 7 shadow"" ""aero ae 45"" ""aero boero ab 95"" ... ```  ---- ## Image URLs  Compressed version of URL text files."
L-Bird,l-bird,DATASET,l-bird,"E.g. ``` ATR-42 ATR-72 Airbus A300B4 ... ```  ### sdog_classes.txt: For each category in the Stanford Dogs dataset: ``` Stanford Dogs class name,freebase id ``` E.g. ``` affenpinscher,/m/01p2dp afghan_hound,/m/011l1 african_hunting_dog,/m/02twvh ... ```  ### bird_classes.txt: For each category in L-Bird: ``` scientific name (estimated),freebase id ``` E.g. ``` ""lithoptila abdounensis"",/m/0100hjcf ""kurrartapu johnnguyeni"",/m/010hmtwz ""zosterops kirki"",/m/011l7vvz ... ```  ### lepidoptera_classes.txt: For each category in L-Butterfly: ``` scientific name (estimated),freebase id ``` E.g. ``` ""sibirarctia kindermanni"",/m/0100bkjp ""zamarada metallicata"",/m/0100jnk9 ""secusio strigata"",/m/0100mnkj ... ```  ### dog_classes.txt: For each category in L-Dog: ``` breed name,freebase id ``` E.g. ``` ""greyhound"",/m/03d12 ""boykin spaniel"",/m/08s0ws ""white shepherd"",/m/05912f ... ```  ### aircraft_classes.txt: For each category in L-Aircraft: ``` category name ``` E.g. ``` ""aai rq 7 shadow"" ""aero ae 45"" ""aero boero ab 95"" ... ```  ---- ## Image URLs  Compressed version of URL text files."
L-Butterfly,NIL,DATASET,NIL,"E.g. ``` ATR-42 ATR-72 Airbus A300B4 ... ```  ### sdog_classes.txt: For each category in the Stanford Dogs dataset: ``` Stanford Dogs class name,freebase id ``` E.g. ``` affenpinscher,/m/01p2dp afghan_hound,/m/011l1 african_hunting_dog,/m/02twvh ... ```  ### bird_classes.txt: For each category in L-Bird: ``` scientific name (estimated),freebase id ``` E.g. ``` ""lithoptila abdounensis"",/m/0100hjcf ""kurrartapu johnnguyeni"",/m/010hmtwz ""zosterops kirki"",/m/011l7vvz ... ```  ### lepidoptera_classes.txt: For each category in L-Butterfly: ``` scientific name (estimated),freebase id ``` E.g. ``` ""sibirarctia kindermanni"",/m/0100bkjp ""zamarada metallicata"",/m/0100jnk9 ""secusio strigata"",/m/0100mnkj ... ```  ### dog_classes.txt: For each category in L-Dog: ``` breed name,freebase id ``` E.g. ``` ""greyhound"",/m/03d12 ""boykin spaniel"",/m/08s0ws ""white shepherd"",/m/05912f ... ```  ### aircraft_classes.txt: For each category in L-Aircraft: ``` category name ``` E.g. ``` ""aai rq 7 shadow"" ""aero ae 45"" ""aero boero ab 95"" ... ```  ---- ## Image URLs  Compressed version of URL text files."
L-Dog,l-bird,DATASET,l-bird,"E.g. ``` ATR-42 ATR-72 Airbus A300B4 ... ```  ### sdog_classes.txt: For each category in the Stanford Dogs dataset: ``` Stanford Dogs class name,freebase id ``` E.g. ``` affenpinscher,/m/01p2dp afghan_hound,/m/011l1 african_hunting_dog,/m/02twvh ... ```  ### bird_classes.txt: For each category in L-Bird: ``` scientific name (estimated),freebase id ``` E.g. ``` ""lithoptila abdounensis"",/m/0100hjcf ""kurrartapu johnnguyeni"",/m/010hmtwz ""zosterops kirki"",/m/011l7vvz ... ```  ### lepidoptera_classes.txt: For each category in L-Butterfly: ``` scientific name (estimated),freebase id ``` E.g. ``` ""sibirarctia kindermanni"",/m/0100bkjp ""zamarada metallicata"",/m/0100jnk9 ""secusio strigata"",/m/0100mnkj ... ```  ### dog_classes.txt: For each category in L-Dog: ``` breed name,freebase id ``` E.g. ``` ""greyhound"",/m/03d12 ""boykin spaniel"",/m/08s0ws ""white shepherd"",/m/05912f ... ```  ### aircraft_classes.txt: For each category in L-Aircraft: ``` category name ``` E.g. ``` ""aai rq 7 shadow"" ""aero ae 45"" ""aero boero ab 95"" ... ```  ---- ## Image URLs  Compressed version of URL text files."
L-Aircraft,NIL,DATASET,NIL,"E.g. ``` ATR-42 ATR-72 Airbus A300B4 ... ```  ### sdog_classes.txt: For each category in the Stanford Dogs dataset: ``` Stanford Dogs class name,freebase id ``` E.g. ``` affenpinscher,/m/01p2dp afghan_hound,/m/011l1 african_hunting_dog,/m/02twvh ... ```  ### bird_classes.txt: For each category in L-Bird: ``` scientific name (estimated),freebase id ``` E.g. ``` ""lithoptila abdounensis"",/m/0100hjcf ""kurrartapu johnnguyeni"",/m/010hmtwz ""zosterops kirki"",/m/011l7vvz ... ```  ### lepidoptera_classes.txt: For each category in L-Butterfly: ``` scientific name (estimated),freebase id ``` E.g. ``` ""sibirarctia kindermanni"",/m/0100bkjp ""zamarada metallicata"",/m/0100jnk9 ""secusio strigata"",/m/0100mnkj ... ```  ### dog_classes.txt: For each category in L-Dog: ``` breed name,freebase id ``` E.g. ``` ""greyhound"",/m/03d12 ""boykin spaniel"",/m/08s0ws ""white shepherd"",/m/05912f ... ```  ### aircraft_classes.txt: For each category in L-Aircraft: ``` category name ``` E.g. ``` ""aai rq 7 shadow"" ""aero ae 45"" ""aero boero ab 95"" ... ```  ---- ## Image URLs  Compressed version of URL text files."
Flickr testing set,flickr1024,DATASET,flickr1024,"id=1535 ... ```  ### flickr_{aircraft,bird,lepidoptera}.txt: For each image in the Flickr testing set: ``` freebase id (or category name for aircraft),urls ``` E.g. ``` ""northrop f 20 tigershark"",http://farm8.staticflickr.com/7151/6387816433_fbf6240392_b.jpg ""transall c 160"",http://farm1.staticflickr.com/292/19707583438_e199092d07_c.jpg ""mikoyan mig 29"",http://farm3.staticflickr.com/2831/9358339349_271a6f28cd_c.jpg ... ```  ---- ## Active learning files  ### sdogs_active_learning_iteration{1,2}.txt:  Images mined from Yahoo Flickr Creative Commons 100M dataset."
Yahoo Flickr Creative Commons 100M,flickr1024,DATASET,flickr1024,"id=1535 ... ```  ### flickr_{aircraft,bird,lepidoptera}.txt: For each image in the Flickr testing set: ``` freebase id (or category name for aircraft),urls ``` E.g. ``` ""northrop f 20 tigershark"",http://farm8.staticflickr.com/7151/6387816433_fbf6240392_b.jpg ""transall c 160"",http://farm1.staticflickr.com/292/19707583438_e199092d07_c.jpg ""mikoyan mig 29"",http://farm3.staticflickr.com/2831/9358339349_271a6f28cd_c.jpg ... ```  ---- ## Active learning files  ### sdogs_active_learning_iteration{1,2}.txt:  Images mined from Yahoo Flickr Creative Commons 100M dataset."
SVAMP,svamp,DATASET,svamp,"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1."
SVAMP,svamp,DATASET,svamp,"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1."
GSM8K,gsm8k,DATASET,gsm8k,"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1."
gsm8k,gsm8k,DATASET,gsm8k,"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1."
MAWPS,mawps,DATASET,mawps,"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1."
mawps,mawps,DATASET,mawps,"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ⚙️ Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1."
gsm8k,gsm8k,DATASET,gsm8k,"Therefore, if you are just looking to explore the method and don't need to re-create everything from scratch, we recommend that you skip this step,   If you would like to reproduce the offline dataset with the llama2 model, you need to follow these steps:   ```  git clone git@github.com:facebookresearch/llama.git  ``` and then move ```Prompt-OIRL/llama_exps/llama_step1_gen_offline.py``` to the ```llama``` folder  then run the following command   ``` torchrun --nproc_per_node 1 llama_step1_gen_offline.py \     --ckpt_dir llama-2-7b-chat/ \     --tokenizer_path tokenizer.model \     --max_seq_len 512 --max_batch_size 8 --prompt_idx 0 --dataset_eval gsm8k  ```  #### Step 2."
XOR,xor-tydi qa,DATASET,xor-tydi qa,"We investigate abstract datasets such as XOR, Spiral, and Circle and some well-known security-specific datasets for intrusion detection of simulated network traffic, using distributional shift detection measures including the Kolmogorov-Smirnov, Kuiper, Anderson-Darling, Wasserstein and mixed Wasserstein-Anderson-Darling measures."
Spiral,NIL,DATASET,NIL,"We investigate abstract datasets such as XOR, Spiral, and Circle and some well-known security-specific datasets for intrusion detection of simulated network traffic, using distributional shift detection measures including the Kolmogorov-Smirnov, Kuiper, Anderson-Darling, Wasserstein and mixed Wasserstein-Anderson-Darling measures."
Circle,circle,DATASET,circle,"We investigate abstract datasets such as XOR, Spiral, and Circle and some well-known security-specific datasets for intrusion detection of simulated network traffic, using distributional shift detection measures including the Kolmogorov-Smirnov, Kuiper, Anderson-Darling, Wasserstein and mixed Wasserstein-Anderson-Darling measures."
Security,NIL,DATASET,NIL,"Application of the SafeML in AI explainability and interpretability</figcaption>  </p> -->  ## Case Studies <ul>   <li><b>Security Datasets</b></li> <p align=""justify""> A Intrusion Detection Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2017.html"">CICIDS2017</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
Intrusion Detection,NIL,DATASET,NIL,"Application of the SafeML in AI explainability and interpretability</figcaption>  </p> -->  ## Case Studies <ul>   <li><b>Security Datasets</b></li> <p align=""justify""> A Intrusion Detection Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2017.html"">CICIDS2017</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
ids-2017,cicids2017,DATASET,cicids2017,"Application of the SafeML in AI explainability and interpretability</figcaption>  </p> -->  ## Case Studies <ul>   <li><b>Security Datasets</b></li> <p align=""justify""> A Intrusion Detection Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2017.html"">CICIDS2017</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
CICIDS2017,cicids2017,DATASET,cicids2017,"Application of the SafeML in AI explainability and interpretability</figcaption>  </p> -->  ## Case Studies <ul>   <li><b>Security Datasets</b></li> <p align=""justify""> A Intrusion Detection Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2017.html"">CICIDS2017</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
Tor-nonTor,NIL,DATASET,NIL,"</li></p> <p align=""justify""> Tor-nonTor Dataset (<a href=""https://www.unb.ca/cic/datasets/tor.html"">ISCXTor2016</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
tor,NIL,DATASET,NIL,"</li></p> <p align=""justify""> Tor-nonTor Dataset (<a href=""https://www.unb.ca/cic/datasets/tor.html"">ISCXTor2016</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
ISCXTor2016,NIL,DATASET,NIL,"</li></p> <p align=""justify""> Tor-nonTor Dataset (<a href=""https://www.unb.ca/cic/datasets/tor.html"">ISCXTor2016</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
DDoS Attach,NIL,DATASET,NIL,"</p> <p align=""justify""> DDoS Attach Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2018.html"">CSE-CIC-IDS2018</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
ids-2018,NIL,DATASET,NIL,"</p> <p align=""justify""> DDoS Attach Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2018.html"">CSE-CIC-IDS2018</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
CSE-CIC-IDS2018,NIL,DATASET,NIL,"</p> <p align=""justify""> DDoS Attach Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2018.html"">CSE-CIC-IDS2018</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
NSL version of KDD Security,NIL,DATASET,NIL,"</p> <p align=""justify""> NSL version of KDD Security Dataset  (<a href=""https://www.unb.ca/cic/datasets/nsl.html"">NSL-KDD</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/NSL_KDD_Security_Dataset"">MATLAB implementation</a>]."
nsl,NIL,DATASET,NIL,"</p> <p align=""justify""> NSL version of KDD Security Dataset  (<a href=""https://www.unb.ca/cic/datasets/nsl.html"">NSL-KDD</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/NSL_KDD_Security_Dataset"">MATLAB implementation</a>]."
NSL-KDD,NIL,DATASET,NIL,"</p> <p align=""justify""> NSL version of KDD Security Dataset  (<a href=""https://www.unb.ca/cic/datasets/nsl.html"">NSL-KDD</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/NSL_KDD_Security_Dataset"">MATLAB implementation</a>]."
NSL_KDD_Security,NIL,DATASET,NIL,"</p> <p align=""justify""> NSL version of KDD Security Dataset  (<a href=""https://www.unb.ca/cic/datasets/nsl.html"">NSL-KDD</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/NSL_KDD_Security_Dataset"">MATLAB implementation</a>]."
DDoS Evaluation,NIL,DATASET,NIL,"</p> <p align=""justify""> DDoS Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ddos-2019.html"">CICDDoS2019</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/CICDDoS2019_Security_Dataset"">MATLAB implementation</a>]."
ddos-2019,NIL,DATASET,NIL,"</p> <p align=""justify""> DDoS Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ddos-2019.html"">CICDDoS2019</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/CICDDoS2019_Security_Dataset"">MATLAB implementation</a>]."
CICDDoS2019,NIL,DATASET,NIL,"</p> <p align=""justify""> DDoS Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ddos-2019.html"">CICDDoS2019</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/CICDDoS2019_Security_Dataset"">MATLAB implementation</a>]."
CICDDoS2019_Security,NIL,DATASET,NIL,"</p> <p align=""justify""> DDoS Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ddos-2019.html"">CICDDoS2019</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/CICDDoS2019_Security_Dataset"">MATLAB implementation</a>]."
CIC DoS,NIL,DATASET,NIL,"</p> <p align=""justify""> CIC DoS Dataset (<a href=""https://www.unb.ca/cic/datasets/dos-dataset.html"">CICDoS2017</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
dos,NIL,DATASET,NIL,"</p> <p align=""justify""> CIC DoS Dataset (<a href=""https://www.unb.ca/cic/datasets/dos-dataset.html"">CICDoS2017</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
CICDoS2017,NIL,DATASET,NIL,"</p> <p align=""justify""> CIC DoS Dataset (<a href=""https://www.unb.ca/cic/datasets/dos-dataset.html"">CICDoS2017</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
VPN-nonVPN,NIL,DATASET,NIL,"</p>  <p align=""justify""> VPN-nonVPN Dataset (<a href=""https://www.unb.ca/cic/datasets/vpn.html"">ISCXVPN2016</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
vpn,NIL,DATASET,NIL,"</p>  <p align=""justify""> VPN-nonVPN Dataset (<a href=""https://www.unb.ca/cic/datasets/vpn.html"">ISCXVPN2016</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
ISCXVPN2016,NIL,DATASET,NIL,"</p>  <p align=""justify""> VPN-nonVPN Dataset (<a href=""https://www.unb.ca/cic/datasets/vpn.html"">ISCXVPN2016</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
Botnet,botnet,DATASET,botnet,"</p>  <p align=""justify""> Botnet Dataset (<a href=""https://www.unb.ca/cic/datasets/botnet.html"">BotNet2014</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
BotNet2014,botnet,DATASET,botnet,"</p>  <p align=""justify""> Botnet Dataset (<a href=""https://www.unb.ca/cic/datasets/botnet.html"">BotNet2014</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>]."
MLBENCH,NIL,DATASET,NIL,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>]."
XOR,xor-tydi qa,DATASET,xor-tydi qa,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>]."
XOR,xor-tydi qa,DATASET,xor-tydi qa,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>]."
Spiral,NIL,DATASET,NIL,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>]."
Spiral,NIL,DATASET,NIL,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>]."
Circle,circle,DATASET,circle,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>]."
Circle,circle,DATASET,circle,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>]."
Smiley,NIL,DATASET,NIL,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>]."
Smiley,NIL,DATASET,NIL,"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>]."
GTSRB,gtsrb,DATASET,gtsrb,"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>]."
German Traffic Sign Recognition Benchmark,NIL,DATASET,NIL,"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>]."
German_Traffic_Sign_Recognition,NIL,DATASET,NIL,"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>]."
GTSR,gtsrb,DATASET,gtsrb,"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>]."
GTSRB,gtsrb,DATASET,gtsrb,"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>]."
cifar,cifar-10,DATASET,cifar-10,"</li>   <li><a href = ""https://www.cs.toronto.edu/~kriz/cifar.html""><b>CIFAR 10/100 Datasets</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_MATLAB/CIFAR_10_Dataset/README.md"">MATLAB implementation</a>]</li>   <li>More datasets will be tested."
CIFAR 10,cifar-10,DATASET,cifar-10,"</li>   <li><a href = ""https://www.cs.toronto.edu/~kriz/cifar.html""><b>CIFAR 10/100 Datasets</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_MATLAB/CIFAR_10_Dataset/README.md"">MATLAB implementation</a>]</li>   <li>More datasets will be tested."
CIFAR_10,cifar-10,DATASET,cifar-10,"</li>   <li><a href = ""https://www.cs.toronto.edu/~kriz/cifar.html""><b>CIFAR 10/100 Datasets</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_MATLAB/CIFAR_10_Dataset/README.md"">MATLAB implementation</a>]</li>   <li>More datasets will be tested."
NarrativeQA,narrativeqa,DATASET,narrativeqa,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
Qasper,qasper,DATASET,qasper,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
MultiFieldQA,multireqa,DATASET,multireqa,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
HotpotQA,hotpotqa,DATASET,hotpotqa,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
2WikiMQA,2wikimultihopqa,DATASET,2wikimultihopqa,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
Musique,musique-ans,DATASET,musique-ans,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
GovReport,govreport,DATASET,govreport,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
QMSum,qmsum,DATASET,qmsum,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
MultiNews,multi-news,DATASET,multi-news,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
TREC,trecvqa,DATASET,trecvqa,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
TriviaQA,triviaqa,DATASET,triviaqa,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
SAMSum,samsum,DATASET,samsum,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
PassageRetrieval,NIL,DATASET,NIL,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
LCC,lcc,DATASET,lcc,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
RepoBench-P,repobench,DATASET,repobench,"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530)."
LongBench,longbench,DATASET,longbench,"However, the LongBench datasets would require some extra preparations."
20480words_10x10x3_7digits,NIL,DATASET,NIL,"Here is an example for InfLLM on PaulGraham Passkey; the results can be found under `<output_folder_dir>`.  ``` compress_ratio=$1 # 2x, 4x, 6x, 8x output_dir_root=$2  task=""paulgraham_passkey"" dataset=""20480words_10x10x3_7digits"" model=""mistral-7b-instruct-v0.2_infinity_model_len"" method=""infllm""  python pipeline/inf_stream_llm/main.py \ --exp_desc ${task}_${dataset}_${model}_${method}_${compress_ratio} \ # this argument is purely cosmetic."
ImageNet,imagenet,DATASET,imagenet,Manual Download  ImageNet is a TFDS Dataset but it needs to be downloaded manually.
imagenet2012,imagenet,DATASET,imagenet,Please check the [instructions](https://www.tensorflow.org/datasets/catalog/imagenet2012).
Pascal VOC 2007,pascal voc 2007,DATASET,pascal voc 2007,"Example  Let's take an example learner (returns always zeros) that we will ""train"" on the DEBUG stream made of Pascal VOC 2007 and Coil100 datasets."
Coil100,coil100-augmented,DATASET,coil100-augmented,"Example  Let's take an example learner (returns always zeros) that we will ""train"" on the DEBUG stream made of Pascal VOC 2007 and Coil100 datasets."
Pascal VOC 2007,pascal voc 2007,DATASET,pascal voc 2007,Pascal VOC 2007 is a TFDS dataset so it will be automatically downloaded when needed.
Coil100,coil100-augmented,DATASET,coil100-augmented,First we download Coil100 dataset:  ```bash .
Coil100,coil100-augmented,DATASET,coil100-augmented,"/build_dataset.sh -e -b -s debug ```  Note that since the DEBUG stream only downloads Coil100, we could also have used `-d coil100` instead of `-s debug`."
coil100,coil100-augmented,DATASET,coil100-augmented,"/build_dataset.sh -e -b -s debug ```  Note that since the DEBUG stream only downloads Coil100, we could also have used `-d coil100` instead of `-s debug`."
ImageNet,imagenet,DATASET,imagenet,"For example on ImageNet, run the configuration `configs/pretrain_imagenet.py`."
Boston Housing,the boston housing dataset,DATASET,the boston housing dataset,"After installing MySQL, you can run the SQL scripts in the directory /db_setups to create two toy databases, one based on the Boston Housing dataset (https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) and the other based on the Iris dataset (https://archive.ics.uci.edu/ml/datasets/iris), which you can find in the directory /data.  ## Use  In file main.hs you can find two examples on how to call function endToEndTranslate, which takes the following input parameters:  * A list of SQL queries defining the objective function of a supervised ML model."
boston,the boston housing dataset,DATASET,the boston housing dataset,"After installing MySQL, you can run the SQL scripts in the directory /db_setups to create two toy databases, one based on the Boston Housing dataset (https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) and the other based on the Iris dataset (https://archive.ics.uci.edu/ml/datasets/iris), which you can find in the directory /data.  ## Use  In file main.hs you can find two examples on how to call function endToEndTranslate, which takes the following input parameters:  * A list of SQL queries defining the objective function of a supervised ML model."
Iris,iris,DATASET,iris,"After installing MySQL, you can run the SQL scripts in the directory /db_setups to create two toy databases, one based on the Boston Housing dataset (https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) and the other based on the Iris dataset (https://archive.ics.uci.edu/ml/datasets/iris), which you can find in the directory /data.  ## Use  In file main.hs you can find two examples on how to call function endToEndTranslate, which takes the following input parameters:  * A list of SQL queries defining the objective function of a supervised ML model."
iris,iris,DATASET,iris,"After installing MySQL, you can run the SQL scripts in the directory /db_setups to create two toy databases, one based on the Boston Housing dataset (https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) and the other based on the Iris dataset (https://archive.ics.uci.edu/ml/datasets/iris), which you can find in the directory /data.  ## Use  In file main.hs you can find two examples on how to call function endToEndTranslate, which takes the following input parameters:  * A list of SQL queries defining the objective function of a supervised ML model."
CADEC,NIL,DATASET,NIL,"/tmp/ichi_bert_base_new --overwrite_output_dir ```  ### Experiments and Results The BERT and MedBERT models were trained and evaluated on two datasets: CADEC (multi-label) and ICHI (multi-class) (Datasets provided in the data directory).   ### Annotated CADEC, a multi-label medical forum question classificaton dataset We annotate CADEC as a multi-label multi-class dataset, for the task of ""Medical Forum Question Classification""."
ICHI,NIL,DATASET,NIL,"/tmp/ichi_bert_base_new --overwrite_output_dir ```  ### Experiments and Results The BERT and MedBERT models were trained and evaluated on two datasets: CADEC (multi-label) and ICHI (multi-class) (Datasets provided in the data directory).   ### Annotated CADEC, a multi-label medical forum question classificaton dataset We annotate CADEC as a multi-label multi-class dataset, for the task of ""Medical Forum Question Classification""."
Annotated CADEC,NIL,DATASET,NIL,"/tmp/ichi_bert_base_new --overwrite_output_dir ```  ### Experiments and Results The BERT and MedBERT models were trained and evaluated on two datasets: CADEC (multi-label) and ICHI (multi-class) (Datasets provided in the data directory).   ### Annotated CADEC, a multi-label medical forum question classificaton dataset We annotate CADEC as a multi-label multi-class dataset, for the task of ""Medical Forum Question Classification""."
CADEC,NIL,DATASET,NIL,"/tmp/ichi_bert_base_new --overwrite_output_dir ```  ### Experiments and Results The BERT and MedBERT models were trained and evaluated on two datasets: CADEC (multi-label) and ICHI (multi-class) (Datasets provided in the data directory).   ### Annotated CADEC, a multi-label medical forum question classificaton dataset We annotate CADEC as a multi-label multi-class dataset, for the task of ""Medical Forum Question Classification""."
CADEC,NIL,DATASET,NIL,"The annotated files can be found at **CADEC-Annotations/**  ### ICHI, a multi-class medical forum question classification dataset We obtain the ICHI data from rakshajalan/ECIR-2018 [Github codebase](https://github.com/rakshajalan/ECIR-2018/tree/master/ECIR-18_medical_question_classification/Dataset), and maintain the same train-test data split.  ## Citing MedBERT If you use the labeled CADEC dataset or use MedBERT model, please cite the paper:   ``` @inproceedings{10.1145/3459637.3482128, author = {Roy, Soumyadeep and Chakraborty, Sudip and Mandal, Aishik and Balde, Gunjan and Sharma, Prakhar and Natarajan, Anandhavelu and Khosla, Megha and Sural, Shamik and Ganguly, Niloy}, title = {Knowledge-Aware Neural Networks for Medical Forum Question Classification}, year = {2021}, isbn = {9781450384469}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3459637.3482128}, doi = {10.1145/3459637.3482128}, abstract = {Online medical forums have become a predominant platform for answering health-related information needs of consumers."
ICHI,NIL,DATASET,NIL,"The annotated files can be found at **CADEC-Annotations/**  ### ICHI, a multi-class medical forum question classification dataset We obtain the ICHI data from rakshajalan/ECIR-2018 [Github codebase](https://github.com/rakshajalan/ECIR-2018/tree/master/ECIR-18_medical_question_classification/Dataset), and maintain the same train-test data split.  ## Citing MedBERT If you use the labeled CADEC dataset or use MedBERT model, please cite the paper:   ``` @inproceedings{10.1145/3459637.3482128, author = {Roy, Soumyadeep and Chakraborty, Sudip and Mandal, Aishik and Balde, Gunjan and Sharma, Prakhar and Natarajan, Anandhavelu and Khosla, Megha and Sural, Shamik and Ganguly, Niloy}, title = {Knowledge-Aware Neural Networks for Medical Forum Question Classification}, year = {2021}, isbn = {9781450384469}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3459637.3482128}, doi = {10.1145/3459637.3482128}, abstract = {Online medical forums have become a predominant platform for answering health-related information needs of consumers."
ICHI,NIL,DATASET,NIL,"The annotated files can be found at **CADEC-Annotations/**  ### ICHI, a multi-class medical forum question classification dataset We obtain the ICHI data from rakshajalan/ECIR-2018 [Github codebase](https://github.com/rakshajalan/ECIR-2018/tree/master/ECIR-18_medical_question_classification/Dataset), and maintain the same train-test data split.  ## Citing MedBERT If you use the labeled CADEC dataset or use MedBERT model, please cite the paper:   ``` @inproceedings{10.1145/3459637.3482128, author = {Roy, Soumyadeep and Chakraborty, Sudip and Mandal, Aishik and Balde, Gunjan and Sharma, Prakhar and Natarajan, Anandhavelu and Khosla, Megha and Sural, Shamik and Ganguly, Niloy}, title = {Knowledge-Aware Neural Networks for Medical Forum Question Classification}, year = {2021}, isbn = {9781450384469}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3459637.3482128}, doi = {10.1145/3459637.3482128}, abstract = {Online medical forums have become a predominant platform for answering health-related information needs of consumers."
CADEC,NIL,DATASET,NIL,"The annotated files can be found at **CADEC-Annotations/**  ### ICHI, a multi-class medical forum question classification dataset We obtain the ICHI data from rakshajalan/ECIR-2018 [Github codebase](https://github.com/rakshajalan/ECIR-2018/tree/master/ECIR-18_medical_question_classification/Dataset), and maintain the same train-test data split.  ## Citing MedBERT If you use the labeled CADEC dataset or use MedBERT model, please cite the paper:   ``` @inproceedings{10.1145/3459637.3482128, author = {Roy, Soumyadeep and Chakraborty, Sudip and Mandal, Aishik and Balde, Gunjan and Sharma, Prakhar and Natarajan, Anandhavelu and Khosla, Megha and Sural, Shamik and Ganguly, Niloy}, title = {Knowledge-Aware Neural Networks for Medical Forum Question Classification}, year = {2021}, isbn = {9781450384469}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3459637.3482128}, doi = {10.1145/3459637.3482128}, abstract = {Online medical forums have become a predominant platform for answering health-related information needs of consumers."
CUB,cub-200-2011,DATASET,cub-200-2011,We provide the bash script in the folder `bash_scripts` for the experiments on the *CUB* dataset.
AltEntities,NIL,DATASET,NIL,"# AltEntities (Alternative Entities) dataset: Resolving Indirect Referring Expressions for Entity Selection  ## COPYRIGHT NOTICE  This is the work of *Mohammad Javad Hosseini*, *Filip Radlinski*, *Silvia Pareti*, and *Annie Louis* from Google LLC, made available under the CC-BY SA 3.0 License."
Alternative Entities,NIL,DATASET,NIL,"# AltEntities (Alternative Entities) dataset: Resolving Indirect Referring Expressions for Entity Selection  ## COPYRIGHT NOTICE  This is the work of *Mohammad Javad Hosseini*, *Filip Radlinski*, *Silvia Pareti*, and *Annie Louis* from Google LLC, made available under the CC-BY SA 3.0 License."
AltEntities,NIL,DATASET,NIL,A full copy of the license can be found at https://creativecommons.org/licenses/by-sa/3.0/  ## The Dataset  AltEntities is a dataset of English-language **alternative questions with their answers**.
HaloQuest,NIL,DATASET,NIL,"# HaloQuest  Welcome to the repository of our ECCV 2024 paper, [**HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning**](https://arxiv.org/abs/2407.15680)."
HaloQuest,NIL,DATASET,NIL,"# HaloQuest  Welcome to the repository of our ECCV 2024 paper, [**HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning**](https://arxiv.org/abs/2407.15680)."
haloquest,NIL,DATASET,NIL,This repository contains a [colab](https://github.com/google/haloquest/blob/main/HaloQuest_Colab.ipynb) that shows how to load the HaloQuest data and how to use the Auto-Eval system.
HaloQuest,NIL,DATASET,NIL,This repository contains a [colab](https://github.com/google/haloquest/blob/main/HaloQuest_Colab.ipynb) that shows how to load the HaloQuest data and how to use the Auto-Eval system.
HaloQuest,NIL,DATASET,NIL,This repository contains a [colab](https://github.com/google/haloquest/blob/main/HaloQuest_Colab.ipynb) that shows how to load the HaloQuest data and how to use the Auto-Eval system.
HaloQuest,NIL,DATASET,NIL,Code to reproduce the experiments in the paper is available [here](https://github.com/ZhecanJamesWang/HaloQuest).
haloquest,NIL,DATASET,NIL,[Unofficial Project Page](https://haloquest.github.io/)  !
HaloQuest,NIL,DATASET,NIL,/example-image.png)  ## Updates - 2024.07.22: Our paper is on arxiv  ## Dataset Description  ### Summary  **HaloQuest** is a novel visual question answering (VQA) dataset that focuses on multimodal hallucination in vision-language models (VLMs).
HaloQuest,NIL,DATASET,NIL,"It contains **7,748** examples with a combination of real and synthetically generated images, annotated with **questions** and **answers** designed to trigger and evaluate hallucinations.  ### Supported Tasks  **HaloQuest** supports tasks related to **hallucination detection and reduction** in VLMs, providing a challenging benchmark for **Visual Question Answering**."
HaloQuest,NIL,DATASET,NIL,"The dataset is useful for both evaluation and fine-tuning purposes, aiming to advance multimodal reasoning.  ## Dataset Details  ### Data Collection HaloQuest includes a mix of real images from the Open Images dataset and synthetic images generated using Midjourney."
Open Images,open images v7,DATASET,open images v7,"The dataset is useful for both evaluation and fine-tuning purposes, aiming to advance multimodal reasoning.  ## Dataset Details  ### Data Collection HaloQuest includes a mix of real images from the Open Images dataset and synthetic images generated using Midjourney."
HaloQuest,NIL,DATASET,NIL,"ZW did some work while at Google DeepMind.)  ## Citing this work  ```latex @inproceedings{wang2024haloquest,   title={HaloQuest: A Visual Hallucination Dataset for Advancing Multimodal Reasoning},   author={Zhecan Wang and Garrett Bingham and Adams Wei Yu and Quoc V."
Open Images Dataset v7,open images v7,DATASET,open images v7,You may obtain a copy of the CC-BY license at: https://creativecommons.org/licenses/by/4.0/legalcode  Image URLs are from the [Open Images Dataset v7](https://storage.googleapis.com/openimages/web/factsfigures_v7.html#publications) and [Midjourney Showcase](https://www.midjourney.com/showcase).
openimages,open images v7,DATASET,open images v7,You may obtain a copy of the CC-BY license at: https://creativecommons.org/licenses/by/4.0/legalcode  Image URLs are from the [Open Images Dataset v7](https://storage.googleapis.com/openimages/web/factsfigures_v7.html#publications) and [Midjourney Showcase](https://www.midjourney.com/showcase).
Sachs,sachs,DATASET,sachs,">You can find the Sachs dataset in the repo, at this [link](https://github.com/TheoBourdais/ComputationalHypergraphDiscovery/blob/main/examples/SachsData.csv)."
Sachs,sachs,DATASET,sachs,"For example, with the Sachs dataset, we can define the following clusters: ```python clusters=[     ['$PKC$','$P38$','$Jnk$'],     ['$Erk$','$Akt$','$PKA$'],     ['$Raf$','$Mek$'],     ['$Plcg$','$PIP2$','$PIP3$'] ] graph_discovery = CHD.GraphDiscovery.from_dataframe(df,clusters=clusters) ```  #### After creating a `GraphDiscovery` object, for the second run  If you already have a `GraphDiscovery` object named `graph_discovery` on which you have run the algorithm, you can choose the clusters based on the results and apply them to get a new `GraphDiscovery` object: ```python graph_discovery2=graph_discovery.prepare_new_graph_with_clusters(clusters) ```  ### Using clusters Once the clusters have been defined, you can use the `GraphDiscovery` object as usual."
MOT17,mot17,DATASET,mot17,"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory."
crowdhuman,crowdhuman,DATASET,crowdhuman,"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory."
MOTSynth,motsynth,DATASET,motsynth,"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory."
crowdhuman,crowdhuman,DATASET,crowdhuman,"The data directory is structured as follows:  ``` data |-- crowdhuman │   ├── annotation_train.odgt │   ├── annotation_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman_train01.zip │   │   ├── CrowdHuman_train02.zip │   │   ├── CrowdHuman_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman_val.zip |-- MOT17 │   ├── train │   ├── test |-- MOTSynth |   ├── videos │   ├── annotations ```  Then, we need to convert the all dataset to COCO format."
MOT17,mot17,DATASET,mot17,"The data directory is structured as follows:  ``` data |-- crowdhuman │   ├── annotation_train.odgt │   ├── annotation_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman_train01.zip │   │   ├── CrowdHuman_train02.zip │   │   ├── CrowdHuman_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman_val.zip |-- MOT17 │   ├── train │   ├── test |-- MOTSynth |   ├── videos │   ├── annotations ```  Then, we need to convert the all dataset to COCO format."
MOTSynth,motsynth,DATASET,motsynth,"The data directory is structured as follows:  ``` data |-- crowdhuman │   ├── annotation_train.odgt │   ├── annotation_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman_train01.zip │   │   ├── CrowdHuman_train02.zip │   │   ├── CrowdHuman_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman_val.zip |-- MOT17 │   ├── train │   ├── test |-- MOTSynth |   ├── videos │   ├── annotations ```  Then, we need to convert the all dataset to COCO format."
crowdhuman,crowdhuman,DATASET,crowdhuman,We provide scripts to do this:  ```shell # crowdhuman python .
MOT17,mot17,DATASET,mot17,/data/crowdhuman/annotations  # MOT17 python .
MOT17,mot17,DATASET,mot17,/data/MOT17/ -o .
MOT17,mot17,DATASET,mot17,/data/MOT17/annotations --split-train --convert-det  # MOTSynth python .
MOTSynth,motsynth,DATASET,motsynth,/data/MOT17/annotations --split-train --convert-det  # MOTSynth python .
MOTSynth,motsynth,DATASET,motsynth,/data/MOTSynth/video --out_dir_path .
MOTSynth,motsynth,DATASET,motsynth,/data/MOTSynth/train/ python .
crowdhuman,crowdhuman,DATASET,crowdhuman,/data/MOTSynth/all_cocoformat.json ```  The processed dataset will be structured as follows:  ``` data |-- crowdhuman │   ├── annotation_train.odgt │   ├── annotation_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman_train01.zip │   │   ├── CrowdHuman_train02.zip │   │   ├── CrowdHuman_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman_val.zip |   ├── annotations │   │   ├── crowdhuman_train.json │   │   ├── crowdhuman_val.json |-- MOT17 │   ├── train │   │   ├── MOT17-02-DPM │   │   ├── ... │   ├── test │   ├── annotations │   │   ├── half-train_cocoformat.json │   │   ├── ... |-- MOTSynth |   ├── videos │   ├── annotations │   ├── train │   │   ├── 000 │   │   │   ├── img1 │   │   │   │   ├── 000001.jpg │   │   │   │   ├── ... │   │   ├── ... │   ├── all_cocoformat.json ``` ### Pretrained models  We use CLIP pretrained ViT models.
MOT17,mot17,DATASET,mot17,/data/MOTSynth/all_cocoformat.json ```  The processed dataset will be structured as follows:  ``` data |-- crowdhuman │   ├── annotation_train.odgt │   ├── annotation_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman_train01.zip │   │   ├── CrowdHuman_train02.zip │   │   ├── CrowdHuman_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman_val.zip |   ├── annotations │   │   ├── crowdhuman_train.json │   │   ├── crowdhuman_val.json |-- MOT17 │   ├── train │   │   ├── MOT17-02-DPM │   │   ├── ... │   ├── test │   ├── annotations │   │   ├── half-train_cocoformat.json │   │   ├── ... |-- MOTSynth |   ├── videos │   ├── annotations │   ├── train │   │   ├── 000 │   │   │   ├── img1 │   │   │   │   ├── 000001.jpg │   │   │   │   ├── ... │   │   ├── ... │   ├── all_cocoformat.json ``` ### Pretrained models  We use CLIP pretrained ViT models.
MOT17,mot17,DATASET,mot17,/data/MOTSynth/all_cocoformat.json ```  The processed dataset will be structured as follows:  ``` data |-- crowdhuman │   ├── annotation_train.odgt │   ├── annotation_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman_train01.zip │   │   ├── CrowdHuman_train02.zip │   │   ├── CrowdHuman_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman_val.zip |   ├── annotations │   │   ├── crowdhuman_train.json │   │   ├── crowdhuman_val.json |-- MOT17 │   ├── train │   │   ├── MOT17-02-DPM │   │   ├── ... │   ├── test │   ├── annotations │   │   ├── half-train_cocoformat.json │   │   ├── ... |-- MOTSynth |   ├── videos │   ├── annotations │   ├── train │   │   ├── 000 │   │   │   ├── img1 │   │   │   │   ├── 000001.jpg │   │   │   │   ├── ... │   │   ├── ... │   ├── all_cocoformat.json ``` ### Pretrained models  We use CLIP pretrained ViT models.
MOTSynth,motsynth,DATASET,motsynth,/data/MOTSynth/all_cocoformat.json ```  The processed dataset will be structured as follows:  ``` data |-- crowdhuman │   ├── annotation_train.odgt │   ├── annotation_val.odgt │   ├── train │   │   ├── Images │   │   ├── CrowdHuman_train01.zip │   │   ├── CrowdHuman_train02.zip │   │   ├── CrowdHuman_train03.zip │   ├── val │   │   ├── Images │   │   ├── CrowdHuman_val.zip |   ├── annotations │   │   ├── crowdhuman_train.json │   │   ├── crowdhuman_val.json |-- MOT17 │   ├── train │   │   ├── MOT17-02-DPM │   │   ├── ... │   ├── test │   ├── annotations │   │   ├── half-train_cocoformat.json │   │   ├── ... |-- MOTSynth |   ├── videos │   ├── annotations │   ├── train │   │   ├── 000 │   │   │   ├── img1 │   │   │   │   ├── 000001.jpg │   │   │   │   ├── ... │   │   ├── ... │   ├── all_cocoformat.json ``` ### Pretrained models  We use CLIP pretrained ViT models.
MOT17,mot17,DATASET,mot17,/pretrain/ViT-B-16.pt ``` Evaluation on MOT17 half validation set ```shell bash .
MOT17,mot17,DATASET,mot17,"/tools/dist_test.sh configs/mot/svm/svm_test.py 8 \    --eval bbox track --checkpoint svm_motsync_ch_mot17half.pth ```  ## Main Results ### MOT17 | Method | Dataset |                Train Data                | MOTA | HOTA | IDF1 |    URL    | | :---: | :---: |:----------------------------------------:|:----:|:----:|:----:|:---------:| | SVM | MOT17 | MOT17 half-train + crowdhuman + MOTSynth | 79.7 | 68.1 | 80.9 | [model](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm_mot17-half-val.pth) |  ## Citation If you find this work useful in your research, please consider citing: ``` @InProceedings{Zhao_2023_CVPR,     author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},     title     = {Streaming Video Model},     booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},     month     = {June},     year      = {2023},     pages     = {14602-14612} } ``` ## Acknowledgement Our code are built on top of [MMTracking](https://github.com/open-mmlab/mmtracking/) and [CLIP](https://github.com/openai/CLIP)."
MOT17,mot17,DATASET,mot17,"/tools/dist_test.sh configs/mot/svm/svm_test.py 8 \    --eval bbox track --checkpoint svm_motsync_ch_mot17half.pth ```  ## Main Results ### MOT17 | Method | Dataset |                Train Data                | MOTA | HOTA | IDF1 |    URL    | | :---: | :---: |:----------------------------------------:|:----:|:----:|:----:|:---------:| | SVM | MOT17 | MOT17 half-train + crowdhuman + MOTSynth | 79.7 | 68.1 | 80.9 | [model](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm_mot17-half-val.pth) |  ## Citation If you find this work useful in your research, please consider citing: ``` @InProceedings{Zhao_2023_CVPR,     author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},     title     = {Streaming Video Model},     booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},     month     = {June},     year      = {2023},     pages     = {14602-14612} } ``` ## Acknowledgement Our code are built on top of [MMTracking](https://github.com/open-mmlab/mmtracking/) and [CLIP](https://github.com/openai/CLIP)."
MOT17,mot17,DATASET,mot17,"/tools/dist_test.sh configs/mot/svm/svm_test.py 8 \    --eval bbox track --checkpoint svm_motsync_ch_mot17half.pth ```  ## Main Results ### MOT17 | Method | Dataset |                Train Data                | MOTA | HOTA | IDF1 |    URL    | | :---: | :---: |:----------------------------------------:|:----:|:----:|:----:|:---------:| | SVM | MOT17 | MOT17 half-train + crowdhuman + MOTSynth | 79.7 | 68.1 | 80.9 | [model](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm_mot17-half-val.pth) |  ## Citation If you find this work useful in your research, please consider citing: ``` @InProceedings{Zhao_2023_CVPR,     author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},     title     = {Streaming Video Model},     booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},     month     = {June},     year      = {2023},     pages     = {14602-14612} } ``` ## Acknowledgement Our code are built on top of [MMTracking](https://github.com/open-mmlab/mmtracking/) and [CLIP](https://github.com/openai/CLIP)."
crowdhuman,crowdhuman,DATASET,crowdhuman,"/tools/dist_test.sh configs/mot/svm/svm_test.py 8 \    --eval bbox track --checkpoint svm_motsync_ch_mot17half.pth ```  ## Main Results ### MOT17 | Method | Dataset |                Train Data                | MOTA | HOTA | IDF1 |    URL    | | :---: | :---: |:----------------------------------------:|:----:|:----:|:----:|:---------:| | SVM | MOT17 | MOT17 half-train + crowdhuman + MOTSynth | 79.7 | 68.1 | 80.9 | [model](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm_mot17-half-val.pth) |  ## Citation If you find this work useful in your research, please consider citing: ``` @InProceedings{Zhao_2023_CVPR,     author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},     title     = {Streaming Video Model},     booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},     month     = {June},     year      = {2023},     pages     = {14602-14612} } ``` ## Acknowledgement Our code are built on top of [MMTracking](https://github.com/open-mmlab/mmtracking/) and [CLIP](https://github.com/openai/CLIP)."
MOTSynth,motsynth,DATASET,motsynth,"/tools/dist_test.sh configs/mot/svm/svm_test.py 8 \    --eval bbox track --checkpoint svm_motsync_ch_mot17half.pth ```  ## Main Results ### MOT17 | Method | Dataset |                Train Data                | MOTA | HOTA | IDF1 |    URL    | | :---: | :---: |:----------------------------------------:|:----:|:----:|:----:|:---------:| | SVM | MOT17 | MOT17 half-train + crowdhuman + MOTSynth | 79.7 | 68.1 | 80.9 | [model](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm_mot17-half-val.pth) |  ## Citation If you find this work useful in your research, please consider citing: ``` @InProceedings{Zhao_2023_CVPR,     author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},     title     = {Streaming Video Model},     booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},     month     = {June},     year      = {2023},     pages     = {14602-14612} } ``` ## Acknowledgement Our code are built on top of [MMTracking](https://github.com/open-mmlab/mmtracking/) and [CLIP](https://github.com/openai/CLIP)."
Fakepedia,NIL,DATASET,NIL,"Locating and Detecting Language Model Grounding with Fakepedia  This repository contains the data and code to reproduce the results of our paper: https://arxiv.org/abs/2312.02073  Please use the following citation:  ``` @misc{monea2023glitch,       title={A Glitch in the Matrix?"
Fakepedia,NIL,DATASET,NIL,"Locating and Detecting Language Model Grounding with Fakepedia},        author={Giovanni Monea and Maxime Peyrard and Martin Josifoski and Vishrav Chaudhary and Jason Eisner and Emre Kıcıman and Hamid Palangi and Barun Patra and Robert West},       year={2023},       eprint={2312.02073},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```  > **Abstract:** Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context."
Fakepedia,NIL,DATASET,NIL,"We present a novel method to study grounding abilities using Fakepedia, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge."
Fakepedia,NIL,DATASET,NIL,"We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries."
Fakepedia,NIL,DATASET,NIL,"We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries."
MRQA,mrqa,DATASET,mrqa,"# Thermometer: Towards Universal Calibration for Large Language Models #### This repository contains official implementation of the paper [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819).  ## Requirements ```bash pip install torch==2.2.1 transformers==4.28.1 evaluate==0.4.1 tqdm pandas ``` ## File Structure #### *src* folder includes all the source code for the experiments. - ### *configs* folder:      includes all the information of training configurations and model hyper-parameter. - ### *data* folder:     includes the dataloader to load and process the datasets. - ### other code files:   - *process_mrqa.py* pre-process the raw data in free-form QA datasets MRQA;    - *extract_features.py* aims to extract labels, features, and logits from pretrained LLMs;   - *train_thermometer.py* and *eval_thermometer.py* contain the main function to train Thermometer,   and the functions to evaluate calibration performance of trained Thermometer, respectively.   ## Usage #### We provide the scripts to help reproduce the results of our paper. - ### Step-1: extract the features and logits from pre-trained LLMs,     ```     exract.sh     ``` - ### Step-2: train Thermometer model,     ```     train.sh     ``` - ### Step-3: evaluate calibration of trained Thermometer model,     ```     eval.sh     ``` - ### Free-form QA task requires an additional step to pre-process the raw data, i.e., append the LLM's response to the prompts,     ```     mrqa.sh     ``` - ### Choose different types of LLMs,     ```     --model_type decoder_only --model_name Llama-2-7b-chat-hf     --model_type encoder_decoder --model_name flan-t5-xl     ```  ## Citation #### If you find this repository helpful for your research, please consider citing our paper,  ``` @InProceedings{pmlr-v235-shen24c,   title = {Thermometer: Towards Universal Calibration for Large Language Models},   author =  {Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W. and Ghosh, Soumya},   booktitle = {Proceedings of the 41st International Conference on Machine Learning},   pages = {44687--44711},   year =  {2024},   editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},   volume =  {235},   series =  {Proceedings of Machine Learning Research},   month = {21--27 Jul},   publisher = {PMLR} } ```"
mrqa,mrqa,DATASET,mrqa,"# Thermometer: Towards Universal Calibration for Large Language Models #### This repository contains official implementation of the paper [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819).  ## Requirements ```bash pip install torch==2.2.1 transformers==4.28.1 evaluate==0.4.1 tqdm pandas ``` ## File Structure #### *src* folder includes all the source code for the experiments. - ### *configs* folder:      includes all the information of training configurations and model hyper-parameter. - ### *data* folder:     includes the dataloader to load and process the datasets. - ### other code files:   - *process_mrqa.py* pre-process the raw data in free-form QA datasets MRQA;    - *extract_features.py* aims to extract labels, features, and logits from pretrained LLMs;   - *train_thermometer.py* and *eval_thermometer.py* contain the main function to train Thermometer,   and the functions to evaluate calibration performance of trained Thermometer, respectively.   ## Usage #### We provide the scripts to help reproduce the results of our paper. - ### Step-1: extract the features and logits from pre-trained LLMs,     ```     exract.sh     ``` - ### Step-2: train Thermometer model,     ```     train.sh     ``` - ### Step-3: evaluate calibration of trained Thermometer model,     ```     eval.sh     ``` - ### Free-form QA task requires an additional step to pre-process the raw data, i.e., append the LLM's response to the prompts,     ```     mrqa.sh     ``` - ### Choose different types of LLMs,     ```     --model_type decoder_only --model_name Llama-2-7b-chat-hf     --model_type encoder_decoder --model_name flan-t5-xl     ```  ## Citation #### If you find this repository helpful for your research, please consider citing our paper,  ``` @InProceedings{pmlr-v235-shen24c,   title = {Thermometer: Towards Universal Calibration for Large Language Models},   author =  {Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W. and Ghosh, Soumya},   booktitle = {Proceedings of the 41st International Conference on Machine Learning},   pages = {44687--44711},   year =  {2024},   editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},   volume =  {235},   series =  {Proceedings of Machine Learning Research},   month = {21--27 Jul},   publisher = {PMLR} } ```"
PAVS10K,NIL,DATASET,NIL,"To support the task, we collect PAVS10K, the first panoramic video dataset for audiovisual salient object detection, which consists of 67 4K-resolution equirectangular videos with per-video labels including hierarchical scene categories and associated attributes depicting specific challenges for conducting PAV-SOD, and 10,465 uniformly sampled video frames with manually annotated object-level and instance-level pixel-wise masks."
PAVS10K,NIL,DATASET,NIL,We further systematically benchmark 13 state-of-the-art salient object detection (SOD)/video object segmentation (VOS) methods based on our PAVS10K.
PAVS10K,NIL,DATASET,NIL,"As a result, our CAV-Net outperforms all competing models and is able to estimate the aleatoric uncertainties within PAVS10K."
PAVS10K,NIL,DATASET,NIL,"We hope that our work could serve as a starting point for advancing SOD towards immersive media.  ------  # PAVS10K  <p align=""center"">     <img src=""."
PAVS10K,NIL,DATASET,NIL,"/figures/fig_teaser.jpg""/> <br />     <em>      Figure 1: An example of our PAVS10K where coarse-to-fine annotations are provided, based on a guidance of fixations acquired from subjective experiments conducted by multiple (N) subjects wearing Head-Mounted Displays (HMDs) and headphones."
PAVS10K,NIL,DATASET,NIL,"/figures/fig_related_datasets.jpg""/> <br />     <em>      Figure 2: Summary of widely used salient object detection (SOD)/video object segmentation (VOS) datasets and PAVS10K."
PAVS10K,NIL,DATASET,NIL,"/figures/fig_dataset_examples.jpg""/> <br />     <em>      Figure 3: Examples of challenging attributes on equirectangular images from our PAVS10K, with instance-level ground truth and fixations as annotation guidance."
PAVS10K,NIL,DATASET,NIL,"/figures/fig_dataset_statistics.jpg""/> <br />     <em>      Figure 4: Statistics of the proposed PAVS10K."
PAVS10K,NIL,DATASET,NIL,"(c) Sound sources of PAVS10K scenes, such as musical instruments, human instances and animals."
PAVS10K,NIL,DATASET,NIL,> Note: The PAVS10K dataset does not own the copyright of videos.
PAVS10K,NIL,DATASET,NIL,"Only researchers and educators who wish to use the videos for non-commercial researches and/or educational purposes, have access to PAVS10K.  ------  # Citation          @article{zhang2023pav,       title={PAV-SOD: A New Task towards Panoramic Audiovisual Saliency Detection},       author={Zhang, Yi and Chao, Fang-Yi and Hamidouche, Wassim and Deforges, Olivier},       journal={ACM Transactions on Multimedia Computing, Communications and Applications},       volume={19},       number={3},       pages={1--26},       year={2023},       publisher={ACM New York, NY}     }  ------  # Contact  yi23zhang.2022@gmail.com or  fangyichao428@gmail.com (for details of head movement and eye fixation data)."
HAGRID,NIL,DATASET,NIL,"# HAGRID: A Human-LLM Collaborative Dataset for Generative Information-seeking with Attribution  <p align=""center""><img src=""https://github.com/project-miracl/hagrid/blob/main/assets/icon.png?"
HAGRID,NIL,DATASET,NIL,"raw=true"" alt=""HAGRID"" width=""20%""><br> </p>  <p align=""center"">     <a href=""https://www.python.org/"">         <img alt=""Build"" src=""https://img.shields.io/badge/Made%20with-Python-1f425f.svg?"
HAGRID,NIL,DATASET,NIL,"color=purple"">     </a>     <a href=""https://github.com/project-miracl/hagrid/blob/master/LICENSE"">         <img alt=""License"" src=""https://img.shields.io/github/license/project-miracl/hagrid"">     </a>     <a href=""https://arxiv.org/abs/2307.16883"">         <img alt=""arXiv"" src=""https://img.shields.io/badge/arXiv-2307.16883-b31b1b.svg"">     </a> </p>  HAGRID (**H**uman-in-the-loop **A**ttributable **G**enerative **R**etrieval for **I**nformation-seeking **D**ataset) is a dataset for generative information-seeking scenarios."
MIRACL,NIL,DATASET,NIL,"It is constructed on top of [MIRACL 🌍 🙌 🌏 ](HTTP://miracl.ai), an information retrieval dataset that consists of queries along with a set of manually labelled relevant passages (quotes)."
miracl,NIL,DATASET,NIL,"It is constructed on top of [MIRACL 🌍 🙌 🌏 ](HTTP://miracl.ai), an information retrieval dataset that consists of queries along with a set of manually labelled relevant passages (quotes)."
HAGRID,NIL,DATASET,NIL,(#baselines-coming-soon)   - [Contact](#contact)   - [License](#license)   - [Citation](#citation)  ## Data  HAGRID is hosted on Hugging Face 🤗 : [link](https://huggingface.co/datasets/miracl/hagrid).
miracl/hagrid,NIL,DATASET,NIL,(#baselines-coming-soon)   - [Contact](#contact)   - [License](#license)   - [Citation](#citation)  ## Data  HAGRID is hosted on Hugging Face 🤗 : [link](https://huggingface.co/datasets/miracl/hagrid).
miracl/hagrid,NIL,DATASET,NIL,"```python import datasets hagrid = datasets.load_dataset(""miracl/hagrid"", split=""train"") print(hagrid[0]) ```  |  Split | #Q | #A       | #Informativeness  | #Attribuatability       | |:-----|:------:|:-------:|:------:|:-------:| | Train     | 1,922 | 3,214 | 3,214 | 754 | | Dev     | 716 | 1,318 | 1,157 | 826 |   ## Baselines (Coming soon!)"
HAGRID,NIL,DATASET,NIL,"See [LICENSE](LICENSE) for details.  ## Citation If you find this dataset and repository helpful, please cite HAGRID as follows: ``` @article{hagrid,       title={{HAGRID}: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution},        author={Ehsan Kamalloo and Aref Jafari and Xinyu Zhang and Nandan Thakur and Jimmy Lin},       year={2023},       journal={arXiv:2307.16883}, } ```"
HAGRID,NIL,DATASET,NIL,"See [LICENSE](LICENSE) for details.  ## Citation If you find this dataset and repository helpful, please cite HAGRID as follows: ``` @article{hagrid,       title={{HAGRID}: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution},        author={Ehsan Kamalloo and Aref Jafari and Xinyu Zhang and Nandan Thakur and Jimmy Lin},       year={2023},       journal={arXiv:2307.16883}, } ```"
SRD,srd,DATASET,srd,"SRD [Train](https://drive.google.com/file/d/1W8vBRJYDG9imMgr9I2XaA13tlFIEHOjS/view)|[BaiduPan](https://pan.baidu.com/s/1mj3BoRQ), [Test](https://drive.google.com/file/d/1GTi4BmQ0SJ7diDMmf-b7x2VismmXtfTo/view)."
AISTD|ISTD+,istd+,DATASET,istd+,AISTD|ISTD+ [[link]](https://github.com/cvlab-stonybrook/SID)  3.
ISTD,istd,DATASET,istd,ISTD [[link]](https://drive.google.com/file/d/1I0qw-65KBA6np8vIZzO6oeiOvcDBttAY/view)  4.
USR: Unpaired Shadow Removal Dataset,NIL,DATASET,NIL,USR: Unpaired Shadow Removal Dataset [[link]](https://drive.google.com/file/d/1PPAX0W4eyfn1cUrb2aBefnbrmhB1htoJ/view)  5.
LRSS: Soft Shadow Dataset,NIL,DATASET,NIL,LRSS: Soft Shadow Dataset [[link]](http://visual.cs.ucl.ac.uk/pubs/softshadows/)<br>    The LRSS dataset contains 134 shadow images (62 pairs of shadow and shadow-free images).
softshadows,NIL,DATASET,NIL,LRSS: Soft Shadow Dataset [[link]](http://visual.cs.ucl.ac.uk/pubs/softshadows/)<br>    The LRSS dataset contains 134 shadow images (62 pairs of shadow and shadow-free images).
LRSS,lrs2,DATASET,lrs2,LRSS: Soft Shadow Dataset [[link]](http://visual.cs.ucl.ac.uk/pubs/softshadows/)<br>    The LRSS dataset contains 134 shadow images (62 pairs of shadow and shadow-free images).
LRSS,lrs2,DATASET,lrs2,"<br>    For shadow-free training images, 28 from LRSS and 72 randomly selected from the USR dataset."
USR,NIL,DATASET,NIL,"<br>    For shadow-free training images, 28 from LRSS and 72 randomly selected from the USR dataset."
ISTD/ISTD+,istd+,DATASET,istd+,pwd=28bv) | | AISTD/ISTD+ | [[Dropbox]](https://www.dropbox.com/scl/fi/k3suqb1ikis4mm6ok6ky4/AISTD_params_0500000.pt?
ISTD,istd,DATASET,istd,pwd=3waf) | | ISTD | [[Dropbox]](https://www.dropbox.com/scl/fi/jgdcftwxpvnwxegawbrqx/ISTD_params_0600000.pt?
USR,NIL,DATASET,NIL,pwd=hh4n) | | USR  | [[Dropbox]](https://www.dropbox.com/scl/fi/kcj4m88ha3razxqx1c7m6/USR_params_0600000.pt?
USR,NIL,DATASET,NIL,pwd=e0a8)  |`results/USR/model/`| [[Dropbox]](https://www.dropbox.com/s/ybmwxtmo7cdljyz/DC-ShadowNet_USR.zip?
USR,NIL,DATASET,NIL,pwd=e0a8)  |`results/USR/model/`| [[Dropbox]](https://www.dropbox.com/s/ybmwxtmo7cdljyz/DC-ShadowNet_USR.zip?
LRSS,lrs2,DATASET,lrs2,pwd=u7ec) | | LRSS  | - | - | -| [[Dropbox]](https://www.dropbox.com/s/wi6g12gr1z0xsqi/DC-ShadowNet_Soft.zip?
SRD,srd,DATASET,srd,Download the pre-trained SRD model [[Dropbox]](https://www.dropbox.com/scl/fi/icj273vu98w1l9zzwjxt7/SRD_params_0500000.pt?
SRD,srd,DATASET,srd,"pwd=zhd2), put in `results/SRD/model/` 2."
SRD,srd,DATASET,srd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
/dataset/SRD/testA/,NIL,DATASET,NIL,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
SRD,srd,DATASET,srd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
SRD,srd,DATASET,srd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
AISTD,aist++,DATASET,aist++,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
USR,NIL,DATASET,NIL,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
SRD,srd,DATASET,srd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
AISTD,aist++,DATASET,aist++,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
AISTD,aist++,DATASET,aist++,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
ISTD,istd,DATASET,istd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
ISTD,istd,DATASET,istd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
USR,NIL,DATASET,NIL,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
USR,NIL,DATASET,NIL,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
SRD,srd,DATASET,srd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
SRD,srd,DATASET,srd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
AISTD,aist++,DATASET,aist++,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
AISTD,aist++,DATASET,aist++,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
ISTD,istd,DATASET,istd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
ISTD,istd,DATASET,istd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
USR,NIL,DATASET,NIL,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
USR,NIL,DATASET,NIL,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
DC-ShadowNet-Hard-and-Soft-Shadow-Remova,NIL,DATASET,NIL,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
SRD,srd,DATASET,srd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
SRD,srd,DATASET,srd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
SRD,srd,DATASET,srd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
SRD,srd,DATASET,srd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
SRD,srd,DATASET,srd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
SRD,srd,DATASET,srd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
SRD,srd,DATASET,srd,"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?"
SRD,srd,DATASET,srd,pwd=srdc)| SRD testC [[Dropbox]](https://www.dropbox.com/scl/fo/2ordu4qggheead9osdfo5/AA-Tqfo2hmTUJhVC-qKS9B0?
ISTD,istd,DATASET,istd,pwd=srdc) | ISTD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/ym4tt6999nqik4aovnmjd/AIPo7LNX9aUmR2X0NQo8_sI?
istd,istd,DATASET,istd,rlkey=fyby4kexn1g009xhb5bkph32e&dl=0) [[BaiduPan(code:istd)]](https://pan.baidu.com/s/1aSPXH6DVpGGc_RfDP7mADQ?
ISTD,istd,DATASET,istd,pwd=istd)| ISTD testC [[Dropbox]](https://www.dropbox.com/scl/fo/cqlu31fsixc48kelb96vs/ANQ6GrWGvGBclT3ThY6ltGA?
istd,istd,DATASET,istd,rlkey=30yg1v0orktptc92tvsettdrv&st=5xpq3fji&dl=0)[[BaiduPan(code:istd)]](https://pan.baidu.com/s/1TmrPgDHwEaP95cyEppfCjQ?
USR,NIL,DATASET,NIL,pwd=istd) | USR trainC [[Dropbox]](https://www.dropbox.com/scl/fo/61437etlg9m4i8b0rhwgx/AMEeRUwkzCMNUAWWEMVxr90?
USR,NIL,DATASET,NIL,pwd=usrc)| USR testC [[Dropbox]](https://www.dropbox.com/scl/fo/8mk01goqz1mllmzwnbgk8/AEi2_Igy7ZM93-IgnU_mmXU?
LRSS,lrs2,DATASET,lrs2,pwd=usrc)  | LRSS trainC [[Dropbox]](https://www.dropbox.com/scl/fo/s3qcyjp754qsnu90853wq/AHZan-_POf1KAQ6UpbyT6b8?
lrss,lrs2,DATASET,lrs2,rlkey=0ghgo6m4fegm6gml0p611f28e&dl=0) [[BaiduPan(code:lrss)]](https://pan.baidu.com/s/1hjIheKDy3vGChoF0WlW-LQ?
LRSS,lrs2,DATASET,lrs2,pwd=lrss)| LRSS testC [[Dropbox]](https://www.dropbox.com/scl/fo/laaymorcfz70ja93y6hwm/ALeXqQp4_f7BKH5PESinWYc?
lrss,lrs2,DATASET,lrs2,rlkey=kpqb5dxeneiroskj9wpfh6rob&st=v1mvl266&dl=0) [[BaiduPan(code:lrss)]](https://pan.baidu.com/s/1SRikZuabEgu43vqqlCIh9A?
SRD,srd,DATASET,srd,"/_Shadow-Free_Chromaticity_python): [inputs](./0_Shadow-Free_Chromaticity_python/input/) and [results](./0_Shadow-Free_Chromaticity_python/sfchroma/) ``` cd 0_Shadow-Free_Chromaticity_python python physics_all.py ```  <p align=""left"">   <img width=450"" src=""teaser/chromaticity.png""> </p>   ## Train with Shadow-Robust Feature Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_pecp_loss True ```  Get the following Figure 5 in the main paper, VGG feature visualization [results](."
SRD,srd,DATASET,srd,"/_Shadow-Free_Chromaticity_python): [inputs](./0_Shadow-Free_Chromaticity_python/input/) and [results](./0_Shadow-Free_Chromaticity_python/sfchroma/) ``` cd 0_Shadow-Free_Chromaticity_python python physics_all.py ```  <p align=""left"">   <img width=450"" src=""teaser/chromaticity.png""> </p>   ## Train with Shadow-Robust Feature Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_pecp_loss True ```  Get the following Figure 5 in the main paper, VGG feature visualization [results](."
SRD,srd,DATASET,srd,SRD Dataset Evaluation set the paths of the [shadow removal result](https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/blob/de9223d55519777e1621b84d5f0067e3989c11dd/evaluation/demo_srd_release.m#L22) and the dataset in `evaluation/demo_srd_release.m` and then run it. ``` demo_srd_release.m ``` Get the following Table 1 in the main paper on the SRD (size: 256x256):  | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|----------|------|------| | **DC-ShadowNet** | Unpaired | **4.66** | 7.70 | 3.39 | | Input Image | N/A | 13.77 | 37.40 | 3.96 |  For SRD (size: 640x840): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|----------|------|------| | **DC-ShadowNet** | Unpaired | **6.57** | **9.84** | **5.52** |  ### 2.
SRD,srd,DATASET,srd,SRD Dataset Evaluation set the paths of the [shadow removal result](https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/blob/de9223d55519777e1621b84d5f0067e3989c11dd/evaluation/demo_srd_release.m#L22) and the dataset in `evaluation/demo_srd_release.m` and then run it. ``` demo_srd_release.m ``` Get the following Table 1 in the main paper on the SRD (size: 256x256):  | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|----------|------|------| | **DC-ShadowNet** | Unpaired | **4.66** | 7.70 | 3.39 | | Input Image | N/A | 13.77 | 37.40 | 3.96 |  For SRD (size: 640x840): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|----------|------|------| | **DC-ShadowNet** | Unpaired | **6.57** | **9.84** | **5.52** |  ### 2.
SRD,srd,DATASET,srd,SRD Dataset Evaluation set the paths of the [shadow removal result](https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/blob/de9223d55519777e1621b84d5f0067e3989c11dd/evaluation/demo_srd_release.m#L22) and the dataset in `evaluation/demo_srd_release.m` and then run it. ``` demo_srd_release.m ``` Get the following Table 1 in the main paper on the SRD (size: 256x256):  | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|----------|------|------| | **DC-ShadowNet** | Unpaired | **4.66** | 7.70 | 3.39 | | Input Image | N/A | 13.77 | 37.40 | 3.96 |  For SRD (size: 640x840): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|----------|------|------| | **DC-ShadowNet** | Unpaired | **6.57** | **9.84** | **5.52** |  ### 2.
AISTD,aist++,DATASET,aist++,AISTD Dataset Evaluation set the paths of the [shadow removal result](https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/blob/eefcc9ee23842847a40f40610c129ecba82f5d21/evaluation/demo_aistd_release.m#L21) and the dataset in `evaluation/demo_aistd_release.m` and then run it. ``` demo_aistd_release.m ```  Get the following Table 2 in the main paper on the AISTD (size: 256x256): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|---------|----------|-----| | **DC-ShadowNet** | Unpaired | **4.7** | **10.6** | 3.7 |  For AISTD (size: 480x640): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|---------|----------|-----| | **DC-ShadowNet** | Unpaired | **6.33** | **11.37** | **5.38** |  ### 3.
AISTD,aist++,DATASET,aist++,AISTD Dataset Evaluation set the paths of the [shadow removal result](https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/blob/eefcc9ee23842847a40f40610c129ecba82f5d21/evaluation/demo_aistd_release.m#L21) and the dataset in `evaluation/demo_aistd_release.m` and then run it. ``` demo_aistd_release.m ```  Get the following Table 2 in the main paper on the AISTD (size: 256x256): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|---------|----------|-----| | **DC-ShadowNet** | Unpaired | **4.7** | **10.6** | 3.7 |  For AISTD (size: 480x640): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|---------|----------|-----| | **DC-ShadowNet** | Unpaired | **6.33** | **11.37** | **5.38** |  ### 3.
AISTD,aist++,DATASET,aist++,AISTD Dataset Evaluation set the paths of the [shadow removal result](https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/blob/eefcc9ee23842847a40f40610c129ecba82f5d21/evaluation/demo_aistd_release.m#L21) and the dataset in `evaluation/demo_aistd_release.m` and then run it. ``` demo_aistd_release.m ```  Get the following Table 2 in the main paper on the AISTD (size: 256x256): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|---------|----------|-----| | **DC-ShadowNet** | Unpaired | **4.7** | **10.6** | 3.7 |  For AISTD (size: 480x640): | Method | Training | All | Shadow | Non-Shadow | |------------------|----------|---------|----------|-----| | **DC-ShadowNet** | Unpaired | **6.33** | **11.37** | **5.38** |  ### 3.
LRSS,lrs2,DATASET,lrs2,"LRSS Soft Shadow Dataset Evaluation set the paths of the shadow removal result and the dataset in `evaluation/demo_lrss_release.m` and then run it. ``` demo_lrss_release.m ```  Get the following Table 3 in the main paper on the LRSS dataset (size: 256x256): | Method | Training | All |  |------------------|----------|----------| | **DC-ShadowNet** | Unpaired | **3.48** | | Input Image | N/A | 12.26 |   ## Acknowledgments Code is implemented based [U-GAT-IT](https://github.com/znxlwm/UGATIT-pytorch), we would like to thank them."
lrss,lrs2,DATASET,lrs2,"LRSS Soft Shadow Dataset Evaluation set the paths of the shadow removal result and the dataset in `evaluation/demo_lrss_release.m` and then run it. ``` demo_lrss_release.m ```  Get the following Table 3 in the main paper on the LRSS dataset (size: 256x256): | Method | Training | All |  |------------------|----------|----------| | **DC-ShadowNet** | Unpaired | **3.48** | | Input Image | N/A | 12.26 |   ## Acknowledgments Code is implemented based [U-GAT-IT](https://github.com/znxlwm/UGATIT-pytorch), we would like to thank them."
lrss,lrs2,DATASET,lrs2,"LRSS Soft Shadow Dataset Evaluation set the paths of the shadow removal result and the dataset in `evaluation/demo_lrss_release.m` and then run it. ``` demo_lrss_release.m ```  Get the following Table 3 in the main paper on the LRSS dataset (size: 256x256): | Method | Training | All |  |------------------|----------|----------| | **DC-ShadowNet** | Unpaired | **3.48** | | Input Image | N/A | 12.26 |   ## Acknowledgments Code is implemented based [U-GAT-IT](https://github.com/znxlwm/UGATIT-pytorch), we would like to thank them."
LRSS,lrs2,DATASET,lrs2,"LRSS Soft Shadow Dataset Evaluation set the paths of the shadow removal result and the dataset in `evaluation/demo_lrss_release.m` and then run it. ``` demo_lrss_release.m ```  Get the following Table 3 in the main paper on the LRSS dataset (size: 256x256): | Method | Training | All |  |------------------|----------|----------| | **DC-ShadowNet** | Unpaired | **3.48** | | Input Image | N/A | 12.26 |   ## Acknowledgments Code is implemented based [U-GAT-IT](https://github.com/znxlwm/UGATIT-pytorch), we would like to thank them."
FaceWarehouse,facewarehouse,DATASET,facewarehouse,"It achieves state-of-the-art performance on multiple datasets such as FaceWarehouse, MICC Florence and BU-3DFE.     ## Features  ### ● Accurate shapes The method reconstructs faces with high accuracy."
MICC Florence,florence,DATASET,florence,"It achieves state-of-the-art performance on multiple datasets such as FaceWarehouse, MICC Florence and BU-3DFE.     ## Features  ### ● Accurate shapes The method reconstructs faces with high accuracy."
BU-3DFE,NIL,DATASET,NIL,"It achieves state-of-the-art performance on multiple datasets such as FaceWarehouse, MICC Florence and BU-3DFE.     ## Features  ### ● Accurate shapes The method reconstructs faces with high accuracy."
FaceWareHouse,facewarehouse,DATASET,facewarehouse,Quantitative evaluations (shape errors in mm) on several benchmarks show its state-of-the-art performance:   |Method|FaceWareHouse|Florence|BU3DFE| |:---:|:---:|:---:|:---:| |[Tewari et al. 17](https://arxiv.org/abs/1703.10580)</center>|2.19±0.54|-|-| |[Tewari et al. 18](https://arxiv.org/abs/1712.02859)|1.84±0.38|-|-| |[Genova et al. 18](https://arxiv.org/abs/1806.06098)|-|1.77±0.53|-| |[Sela et al. 17](https://arxiv.org/abs/1703.10131)|-|-|2.91±0.60| |[PRN 18](https://arxiv.org/abs/1803.07835)|-|-|1.86±0.47| |Ours|**1.81±0.50**|**1.67±0.50**|**1.40±0.31**|  (Please refer to our paper for more details about these results)  ### ● High fidelity textures The method produces high fidelity face textures meanwhile preserves identity information of input images.
Florence,florence,DATASET,florence,Quantitative evaluations (shape errors in mm) on several benchmarks show its state-of-the-art performance:   |Method|FaceWareHouse|Florence|BU3DFE| |:---:|:---:|:---:|:---:| |[Tewari et al. 17](https://arxiv.org/abs/1703.10580)</center>|2.19±0.54|-|-| |[Tewari et al. 18](https://arxiv.org/abs/1712.02859)|1.84±0.38|-|-| |[Genova et al. 18](https://arxiv.org/abs/1806.06098)|-|1.77±0.53|-| |[Sela et al. 17](https://arxiv.org/abs/1703.10131)|-|-|2.91±0.60| |[PRN 18](https://arxiv.org/abs/1803.07835)|-|-|1.86±0.47| |Ours|**1.81±0.50**|**1.67±0.50**|**1.40±0.31**|  (Please refer to our paper for more details about these results)  ### ● High fidelity textures The method produces high fidelity face textures meanwhile preserves identity information of input images.
BU3DFE,NIL,DATASET,NIL,Quantitative evaluations (shape errors in mm) on several benchmarks show its state-of-the-art performance:   |Method|FaceWareHouse|Florence|BU3DFE| |:---:|:---:|:---:|:---:| |[Tewari et al. 17](https://arxiv.org/abs/1703.10580)</center>|2.19±0.54|-|-| |[Tewari et al. 18](https://arxiv.org/abs/1712.02859)|1.84±0.38|-|-| |[Genova et al. 18](https://arxiv.org/abs/1806.06098)|-|1.77±0.53|-| |[Sela et al. 17](https://arxiv.org/abs/1703.10131)|-|-|2.91±0.60| |[PRN 18](https://arxiv.org/abs/1803.07835)|-|-|1.86±0.47| |Ours|**1.81±0.50**|**1.67±0.50**|**1.40±0.31**|  (Please refer to our paper for more details about these results)  ### ● High fidelity textures The method produces high fidelity face textures meanwhile preserves identity information of input images.
AFLW_2000,aflw,DATASET,aflw,"We conduct an experiment on AFLW_2000 dataset (NME) to evaluate the performance, as shown in the table below: <p align=""center"">  <img src=""/images/alignment.png""> </p>  |Method|[0°,30°]|[30°,60°]|[60°,90°]|Overall| |:---:|:---:|:---:|:---:|:---:| |[3DDFA 16](https://arxiv.org/abs/1511.07212)</center>|3.78|4.54|7.93|5.42| |[3DDFA+SDM 16](https://arxiv.org/abs/1511.07212)|3.43|4.24|7.17|4.94| |[Bulat et al. 17](https://arxiv.org/abs/1703.00862)|**2.47**|**3.01**|**4.31**|**3.26**| |[PRN 18](https://arxiv.org/abs/1803.07835)|2.75|3.51|4.61|3.62| |Ours|2.56|3.11|4.45|3.37|   ### ● Easy and Fast Faces are represented with Basel Face Model 2009, which is easy for further manipulations (e.g expression transfer)."
CoarseData,NIL,DATASET,NIL,"(https://github.com/Juyong/3DFace) You can find a link named ""CoarseData"" in the first row of Introduction part in their repository."
Coarse,NIL,DATASET,NIL,Download and unzip the Coarse_Dataset.zip.
Facewarehouse,facewarehouse,DATASET,facewarehouse,The expression basis are constructed using [Facewarehouse](http://kunzhou.net/zjugaps/facewarehouse/) data and transferred to BFM topology.  3.
ImageNet,imagenet,DATASET,imagenet,Training process has been tested with this model to ensure similar results. - [Resnet50-v1](https://github.com/tensorflow/models/blob/master/research/slim/README.md) pre-trained on ImageNet from Tensorflow Slim.
CelebA,celeba,DATASET,celeba,"If you have any further questions, please contact Yu Deng (dengyu2008@hotmail.com) and Jiaolong Yang (jiaoyan@microsoft.com).   ## Citation  Please cite the following paper if this model helps your research:   @inproceedings{deng2019accurate,      title={Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set},      author={Yu Deng and Jiaolong Yang and Sicheng Xu and Dong Chen and Yunde Jia and Xin Tong},      booktitle={IEEE Computer Vision and Pattern Recognition Workshops},      year={2019}  } ## The face images on this page are from the public [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset released by MMLab, CUHK."
CelebA,celeba,DATASET,celeba,"If you have any further questions, please contact Yu Deng (dengyu2008@hotmail.com) and Jiaolong Yang (jiaoyan@microsoft.com).   ## Citation  Please cite the following paper if this model helps your research:   @inproceedings{deng2019accurate,      title={Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set},      author={Yu Deng and Jiaolong Yang and Sicheng Xu and Dong Chen and Yunde Jia and Xin Tong},      booktitle={IEEE Computer Vision and Pattern Recognition Workshops},      year={2019}  } ## The face images on this page are from the public [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset released by MMLab, CUHK."
cityscapes,cityscapes,DATASET,cityscapes,/tools/dist_test.sh configs/EvPSNet_unc_mutigpu.py ${CHECKPOINT_FILE} ${GPU_NUM} --eval panoptic ```  ## Additional Notes:    * tool/cityscapes_inference.py: saves predictions in the official cityscapes panoptic format
BSARD,bsard,DATASET,bsard,"label=🤗 %20Datasets&message=BSARD&color=FF9900)](https://huggingface.co/datasets/maastrichtlawtech/bsard)  # A Statutory Article Retrieval Dataset in French  This repository contains the code for reproducing the experimental results presented in the ACL 2022 paper [""A Statutory Article Retrieval Dataset in French""](https://aclanthology.org/2022.acl-long.468/) by [Antoine Louis](https:/antoinelouis.co/work/) and [Jerry Spanakis](https://dke.maastrichtuniversity.nl/jerry.spanakis/)."
Belgian Statutory Article Retrieval Dataset,NIL,DATASET,NIL,"To address this bottleneck, we introduce the Belgian Statutory Article Retrieval Dataset (BSARD), which consists of 1,100+ French native legal questions labeled by experienced jurists with relevant articles from a corpus of 22,600+ Belgian law articles."
BSARD,bsard,DATASET,bsard,"To address this bottleneck, we introduce the Belgian Statutory Article Retrieval Dataset (BSARD), which consists of 1,100+ French native legal questions labeled by experienced jurists with relevant articles from a corpus of 22,600+ Belgian law articles."
BSARD,bsard,DATASET,bsard,"Using BSARD, we benchmark several state-of-the-art retrieval approaches, including lexical and dense architectures, both in zero-shot and supervised setups."
BSARD,bsard,DATASET,bsard,"By the specificity of the domain and addressed task, BSARD presents a unique challenge problem for future research on legal information retrieval.  ## Documentation  Detailed documentation on the dataset and how to reproduce the main experimental results can be found [here](docs/README.md).  ## Citation  For attribution in academic contexts, please cite this work as:  ```latex @inproceedings{louis2022statutory,   title = {A Statutory Article Retrieval Dataset in French},   author = {Louis, Antoine and Spanakis, Gerasimos},   booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},   month = may,   year = {2022},   address = {Dublin, Ireland},   publisher = {Association for Computational Linguistics},   url = {https://aclanthology.org/2022.acl-long.468},   pages = {6789--6803}, } ```  ## License  This repository is MIT-licensed."
ASSIN2,assin2,DATASET,assin2,)            | 100 languages |   ASSIN2        |   0.8680     |   0.8680    | | PTT5 (Carmo et al
ASSIN2,assin2,DATASET,assin2,)             | EN & PT       |   ASSIN2        |   0.8850     |   0.8860    | | BERTimbau Large (Souza et al
ASSIN2,assin2,DATASET,assin2,)  | EN & PT       |   ASSIN2        |   0.9000     |   0.9000    | | BERT-pt (ours)                  | EN & PT       |  MNLI + ASSIN2  |   0.9207     |   0.9207    |                       ## How to Translate   We made available the following data and the respectives notebooks with translation code: - [SQuAD](https://colab.research.google.com/drive/1CSNwfWJCwhFgYTtjxsDvUdN1DMuU_P4-?
MNLI,NIL,DATASET,NIL,)  | EN & PT       |   ASSIN2        |   0.9000     |   0.9000    | | BERT-pt (ours)                  | EN & PT       |  MNLI + ASSIN2  |   0.9207     |   0.9207    |                       ## How to Translate   We made available the following data and the respectives notebooks with translation code: - [SQuAD](https://colab.research.google.com/drive/1CSNwfWJCwhFgYTtjxsDvUdN1DMuU_P4-?
ASSIN2,assin2,DATASET,assin2,)  | EN & PT       |   ASSIN2        |   0.9000     |   0.9000    | | BERT-pt (ours)                  | EN & PT       |  MNLI + ASSIN2  |   0.9207     |   0.9207    |                       ## How to Translate   We made available the following data and the respectives notebooks with translation code: - [SQuAD](https://colab.research.google.com/drive/1CSNwfWJCwhFgYTtjxsDvUdN1DMuU_P4-?
SQuAD,squad,DATASET,squad,)  | EN & PT       |   ASSIN2        |   0.9000     |   0.9000    | | BERT-pt (ours)                  | EN & PT       |  MNLI + ASSIN2  |   0.9207     |   0.9207    |                       ## How to Translate   We made available the following data and the respectives notebooks with translation code: - [SQuAD](https://colab.research.google.com/drive/1CSNwfWJCwhFgYTtjxsDvUdN1DMuU_P4-?
FaQuAD,faquad,DATASET,faquad,usp=sharing) (Q&A) - [FaQuAD](https://colab.research.google.com/drive/1HdPjzn61genPyZfiDG5fqAwPNI4vhegw?
MNLI,NIL,DATASET,NIL,usp=sharing) (Q&A) - [MNLI](https://colab.research.google.com/drive/1Y9ZaJuN-SVo0fmwypPzcJCOetFw-A2tx?
ASSIN2,assin2,DATASET,assin2,usp=sharing) (NLI) - [ASSIN2](https://colab.research.google.com/drive/1S5zwaw8KWee8y6Vyq-XHC8Am3GmGFpv9?
SQuAD,squad,DATASET,squad,usp=sharing) (NLI)  The datasets SQuAD and MNLI are directly downloaded from the notebooks of this repository.
MNLI,NIL,DATASET,NIL,usp=sharing) (NLI)  The datasets SQuAD and MNLI are directly downloaded from the notebooks of this repository.
FaQuAD,faquad,DATASET,faquad,We also provide the FaQuAD and ASSIN2 datasets
ASSIN2,assin2,DATASET,assin2,We also provide the FaQuAD and ASSIN2 datasets
SQuAD,squad,DATASET,squad,"|                                 |   SQuAD    | FaQuAD   | MNLI       | ASSIN2   |  | ------------------------------- | ---------- | -------- | ---------- |--------- |  | Training examples               |  86,288    |   837    |   392,702  |  6,500   |    | Test examples                   |  21,557    |   63     |   20,000   |  2,448   |     | Translate Train (Batch size = 1)|    34h     |   -      |    36h     |    -     |   | Translate Test (Batch size = 1) |    -       |  1m 30s  |    -       |   31m    | ## References  [1] [BERTimbau: Pretrained BERT Models for Brazilian Portuguese](https://www.researchgate.net/publication/345395208_BERTimbau_Pretrained_BERT_Models_for_Brazilian_Portuguese)  [2] [PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data](https://arxiv.org/abs/2008.09144)  ## How do I cite this work?"
FaQuAD,faquad,DATASET,faquad,"|                                 |   SQuAD    | FaQuAD   | MNLI       | ASSIN2   |  | ------------------------------- | ---------- | -------- | ---------- |--------- |  | Training examples               |  86,288    |   837    |   392,702  |  6,500   |    | Test examples                   |  21,557    |   63     |   20,000   |  2,448   |     | Translate Train (Batch size = 1)|    34h     |   -      |    36h     |    -     |   | Translate Test (Batch size = 1) |    -       |  1m 30s  |    -       |   31m    | ## References  [1] [BERTimbau: Pretrained BERT Models for Brazilian Portuguese](https://www.researchgate.net/publication/345395208_BERTimbau_Pretrained_BERT_Models_for_Brazilian_Portuguese)  [2] [PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data](https://arxiv.org/abs/2008.09144)  ## How do I cite this work?"
MNLI,NIL,DATASET,NIL,"|                                 |   SQuAD    | FaQuAD   | MNLI       | ASSIN2   |  | ------------------------------- | ---------- | -------- | ---------- |--------- |  | Training examples               |  86,288    |   837    |   392,702  |  6,500   |    | Test examples                   |  21,557    |   63     |   20,000   |  2,448   |     | Translate Train (Batch size = 1)|    34h     |   -      |    36h     |    -     |   | Translate Test (Batch size = 1) |    -       |  1m 30s  |    -       |   31m    | ## References  [1] [BERTimbau: Pretrained BERT Models for Brazilian Portuguese](https://www.researchgate.net/publication/345395208_BERTimbau_Pretrained_BERT_Models_for_Brazilian_Portuguese)  [2] [PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data](https://arxiv.org/abs/2008.09144)  ## How do I cite this work?"
ASSIN2,assin2,DATASET,assin2,"|                                 |   SQuAD    | FaQuAD   | MNLI       | ASSIN2   |  | ------------------------------- | ---------- | -------- | ---------- |--------- |  | Training examples               |  86,288    |   837    |   392,702  |  6,500   |    | Test examples                   |  21,557    |   63     |   20,000   |  2,448   |     | Translate Train (Batch size = 1)|    34h     |   -      |    36h     |    -     |   | Translate Test (Batch size = 1) |    -       |  1m 30s  |    -       |   31m    | ## References  [1] [BERTimbau: Pretrained BERT Models for Brazilian Portuguese](https://www.researchgate.net/publication/345395208_BERTimbau_Pretrained_BERT_Models_for_Brazilian_Portuguese)  [2] [PTT5: Pretraining and validating the T5 model on Brazilian Portuguese data](https://arxiv.org/abs/2008.09144)  ## How do I cite this work?"
Cora,cora,DATASET,cora,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541)."
CORA,cora,DATASET,cora,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541)."
Citeseer,citeseer,DATASET,citeseer,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541)."
Pubmed,pubmed,DATASET,pubmed,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541)."
DBLP,dblp,DATASET,dblp,"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes’ attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541)."
Cora,cora,DATASET,cora,Take Cora dataset as an example:  - make a folder called **dataset** at root directory
cora,cora,DATASET,cora,- make a folder called **cora** in **dataset** directory
Cora,cora,DATASET,cora,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```"
cora,cora,DATASET,cora,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```"
cora,cora,DATASET,cora,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```"
cora,cora,DATASET,cora,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```"
cora,cora,DATASET,cora,"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```"
SQuAD,squad,DATASET,squad,Framework for Robustness Validation of Machine Comprehension Systems  This repository contains dataset for robustness validation of machine comprehension systems trained on SQuAD dataset.
Visual Genome,visual genome,DATASET,visual genome,"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell ."
VG_100K,NIL,DATASET,NIL,/experiments/scripts/fetch_images.sh ``` This will populate the `DrillDown/data` folder with `vg/VG_100K` and `vg/VG_100K_2`
VG_100K_2,NIL,DATASET,NIL,/experiments/scripts/fetch_images.sh ``` This will populate the `DrillDown/data` folder with `vg/VG_100K` and `vg/VG_100K_2`
BlackScholes,NIL,DATASET,NIL,"Then:  ```setup pip install -r requirements.txt ```   ## Training & Testing  To train and test the model(s) of the paper, run these commands (other  hyperparameters can be changed in the main section of demo.py):  go to the source directory: ```train cd NJODE ```  for Black-Scholes model: ```train python demo.py --dataset='BlackScholes' ```  for Heston model: ```train python demo.py --dataset='Heston' ```  for Ornstein-Uhlenbeck model: ```train python demo.py --dataset='OrnsteinUhlenbeck' ```  If no dataset for the model was generated yet, it will be generated  automatically before the training starts."
Heston,NIL,DATASET,NIL,"Then:  ```setup pip install -r requirements.txt ```   ## Training & Testing  To train and test the model(s) of the paper, run these commands (other  hyperparameters can be changed in the main section of demo.py):  go to the source directory: ```train cd NJODE ```  for Black-Scholes model: ```train python demo.py --dataset='BlackScholes' ```  for Heston model: ```train python demo.py --dataset='Heston' ```  for Ornstein-Uhlenbeck model: ```train python demo.py --dataset='OrnsteinUhlenbeck' ```  If no dataset for the model was generated yet, it will be generated  automatically before the training starts."
Ornstein-Uhlenbeck,NIL,DATASET,NIL,"Then:  ```setup pip install -r requirements.txt ```   ## Training & Testing  To train and test the model(s) of the paper, run these commands (other  hyperparameters can be changed in the main section of demo.py):  go to the source directory: ```train cd NJODE ```  for Black-Scholes model: ```train python demo.py --dataset='BlackScholes' ```  for Heston model: ```train python demo.py --dataset='Heston' ```  for Ornstein-Uhlenbeck model: ```train python demo.py --dataset='OrnsteinUhlenbeck' ```  If no dataset for the model was generated yet, it will be generated  automatically before the training starts."
OrnsteinUhlenbeck,NIL,DATASET,NIL,"Then:  ```setup pip install -r requirements.txt ```   ## Training & Testing  To train and test the model(s) of the paper, run these commands (other  hyperparameters can be changed in the main section of demo.py):  go to the source directory: ```train cd NJODE ```  for Black-Scholes model: ```train python demo.py --dataset='BlackScholes' ```  for Heston model: ```train python demo.py --dataset='Heston' ```  for Ornstein-Uhlenbeck model: ```train python demo.py --dataset='OrnsteinUhlenbeck' ```  If no dataset for the model was generated yet, it will be generated  automatically before the training starts."
Heston,NIL,DATASET,NIL,For training: uncomment the code below the model params and run the file.   ## Heston dataset without Feller condition Models for the Heston dataset without Feller condition were trained using  parallel_train.py with the model parameters specified below ``` # ========================================================================== # parallel training for Heston without Feller # ========================================================================== ``` in the main part of the file parallel_train.py.
Heston,NIL,DATASET,NIL,For training: uncomment the code below the model params and run the file.   ## Heston dataset without Feller condition Models for the Heston dataset without Feller condition were trained using  parallel_train.py with the model parameters specified below ``` # ========================================================================== # parallel training for Heston without Feller # ========================================================================== ``` in the main part of the file parallel_train.py.
Physionet,physion,DATASET,physion,"For performing/evaluating the cross-validation use the function-call: ``` get_training_overview(     params_extract_desc=('dataset', 'network_size', 'dropout_rate',                          'hidden_size', 'data_index'),     val_test_params_extract=((""max"", ""epoch"", ""epoch"", ""epochs_trained""),                              (""min"", ""eval_metric"",                               ""eval_metric"", ""eval_metric_min""),                              (""min"", ""test_metric"",                               ""test_metric"", ""test_metric_min""),                              (""min"", ""eval_metric"",                               ""test_metric"", ""test_metric_evaluation_min""),                              (""min"", ""eval_loss"",                               ""test_metric"", ""test_metric_eval_loss_min""),                              ) )  get_climate_cross_validation(early_stop_after_epoch=100) ```  in extras.py.   ## Training on Physionet Dataset The Physionet dataset that was used by  [Latent ODEs for Irregularly-Sampled Time Series](https://arxiv.org/abs/1907.03907) is downloaded and saved automatically when training for the first time."
Physionet,physion,DATASET,physion,"For performing/evaluating the cross-validation use the function-call: ``` get_training_overview(     params_extract_desc=('dataset', 'network_size', 'dropout_rate',                          'hidden_size', 'data_index'),     val_test_params_extract=((""max"", ""epoch"", ""epoch"", ""epochs_trained""),                              (""min"", ""eval_metric"",                               ""eval_metric"", ""eval_metric_min""),                              (""min"", ""test_metric"",                               ""test_metric"", ""test_metric_min""),                              (""min"", ""eval_metric"",                               ""test_metric"", ""test_metric_evaluation_min""),                              (""min"", ""eval_loss"",                               ""test_metric"", ""test_metric_eval_loss_min""),                              ) )  get_climate_cross_validation(early_stop_after_epoch=100) ```  in extras.py.   ## Training on Physionet Dataset The Physionet dataset that was used by  [Latent ODEs for Irregularly-Sampled Time Series](https://arxiv.org/abs/1907.03907) is downloaded and saved automatically when training for the first time."
Physionet,physion,DATASET,physion,Models for validation on the Physionet dataset were trained using  parallel_train.py with the model parameters specified below ``` # ========================================================================== # parallel training on physionet dataset # ========================================================================== ``` in the main part of the file parallel_train.py.
physionet,physion,DATASET,physion,Models for validation on the Physionet dataset were trained using  parallel_train.py with the model parameters specified below ``` # ========================================================================== # parallel training on physionet dataset # ========================================================================== ``` in the main part of the file parallel_train.py.
MNIST,mnist,DATASET,mnist,The source code is currently set up for the configuration of three clients performing secure and differentially private federated learning using logistic regresion on the MNIST dataset.
MNIST,mnist,DATASET,mnist,"In our example, it loads the MNIST dataset and processes it for the client agent instances."
WordNet,languagenet,DATASET,languagenet,"Currently surfaced using [WordNet](https://wordnet.princeton.edu/) synsets. ``` msb.get_similar_concepts(""your concept term"") ```  ### Compare concepts to ground truth labels This function displays the Pearson correlation coefficients between each of the selected concepts and the ground truth label column."
wordnet,languagenet,DATASET,languagenet,"Currently surfaced using [WordNet](https://wordnet.princeton.edu/) synsets. ``` msb.get_similar_concepts(""your concept term"") ```  ### Compare concepts to ground truth labels This function displays the Pearson correlation coefficients between each of the selected concepts and the ground truth label column."
KITTI,kitti,DATASET,kitti,This method holds the **1st place** entry on the [KITTI depth completion benchmark](http://www.cvlibs.net/datasets/kitti/eval_depth.php?
kitti,kitti,DATASET,kitti,This method holds the **1st place** entry on the [KITTI depth completion benchmark](http://www.cvlibs.net/datasets/kitti/eval_depth.php?
KITTI,kitti,DATASET,kitti,This is the late fusion technique used in our framework. * This method ranks first on the KITTI depth completion benchmark without using additional data or postprocessing.
KITTI,kitti,DATASET,kitti,The predictions of our model for the KITTI test set can be downloaded [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x).  !
Kitti,kitti,DATASET,kitti,(Works with Pytorch 1.1)   ## Dataset The [Kitti dataset](www.cvlibs.net/datasets/kitti/) has been used.
kitti,kitti,DATASET,kitti,(Works with Pytorch 1.1)   ## Dataset The [Kitti dataset](www.cvlibs.net/datasets/kitti/) has been used.
kitti,kitti,DATASET,kitti,"Secondly, you'll need to unzip and download the camera images from kitti."
Cityscapes,cityscapes,DATASET,cityscapes,"|--validation ```   ## Run Code To run the code:  `python main.py --data_path /path/to/data/ --lr_policy plateau`  Flags: - Set flag ""input_type"" to rgb or depth. - Set flag ""pretrained"" to true or false to use a model pretrained on Cityscapes for the global branch. - See `python main.py --help` for more information."
Cityscapes,cityscapes,DATASET,cityscapes,You can find the model pretrained on Cityscapes [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?
KITTI,kitti,DATASET,kitti,You can find a fully trained model and its corresponding predictions for the KITTI test set [here](https://drive.google.com/drive/folders/1U7dvH4sC85KRVuV19fRpaMzJjE-m3D9x?
KITTI,kitti,DATASET,kitti,"and execute the following command:  `source Test/test.sh /path/to/directory_with_saved_model/ $num_samples /path/to/dataset/ /path/to/directory_with_ground_truth_for_selected_validation_files/`  (You might have to recompile the C files for testing, provided by KITTI, if your architecture is different from mine)  ## Results  Comparision with state-of-the-art:  !"
multi-centric heterogeneous cine-SSFPs CMR TOF,NIL,DATASET,NIL,"Generated with `conda env export > environment.yml`     │     ├── setup.py           <- Makes project pip installable (pip install -e .) so src can be imported     ├── src                <- Python modules with classes and functions that will be orchestrated in the notebooks.         ├── data           <- Python modules - Generators, preprocess and postprocess functions         ├── models         <- Python modules - TF.keras 2.X Model and Layer definition         ├── utils          <- Python modules - Metrics, losses, prediction, evaluation code, TF-callbacks and io-utils         └── visualization  <- Python modules - Plot functions for the evaluation and data description  ## Dataset For this work a multi-centric heterogeneous cine-SSFPs CMR TOF data set from the [German Competence Network for Congenital Heart Defects](https://www.kompetenznetz-ahf.de/en/about-us/competence-network/)) was used (study identifier: NCT00266188, title: Non-invasive Imaging and Exercise Tolerance Tests in Post-repair Tetralogy of Fallot -Intervention and Course in Patients Over 8 Years Old)."
TOF,tot,DATASET,tot,This TOF dataset constitutes one of the largest compiled data set of this pathology to date.
NELA-GT-2019,nela-gt-2019,DATASET,nela-gt-2019,# NELA-GT-2019  This repository contain examples of how to use the NELA-GT-2019 data set with Python 3.
NELA-GT-2019,nela-gt-2019,DATASET,nela-gt-2019,# NELA-GT-2019  This repository contain examples of how to use the NELA-GT-2019 data set with Python 3.
NELA-GT-2019,nela-gt-2019,DATASET,nela-gt-2019,"persistentId=doi:10.7910/DVN/O7FWPO  __For more details about this dataset, check the paper__: https://arxiv.org/abs/2003.08444  If you use this dataset in your work, please cite us as follows: <br> ``` @misc{     gruppi2020nelagt2019,     title={NELA-GT-2019: A Large Multi-Labelled News Dataset for The Study of Misinformation in News Articles},     author={Maurício Gruppi and Benjamin D."
NELA-GT-2019,nela-gt-2019,DATASET,nela-gt-2019,"Horne and Sibel Adalı},     year={2020},     eprint={2003.08444},     archivePrefix={arXiv},     primaryClass={cs.CY} } ``` ## Data  Metadata|| ---|--- Dataset name|`NELA-GT-2019` Formats|`Sqlite3`,`JSON` No. of articles|`1118821` No. of sources|`261` Collection period|`2019-01-01` to `2019-12-31`  ### Fields  Each data point collected corresponds to an article and contains the fields described below."
NELA,NIL,DATASET,NIL,+ Loading data from single or multiple sources from the database   + Loading data from the database into a Pandas dataframe  Usage: ``` python3 load-sqlite3.py <path-to-database> ```  ###  load-json.py  * How to load NELA in JSON format with Python 3
NELA,NIL,DATASET,NIL,+ Loading a single source's JSON   + Loading a directory of NELA JSON files - **WARNING**: this consumes a lot of memory  Usage: ``` python3 load-json.py <path-to-file> ```
COCO,ms coco,DATASET,ms coco,"## Fully Convolutional Instance-aware Semantic Segmentation  The major contributors of this repository include [Haozhi Qi](https://github.com/Oh233), [Yi Li](https://github.com/liyi14), [Guodong Zhang](https://github.com/gd-zhang), [Haochen Zhang](https://github.com/Braininvat), [Jifeng Dai](https://github.com/daijifeng001), and [Yichen Wei](https://github.com/YichenWei).  ### Introduction  **FCIS** is a fully convolutional end-to-end solution for instance segmentation, which won the first place in COCO segmentation challenge 2016."
COCO,ms coco,DATASET,ms coco,Visual results on the first 5k images from COCO test set of our ***COCO 2016 challenge entry***: [OneDrive](https://onedrive.live.com/?
COCO,ms coco,DATASET,ms coco,Visual results on the first 5k images from COCO test set of our ***COCO 2016 challenge entry***: [OneDrive](https://onedrive.live.com/?
COCO,ms coco,DATASET,ms coco,Slides in [ImageNet ILSVRC and COCO workshop 2016](http://image-net.org/challenges/ilsvrc+coco2016): [OneDrive](https://onedrive.live.com/?
ImageNet,imagenet,DATASET,imagenet,* We trained our model based on the ImageNet pre-trained [ResNet-v1-101](https://github.com/KaimingHe/deep-residual-networks) using a [model converter](https://github.com/dmlc/mxnet/tree/430ea7bfbbda67d993996d81c7fd44d3a20ef846/tools/caffe_converter).
COCO trainval35k,ms coco,DATASET,ms coco,"To run the demo with our trained model (on COCO trainval35k), please download the model manually from [OneDrive](https://1drv.ms/u/s!"
VOC 2012,pascal voc 2011,DATASET,pascal voc 2011,Please download VOC 2012 dataset with additional annotations from [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html).
SBD,sbd,DATASET,sbd,Please download VOC 2012 dataset with additional annotations from [SBD](http://home.bharathh.info/pubs/codes/SBD/download.html).
COCO,ms coco,DATASET,ms coco,Please download [COCO dataset](http://mscoco.org/dataset/#download) and annotations for the 5k image [minival](https://dl.dropboxusercontent.com/s/o43o90bna78omob/instances_minival2014.json.zip?
minival,NIL,DATASET,NIL,Please download [COCO dataset](http://mscoco.org/dataset/#download) and annotations for the 5k image [minival](https://dl.dropboxusercontent.com/s/o43o90bna78omob/instances_minival2014.json.zip?
minival,NIL,DATASET,NIL,Please download [COCO dataset](http://mscoco.org/dataset/#download) and annotations for the 5k image [minival](https://dl.dropboxusercontent.com/s/o43o90bna78omob/instances_minival2014.json.zip?
val2014 minus minival (val35k),NIL,DATASET,NIL,dl=0) subset and [val2014 minus minival (val35k)](https://dl.dropboxusercontent.com/s/s3tw5zcg7395368/instances_valminusminival2014.json.zip?
valminusminival2014,NIL,DATASET,NIL,Make sure it looks like this:  ```  .data/coco/  .data/coco/annotations/instances_valminusminival2014.json  .data/coco/annotations/instances_minival2014.json  ```  3.
minival2014,NIL,DATASET,NIL,Make sure it looks like this:  ```  .data/coco/  .data/coco/annotations/instances_valminusminival2014.json  .data/coco/annotations/instances_minival2014.json  ```  3.
COCO,ms coco,DATASET,ms coco,Two config files have been provided so far: FCIS@COCO with OHEM and FCIS@VOC without OHEM.
VOC,pascal voc 2007,DATASET,pascal voc 2007,Two config files have been provided so far: FCIS@COCO with OHEM and FCIS@VOC without OHEM.
COCO,ms coco,DATASET,ms coco,"We use 8 and 4 GPUs to train models on COCO and on VOC, respectively. 3."
VOC,pascal voc 2007,DATASET,pascal voc 2007,"We use 8 and 4 GPUs to train models on COCO and on VOC, respectively. 3."
COCO,ms coco,DATASET,ms coco,"For example, to train and test FCIS on COCO with ResNet-v1-101, use the following command     ```     python experiments/fcis/fcis_end2end_train_test.py --cfg experiments/fcis/cfgs/resnet_v1_101_coco_fcis_end2end_ohem.yaml     ```     A cache folder would be created automatically to save the model and the log under `output/fcis/coco/` or `output/fcis/voc/`. 4."
S2ORC,s2orc,DATASET,s2orc,- `logs/fos_senses/es-True_res-0.0/` includes senses and their npmi scores in each field  ### Discipline-specific word types   - `logs/type_npmi/fos_set-False_lemma-True/` includes lists of word types in each subfield and their npmi scores  ## Code pipeline  **Data filtering**  Information on accessing S2ORC can be found [here](https://github.com/allenai/s2orc)
s2orc,s2orc,DATASET,s2orc,- `logs/fos_senses/es-True_res-0.0/` includes senses and their npmi scores in each field  ### Discipline-specific word types   - `logs/type_npmi/fos_set-False_lemma-True/` includes lists of word types in each subfield and their npmi scores  ## Code pipeline  **Data filtering**  Information on accessing S2ORC can be found [here](https://github.com/allenai/s2orc)
s2orc,s2orc,DATASET,s2orc,This generates `s2orc_fos.json`  **Word type pipeline**  In the `type_jargon` folder:   FOS - `word_counts_per_fos.py`: count words per field of study.
S2ORC,s2orc,DATASET,s2orc,"Run `bash prepare_sense_input.sh 2>&1 | tee temp.log` to do the next three scripts:   - `write_mask_preds/wsi_vocab.py`: determine vocabulary of words to perform WSI - `val_data_process/process_wiktionary.py`: get wiktionary definitions for vocabulary words - `write_mask_preds/wsi_preprocessing.py`: input preparation, also copy vocab file into output folder  Then, run the following script on S2ORC and Wikipedia:   - `write_mask_preds/write_mask_preds.py`: write replacements   We recommend splitting input files into numbered parts and running the script on ranges of file numbers."
S2ORC,s2orc,DATASET,s2orc,Usage example for S2ORC:  ``` python write_mask_preds.py --data_dir /data/actual_data --out_dir /output --dataset s2orc --model scholarBERT --max_tokens_per_batch 16384 --write_specific_replacements --vocab_path /data/wsi_vocab_set_98_50.txt --overwrite_cache --files_range 0-24 ```  Usage example for Wikipedia:  ``` python write_mask_preds.py --data_dir /data/actual_data --out_dir /output --dataset wikipedia --model scholarBERT --max_tokens_per_batch 16384 --write_specific_replacements --vocab_path /data/wsi_vocab_set_98_50.txt --overwrite_cache --files_range 0-24 ```  In the `WSIatScale` folder run `wsi_pipeline.sh` to do WSI pipeline for the entire vocab.
s2orc,s2orc,DATASET,s2orc,Usage example for S2ORC:  ``` python write_mask_preds.py --data_dir /data/actual_data --out_dir /output --dataset s2orc --model scholarBERT --max_tokens_per_batch 16384 --write_specific_replacements --vocab_path /data/wsi_vocab_set_98_50.txt --overwrite_cache --files_range 0-24 ```  Usage example for Wikipedia:  ``` python write_mask_preds.py --data_dir /data/actual_data --out_dir /output --dataset wikipedia --model scholarBERT --max_tokens_per_batch 16384 --write_specific_replacements --vocab_path /data/wsi_vocab_set_98_50.txt --overwrite_cache --files_range 0-24 ```  In the `WSIatScale` folder run `wsi_pipeline.sh` to do WSI pipeline for the entire vocab.
s2orc,s2orc,DATASET,s2orc,"- `create_inverted_index.py`: create inverted index  ``` python create_inverted_index.py --replacements_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --vocab_path /home/lucyl/language-map-of-science/logs/sense_vocab/wsi_vocab_set_98_50.txt --outdir /home/lucyl/language-map-of-science/logs/inverted_index --input_ids_path /home/lucyl/language-map-of-science/data/input_paper_ids/journal_analysis.txt ```  - `cluster_reps_per_token.py`: cluster the reps  Lemmatized, specifying resolution:   ``` python cluster_reps_per_token.py --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --out_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  After clustering for the whole dataset, use `Wiktionary Validation.ipynb` notebook to get FOS to words json."
s2orc,s2orc,DATASET,s2orc,"- `create_inverted_index.py`: create inverted index  ``` python create_inverted_index.py --replacements_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --vocab_path /home/lucyl/language-map-of-science/logs/sense_vocab/wsi_vocab_set_98_50.txt --outdir /home/lucyl/language-map-of-science/logs/inverted_index --input_ids_path /home/lucyl/language-map-of-science/data/input_paper_ids/journal_analysis.txt ```  - `cluster_reps_per_token.py`: cluster the reps  Lemmatized, specifying resolution:   ``` python cluster_reps_per_token.py --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --out_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  After clustering for the whole dataset, use `Wiktionary Validation.ipynb` notebook to get FOS to words json."
s2orc,s2orc,DATASET,s2orc,"Cluster only wiktionary words, lemmatized, specifying resolution:   ``` python cluster_reps_per_token.py --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --out_dir /home/lucyl/language-map-of-science/logs/word_clusters_eval --lemmatize True --wiki_eval True --resolution 0.0 ```  Can check the coverage of words that appear in FOS in `Wiktionary Validation.ipynb`"
s2orc,s2orc,DATASET,s2orc,"- `assign_clusters_to_tokens.py`: assign everyone to a cluster  Lemmatized, specifying resolution:   ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_lemmed --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  Assign only wiktionary words, lemmatized, specifying resolution:  ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_eval --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_eval --lemmatize True --wiki_eval True --resolution 0.5 ```  Sense NPMI - run `get_documentID_maps()` in `get_docID_to_group.py` - `word_sense.py`, for journals, fos, and wiktionary evaluation - `Wiktionary Validation.ipynb` is the notebook that contains Wiktionary evaluation results."
s2orc,s2orc,DATASET,s2orc,"- `assign_clusters_to_tokens.py`: assign everyone to a cluster  Lemmatized, specifying resolution:   ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_lemmed --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  Assign only wiktionary words, lemmatized, specifying resolution:  ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_eval --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_eval --lemmatize True --wiki_eval True --resolution 0.5 ```  Sense NPMI - run `get_documentID_maps()` in `get_docID_to_group.py` - `word_sense.py`, for journals, fos, and wiktionary evaluation - `Wiktionary Validation.ipynb` is the notebook that contains Wiktionary evaluation results."
S2ORC,s2orc,DATASET,s2orc,"- `get_discipline_specific.py`: get discipline specific journals and their papers, for the audience design experiment - `jargonyness_per_paper.py`: calculate amount of jargon per abstract  Example usage:  ``` python jargonyness_per_paper.py --cutoff 0.1 --exp_name general_specific  python jargonyness_per_paper.py --cutoff 0.1 --exp_name regression_sample ```  - `expected_max_npmi.py`: expected max NPMI over token positions in abstract, for audience design experiment - `Paper Jargon Rate.ipynb`: audience design plots - `get_paper_time_and_place.py`: get FOS and year of potential papers that may cite the papers in regression study - `General Dataset Statistics`: get data used for regression  - `regression_variables.py`: get some of the simpler regression variables - `citations_per_journal.py`: for calculating the average number of citations per journal, a regression variable - `Paper Success Regression.ipynb`: notebook that runs regressions - `get_fos_citation_matrix.py`: for calculating similarity among disciplines, part of interdisciplinarity regression  **Citation graph**  Future work may want to run analysis on the S2ORC citation graph."
S2ORC,s2orc,DATASET,s2orc,"The below script supports the conversion of S2ORC data to a `graph-tool` network, where nodes are papers labeled with paper ID"
refcoco,refcoco,DATASET,refcoco,Run following script to setup `cache` directory: ``` sh scripts/prepare_data.sh ``` This should generate following files under `cache` directory: - vocabulary file: `std_vocab_<dataset>_<split_by>.txt` - selected GloVe feature: `std_glove_<dataset>_<split_by>.npy` - referring expression database: `std_refdb_<dataset>_<split_by>.json` - critical objects database: `std_ctxdb_<dataset>_<split_by>.json`   ## Train **Train with binary XE loss:** ``` PYTHONPATH=$PWD python tools/train_att_vanilla.py --dataset refcoco --split-by unc ```  **Train with ranking loss:** ``` PYTHONPATH=$PWD python tools/train_att_rank.py --dataset refcoco --split-by unc ```  We use tensorboard to monitor the training process.
refcoco,refcoco,DATASET,refcoco,Run following script to setup `cache` directory: ``` sh scripts/prepare_data.sh ``` This should generate following files under `cache` directory: - vocabulary file: `std_vocab_<dataset>_<split_by>.txt` - selected GloVe feature: `std_glove_<dataset>_<split_by>.npy` - referring expression database: `std_refdb_<dataset>_<split_by>.json` - critical objects database: `std_ctxdb_<dataset>_<split_by>.json`   ## Train **Train with binary XE loss:** ``` PYTHONPATH=$PWD python tools/train_att_vanilla.py --dataset refcoco --split-by unc ```  **Train with ranking loss:** ``` PYTHONPATH=$PWD python tools/train_att_rank.py --dataset refcoco --split-by unc ```  We use tensorboard to monitor the training process.
refcoco,refcoco,DATASET,refcoco,The log file can be found in `tb` folder.  ## Evaluate Recall **Save Ref-NMS proposals:** ``` PYTHONPATH=$PWD python tools/save_ref_nms_proposals.py --dataset refcoco --split-by unc --tid <tid> --m <loss_type> ``` `<loss_type>` can be either `att_vanilla` for binary XE loss or `att_rank` for rank loss.
refcoco,refcoco,DATASET,refcoco,**Evaluate recall on referent object:** ``` PYTHONPATH=$PWD python tools/eval_proposal_hit_rate.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ``` `conf` parameter is the score threshold used to filter Ref-NMS proposals.
refcoco,refcoco,DATASET,refcoco,**Evaluate recall on critical objects:** ``` PYTHONPATH=$PWD python tools/eval_proposal_ctx_recall.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ```  ## Evaluate REG Performance Save MAttNet-style detection file: ``` PYTHONPATH=$PWD python tools/save_matt_dets.py --dataset refcoco --split-by unc --m <loss_type> --tid <tid> --conf <conf> ``` This script will save all the detection information needed for downstream REG evaluation to `output/matt_dets_<loss_type>_<tid>_<dataset>_<split_by>_<top_N>.json`.
refcoco,refcoco,DATASET,refcoco,**Evaluate recall on critical objects:** ``` PYTHONPATH=$PWD python tools/eval_proposal_ctx_recall.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ```  ## Evaluate REG Performance Save MAttNet-style detection file: ``` PYTHONPATH=$PWD python tools/save_matt_dets.py --dataset refcoco --split-by unc --m <loss_type> --tid <tid> --conf <conf> ``` This script will save all the detection information needed for downstream REG evaluation to `output/matt_dets_<loss_type>_<tid>_<dataset>_<split_by>_<top_N>.json`.
refcoco,refcoco,DATASET,refcoco,"Then, run the following commands to evaluate on REC and RES task: ``` # Evaluate REC performance python tools/extract_mrcn_ref_feats.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> # Evaluate RES performance python tools/run_propose_to_mask.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref_masks.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> --save ```  ## Pretrained Models We provide pre-trained model weights as long as the corresponding **MAttNet-style detection file** (note the MattNet-style detection files can be directly used to evaluate downstream REG task performance)."
refcoco,refcoco,DATASET,refcoco,"Then, run the following commands to evaluate on REC and RES task: ``` # Evaluate REC performance python tools/extract_mrcn_ref_feats.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> # Evaluate RES performance python tools/run_propose_to_mask.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref_masks.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> --save ```  ## Pretrained Models We provide pre-trained model weights as long as the corresponding **MAttNet-style detection file** (note the MattNet-style detection files can be directly used to evaluate downstream REG task performance)."
refcoco,refcoco,DATASET,refcoco,"Then, run the following commands to evaluate on REC and RES task: ``` # Evaluate REC performance python tools/extract_mrcn_ref_feats.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> # Evaluate RES performance python tools/run_propose_to_mask.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> python tools/eval_ref_masks.py --dataset refcoco --splitBy unc --tid <tid> --top-N 0 --m <loss_type> --save ```  ## Pretrained Models We provide pre-trained model weights as long as the corresponding **MAttNet-style detection file** (note the MattNet-style detection files can be directly used to evaluate downstream REG task performance)."
SC-PDB,NIL,DATASET,NIL,"The processed datasets will be released soon, because of the large size of the datasets (graph with esm embeddings), we will upload the pipeline to process the raw data.  ### Raw Datasets  #### SC-PDB  <https://github.com/jivankandel/PUResNet/blob/main/scpdb_subset.zip>  #### COACH420/HOLO4K  <https://github.com/rdk/p2rank-datasets>  These two datasets do not include ligands in a pre-extracted format."
scpdb,NIL,DATASET,NIL,"The processed datasets will be released soon, because of the large size of the datasets (graph with esm embeddings), we will upload the pipeline to process the raw data.  ### Raw Datasets  #### SC-PDB  <https://github.com/jivankandel/PUResNet/blob/main/scpdb_subset.zip>  #### COACH420/HOLO4K  <https://github.com/rdk/p2rank-datasets>  These two datasets do not include ligands in a pre-extracted format."
COACH420,NIL,DATASET,NIL,"The processed datasets will be released soon, because of the large size of the datasets (graph with esm embeddings), we will upload the pipeline to process the raw data.  ### Raw Datasets  #### SC-PDB  <https://github.com/jivankandel/PUResNet/blob/main/scpdb_subset.zip>  #### COACH420/HOLO4K  <https://github.com/rdk/p2rank-datasets>  These two datasets do not include ligands in a pre-extracted format."
HOLO4K,NIL,DATASET,NIL,"The processed datasets will be released soon, because of the large size of the datasets (graph with esm embeddings), we will upload the pipeline to process the raw data.  ### Raw Datasets  #### SC-PDB  <https://github.com/jivankandel/PUResNet/blob/main/scpdb_subset.zip>  #### COACH420/HOLO4K  <https://github.com/rdk/p2rank-datasets>  These two datasets do not include ligands in a pre-extracted format."
MOAD,NIL,DATASET,NIL,"Instead, the MOAD database contains the ligand information, according to the criteria of *relevant ligand* as specified in the MOAD database."
MOAD,NIL,DATASET,NIL,"Instead, the MOAD database contains the ligand information, according to the criteria of *relevant ligand* as specified in the MOAD database."
PDBBind,NIL,DATASET,NIL,"Ligand Information: <http://www.bindingmoad.org/files/csv/every_bind.csv>  #### PDBBind  <http://www.pdbbind.org.cn/download/PDBbind_v2020_other_PL.tar.gz>  ## Replicate results  If you want to replicated the results from the paper, put the downloaded datasets in the specific folders"
PDBbind_v2020,NIL,DATASET,NIL,"Ligand Information: <http://www.bindingmoad.org/files/csv/every_bind.csv>  #### PDBBind  <http://www.pdbbind.org.cn/download/PDBbind_v2020_other_PL.tar.gz>  ## Replicate results  If you want to replicated the results from the paper, put the downloaded datasets in the specific folders"
coach420,NIL,DATASET,NIL,"- `bindingsite_test_data/coach420/raw`  - `bindingsite_test_data/holo4k/raw`  - `bindingsite_test_data/holo4k_split_chain/raw`    This dataset is the same as the holo4k dataset, but the chains are split into separate files."
holo4k,NIL,DATASET,NIL,"- `bindingsite_test_data/coach420/raw`  - `bindingsite_test_data/holo4k/raw`  - `bindingsite_test_data/holo4k_split_chain/raw`    This dataset is the same as the holo4k dataset, but the chains are split into separate files."
holo4k,NIL,DATASET,NIL,"- `bindingsite_test_data/coach420/raw`  - `bindingsite_test_data/holo4k/raw`  - `bindingsite_test_data/holo4k_split_chain/raw`    This dataset is the same as the holo4k dataset, but the chains are split into separate files."
holo4k,NIL,DATASET,NIL,"- `bindingsite_test_data/coach420/raw`  - `bindingsite_test_data/holo4k/raw`  - `bindingsite_test_data/holo4k_split_chain/raw`    This dataset is the same as the holo4k dataset, but the chains are split into separate files."
pdbbind2020,NIL,DATASET,NIL,- `bindingsite_test_data/pdbbind2020/raw`  The holo4k and the coach420 dataset were created by extracting the relevant ligans from the MOAD database.
holo4k,NIL,DATASET,NIL,- `bindingsite_test_data/pdbbind2020/raw`  The holo4k and the coach420 dataset were created by extracting the relevant ligans from the MOAD database.
coach420,NIL,DATASET,NIL,- `bindingsite_test_data/pdbbind2020/raw`  The holo4k and the coach420 dataset were created by extracting the relevant ligans from the MOAD database.
MOAD,NIL,DATASET,NIL,- `bindingsite_test_data/pdbbind2020/raw`  The holo4k and the coach420 dataset were created by extracting the relevant ligans from the MOAD database.
scpdb,NIL,DATASET,NIL,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.
scpdb,NIL,DATASET,NIL,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.
scpdb,NIL,DATASET,NIL,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.
scpdb,NIL,DATASET,NIL,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.
coach420,NIL,DATASET,NIL,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.
coach420,NIL,DATASET,NIL,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.
holo4k,NIL,DATASET,NIL,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.
holo4k,NIL,DATASET,NIL,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.
holo4k,NIL,DATASET,NIL,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.
holo4k,NIL,DATASET,NIL,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.
pdbbind2020,NIL,DATASET,NIL,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.
pdbbind2020,NIL,DATASET,NIL,The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.
coach420,NIL,DATASET,NIL,"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and Günter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria."
coach420,NIL,DATASET,NIL,"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and Günter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria."
pdbbind2020,NIL,DATASET,NIL,"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and Günter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria."
holo4k,NIL,DATASET,NIL,"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and Günter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria."
holo4k,NIL,DATASET,NIL,"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and Günter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria."
KILT,kilt,DATASET,kilt,Download KILT wikipedia knowledge base [here](https://github.com/facebookresearch/KILT) and put it under a kb directory like /raw_kb/  \ 2.
AIDA CoNLL,NIL,DATASET,NIL,Download AIDA CoNLL datasets [here](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aida/downloads) and place them under a raw aida directory like /raw_aida/ \ 4.
AIDA,NIL,DATASET,NIL,"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments."
KILT,kilt,DATASET,kilt,"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments."
AIDA,NIL,DATASET,NIL,"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments."
KILT,kilt,DATASET,kilt,"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments."
AIDA testb,NIL,DATASET,NIL,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|"
MSNBC,NIL,DATASET,NIL,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|"
Der,NIL,DATASET,NIL,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|"
K50,NIL,DATASET,NIL,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|"
R128,NIL,DATASET,NIL,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|"
R500,NIL,DATASET,NIL,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|"
OKE15,NIL,DATASET,NIL,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|"
OKE16,NIL,DATASET,NIL,"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|"
yelp_dataset,NIL,DATASET,NIL,Download Yelp data: https://www.yelp.com/dataset and place files in ```datasets/yelp_dataset/``` 2.
yelp_dataset,NIL,DATASET,NIL,Download subword tokenizer built on Yelp and place in  ```datasets/yelp_dataset/processed/```:  [link](https://s3.us-east-2.amazonaws.com/unsup-sum/subwordenc_32000_maxrevs260_fixed.pkl)  ### Pre-trained models  1.
BOOKSUM,booksum,DATASET,booksum,"# BOOKSUM: A Collection of Datasets for Long-form Narrative Summarization Authors: [Wojciech Kryściński](https://twitter.com/iam_wkr), [Nazneen Rajani](https://twitter.com/nazneenrajani), [Divyansh Agarwal](https://twitter.com/jigsaw2212), [Caiming Xiong](https://twitter.com/caimingxiong), [Dragomir Radev](http://www.cs.yale.edu/homes/radev/)  ## Introduction The majority of available text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases."
BookSum,booksum,DATASET,booksum,"We address these issues by introducing BookSum, a collection of datasets for long-form narrative summarization."
BookSum,booksum,DATASET,booksum,"[Get Involved](#get-involved)  ## Updates #### 4/15/2021 Initial commit   ## Citation ``` @article{kryscinski2021booksum,       title={BookSum: A Collection of Datasets for Long-form Narrative Summarization},        author={Wojciech Kry{\'s}ci{\'n}ski and Nazneen Rajani and Divyansh Agarwal and Caiming Xiong and Dragomir Radev},       year={2021},       eprint={2105.08209},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```  ## Legal Note By downloading or using the resources, including any code or scripts, shared in this code repository, you hereby agree to the following terms, and your use of the resources is conditioned on and subject to these terms. 1."
sop,NIL,DATASET,NIL,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16."
cub,cub-200-2011,DATASET,cub-200-2011,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16."
CUB200-2011,cub-200-2011,DATASET,cub-200-2011,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16."
cars,car,DATASET,car,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16."
CARS196,cars196,DATASET,cars196,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16."
sop,NIL,DATASET,NIL,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16."
Standford Online Porducts,NIL,DATASET,NIL,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16."
inshop,NIL,DATASET,NIL,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16."
In-Shop cloths retireval,NIL,DATASET,NIL,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16."
vid,NIL,DATASET,NIL,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16."
PKU Vehicle id,NIL,DATASET,NIL,"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16."
CUB200-2011,cub-200-2011,DATASET,cub-200-2011,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.
CUB-200,cub-200-2011,DATASET,cub-200-2011,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.
CARS196,cars196,DATASET,cars196,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.
cars,car,DATASET,car,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.
Stanford Online Products,stanford online products,DATASET,stanford online products,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.
In-shop Clothes Retrieval Benchmark,NIL,DATASET,NIL,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.
PKU VehicleID,vehicleid,DATASET,vehicleid,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.
pku-vehicleid,vehicleid,DATASET,vehicleid,You can print a summary of the results with `python browse_results <log path>`.     ### Datasets:  The method is tested on the following datasets:  * CUB200-2011 (http://www.vision.caltech.edu/visipedia/CUB-200.html) * CARS196 (https://ai.stanford.edu/~jkrause/cars/car_dataset.html) * Stanford Online Products (http://cvgl.stanford.edu/projects/lifted_struct/) * In-shop Clothes Retrieval Benchmark (http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) * PKU VehicleID (https://www.pkuml.org/resources/pku-vehicleid.html)  Assuming your folder is placed in e.g.
cars196,cars196,DATASET,cars196,"`<$datadir/cars196>`, pass `$datadir` as input to `--dataset-dir`, by default."
CUB200-2011,cub-200-2011,DATASET,cub-200-2011,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |..."
cub-200-2011,cub-200-2011,DATASET,cub-200-2011,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |..."
Cars196,cars196,DATASET,cars196,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |..."
cars,car,DATASET,car,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |..."
cars196,cars196,DATASET,cars196,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |..."
Stanford Online Products,stanford online products,DATASET,stanford online products,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |..."
sop,NIL,DATASET,NIL,"To avoid conflicts between the folder structure and our pipeline, please make sure that the datasets  have the following internal structure:  * For CUB200-2011, ``` cub-200-2011 └───images |    └───001.Black_footed_Albatross |           │   Black_Footed_Albatross_0001_796111 |           │   ... |    ... ``` * For Cars196, please download the tar of all images, all bounding boxes, labels for both training and test  [here](http://ai.stanford.edu/~jkrause/cars/car_dataset.html) and unzip them, which is consistent with our dataloader. ``` cars196 └───car_ims |    │016180.jpg |    │   ... |    │016185.jpg └───cars_train |    │08092.jpg |    │   ... |    │08144.jpg └───cars_annos.mat  ```  * For Stanford Online Products: ``` sop └───bicycle_final |   │   111085122871_0.jpg |           ... |..."
In-shop Clothes,NIL,DATASET,NIL,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0)."
PKU Vehicle id,NIL,DATASET,NIL,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0)."
CUB200,cub-200-2011,DATASET,cub-200-2011,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0)."
Cars196,cars196,DATASET,cars196,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0)."
In-Shop Clothes,in-shop,DATASET,in-shop,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0)."
Online Products,stanford online products,DATASET,stanford online products,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0)."
VID,NIL,DATASET,NIL,"└───cabinet_final |   │   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0)."
BIOSSES,biosses,DATASET,biosses,"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g."
biosses,biosses,DATASET,biosses,"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g."
mteb/sts12-sts,mteb,DATASET,mteb,"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g."
mteb/sts13-sts,mteb,DATASET,mteb,"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g."
mteb/sts14-sts,mteb,DATASET,mteb,"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g."
mteb/sts15-sts,mteb,DATASET,mteb,"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g."
mteb/sts16-sts,mteb,DATASET,mteb,"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g."
BIOSSES,biosses,DATASET,biosses,"- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts)."
biosses,biosses,DATASET,biosses,"- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts)."
mteb/sts12-sts,mteb,DATASET,mteb,"- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts)."
mteb/sts13-sts,mteb,DATASET,mteb,"- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts)."
mteb/sts14-sts,mteb,DATASET,mteb,"- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts)."
mteb/sts15-sts,mteb,DATASET,mteb,"- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts)."
mteb/sts16-sts,mteb,DATASET,mteb,"- While the ontological knowledge infusion process proceeds, the selected embedding-LLM is  evaluated at regular intervals: evaluation is performed against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts)."
FewEvent,fewevent,DATASET,fewevent,"# Low_Resource_KBP knowledge graph population in low resource conditions   The file ""*Few-Shot_ED.json.zip*"" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[""Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection""](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)"
ACE-2005 corpus,NIL,DATASET,NIL,"# Low_Resource_KBP knowledge graph population in low resource conditions   The file ""*Few-Shot_ED.json.zip*"" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[""Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection""](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)"
TAC-KBP-2017 Event Track Data,NIL,DATASET,NIL,"# Low_Resource_KBP knowledge graph population in low resource conditions   The file ""*Few-Shot_ED.json.zip*"" is the ***FewEvent*** dataset for the paper accepted by WSDM 2020 ***[""Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection""](https://arxiv.org/abs/1910.11621)***   ## Source of Raw Data * We first scale up the number of event types in existing datasets, including the [ACE-2005 corpus](http://projects.ldc.upenn.edu/ace/), and [TAC-KBP-2017 Event Track Data](https://tac.nist.gov/2017/KBP/Event/index.html)"
NuScenes-QA,nuscenes,DATASET,nuscenes,"- `2023.09.04`  Our NuScenes-QA dataset v1.0 released.  ## :hourglass_flowing_sand: To Do  - [x] Release question & anwswer data - [x] Release visual feature - [x] Release training and testing code  ## :running: Getting Started  ### Data Preparation  We have released our question-answer annotations, please download it from [HERE](https://drive.google.com/drive/folders/1jIkICT23wZWZYPrWCa0x-ubjpClSzOuU?"
nuScenes,nuscenes,DATASET,nuscenes,"As an alternative, you can also download the origin nuScenes dataset from [HERE](https://www.nuscenes.org/download), and extract the object-level features refer to this [LINK](https://mmdetection3d.readthedocs.io/en/v0.16.0/datasets/nuscenes_det.html) with different backbones."
NuScenes-QA,nuscenes,DATASET,nuscenes,"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods."
so-ds-feb20,NIL,DATASET,NIL,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded."
so-ds-feb20,NIL,DATASET,NIL,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded."
staqc-py,staqc,DATASET,staqc,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded."
staqc-py-cleaned,staqc,DATASET,staqc,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded."
so-ds-feb20,NIL,DATASET,NIL,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded."
so-python-question-titles-feb20,NIL,DATASET,NIL,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded."
so-duplicates-pacsv1-train,NIL,DATASET,NIL,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded."
staqc-py-cleaned,NIL,DATASET,NIL,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded."
so-duplicates-pacs-train,NIL,DATASET,NIL,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded."
so-ds-feb20,NIL,DATASET,NIL,"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded."
staqc-py-cleaned,staqc,DATASET,staqc,"The staqc-py-cleaned, conala-curated, and codesearchnet collections are derived from existing datasets."
conala-curated,conala,DATASET,conala,"The staqc-py-cleaned, conala-curated, and codesearchnet collections are derived from existing datasets."
staqc-py,staqc,DATASET,staqc,"For staqc-py and conala-curated we did some additional processing, for the codesearchnet collections we merely load the original dataset in a format that is consistent with our code."
conala-curated,conala,DATASET,conala,"For staqc-py and conala-curated we did some additional processing, for the codesearchnet collections we merely load the original dataset in a format that is consistent with our code."
so-ds-feb20,NIL,DATASET,NIL,| name                                          | description                                                                                                                  | |-----------------------------------------------|------------------------------------------------------------------------------------------------------------------------------| | so-ds-feb20                                   | Mined from Python Stack Overflow posts related to data science.
staqc-py-cleaned,staqc,DATASET,staqc,"Stack Overflow dumps can be found here: https://archive.org/details/stackexchange, [LICENSE](https://creativecommons.org/licenses/by-sa/4.0/)                                                             | | staqc-py-cleaned                     | Derived from the Python StaQC snippets (additional cleaning was done as decribed in the paper)."
StackOverflow,NIL,DATASET,NIL,"See https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset, [LICENSE](https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset/blob/master/LICENSE.txt)                               | | conala-curated                                | Derived from the curated snippets of the CoNaLa benchmark."
conala-curated,conala,DATASET,conala,"See https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset, [LICENSE](https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset/blob/master/LICENSE.txt)                               | | conala-curated                                | Derived from the curated snippets of the CoNaLa benchmark."
CoNaLa,conala,DATASET,conala,"See https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset, [LICENSE](https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset/blob/master/LICENSE.txt)                               | | conala-curated                                | Derived from the curated snippets of the CoNaLa benchmark."
conala,conala,DATASET,conala,"See https://conala-corpus.github.io/ , [LICENSE](https://creativecommons.org/licenses/by-sa/4.0/)                                                                                         | | codesearchnet-{language}-{train\|valid\|test} | The CodeSearchNet snippet collections used for training/MRR validation/MRR testing."
codesearchnet,codesearchnet,DATASET,codesearchnet,"See https://conala-corpus.github.io/ , [LICENSE](https://creativecommons.org/licenses/by-sa/4.0/)                                                                                         | | codesearchnet-{language}-{train\|valid\|test} | The CodeSearchNet snippet collections used for training/MRR validation/MRR testing."
CodeSearchNet,codesearchnet,DATASET,codesearchnet,"See https://conala-corpus.github.io/ , [LICENSE](https://creativecommons.org/licenses/by-sa/4.0/)                                                                                         | | codesearchnet-{language}-{train\|valid\|test} | The CodeSearchNet snippet collections used for training/MRR validation/MRR testing."
CodeSearchNet,codesearchnet,DATASET,codesearchnet,See https://github.com/github/CodeSearchNet.
codesearchnet,codesearchnet,DATASET,codesearchnet,| | codesearchnet-{language}                      | The CodeSearchNet snippet collections used for the weights and biases benchmark.
CodeSearchNet,codesearchnet,DATASET,codesearchnet,| | codesearchnet-{language}                      | The CodeSearchNet snippet collections used for the weights and biases benchmark.
CodeSearchNet,codesearchnet,DATASET,codesearchnet,See https://github.com/github/CodeSearchNet.
so-ds-feb20,NIL,DATASET,NIL,"**Note**: not all of these snippets have descriptions |  ### Evaluation data Evaluation datasets link queries to relevant snippets in one of the above snippet collections.   #### Example: load an evaluation dataset ```python from codesearch.data import load_eval_dataset queries, query2ids = load_eval_dataset(""so-ds-feb20-valid"") ```  #### Available evaluation datasets | name                           | description                                                                     | |--------------------------------|---------------------------------------------------------------------------------| | so-ds-feb20-{valid\|test}      | Queries paired to relevant snippets in the so-ds-feb20 snippet collection"
so-ds-feb20,NIL,DATASET,NIL,"**Note**: not all of these snippets have descriptions |  ### Evaluation data Evaluation datasets link queries to relevant snippets in one of the above snippet collections.   #### Example: load an evaluation dataset ```python from codesearch.data import load_eval_dataset queries, query2ids = load_eval_dataset(""so-ds-feb20-valid"") ```  #### Available evaluation datasets | name                           | description                                                                     | |--------------------------------|---------------------------------------------------------------------------------| | so-ds-feb20-{valid\|test}      | Queries paired to relevant snippets in the so-ds-feb20 snippet collection"
staqc-py-cleaned,NIL,DATASET,NIL,| | staqc-py-cleaned-{valid\|test} | Queries paired to relevant snippets in the staqc-py-cleaned snippet collection. | | conala-curated-0.5-test        | Queries paired to relevant snippets in the CoNaLa benchmark                     |   It is also possible to load a snippet collection as evaluation data.
staqc-py-cleaned,NIL,DATASET,NIL,| | staqc-py-cleaned-{valid\|test} | Queries paired to relevant snippets in the staqc-py-cleaned snippet collection. | | conala-curated-0.5-test        | Queries paired to relevant snippets in the CoNaLa benchmark                     |   It is also possible to load a snippet collection as evaluation data.
conala-curated-0.5-test,NIL,DATASET,NIL,| | staqc-py-cleaned-{valid\|test} | Queries paired to relevant snippets in the staqc-py-cleaned snippet collection. | | conala-curated-0.5-test        | Queries paired to relevant snippets in the CoNaLa benchmark                     |   It is also possible to load a snippet collection as evaluation data.
CoNaLa,conala,DATASET,conala,| | staqc-py-cleaned-{valid\|test} | Queries paired to relevant snippets in the staqc-py-cleaned snippet collection. | | conala-curated-0.5-test        | Queries paired to relevant snippets in the CoNaLa benchmark                     |   It is also possible to load a snippet collection as evaluation data.
so-ds-feb20,NIL,DATASET,NIL,"To download and load the title pairs from Stack Overflow duplicate posts run:  ```python from codesearch.data import load_train_dataset duplicate_records = load_train_dataset(""so-duplicates-pacs-train"") ```  These duplicate records have been filtered to ensure that there is no overlap with the `so-ds-feb20` and `staqc-py` evaluation datasets."
staqc-py,staqc,DATASET,staqc,"To download and load the title pairs from Stack Overflow duplicate posts run:  ```python from codesearch.data import load_train_dataset duplicate_records = load_train_dataset(""so-duplicates-pacs-train"") ```  These duplicate records have been filtered to ensure that there is no overlap with the `so-ds-feb20` and `staqc-py` evaluation datasets."
PACS,pacs,DATASET,pacs,"To download a text file with Stack Overflow post titles tagged with Python (used for the TNBOW baseline) run:   ```python from codesearch.data import load_train_dataset filename = load_train_dataset(""so-python-question-titles-feb20"") ```  ## Demo notebook   You can run the demo notebook `nbs/demo/demo.ipynb` to quickly try out any of the pretrained models on one of the snippet collections.  ## Benchmark on PACS  To replicate the results of our paper or evaluate your own model on the PACS benchmark, have a look at `nbs/evaluate.ipynb` and `codesearch/benchmark.ipynb`."
PACS,pacs,DATASET,pacs,"To download a text file with Stack Overflow post titles tagged with Python (used for the TNBOW baseline) run:   ```python from codesearch.data import load_train_dataset filename = load_train_dataset(""so-python-question-titles-feb20"") ```  ## Demo notebook   You can run the demo notebook `nbs/demo/demo.ipynb` to quickly try out any of the pretrained models on one of the snippet collections.  ## Benchmark on PACS  To replicate the results of our paper or evaluate your own model on the PACS benchmark, have a look at `nbs/evaluate.ipynb` and `codesearch/benchmark.ipynb`."
PACS,pacs,DATASET,pacs,"A custom embedding model class should implement the `embed_snippets` and `embed_queries` functions (similar to `codesearch/tuse/tuse_embedder.py`, `codesearch/tnbow/tnbow_embedder.py`, `codesearch/ncs/ncs_embedder.py` etc.).  #### Example: Benchmark a model on PACS  ```python from codesearch.benchmark import benchmark_on_pacs  benchmark_on_pacs(     model_path=model_path, # one of the pretrained model names or a path to a model that can be loaded with `codesearch.utils.load_model`     output_dir=output_dir ) ```"
pacs,pacs,DATASET,pacs,"A custom embedding model class should implement the `embed_snippets` and `embed_queries` functions (similar to `codesearch/tuse/tuse_embedder.py`, `codesearch/tnbow/tnbow_embedder.py`, `codesearch/ncs/ncs_embedder.py` etc.).  #### Example: Benchmark a model on PACS  ```python from codesearch.benchmark import benchmark_on_pacs  benchmark_on_pacs(     model_path=model_path, # one of the pretrained model names or a path to a model that can be loaded with `codesearch.utils.load_model`     output_dir=output_dir ) ```"
pacs,pacs,DATASET,pacs,"A custom embedding model class should implement the `embed_snippets` and `embed_queries` functions (similar to `codesearch/tuse/tuse_embedder.py`, `codesearch/tnbow/tnbow_embedder.py`, `codesearch/ncs/ncs_embedder.py` etc.).  #### Example: Benchmark a model on PACS  ```python from codesearch.benchmark import benchmark_on_pacs  benchmark_on_pacs(     model_path=model_path, # one of the pretrained model names or a path to a model that can be loaded with `codesearch.utils.load_model`     output_dir=output_dir ) ```"
Movielens-1M,movielens,DATASET,movielens,Run experiments This is one example of reproducing results for Movielens-1M with BPR.
Amazon-Book,amazon-book,DATASET,amazon-book,||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.
Yelp-2018,yelp2018,DATASET,yelp2018,||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.
Gowalla,gowalla,DATASET,gowalla,||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.
MovieLens-1M,movielens,DATASET,movielens,||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.
iSQUAD,NIL,DATASET,NIL,"Run experiments in the shell of the Docker container following the usage table as follows.   ### Usage |Algorithm|Usage| |---|---| |DejaVu|Run for dataset A1: `python exp/run_GAT_node_classification.py -H=4 -L=8 -fe=GRU -bal=True --data_dir=data/A1`| |JSS'20|Run for dataset A1: `python exp/DejaVu/run_JSS20.py --data_dir=data/A1`| |iSQUAD|Run for dataset A1: `python exp/DejaVu/run_iSQ.py --data_dir=data/A1`| |Decision Tree|Run for dataset A1: `python exp/run_DT_node_classification.py --data_dir=data/A1`| |RandomWalk@Metric|Run for dataset A1: `python exp/DejaVu/run_random_walk_single_metric.py --data_dir=data/A1 --window_size 60 10 --score_aggregation_method=min`| |RandomWalk@FI|Run for dataset A1: `python exp/DejaVu/run_random_walk_failure_instance.py --data_dir=data/A1 --window_size 60 10 --anomaly_score_aggregation_method=min --corr_aggregation_method=max`| |Global interpretation|Run `notebooks/explain.py` as a jupyter notebook with `jupytext`| |Local interpretation|`DejaVu/explanability/similar_faults.py`|  The commands would print a `one-line summary` in the end, including the following fields: `A@1`, `A@2`, `A@3`, `A@5`, `MAR`, `Time`, `Epoch`, `Valid Epoch`, `output_dir`, `val_loss`, `val_MAR`, `val_A@1`, `command`, `git_commit_url`, which are the desrired results."
Google Speech Commands,speech commands,DATASET,speech commands,"Interspeech 2021},   pages={4249--4253},   doi={10.21437/Interspeech.2021-1286} } ```  ## Setup  ### Download Google Speech Commands  There are two versions of the dataset, V1 and V2."
speech_commands_v0.02,speech commands,DATASET,speech commands,"To download and extract dataset V2, run:  ```shell wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz mkdir data2 mv ."
speech_commands_v0.02,speech commands,DATASET,speech commands,/speech_commands_v0.02.tar.gz .
speech_commands_v0.02,speech commands,DATASET,speech commands,/speech_commands_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz mkdir data1 mv .
wget,NIL,DATASET,NIL,/speech_commands_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz mkdir data1 mv .
speech_commands_v0.01,speech commands,DATASET,speech commands,/speech_commands_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz mkdir data1 mv .
speech_commands_v0.01,speech commands,DATASET,speech commands,/speech_commands_v0.01.tar.gz .
speech_commands_v0.01,speech commands,DATASET,speech commands,/speech_commands_v0.01.tar.gz cd ../ ```  ### Install dependencies  Set up a new virtual environment:  ```shell pip install virtualenv virtualenv --system-site-packages -p python3 .
Speech Commands V2,speech commands,DATASET,speech commands,"There are three variants of the Keyword-Transformer model:  * **Time-domain attention**: each time-window is treated as a patch, self-attention is computed between time-windows * **Frequency-domain attention**: each frequency is treated as a patch self-attention is computed between frequencies * **Combination of both**: The signal is fed into both a time- and a frequency-domain transformer and the outputs are combined * **Patch-wise attention**: Similar to the vision transformer, it extracts rectangular patches from the spectrogram, so attention happens both in the time and frequency domain simultaneously.  ## Training a model from scratch To train KWT-3 from scratch on Speech Commands V2, run    ```shell sh train.sh ```  Please note that the train directory (given by the argument  `--train_dir`) cannot exist prior to start script."
Google Speech Commands v2,google speech commands - musan,DATASET,google speech commands - musan,"|Model name|embedding dim|mlp-dim|heads|depth|#params|V2-12 accuracy|pre-trained| |:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:| |KWT-1|64|128|1|12|607K|97.7|[here](models_data_v2_12_labels/kwt1)| |KWT-2|128|256|2|12|2.4M|98.2|[here](models_data_v2_12_labels/kwt2)| |KWT-3|192|768|3|12|5.5M|98.7|[here](models_data_v2_12_labels/kwt3)|  To perform inference on Google Speech Commands v2 with 12 labels, run  ```shell sh eval.sh ```  ## Acknowledgements  The code heavily borrows from the [KWS streaming work](https://github.com/google-research/google-research/tree/master/kws_streaming) by Google Research."
WIDER FACE,wider,DATASET,wider,"The naming convention we follow for the pruned models is straightforward; for instance, `ERES10_FPGM` refers to the EResFD model pruned with 10% sparsity using the FPGM technique. - `torchscript/`: All the required files for android deployment of the EResFD model (and its pruned versions) using the torchscript framework.  ## Prerequisites  Before running the pruning scripts, the user needs to prepare the necessary dataset:  ### WIDER FACE Dataset  The models are trained and evaluated using the WIDER FACE dataset."
WIDER FACE,wider,DATASET,wider,Download the WIDER FACE dataset from [here](https://shuoyang1213.me/WIDERFACE/). 2.
WIDERFACE,wider,DATASET,wider,Download the WIDER FACE dataset from [here](https://shuoyang1213.me/WIDERFACE/). 2.
MaSTr1325,NIL,DATASET,NIL,Download the [MaSTr1325 dataset](https://box.vicos.si/borja/viamaro/index.html) and corresponding [weak annotations](https://github.com/lojzezust/SLR/releases/download/weights_v2/mastr_slr.zip).
MaSTr1325,NIL,DATASET,NIL,All models are trained on the MaSTr1325 dataset using SLR and weak annotations and evaluated on the [MODS benchmark](https://github.com/bborja/mods_evaluation).
RELISH,NIL,DATASET,NIL,"/aspire-biencoder-compsci-spec-full'  # Load hyperparameters from disk. with codecs.open(os.path.join(model_path, 'run_info.json'), 'r') as fp:     hparams = json.load(fp)     model_hparams = hparams['all_hparams']  # Initialize the tokenizer and model. aspire_tok = AutoTokenizer.from_pretrained(model_hparams['base-pt-layer']) aspire_bienc = AspireBiEnc(model_hparams)  # Load model parameters from disk. model_fname = os.path.join(model_path, 'model_cur_best.pt') aspire_bienc.load_state_dict(torch.load(model_fname))  # Encode example input. title = ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific ""         ""Document Similarity"" abstract = ""We present a new scientific document similarity model based on matching ""            ""fine-grained aspects of texts."" d = [title + aspire_tok.sep_token + abstract]  inputs = aspire_tok(d, padding=True, truncation=True, return_tensors=""pt"", max_length=512) clsrep = aspire_bienc.forward(inputs)  ```   #### Evaluation Datasets <a name=""evaldata""></a>  The paper uses the following evaluation datasets:  - RELISH was created in [Brown et al. 2019](https://academic.oup.com/database/article/doi/10.1093/database/baz085/5608006?"
TRECCOVID,NIL,DATASET,NIL,- TRECCOVID presents an ad-hoc search dataset.
CORD-19,cord-19,DATASET,cord-19,"The versions of the dataset used may be accessed here: [query topics](https://ir.nist.gov/covidSubmit/data/topics-rnd5.xml), [relevance annotations](https://ir.nist.gov/covidSubmit/data/qrels-covid_d5_j0.5-5.txt), and the metadata for papers is obtained from the [CORD-19](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html) dataset in the [2021-06-21](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2021-06-21/metadata.csv) release."
treccovid,NIL,DATASET,NIL,"The function `get_qbe_pools` in `pre_proc_treccovid.py`, converts the dataset in its original form to the reformulated form, TRECCOVID-RF, used in the paper."
TRECCOVID,NIL,DATASET,NIL,"The function `get_qbe_pools` in `pre_proc_treccovid.py`, converts the dataset in its original form to the reformulated form, TRECCOVID-RF, used in the paper."
treccovid,NIL,DATASET,NIL,Dataset splits are created in `pre_proc_treccovid.py`
SciDocs,scidocs,DATASET,scidocs,- SciDocs is obtained from: [link](https://github.com/allenai/scidocs).
scidocs,scidocs,DATASET,scidocs,- SciDocs is obtained from: [link](https://github.com/allenai/scidocs).
CSFCube,csfcube,DATASET,csfcube,- CSFCube is obtained from: [link](https://github.com/iesl/CSFCube).
CSFCube,csfcube,DATASET,csfcube,- CSFCube is obtained from: [link](https://github.com/iesl/CSFCube).
S2ORC,s2orc,DATASET,s2orc,"Complete evaluation datasets used in the paper can be downloaded here: [`datasets/datasets.md`](https://github.com/allenai/aspire/blob/main/datasets/datasets.md)   ### Repository Contents <a name=""repocontents""></a>      ├── bin     ├── config     │             └── models_config     │                 ├── s2orcbiomed     │                 ├── s2orccompsci     │                 └── s2orcscidocs     ├── scripts     └── src         ├── evaluation         │             ├── utils         │             │             ├── datasets.py         │             │             ├── metrics.py         │             │             ├── models.py         │             │             └── utils.py         │             └── evaluate.py         ├── learning         │             ├── facetid_models         │             │             ├── disent_models.py         │             │             ├── pair_distances.py         │             │             └── sentsim_models.py         │             ├── main_fsim.py         │             ├── batchers.py         │             └── trainer.py         └── pre_process             ├── extract_entities.py             ├── pp_settings.py             ├── pre_proc_cocits.py             ├── pre_proc_gorc.py             ├── pre_proc_relish.py             ├── pre_proc_scidocs.py             ├── pre_proc_treccovid.py             ├── pp_gen_nearest.py             └── pre_proc_buildreps.py   **The repository is organized broadly as:**  `src/pre_process/`: Scripts to 1) generate gather and filter co-citations data from the [S2ORC](https://github.com/allenai/s2orc) corpus 2) generate training examples with co-citation data 3) pre-process the evaluation datasets into apt formats for use with models 4) extract NER entities from datasets."
s2orc,s2orc,DATASET,s2orc,"Complete evaluation datasets used in the paper can be downloaded here: [`datasets/datasets.md`](https://github.com/allenai/aspire/blob/main/datasets/datasets.md)   ### Repository Contents <a name=""repocontents""></a>      ├── bin     ├── config     │             └── models_config     │                 ├── s2orcbiomed     │                 ├── s2orccompsci     │                 └── s2orcscidocs     ├── scripts     └── src         ├── evaluation         │             ├── utils         │             │             ├── datasets.py         │             │             ├── metrics.py         │             │             ├── models.py         │             │             └── utils.py         │             └── evaluate.py         ├── learning         │             ├── facetid_models         │             │             ├── disent_models.py         │             │             ├── pair_distances.py         │             │             └── sentsim_models.py         │             ├── main_fsim.py         │             ├── batchers.py         │             └── trainer.py         └── pre_process             ├── extract_entities.py             ├── pp_settings.py             ├── pre_proc_cocits.py             ├── pre_proc_gorc.py             ├── pre_proc_relish.py             ├── pre_proc_scidocs.py             ├── pre_proc_treccovid.py             ├── pp_gen_nearest.py             └── pre_proc_buildreps.py   **The repository is organized broadly as:**  `src/pre_process/`: Scripts to 1) generate gather and filter co-citations data from the [S2ORC](https://github.com/allenai/s2orc) corpus 2) generate training examples with co-citation data 3) pre-process the evaluation datasets into apt formats for use with models 4) extract NER entities from datasets."
RELISH,NIL,DATASET,NIL,"Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain."
TRECCOVID,NIL,DATASET,NIL,"Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain."
CSFCube,csfcube,DATASET,csfcube,"Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain."
SciDocs,scidocs,DATASET,scidocs,"Since we evaluate on datasets in the Biomedical (RELISH, TRECCOVID-RF), Computer Science (CSFCube), and mixed domains (SciDocs) we train separate models for these domains, the sub-directories named `s2orcbiomed`, `s2orccompsci`, and `s2orcscidocs` contain config files for the models trained for each domain."
S2ORC,s2orc,DATASET,s2orc,"`src/pre_process/pre_proc_gorc.py:` Code to gather full text articles from the [S2ORC](https://github.com/allenai/s2orc) corpus, exclude noisy data, and gather co-citations for different domains used in the paper (biomedical papers and computer science papers)."
s2orc,s2orc,DATASET,s2orc,"`src/pre_process/pre_proc_gorc.py:` Code to gather full text articles from the [S2ORC](https://github.com/allenai/s2orc) corpus, exclude noisy data, and gather co-citations for different domains used in the paper (biomedical papers and computer science papers)."
S2ORC,s2orc,DATASET,s2orc,This code assumes the 2019-09-28 release of S2ORC.
relish,NIL,DATASET,NIL,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts."
scidocs,scidocs,DATASET,scidocs,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts."
treccovid,NIL,DATASET,NIL,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts."
RELISH,NIL,DATASET,NIL,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts."
TRECCOVID,NIL,DATASET,NIL,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts."
SciDocs,scidocs,DATASET,scidocs,"`src/pre_process/pre_proc_{relish/scidocs/treccovid}.py`: Pre-process the evaluation datasets (RELISH, TRECCOVID, and SciDocs) into a format consumed by trained models and evaluation scripts."
CSFCube,csfcube,DATASET,csfcube,CSFCube data format matches the assumed format.
Semantic Scholar Open Research Corpus,s2orc,DATASET,s2orc,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1."
s2orc,s2orc,DATASET,s2orc,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1."
S2ORC,s2orc,DATASET,s2orc,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1."
RELISH,NIL,DATASET,NIL,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1."
TRECCOVID,s2orc,DATASET,s2orc,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1."
SciDocs,scidocs,DATASET,scidocs,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1."
CSFCube,csfcube,DATASET,csfcube,"For info on how to run this file see src/pre_process/README_NER     <div style=""margin-left: auto;             margin-right: auto;             width: 95%"">  | Model name in paper         | Config under `config/models_config/{<domain>}`  | Model class in code   | |-----------------------------|:-------------------------:|:-----------:| | cosentbert              |        `cosentbert`        |  `facetid_models.sentsim_models.SentBERTWrapper` | | ICTSentBert              |        `ictsentbert`        |  `facetid_models.sentsim_models.ICTBERTWrapper` | | SPECTER-CoCite              |        `hparam_opt/cospecter-best`/`hparam_opt/cospecter-specinit-best`        |  `facetid_models.disent_models.MySPECTER`  | | tsAspire                    |        `hparam_opt/sbalisentbienc-sup-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | otAspire                    |        `hparam_opt/miswordbienc-otstuni-best`        |      `facetid_models.disent_models.WordSentAlignBiEnc`   | | ts+otAspire                 |        `hparam_opt/sbalisentbienc-otuni-best`        |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | maxAspire                 |          `hparam_opt/miswordbienc-l2max-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc` | | absAspire                 |          `hparam_opt/sbalisentbienc-sup-absali-best`      |        `facetid_models.disent_models.WordSentAbsSupAlignBiEnc`   | | attAspire                 |          `hparam_opt/miswordbienc-cdatt-best`      |        `facetid_models.disent_models.WordSentAlignBiEnc`   |  </div>   ### Acknowledgements <a name=""acks""></a>  This work relies on: (1) Data from the [Semantic Scholar Open Research Corpus](https://github.com/allenai/s2orc) (S2ORC) and the evaluation datasets RELISH (kindly shared by [Mariana Neves](https://mariananeves.github.io/)), TRECCOVID, SciDocs, and CSFCube linked above. (2) The pre-trained models of [SPECTER](https://github.com/allenai/specter). (3) The software packages: [GeomLoss](https://www.kernel-operations.io/geomloss/index.html) and [sentence-transformers](https://www.sbert.net/).   ### Citation <a name=""citation""></a>  Please cite the [Aspire paper](https://arxiv.org/pdf/2004.07180.pdf) as:    ```bibtex @misc{mysore2021aspire,       title={Multi-Vector Models with Textual Guidance for Fine-Grained Scientific Document Similarity},        author={Sheshera Mysore and Arman Cohan and Tom Hope},       year={2021},       eprint={2111.08366},       archivePrefix={arXiv},       primaryClass={cs.CL} } ```   ### TODOs <a name=""todos""></a>  1."
