sentence,entity_text,entity_type
Metrics --------  Given the artificial data generated in the previous step it is now possible to measure the internal MSE and SSMI.,MSE,EVALMETRIC
Metrics --------  Given the artificial data generated in the previous step it is now possible to measure the internal MSE and SSMI.,SSMI,EVALMETRIC
"Moreover, the MSE and SSMI will be both printed on terminal and saved on a CSV file in the `metrics` folder.",MSE,EVALMETRIC
"Moreover, the MSE and SSMI will be both printed on terminal and saved on a CSV file in the `metrics` folder.",SSMI,EVALMETRIC
"The command will be similar to this one:  ``` python test.py --arch=""lenet"" --type=""accuracy"" --batch=30000 --resume="".",accuracy,EVALMETRIC
"/results/cae_ep10_wdecay0.0001_units16/gendata/132252_27032019""  ```  The command will print the loss and accuracy on this dataset.",accuracy,EVALMETRIC
"We also quantify the contribution of each modality and data source using Shapley values, which demonstrates the heterogeneity in data modality importance and the necessity of multimodal inputs across different healthcare-relevant tasks.",Shapley values,EVALMETRIC
"To run our code without them just comment import and usage of these notes.  2 - Modeling of our three tasks: mortality prediction, length of stay prediction, chest pathology classification  3 - Result Generating: Including reporting of the AUROC, AUPRC, F1 scores, as well as code to generate the plots reported in the paper.",AUROC,EVALMETRIC
"To run our code without them just comment import and usage of these notes.  2 - Modeling of our three tasks: mortality prediction, length of stay prediction, chest pathology classification  3 - Result Generating: Including reporting of the AUROC, AUPRC, F1 scores, as well as code to generate the plots reported in the paper.",AUPRC,EVALMETRIC
"To run our code without them just comment import and usage of these notes.  2 - Modeling of our three tasks: mortality prediction, length of stay prediction, chest pathology classification  3 - Result Generating: Including reporting of the AUROC, AUPRC, F1 scores, as well as code to generate the plots reported in the paper.",F1 scores,EVALMETRIC
</strong> We report the test macro F1-score (mean Â± stdev for a 5-fold CV) with optimized parameters.,macro F1-score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",accuracy,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",accuracy_score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",f1_score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",f1_score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",roc_auc,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",roc_auc_score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",Accuracy,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",accuracy,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",F1 Score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",f1_score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",ROC AUC,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",roc_auc,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",accuracy,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",accuracy_score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",f1_score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",f1_score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",roc_auc,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",roc_auc_score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",Accuracy,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",accuracy,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",F1 Score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",f1_score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",ROC AUC,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",roc_auc,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",mean_absolute_error,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",mean_absolute_error,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",r2_score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",r2_score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",MAE,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",mean_absolute_error,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",R2 Score,EVALMETRIC
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",r2_score,EVALMETRIC
"In our experiments on TACRED we attain 63{\%} F1 zero-shot, 69{\%} with 16 examples per relation (17{\%} points better than the best supervised system on the same conditions), and only 4 points short to the state-of-the-art (which uses 20 times more training data).",F1,EVALMETRIC
"* Benchmark Results:      * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |     | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |     | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 Ã RTX 3090    | 17.25 hours |     $17.25     |     | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 Ã A100      |  5.5 hours  |    $ 22.55     |     | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 Ã A100      |  5.5 hours  |    $ 22.55     |     | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 Ã RTX 3090    | 6.47 hours  |     $ 6.47     |     | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 Ã RTX 3090    | 4.15 hours  |     $ 4.15     |     | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |     | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |     | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 Ã NVIDIA K80 GPU |      -      |       -        |     | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 Ã A100     |   21 days   | $ 4.23 million |     | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 Ã A100     |   53 days   | $ 2.67 million |        **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs.",Weighted F1,EVALMETRIC
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| Worldâs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",accuracy,EVALMETRIC
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| Worldâs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",Weighted F1,EVALMETRIC
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| Worldâs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",Acc,EVALMETRIC
"---  **(Fast and accurate) Sentiment Analysis**     GPT-3 can help study customer surveys, social media tweets from customers/users.",accurate,EVALMETRIC
"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1.",Accuracy,EVALMETRIC
"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1.",Accuracy,EVALMETRIC
"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1.",Accuracy,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",TP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",TP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",FP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",FN,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",TP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",FP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",FN,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",TP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",TP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",FP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",FN,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",TP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",FP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",FN,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",IoU,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",TP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",TP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",FP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",FN,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",TP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",FP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",FN,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",IoU,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",F1,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",F1,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",F1,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",TP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",F1,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",TP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",TP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",F1,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",TP,EVALMETRIC
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",TP,EVALMETRIC
Edit `IMAGENET_ROOT` in `example_imagenet_eval.py` to match the location on your machine. ``` python example_imagenet_eval.py ``` For DiHT-L/14@336 the output should look like: ``` ImageNet1K acc@1 for diht_vitl14_336px: 77.9 ```  ## Example retrieval zero-shot evaluation  A simple retrieval zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download COCO and Flickr30K datasets from the original websites.,acc@1,EVALMETRIC
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",r@1,EVALMETRIC
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",r@1,EVALMETRIC
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",r@1,EVALMETRIC
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",r@1,EVALMETRIC
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",Accuracy@1,EVALMETRIC
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",Recall@1,EVALMETRIC
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",Recall@1,EVALMETRIC
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",Recall@1,EVALMETRIC
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",Recall@1,EVALMETRIC
"Furthermore, we introduce an adapted metric POKS complying with the concept of keypoint-guided polylines.",POKS,EVALMETRIC
"In our experimental evaluation, we provide baseline results for our newly introduced dataset while showcasing the benefits of POKS over OKS.",POKS,EVALMETRIC
"In our experimental evaluation, we provide baseline results for our newly introduced dataset while showcasing the benefits of POKS over OKS.",OKS,EVALMETRIC
|    name  | dataset  | backbone |  mAP  | top-1 |  mAP+ | top-1+  | download| | :-------------: | :-----: | :-----: | :-------------------: | :-----: | :-----: | :------: | :-----------------: | |     PSTR | PRW    | PVTv2-B2  |   57.46  |   90.57   |58.07   |    92.03     |          [model](https://drive.google.com/file/d/1hrmyvS9f8fzflpoIlEhWQ-XDyNp_qCGq/view?,mAP,EVALMETRIC
"Totally, the main experiment commands of DejaVu should output as follows: - FDG message, including the data paths, edge types, the number of nodes (failure units), the number of metrics, the metrics of each failure class. - Traning setup message: the faults used for training, validation and testing. - Model architecture: model parameters in each part, total params - Training process: the training/validation/testing loss and accuracy - Time Report. - command output one-line summary.  ### Example See https://github.com/NetManAIOps/DejaVu/issues/4  ## Datasets  The datasets A, B, C, D are public at : - https://www.dropbox.com/sh/ist4ojr03e2oeuw/AAD5NkpAFg1nOI2Ttug3h2qja?",accuracy,EVALMETRIC
"Then for two similar failures which occur at $v_1$ and $v_2$ respectively, their feature vectors are $(1, 0, 0, 0)$ and $(0, 1, 0, 0)$ respectively, which are dissimilar with respect to common similarity metrics (e.g., Manhattan or Euclidean).",Manhattan,EVALMETRIC
"Then for two similar failures which occur at $v_1$ and $v_2$ respectively, their feature vectors are $(1, 0, 0, 0)$ and $(0, 1, 0, 0)$ respectively, which are dissimilar with respect to common similarity metrics (e.g., Manhattan or Euclidean).",Euclidean,EVALMETRIC
"[RSSI](https://github.com/dimisik/BLEBeacon-Dataset/blob/master/images/ARCH.png) * RSSI Report: all advertisement packet receptions from beacon devices are directly reported to a server with a message that contains the beacon/user ID, the packet's Received Signal Strength Indicator (RSSI), a reception timestamp, and finally the ID of the RPi that received the advertisement (Fig. 1).   !",RSSI,EVALMETRIC
"[RSSI](https://github.com/dimisik/BLEBeacon-Dataset/blob/master/images/ARCH.png) * RSSI Report: all advertisement packet receptions from beacon devices are directly reported to a server with a message that contains the beacon/user ID, the packet's Received Signal Strength Indicator (RSSI), a reception timestamp, and finally the ID of the RPi that received the advertisement (Fig. 1).   !",RSSI,EVALMETRIC
"[RSSI](https://github.com/dimisik/BLEBeacon-Dataset/blob/master/images/ARCH.png) * RSSI Report: all advertisement packet receptions from beacon devices are directly reported to a server with a message that contains the beacon/user ID, the packet's Received Signal Strength Indicator (RSSI), a reception timestamp, and finally the ID of the RPi that received the advertisement (Fig. 1).   !",Received Signal Strength Indicator,EVALMETRIC
"[RSSI](https://github.com/dimisik/BLEBeacon-Dataset/blob/master/images/ARCH.png) * RSSI Report: all advertisement packet receptions from beacon devices are directly reported to a server with a message that contains the beacon/user ID, the packet's Received Signal Strength Indicator (RSSI), a reception timestamp, and finally the ID of the RPi that received the advertisement (Fig. 1).   !",RSSI,EVALMETRIC
"A thirty-second period is used to ensure that the occupant exited the RPi proximity.   ## Dataset  The BLEBeacon dataset contains two files, one with the trial readings from the RSSI report operation (RSSI Report file) and the other from the Check-In/Check-Out report operation (Check-In Check-Out Report file).",RSSI,EVALMETRIC
"A thirty-second period is used to ensure that the occupant exited the RPi proximity.   ## Dataset  The BLEBeacon dataset contains two files, one with the trial readings from the RSSI report operation (RSSI Report file) and the other from the Check-In/Check-Out report operation (Check-In Check-Out Report file).",RSSI,EVALMETRIC
The RSSI Report file contains the following entries: * Entry_id: unique identifier of a packet in the dataset. * Beacon_id: unique identifier of the occupant/beacon. * RSSI: the Received Signal Strength Indicator (RSSI) in dB. * Timestamp: Date (Month/Day/Year) and Unix time (Hour:Second) of the advertisement packet reception moment from the Rpi. * RPi_id: RPi that received the packet.,RSSI,EVALMETRIC
The RSSI Report file contains the following entries: * Entry_id: unique identifier of a packet in the dataset. * Beacon_id: unique identifier of the occupant/beacon. * RSSI: the Received Signal Strength Indicator (RSSI) in dB. * Timestamp: Date (Month/Day/Year) and Unix time (Hour:Second) of the advertisement packet reception moment from the Rpi. * RPi_id: RPi that received the packet.,RSSI,EVALMETRIC
The RSSI Report file contains the following entries: * Entry_id: unique identifier of a packet in the dataset. * Beacon_id: unique identifier of the occupant/beacon. * RSSI: the Received Signal Strength Indicator (RSSI) in dB. * Timestamp: Date (Month/Day/Year) and Unix time (Hour:Second) of the advertisement packet reception moment from the Rpi. * RPi_id: RPi that received the packet.,Received Signal Strength Indicator,EVALMETRIC
The RSSI Report file contains the following entries: * Entry_id: unique identifier of a packet in the dataset. * Beacon_id: unique identifier of the occupant/beacon. * RSSI: the Received Signal Strength Indicator (RSSI) in dB. * Timestamp: Date (Month/Day/Year) and Unix time (Hour:Second) of the advertisement packet reception moment from the Rpi. * RPi_id: RPi that received the packet.,RSSI,EVALMETRIC
"For image-text retrieval on Flickr30K and MSCOCO, we compute IR@1 and TR@1 for the Recall@1 on image-retrieval (IR) and text-retrieval (TR).",IR@1,EVALMETRIC
"For image-text retrieval on Flickr30K and MSCOCO, we compute IR@1 and TR@1 for the Recall@1 on image-retrieval (IR) and text-retrieval (TR).",TR@1,EVALMETRIC
"For image-text retrieval on Flickr30K and MSCOCO, we compute IR@1 and TR@1 for the Recall@1 on image-retrieval (IR) and text-retrieval (TR).",Recall@1,EVALMETRIC
"For classification tasks, we compute top-1 accuracy (\%).",top-1 accuracy,EVALMETRIC
"|  Model | Dataset  | # S  | Accuracy | | ------------------ |---------------- | -------------- | -------------- | | TnT  |    MNIST        |      600      |     90.87Â±0.31       | | CART  |     MNIST         |      1.1k       |    88.59Â±0.14      | | TnT  |    Connect-4        |      864      |     78.85Â±0.46       | | CART  |     Connect-4         |      931       |    77.23Â±0.01      | | TnT  |    Letter      |      1.2k     |     86.62Â±0.02       | | CART  |     Letter         |      1.3k       |    86.26Â±0.15      | | TnT  |    Optical recognition     |      174     |     86.32Â±0.24       | | CART  |     Optical recognition         |      193       |    85.56Â±0.46      | | TnT  |    Pendigits       |      125     |     92.61Â±0.53       | | CART  |     Pendigits         |    166       |    91.74Â±0.13      | | TnT  |    Protein       |      69      |     57.26       | | CART  |     Protein         |     76       |    55.30      | | TnT  |    SenseIT     |      198    |     80.48Â±0.42       | | CART  |     SenseIT         |      345       |    79.40      | | TnT  |    USPS       |      31      |     88.76Â±1.36       | | CART  |     USPS         |      109       |    87.35Â±0.15      |  ## Citation If you use this code for research, please consider citing our paper: ``` @misc{zhu2021tree,       title={Tree in Tree: from Decision Trees to Decision Graphs},        author={Bingzhao Zhu and Mahsa Shoaran},       year={2021},       eprint={2110.00392},       archivePrefix={arXiv},       primaryClass={cs.LG} }",Accuracy,EVALMETRIC
"**FRNet** achieves an appealing balance between accuracy and efficiency, enabling real-time LiDAR segmentation",accuracy,EVALMETRIC
"| <img src=""docs/figs/teaser1.png"" align=""center"" width=""91%""> | <img src=""docs/figs/teaser2.png"" align=""center"" width=""89%""> | | :----------------------------------------------------------: | :----------------------------------------------------------: | |                     Speed *vs.* Accuracy                     |                    Speed *vs.* Robustness                    |  Visit our [project page](https://xiangxu-0103.github.io/FRNet) to explore more examples.",Accuracy,EVALMETRIC
"| <img src=""docs/figs/teaser1.png"" align=""center"" width=""91%""> | <img src=""docs/figs/teaser2.png"" align=""center"" width=""89%""> | | :----------------------------------------------------------: | :----------------------------------------------------------: | |                     Speed *vs.* Accuracy                     |                    Speed *vs.* Robustness                    |  Visit our [project page](https://xiangxu-0103.github.io/FRNet) to explore more examples.",Robustness,EVALMETRIC
"-only</td>         <td>44.9</td> <td>60.4</td> <td>61.8</td> <td>63.1</td>         <td>51.9</td> <td>68.1</td> <td>70.9</td> <td>74.6</td>         <td>42.4</td> <td>53.5</td> <td>55.1</td> <td>57.0</td>     </tr>     <tr>         <td>LaserMix</td>         <td>52.9</td> <td>62.9</td> <td>63.2</td> <td>65.0</td>         <td>58.7</td> <td>71.5</td> <td>72.3</td> <td>75.0</td>         <td>45.8</td> <td>56.8</td> <td>57.7</td> <td>59.0</td>     </tr>     <tr>         <td><strong>FrustumMix</strong></td>         <td>55.8</td> <td>64.8</td> <td>65.2</td> <td>65.4</td>         <td>61.2</td> <td>72.2</td> <td>74.6</td> <td>75.4</td>         <td>46.6</td> <td>57.0</td> <td>59.5</td> <td>61.2</td>     </tr> </table>  ### Robustness  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""2"">SemKITTI-C</th>         <th colspan=""2"">nuScenes-C</th>     </tr>     <tr>         <td>mCE</td> <td>mRR</td>         <td>mCE</td> <td>mRR</td>     </tr>     <tr>         <td>CENet</td>         <td>103.4</td> <td>81.3</td>         <td>112.8</td> <td>76.0</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>96.8</td> <td>80.0</td>         <td>98.6</td> <td>77.5</td>     </tr> </table>  **:memo: Note**:  - **mCE (the lower the better)**: The *average corruption error* (in percentage) of a candidate model compared to the baseline model, which is calculated among all corruption types across three severity levels. - **mRR (the higher the better)**: The *average resilience rate* (in percentage) of a candidate model compared to its ""clean"" performance, which is calculated among all corruption types across three severity levels.  ### :round_pushpin: Pre-Trained Checkpoints  We provide the trained models for SemanticKITTI and nuScenes.",mCE,EVALMETRIC
"-only</td>         <td>44.9</td> <td>60.4</td> <td>61.8</td> <td>63.1</td>         <td>51.9</td> <td>68.1</td> <td>70.9</td> <td>74.6</td>         <td>42.4</td> <td>53.5</td> <td>55.1</td> <td>57.0</td>     </tr>     <tr>         <td>LaserMix</td>         <td>52.9</td> <td>62.9</td> <td>63.2</td> <td>65.0</td>         <td>58.7</td> <td>71.5</td> <td>72.3</td> <td>75.0</td>         <td>45.8</td> <td>56.8</td> <td>57.7</td> <td>59.0</td>     </tr>     <tr>         <td><strong>FrustumMix</strong></td>         <td>55.8</td> <td>64.8</td> <td>65.2</td> <td>65.4</td>         <td>61.2</td> <td>72.2</td> <td>74.6</td> <td>75.4</td>         <td>46.6</td> <td>57.0</td> <td>59.5</td> <td>61.2</td>     </tr> </table>  ### Robustness  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""2"">SemKITTI-C</th>         <th colspan=""2"">nuScenes-C</th>     </tr>     <tr>         <td>mCE</td> <td>mRR</td>         <td>mCE</td> <td>mRR</td>     </tr>     <tr>         <td>CENet</td>         <td>103.4</td> <td>81.3</td>         <td>112.8</td> <td>76.0</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>96.8</td> <td>80.0</td>         <td>98.6</td> <td>77.5</td>     </tr> </table>  **:memo: Note**:  - **mCE (the lower the better)**: The *average corruption error* (in percentage) of a candidate model compared to the baseline model, which is calculated among all corruption types across three severity levels. - **mRR (the higher the better)**: The *average resilience rate* (in percentage) of a candidate model compared to its ""clean"" performance, which is calculated among all corruption types across three severity levels.  ### :round_pushpin: Pre-Trained Checkpoints  We provide the trained models for SemanticKITTI and nuScenes.",average corruption error,EVALMETRIC
"-only</td>         <td>44.9</td> <td>60.4</td> <td>61.8</td> <td>63.1</td>         <td>51.9</td> <td>68.1</td> <td>70.9</td> <td>74.6</td>         <td>42.4</td> <td>53.5</td> <td>55.1</td> <td>57.0</td>     </tr>     <tr>         <td>LaserMix</td>         <td>52.9</td> <td>62.9</td> <td>63.2</td> <td>65.0</td>         <td>58.7</td> <td>71.5</td> <td>72.3</td> <td>75.0</td>         <td>45.8</td> <td>56.8</td> <td>57.7</td> <td>59.0</td>     </tr>     <tr>         <td><strong>FrustumMix</strong></td>         <td>55.8</td> <td>64.8</td> <td>65.2</td> <td>65.4</td>         <td>61.2</td> <td>72.2</td> <td>74.6</td> <td>75.4</td>         <td>46.6</td> <td>57.0</td> <td>59.5</td> <td>61.2</td>     </tr> </table>  ### Robustness  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""2"">SemKITTI-C</th>         <th colspan=""2"">nuScenes-C</th>     </tr>     <tr>         <td>mCE</td> <td>mRR</td>         <td>mCE</td> <td>mRR</td>     </tr>     <tr>         <td>CENet</td>         <td>103.4</td> <td>81.3</td>         <td>112.8</td> <td>76.0</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>96.8</td> <td>80.0</td>         <td>98.6</td> <td>77.5</td>     </tr> </table>  **:memo: Note**:  - **mCE (the lower the better)**: The *average corruption error* (in percentage) of a candidate model compared to the baseline model, which is calculated among all corruption types across three severity levels. - **mRR (the higher the better)**: The *average resilience rate* (in percentage) of a candidate model compared to its ""clean"" performance, which is calculated among all corruption types across three severity levels.  ### :round_pushpin: Pre-Trained Checkpoints  We provide the trained models for SemanticKITTI and nuScenes.",mRR,EVALMETRIC
"-only</td>         <td>44.9</td> <td>60.4</td> <td>61.8</td> <td>63.1</td>         <td>51.9</td> <td>68.1</td> <td>70.9</td> <td>74.6</td>         <td>42.4</td> <td>53.5</td> <td>55.1</td> <td>57.0</td>     </tr>     <tr>         <td>LaserMix</td>         <td>52.9</td> <td>62.9</td> <td>63.2</td> <td>65.0</td>         <td>58.7</td> <td>71.5</td> <td>72.3</td> <td>75.0</td>         <td>45.8</td> <td>56.8</td> <td>57.7</td> <td>59.0</td>     </tr>     <tr>         <td><strong>FrustumMix</strong></td>         <td>55.8</td> <td>64.8</td> <td>65.2</td> <td>65.4</td>         <td>61.2</td> <td>72.2</td> <td>74.6</td> <td>75.4</td>         <td>46.6</td> <td>57.0</td> <td>59.5</td> <td>61.2</td>     </tr> </table>  ### Robustness  <table>     <tr>         <th rowspan=""2"">Method</th>         <th colspan=""2"">SemKITTI-C</th>         <th colspan=""2"">nuScenes-C</th>     </tr>     <tr>         <td>mCE</td> <td>mRR</td>         <td>mCE</td> <td>mRR</td>     </tr>     <tr>         <td>CENet</td>         <td>103.4</td> <td>81.3</td>         <td>112.8</td> <td>76.0</td>     </tr>     <tr>         <td><strong>FRNet</strong></td>         <td>96.8</td> <td>80.0</td>         <td>98.6</td> <td>77.5</td>     </tr> </table>  **:memo: Note**:  - **mCE (the lower the better)**: The *average corruption error* (in percentage) of a candidate model compared to the baseline model, which is calculated among all corruption types across three severity levels. - **mRR (the higher the better)**: The *average resilience rate* (in percentage) of a candidate model compared to its ""clean"" performance, which is calculated among all corruption types across three severity levels.  ### :round_pushpin: Pre-Trained Checkpoints  We provide the trained models for SemanticKITTI and nuScenes.",average resilience rate,EVALMETRIC
"The changes in `goatools.obo_parser` are as follows:  ```python # line 132 elif line[:5] == ""def: "":     rec_curr.definition = line[5:]  # line 169 self.definition = """" ```  ### Environment for OntoProtein pre-training <span id=""environment-for-ontoprotein-pre-training""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / deepspeed 0.5.1/ lmdb /   ### Environment for protein-related tasks <span id=""environment-for-protein-related-tasks""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / lmdb / tape_proteins  Specially, in library `tape_proteins`, it only implements the calculation of metric `P@L` for the contact prediction task.",P@L,EVALMETRIC
"So, for reporting the metrics P@K taking different K values, in which the metrics P@K are precisions for the top K contacts, we made some changes in the library.",P@K,EVALMETRIC
"So, for reporting the metrics P@K taking different K values, in which the metrics P@K are precisions for the top K contacts, we made some changes in the library.",P@K,EVALMETRIC
"- Verified Labels (CoNLL++): https://github.com/ZihanWangKi/CrossWeigh/tree/master/data   #### Experiments  ---   `token-classification-benchmark.ipynb`: We implement 11 different methods of aggregating the label quality scores for each token to obtain an overall score per sentence, and evaluate the precision-recall curve and related label-error detection metrics for each method.",precision-recall curve,EVALMETRIC
"pid=diva2%3A1581005&dswid=5229), [Robust Explanations for Private Support Vector Machines](https://arxiv.org/abs/2102.03785) | 2021 | _ICML-Workshop_ | Counterfactual | - |  Private SVM | [[Code]](https://github.com/rami-mochaourab/robust-explanation-SVM) | | [When Differential Privacy Meets Interpretability: A Case Study](https://arxiv.org/abs/2106.13203) | 2021 | _RCV-CVPR_ | Interpretable Models | - | Differential Privacy | - | | [Differentially Private Quantiles](https://proceedings.mlr.press/v139/gillenwater21a.html) | 2021 | _ICML_ | Quantiles | - | Differential Privacy | [[Code]](https://github.com/google-research/google-research/tree/master/dp_multiq) | | [FOX: Fooling with Explanations : Privacy Protection with Adversarial Reactions in Social Media](https://ieeexplore.ieee.org/document/9647778) | 2021 | _PST_ | - | Attribute Inference | Privacy-Protecting Explanation | - | | [Privacy-preserving generative adversarial network for case-based explainability in medical image analysis](https://ieeexplore.ieee.org/abstract/document/9598877/) | 2021 | _IEEE Access_ | Example-based | - | Generative Anonymisation | - | | [Interpretable and Differentially Private Predictions](https://ojs.aaai.org/index.php/AAAI/article/view/5827) | 2020 | _AAAI_ | Locally linear maps | - | Differential Privacy | [[Code]](https://github.com/frhrdr/dp-llm) | | [Model extraction from counterfactual explanations](https://arxiv.org/abs/2009.01884) | 2020 | _arXiv_ | Counterfactual | Model Extraction | - | [[Code]](https://github.com/aivodji/mrce) | | [Model Reconstruction from Model Explanations](https://dl.acm.org/doi/10.1145/3287560.3287562) | 2019 | _FAT*_ | Gradient-based | Model Reconstruction, Model Extraction | - | - | | [Interpret Federated Learning with Shapley Values](https://arxiv.org/abs/1905.04519) | 2019 | __ |  Shapley | - | Federated | [[Code]](https://github.com/crownpku/federated_shap) | | [Collaborative Explanation of Deep Models with Limited Interaction for Trade Secret and Privacy Preservation](https://dl.acm.org/doi/10.1145/3308560.3317586) | 2019 | _WWW_ | Feature-based | - | Collaborative rule-based model | - | | [Model inversion attacks that exploit confidence information and basic countermeasures](https://dl.acm.org/doi/abs/10.1145/2810103.2813677) | 2015 | _CCS_ | Confidence scores | Reconstruction, Model Inversion | - | - |  ----------  ## Datasets  ### Type: Image | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [MNIST](www.kaggle.com/datasets/hojjatk/mnist-dataset) | 70K | 11MB | Counterfactuals, Gradient | 4 | | [CIFAR](www.cs.toronto.edu/~kriz/cifar.html) | 60K | 163MB | Gradient | 4 | | [SVHN](ufldl.stanford.edu/housenumbers/) | 600K | 400MB+ | Gradient | 1 | | [Food101](www.kaggle.com/datasets/dansbecker/food-101) | 100K+ | 10GB | Case-based | 1 | | [Flowers102](www.robots.ox.ac.uk/~vgg/data/flowers/102/) | 8K+ | 300MB+ | Case-based | 1 | | [Cervical](www.kaggle.com/competitions/intel-mobileodt-cervical-cancer-screening) | 8K+ | 46GB+ | Case-based, Interpretable Models | 1 | | [CheXpert](stanfordmlgroup.github.io/competitions/chexpert/) | 220K+ | GBs | Black-box | 1 | | [Facial Expression](www.kaggle.com/datasets/msambare/fer2013) | 12K+ | 63MB | Gradient | 1 | | [Celeb](mmlab.ie.cuhk.edu.hk/projects/CelebA.html) | 200K | GBs | Counterfactuals, Shapley, Gradient, Perturbation | 1 |  ### Type: Tablular | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Adult](archive.ics.uci.edu/ml/datasets/adult) | 48K+ | 10MB | Counterfactuals, Shapley | 10+ | | [COMPAS](www.kaggle.com/datasets/danofer/compass) | 7K+ | 25MB | Counterfactuals, Shapley | 2 | | [FICO](community.fico.com/s/explainable-machine-learning-challenge) | 10K+ | â¤ 1MB | Counterfactuals, Shapley | 4 | | [Boston Housing](www.kaggle.com/code/prasadperera/the-boston-housing-dataset) | 500+ | â¤ 1MB | Counterfactuals, Shapley | 1 | | [German Credit](archive.ics.uci.edu/dataset/144/statlog+german+credit+data) | 1K | â¤ 1MB | Counterfactuals, Shapley | 4 | | [Student Admission](www.kaggle.com/datasets/mohansacharya/graduate-admissions) | 500 | â¤ 1MB | Counterfactuals, Shapley, Gradient, Perturbation | 1 | | [Student Performance](www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression) | 10K | â¤ 1MB | Counterfactuals, Shapley | 1 | | [GMSC](www.kaggle.com/c/GiveMeSomeCredit/data) | 150K+ | 15MB | Interpretable models, Counterfactuals | 2 | | [Diabetes](archive.ics.uci.edu/dataset/296/diabetes+130-us+hospitals+for+years+1999-2008) | 100K+ | 20MB | Feature-based | 5 | | [Breast Cancer](archive.ics.uci.edu/ml/datasets/breast+cancer) | 569 | < 1MB | Feature-based | 1 |  ### Type: Graph | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [Cora](relational.fit.cvut.cz/dataset/CORA) | 2K+ | 4.5MB | Feature-based | 1 | | [Bitcoin](snap.stanford.edu/data/soc-sign-bitcoin-alpha.html) | 30K | â¤ 1MB | Counterfactuals | 1 | | [CIC-IDS2017](www.unb.ca/cic/datasets/ids-2017.html) | 2.8M+ | 500MB | Black-box | 1 |  ### Type: Text | Dataset | #Items | Disk Size | Downstream Explanations | #Papers Used | | :-- | --- | --- | --- | --- | | [IMDB Review](ai.stanford.edu/~amaas/data/sentiment/) | 50K | 66MB | Black-box | 1 |  ---------- ## Evaluation Metrics  | **Category**           | **Evaluation Metrics**                    | **Formula/Description**    | **Usage**                | |-----------|-----------|---------|---------------| | **Explanation Utility** | Counterfactual validity | $\text{Pureness} = \frac{\text{no. value combinations with desired outcome}}{\text{no. value combinations}}$        |  <div style=""width:500px""> Assess the range of attribute values within k-anonymous counterfactual instances.",Counterfactual validity,EVALMETRIC
"Consider all attributes, including those beyond quasi-identifiers  </div>                                                                     | |                        | Classification metric | $CM = \frac{\sum\limits_{i=1}^{N} \text{penalty}(tuple_i)}{N}$                                                             | <div style=""width:500px""> Assess equivalence classes within anonymized datasets, focusing on class label uniformity.",Classification metric,EVALMETRIC
"</div>                                        | |                        | Faithfulness </br> (RDT-Fidelity) | $\mathcal{F}(\mathcal{E}_X)$ (see our paper) | <div style=""width:500px""> Reflect how often the model's predictions are unchanged despite perturbations to the input, which would suggest that the explanation is effectively capturing the reasoning behind the model's predictions.",Faithfulness,EVALMETRIC
"</div>                                        | |                        | Faithfulness </br> (RDT-Fidelity) | $\mathcal{F}(\mathcal{E}_X)$ (see our paper) | <div style=""width:500px""> Reflect how often the model's predictions are unchanged despite perturbations to the input, which would suggest that the explanation is effectively capturing the reasoning behind the model's predictions.",RDT-Fidelity,EVALMETRIC
"</div>          | |                        | Sparsity | $H(p) = -\sum_{f \in M} p(f) \log p(f)$                                                                              | <div style=""width:500px""> A complete and faithful explanation to the model should inherently be sparse, focusing only on a select subset of features that are most predictive of the model's decision.",Sparsity,EVALMETRIC
"</div>            | | **Information Loss**   | Normalised Certainty Penalty </br> (NCP) | $\text{NCP}(G) = \sum\limits_{i=1}^{d} w_i \cdot \text{NCP}_{A_i}(G)$                                                       | <div style=""width:500px""> Higher NCP values indicate a greater degree of generalization and more information loss.",Normalised Certainty Penalty,EVALMETRIC
"</div>            | | **Information Loss**   | Normalised Certainty Penalty </br> (NCP) | $\text{NCP}(G) = \sum\limits_{i=1}^{d} w_i \cdot \text{NCP}_{A_i}(G)$                                                       | <div style=""width:500px""> Higher NCP values indicate a greater degree of generalization and more information loss.",NCP,EVALMETRIC
"</div>            | | **Information Loss**   | Normalised Certainty Penalty </br> (NCP) | $\text{NCP}(G) = \sum\limits_{i=1}^{d} w_i \cdot \text{NCP}_{A_i}(G)$                                                       | <div style=""width:500px""> Higher NCP values indicate a greater degree of generalization and more information loss.",NCP,EVALMETRIC
"</div>            | | **Information Loss**   | Normalised Certainty Penalty </br> (NCP) | $\text{NCP}(G) = \sum\limits_{i=1}^{d} w_i \cdot \text{NCP}_{A_i}(G)$                                                       | <div style=""width:500px""> Higher NCP values indicate a greater degree of generalization and more information loss.",NCP,EVALMETRIC
"</div>            | | **Information Loss**   | Normalised Certainty Penalty </br> (NCP) | $\text{NCP}(G) = \sum\limits_{i=1}^{d} w_i \cdot \text{NCP}_{A_i}(G)$                                                       | <div style=""width:500px""> Higher NCP values indicate a greater degree of generalization and more information loss.",NCP,EVALMETRIC
"</div>  | |                        | Information Leakage | <div style=""width:300px""> $Pr_{i=1..k}\hat{\phi}(\mathbf{z_i}, X, f_D(X)) \leq e^{\hat{\varepsilon}} \cdot Pr[\hat{\phi}(\mathbf{z_i}, X, f'_D(X)) : \forall i] + \hat{\delta}$     </div>          | <div style=""width:500px""> If an adversary can access model explanations, they would not gain any additional information that could help in inferring something about the training data beyond what could be learned from the model predictions alone.",Information Leakage,EVALMETRIC
"</div>                           | | **Attack Success**     | Precision/Recall/F1 | $Prec = \frac{TP}{TP+FP}$, </br> $Rec = \frac{TP}{TP+FN}$, </br> $F1 = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}$ | <div style=""width:500px""> Evaluate an attack's effectiveness in correctly and completely identifying the properties it is designed to infer.",Precision,EVALMETRIC
"</div>                           | | **Attack Success**     | Precision/Recall/F1 | $Prec = \frac{TP}{TP+FP}$, </br> $Rec = \frac{TP}{TP+FN}$, </br> $F1 = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}$ | <div style=""width:500px""> Evaluate an attack's effectiveness in correctly and completely identifying the properties it is designed to infer.",Recall,EVALMETRIC
"</div>                           | | **Attack Success**     | Precision/Recall/F1 | $Prec = \frac{TP}{TP+FP}$, </br> $Rec = \frac{TP}{TP+FN}$, </br> $F1 = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}$ | <div style=""width:500px""> Evaluate an attack's effectiveness in correctly and completely identifying the properties it is designed to infer.",F1,EVALMETRIC
"</div>                           | | **Attack Success**     | Precision/Recall/F1 | $Prec = \frac{TP}{TP+FP}$, </br> $Rec = \frac{TP}{TP+FN}$, </br> $F1 = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}$ | <div style=""width:500px""> Evaluate an attack's effectiveness in correctly and completely identifying the properties it is designed to infer.",F1,EVALMETRIC
"</div>                           | | **Attack Success**     | Precision/Recall/F1 | $Prec = \frac{TP}{TP+FP}$, </br> $Rec = \frac{TP}{TP+FN}$, </br> $F1 = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}$ | <div style=""width:500px""> Evaluate an attack's effectiveness in correctly and completely identifying the properties it is designed to infer.",precision,EVALMETRIC
"</div>                           | | **Attack Success**     | Precision/Recall/F1 | $Prec = \frac{TP}{TP+FP}$, </br> $Rec = \frac{TP}{TP+FN}$, </br> $F1 = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}$ | <div style=""width:500px""> Evaluate an attack's effectiveness in correctly and completely identifying the properties it is designed to infer.",recall,EVALMETRIC
"</div>                           | | **Attack Success**     | Precision/Recall/F1 | $Prec = \frac{TP}{TP+FP}$, </br> $Rec = \frac{TP}{TP+FN}$, </br> $F1 = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}$ | <div style=""width:500px""> Evaluate an attack's effectiveness in correctly and completely identifying the properties it is designed to infer.",precision,EVALMETRIC
"</div>                           | | **Attack Success**     | Precision/Recall/F1 | $Prec = \frac{TP}{TP+FP}$, </br> $Rec = \frac{TP}{TP+FN}$, </br> $F1 = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}$ | <div style=""width:500px""> Evaluate an attack's effectiveness in correctly and completely identifying the properties it is designed to infer.",recall,EVALMETRIC
"</div>                                                   | |                        | Balanced Accuracy | $BA = \frac{TPR + TNR}{2}$                                                                                         | <div style=""width:500px""> Measures the accuracy of attack (e.g., membership prediction in membership inference attacks), on a balanced dataset of members and non-members.",Balanced Accuracy,EVALMETRIC
"</div>                                                   | |                        | Balanced Accuracy | $BA = \frac{TPR + TNR}{2}$                                                                                         | <div style=""width:500px""> Measures the accuracy of attack (e.g., membership prediction in membership inference attacks), on a balanced dataset of members and non-members.",BA,EVALMETRIC
"</div>                | |                        | ROC/AUC | <div style=""width:300px""> The ROC curve plots the true positive rate against the false positive rate at various threshold settings.",ROC,EVALMETRIC
"</div>                | |                        | ROC/AUC | <div style=""width:300px""> The ROC curve plots the true positive rate against the false positive rate at various threshold settings.",AUC,EVALMETRIC
"</div>                | |                        | ROC/AUC | <div style=""width:300px""> The ROC curve plots the true positive rate against the false positive rate at various threshold settings.",ROC,EVALMETRIC
"</div>       | <div style=""width:500px""> An AUC near 1 indicates a highly successful privacy attack, while an AUC close to 0.5 suggests no better performance than random guessing.",AUC,EVALMETRIC
"</div>       | <div style=""width:500px""> An AUC near 1 indicates a highly successful privacy attack, while an AUC close to 0.5 suggests no better performance than random guessing.",AUC,EVALMETRIC
"</div>                                    | |                        | TPR at Low FPR | Report TPR at a fixed FPR (e.g., 0.1%)",TPR at Low FPR,EVALMETRIC
"</div>                                    | |                        | TPR at Low FPR | Report TPR at a fixed FPR (e.g., 0.1%)",TPR,EVALMETRIC
"</div>                                    | |                        | TPR at Low FPR | Report TPR at a fixed FPR (e.g., 0.1%)",FPR,EVALMETRIC
"</div>               | |                        | Mean Absolute Error (MAE) | $\ell_1 (\hat{x}, x) = \frac{1}{mn} \sum\limits_{j=1}^{m} \sum\limits_{i=1}^{n} \| \hat{x}_i^j - x_i^j \|,$                          | <div style=""width:500px""> Gives an overview of how accurately an attack can reconstruct private inputs by averaging the absolute differences across all samples and features.",Mean Absolute Error,EVALMETRIC
"</div>               | |                        | Mean Absolute Error (MAE) | $\ell_1 (\hat{x}, x) = \frac{1}{mn} \sum\limits_{j=1}^{m} \sum\limits_{i=1}^{n} \| \hat{x}_i^j - x_i^j \|,$                          | <div style=""width:500px""> Gives an overview of how accurately an attack can reconstruct private inputs by averaging the absolute differences across all samples and features.",MAE,EVALMETRIC
"</div>                         | |                        | Success Rate (SR) | $SR = \frac{\|\hat{X}_{val} \neq \perp\|}{mn}$                                                                        | <div style=""width:500px""> The ratio of successfully reconstructed features to the total number of features across all samples                                                                                                    </div>                    | |                        | Model Agreement | $\text{Agreement} = \frac{1}{n} \sum\limits_{i=1}^{n} 1_{f_\theta(x_i) = h_\phi(x_i)}.$                                    | <div style=""width:500px""> A higher agreement indicates that the substitute model is more similar to the original model.",Success Rate,EVALMETRIC
"</div>                         | |                        | Success Rate (SR) | $SR = \frac{\|\hat{X}_{val} \neq \perp\|}{mn}$                                                                        | <div style=""width:500px""> The ratio of successfully reconstructed features to the total number of features across all samples                                                                                                    </div>                    | |                        | Model Agreement | $\text{Agreement} = \frac{1}{n} \sum\limits_{i=1}^{n} 1_{f_\theta(x_i) = h_\phi(x_i)}.$                                    | <div style=""width:500px""> A higher agreement indicates that the substitute model is more similar to the original model.",SR,EVALMETRIC
"</div>                         | |                        | Success Rate (SR) | $SR = \frac{\|\hat{X}_{val} \neq \perp\|}{mn}$                                                                        | <div style=""width:500px""> The ratio of successfully reconstructed features to the total number of features across all samples                                                                                                    </div>                    | |                        | Model Agreement | $\text{Agreement} = \frac{1}{n} \sum\limits_{i=1}^{n} 1_{f_\theta(x_i) = h_\phi(x_i)}.$                                    | <div style=""width:500px""> A higher agreement indicates that the substitute model is more similar to the original model.",SR,EVALMETRIC
"</div> | |                        | Average Uncertainty Reduction | $Dist(\mathcal{D}^M, \mathcal{D}^{Orig}) = \frac{1}{n \cdot d} \sum\limits_{i=1}^{n} \sum\limits_{k=1}^{d} \frac{H(\mathcal{D}^M_{i,k})}{H(\mathcal{D}_{i,k})}$ | <div style=""width:500px""> The degree to which a data reconstruction attack is accurate, measured by the reduction in uncertainty across all features of all samples in the dataset                       </div>                                               |    ---------- **Disclaimer**  Feel free to contact us if you have any queries or exciting news.",Average Uncertainty Reduction,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",cross-entropy loss,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",CO_cel,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",cross-entropy loss,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",CO_cos,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",top-k accuracy,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",jaccard similarity coefficient,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",JSC,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",cosine similarity,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",CS,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",JSC,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",CS,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",CO_cel,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",CO_cos,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",JSC,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",CS,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",CO_cel,EVALMETRIC
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ## Download Ontology, Dataset and Models  You can automatically download the files (ontologies, models, etc.) that are required for [inference](#inference) and [test](#test) with the following command:  ```shell script python download_resources.py ```  The files will be stored in a folder called ```resources/``` relative to the repository path.   ## Models  We provide the trained models for the following approaches:  - Classification baseline (denoted as ```C```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/727c3ee1-4107-4996-878d-1caf537730e8/download/vise_c.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cel```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/7c672f2b-f45e-40aa-b6bb-01fb2e9bf5e7/download/vise_co_cel.tar.gz) - Best ontology driven approach using the cross-entropy loss (denoted as ```CO_cos```): [link](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b105c1aa-3bc4-4233-8103-8f4616948d85/download/vise_co_cos.tar.gz)  The performance of these models regarding the top-k accuracy, jaccard similarity coefficient (JSC), and cosine similarity (CS) on the *VisE-Bing* and *VisE-Wiki* testsets is listed below using the provided [singularity image](#setup-with-singularity-for-reproducibility):  \ **VisE-Bing** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   77.4   |   89.8   |   93.6   |   84.7   |   87.7   | | CO_cel |   81.5   | **91.8** | **94.3** |   87.5   |   90.0   | | CO_cos | **81.9** |   90.8   |   93.2   | **87.9** | **90.4** |  \ **VisE-Wiki** | Model  |  Top-1   |  Top-3   |  Top-5   |   JSC    |    CS    | | :----- | :------: | :------: | :------: | :------: | :------: | | C      |   61.7   |   74.6   | **79.2** |   72.7   |   77.8   | | CO_cel |   63.4   | **74.7** |   78.8   |   73.9   |   78.7   | | CO_cos | **63.5** |   74.3   |   78.8   | **74.1** | **79.0** |   ## Inference  In order to apply our [models](#models) on an image or a list of images, please execute the following command:  ```shell script python infer.py -c </path/to/model.yml> -i </path/to/image(s)> ```  If you followed the instructions in [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models) the model config is placed in ```resources/VisE-D/models/<modelname>.yml``` relative to the repository path.",CO_cos,EVALMETRIC
"For example, the following command computes the bleu score  ```shell vec2sent_evaluate --metric BLEU --file hier.txt ```  The following metrics are available  | Parameter | Explanation                                                                       | |-----------|-----------------------------------------------------------------------------------| | ID        | Fraction of all sentences where the output is identical to the input              | | PERM      | Fraction of all output sentences that can be formed as a permutation of the input | | ID_PERM   | Fraction of all permuations that are identical to the input                       | | BLEU      | Document BLEU score                                                               | | MOVER     | Average Mover Score between input and output sentences                            |   > [!",BLEU,EVALMETRIC
"For example, the following command computes the bleu score  ```shell vec2sent_evaluate --metric BLEU --file hier.txt ```  The following metrics are available  | Parameter | Explanation                                                                       | |-----------|-----------------------------------------------------------------------------------| | ID        | Fraction of all sentences where the output is identical to the input              | | PERM      | Fraction of all output sentences that can be formed as a permutation of the input | | ID_PERM   | Fraction of all permuations that are identical to the input                       | | BLEU      | Document BLEU score                                                               | | MOVER     | Average Mover Score between input and output sentences                            |   > [!",ID,EVALMETRIC
"For example, the following command computes the bleu score  ```shell vec2sent_evaluate --metric BLEU --file hier.txt ```  The following metrics are available  | Parameter | Explanation                                                                       | |-----------|-----------------------------------------------------------------------------------| | ID        | Fraction of all sentences where the output is identical to the input              | | PERM      | Fraction of all output sentences that can be formed as a permutation of the input | | ID_PERM   | Fraction of all permuations that are identical to the input                       | | BLEU      | Document BLEU score                                                               | | MOVER     | Average Mover Score between input and output sentences                            |   > [!",PERM,EVALMETRIC
"For example, the following command computes the bleu score  ```shell vec2sent_evaluate --metric BLEU --file hier.txt ```  The following metrics are available  | Parameter | Explanation                                                                       | |-----------|-----------------------------------------------------------------------------------| | ID        | Fraction of all sentences where the output is identical to the input              | | PERM      | Fraction of all output sentences that can be formed as a permutation of the input | | ID_PERM   | Fraction of all permuations that are identical to the input                       | | BLEU      | Document BLEU score                                                               | | MOVER     | Average Mover Score between input and output sentences                            |   > [!",ID_PERM,EVALMETRIC
"For example, the following command computes the bleu score  ```shell vec2sent_evaluate --metric BLEU --file hier.txt ```  The following metrics are available  | Parameter | Explanation                                                                       | |-----------|-----------------------------------------------------------------------------------| | ID        | Fraction of all sentences where the output is identical to the input              | | PERM      | Fraction of all output sentences that can be formed as a permutation of the input | | ID_PERM   | Fraction of all permuations that are identical to the input                       | | BLEU      | Document BLEU score                                                               | | MOVER     | Average Mover Score between input and output sentences                            |   > [!",BLEU,EVALMETRIC
"For example, the following command computes the bleu score  ```shell vec2sent_evaluate --metric BLEU --file hier.txt ```  The following metrics are available  | Parameter | Explanation                                                                       | |-----------|-----------------------------------------------------------------------------------| | ID        | Fraction of all sentences where the output is identical to the input              | | PERM      | Fraction of all output sentences that can be formed as a permutation of the input | | ID_PERM   | Fraction of all permuations that are identical to the input                       | | BLEU      | Document BLEU score                                                               | | MOVER     | Average Mover Score between input and output sentences                            |   > [!",BLEU score,EVALMETRIC
"For example, the following command computes the bleu score  ```shell vec2sent_evaluate --metric BLEU --file hier.txt ```  The following metrics are available  | Parameter | Explanation                                                                       | |-----------|-----------------------------------------------------------------------------------| | ID        | Fraction of all sentences where the output is identical to the input              | | PERM      | Fraction of all output sentences that can be formed as a permutation of the input | | ID_PERM   | Fraction of all permuations that are identical to the input                       | | BLEU      | Document BLEU score                                                               | | MOVER     | Average Mover Score between input and output sentences                            |   > [!",MOVER,EVALMETRIC
"For example, the following command computes the bleu score  ```shell vec2sent_evaluate --metric BLEU --file hier.txt ```  The following metrics are available  | Parameter | Explanation                                                                       | |-----------|-----------------------------------------------------------------------------------| | ID        | Fraction of all sentences where the output is identical to the input              | | PERM      | Fraction of all output sentences that can be formed as a permutation of the input | | ID_PERM   | Fraction of all permuations that are identical to the input                       | | BLEU      | Document BLEU score                                                               | | MOVER     | Average Mover Score between input and output sentences                            |   > [!",Average Mover Score,EVALMETRIC
"Note that we also need `instances_<train | val>2014.json` for computing PMRP score.  ### CUB Caption  Download images (CUB-200-2011) from [this link](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html), and download caption from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016).",PMRP,EVALMETRIC
"You can use the image path and the caption path separately in the code.  ## Evaluate pretrained models  NOTE: the current implementation of plausible match R-Precision (PMRP) is not efficient: <br> It first dumps all ranked items for each item to a local file, and compute R-precision.",plausible match R-Precision,EVALMETRIC
"You can use the image path and the caption path separately in the code.  ## Evaluate pretrained models  NOTE: the current implementation of plausible match R-Precision (PMRP) is not efficient: <br> It first dumps all ranked items for each item to a local file, and compute R-precision.",PMRP,EVALMETRIC
"You can use the image path and the caption path separately in the code.  ## Evaluate pretrained models  NOTE: the current implementation of plausible match R-Precision (PMRP) is not efficient: <br> It first dumps all ranked items for each item to a local file, and compute R-precision.",R-precision,EVALMETRIC
<br> We are planning to re-implement efficient PMRP as soon as possible.  ### COCO Caption  ``` # Compute recall metrics python evaluate_recall_coco.py .,PMRP,EVALMETRIC
<br> We are planning to re-implement efficient PMRP as soon as possible.  ### COCO Caption  ``` # Compute recall metrics python evaluate_recall_coco.py .,recall,EVALMETRIC
<br> We are planning to re-implement efficient PMRP as soon as possible.  ### COCO Caption  ``` # Compute recall metrics python evaluate_recall_coco.py .,recall,EVALMETRIC
/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image ```  ``` # Compute plausible match R-Precision (PMRP) metric python extract_rankings_coco.py .,plausible match R-Precision,EVALMETRIC
/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image ```  ``` # Compute plausible match R-Precision (PMRP) metric python extract_rankings_coco.py .,PMRP,EVALMETRIC
"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",pmrp,EVALMETRIC
"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",PMRP,EVALMETRIC
"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",R@1,EVALMETRIC
"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",mAP@R,EVALMETRIC
"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",PMRP,EVALMETRIC
"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",R@1,EVALMETRIC
"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",mAP@R,EVALMETRIC
"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",mAP@R,EVALMETRIC
"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image ```  NOTE: If you just download file from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016), then `caption_root` will be `cvpr2016_cub/text_c10`  If you want to test other probabilistic distances, such as Wasserstein distance or KL-divergence, try the following command:  ``` python evaluate_cub.py .",Wasserstein distance,EVALMETRIC
"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image ```  NOTE: If you just download file from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016), then `caption_root` will be `cvpr2016_cub/text_c10`  If you want to test other probabilistic distances, such as Wasserstein distance or KL-divergence, try the following command:  ``` python evaluate_cub.py .",KL-divergence,EVALMETRIC
"For each comment, we randomly hold out one annotator to act as our 'human model' and use the aggregated score of the other annotators as the ground truth to compute the ROC AUC (repeated 5 times then averaged).",ROC AUC,EVALMETRIC
We use the same test sets to compute the ROC AUC of the trained BERT model and average those scores as well.,ROC AUC,EVALMETRIC
| Attribute}  | Human AUC | BERT AUC | |------------ |-----------|----------| |Antagonistic |0.71       |  0.82    |  |Condescending| 0.72      |    0.78  | |Dismissive   | 0.68      | 0.82     | |Generalisation | 0.73    | 0.74     | |Hostile       |0.76      |0.84      | |Sarcastic     |0.72      |0.64      | |Unhealthy     | 0.62     | 0.69   |   Code for  this analysis is in `AUC_analysis.ipynb`.  _____   This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. https://creativecommons.org/licenses/by-nc-sa/4.0/,Human AUC,EVALMETRIC
| Attribute}  | Human AUC | BERT AUC | |------------ |-----------|----------| |Antagonistic |0.71       |  0.82    |  |Condescending| 0.72      |    0.78  | |Dismissive   | 0.68      | 0.82     | |Generalisation | 0.73    | 0.74     | |Hostile       |0.76      |0.84      | |Sarcastic     |0.72      |0.64      | |Unhealthy     | 0.62     | 0.69   |   Code for  this analysis is in `AUC_analysis.ipynb`.  _____   This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. https://creativecommons.org/licenses/by-nc-sa/4.0/,AUC,EVALMETRIC
| Attribute}  | Human AUC | BERT AUC | |------------ |-----------|----------| |Antagonistic |0.71       |  0.82    |  |Condescending| 0.72      |    0.78  | |Dismissive   | 0.68      | 0.82     | |Generalisation | 0.73    | 0.74     | |Hostile       |0.76      |0.84      | |Sarcastic     |0.72      |0.64      | |Unhealthy     | 0.62     | 0.69   |   Code for  this analysis is in `AUC_analysis.ipynb`.  _____   This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. https://creativecommons.org/licenses/by-nc-sa/4.0/,AUC,EVALMETRIC
| backbone  |  FPS |  AP  | AP50 | AP75 |  AR1 |  AR10 | download | |:----------|:----:|:----:|:----:|:----:|:----:|:-----:|:--------:| | [ResNet-50](projects/VISOLO/configs/base_ytvis_coco.yaml) | 40.0 | 38.6 | 56.3 | 43.7 | 35.7 | 42.5 | [model](https://drive.google.com/file/d/1kgr2WPXB1rv8U4aaE1FKjPNezCDzqMY3/view?,FPS,EVALMETRIC
| backbone  |  FPS |  AP  | AP50 | AP75 |  AR1 |  AR10 | download | |:----------|:----:|:----:|:----:|:----:|:----:|:-----:|:--------:| | [ResNet-50](projects/VISOLO/configs/base_ytvis_coco.yaml) | 40.0 | 38.6 | 56.3 | 43.7 | 35.7 | 42.5 | [model](https://drive.google.com/file/d/1kgr2WPXB1rv8U4aaE1FKjPNezCDzqMY3/view?,AP,EVALMETRIC
| backbone  |  FPS |  AP  | AP50 | AP75 |  AR1 |  AR10 | download | |:----------|:----:|:----:|:----:|:----:|:----:|:-----:|:--------:| | [ResNet-50](projects/VISOLO/configs/base_ytvis_coco.yaml) | 40.0 | 38.6 | 56.3 | 43.7 | 35.7 | 42.5 | [model](https://drive.google.com/file/d/1kgr2WPXB1rv8U4aaE1FKjPNezCDzqMY3/view?,AP50,EVALMETRIC
| backbone  |  FPS |  AP  | AP50 | AP75 |  AR1 |  AR10 | download | |:----------|:----:|:----:|:----:|:----:|:----:|:-----:|:--------:| | [ResNet-50](projects/VISOLO/configs/base_ytvis_coco.yaml) | 40.0 | 38.6 | 56.3 | 43.7 | 35.7 | 42.5 | [model](https://drive.google.com/file/d/1kgr2WPXB1rv8U4aaE1FKjPNezCDzqMY3/view?,AP75,EVALMETRIC
| backbone  |  FPS |  AP  | AP50 | AP75 |  AR1 |  AR10 | download | |:----------|:----:|:----:|:----:|:----:|:----:|:-----:|:--------:| | [ResNet-50](projects/VISOLO/configs/base_ytvis_coco.yaml) | 40.0 | 38.6 | 56.3 | 43.7 | 35.7 | 42.5 | [model](https://drive.google.com/file/d/1kgr2WPXB1rv8U4aaE1FKjPNezCDzqMY3/view?,AR1,EVALMETRIC
| backbone  |  FPS |  AP  | AP50 | AP75 |  AR1 |  AR10 | download | |:----------|:----:|:----:|:----:|:----:|:----:|:-----:|:--------:| | [ResNet-50](projects/VISOLO/configs/base_ytvis_coco.yaml) | 40.0 | 38.6 | 56.3 | 43.7 | 35.7 | 42.5 | [model](https://drive.google.com/file/d/1kgr2WPXB1rv8U4aaE1FKjPNezCDzqMY3/view?,AR10,EVALMETRIC
Our best performing model with XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23% showing that there is substantial room for improvement.  ## Dataset Description We derive [SciNLI](https://drive.google.com/drive/folders/1kjBTVBV1HlMWW5xK8V096LahsU3pULHU?,Macro F1 score,EVALMETRIC
Our best performing model with XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23% showing that there is substantial room for improvement.  ## Dataset Description We derive [SciNLI](https://drive.google.com/drive/folders/1kjBTVBV1HlMWW5xK8V096LahsU3pULHU?,accuracy,EVALMETRIC
"Fast R-CNN  - trains state-of-the-art models, like VGG16, 9x faster than traditional R-CNN and 3x faster than SPPnet,  - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time,  - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet,  - and is written in Python and C++/Caffe.",mAP,EVALMETRIC
"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",Dice Similarity Coefficient,EVALMETRIC
"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",DSC,EVALMETRIC
"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",Sensitivity,EVALMETRIC
"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",True Positive Rate,EVALMETRIC
"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",Specificity,EVALMETRIC
"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",True Negative Rate,EVALMETRIC
"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",Dice Similarity Coefficient,EVALMETRIC
"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",DSC,EVALMETRIC
"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",Area Under the Curve,EVALMETRIC
"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",AUC,EVALMETRIC
"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",Competition Performance Metric,EVALMETRIC
"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",CPM,EVALMETRIC
"Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points and crossing the 60% mAP for the first time.",mAP,EVALMETRIC
"Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points and crossing the 60% mAP for the first time.",tIoU,EVALMETRIC
"Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points and crossing the 60% mAP for the first time.",mAP,EVALMETRIC
"Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.56% average mAP) and the more challenging EPIC-Kitchens 100 (+13.5% average mAP over prior works).",mAP,EVALMETRIC
"Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.56% average mAP) and the more challenging EPIC-Kitchens 100 (+13.5% average mAP over prior works).",mAP,EVALMETRIC
"Our submission in particular is ranked 2nd with a record 21.76% average mAP and 42.54% Recall@1x, tIoU=0.5, nearly three times higher than the official baseline.",mAP,EVALMETRIC
"Our submission in particular is ranked 2nd with a record 21.76% average mAP and 42.54% Recall@1x, tIoU=0.5, nearly three times higher than the official baseline.",Recall@1x,EVALMETRIC
"Our submission in particular is ranked 2nd with a record 21.76% average mAP and 42.54% Recall@1x, tIoU=0.5, nearly three times higher than the official baseline.",tIoU,EVALMETRIC
"* 05/08/2022: We have updated the code repo based on the community feedback and our code review, leading to significantly better average mAP on THUMOS14 (>66.0%) and slightly improved results on ActivityNet and EPIC-Kitchens 100.   ## Code Overview The structure of this code repo is heavily inspired by Detectron2.",mAP,EVALMETRIC
The expected average mAP should be around 62.6(%) as in Table 1 of our main paper.,mAP,EVALMETRIC
"**With recent commits, the expected average mAP should be higher than 66.0(%)**.",mAP,EVALMETRIC
/pretrained/thumos_i3d_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method            |  0.3  |  0.4  |  0.5  |  0.6  |  0.7  |  Avg  | |-------------------|-------|-------|-------|-------|-------|-------| | ActionFormer      | 82.13 | 77.80 | 70.95 | 59.40 | 43.87 | 66.83 |   ## To Reproduce Our Results on ActivityNet 1.3 **Download Features and Annotations** * Download *anet_1.3.tar.gz* (`md5sum c415f50120b9425ee1ede9ac3ce11203`) from [this Box link](https://uwmadison.box.com/s/aisdoymowukc99zoc7gpqegxbb4whikx) or [this Google Drive Link](https://drive.google.com/file/d/1VW8px1Nz9A17i0wMVUfxh6YsPCLVqL-S/view?,mAP,EVALMETRIC
/pretrained/thumos_i3d_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method            |  0.3  |  0.4  |  0.5  |  0.6  |  0.7  |  Avg  | |-------------------|-------|-------|-------|-------|-------|-------| | ActionFormer      | 82.13 | 77.80 | 70.95 | 59.40 | 43.87 | 66.83 |   ## To Reproduce Our Results on ActivityNet 1.3 **Download Features and Annotations** * Download *anet_1.3.tar.gz* (`md5sum c415f50120b9425ee1ede9ac3ce11203`) from [this Box link](https://uwmadison.box.com/s/aisdoymowukc99zoc7gpqegxbb4whikx) or [this Google Drive Link](https://drive.google.com/file/d/1VW8px1Nz9A17i0wMVUfxh6YsPCLVqL-S/view?,tIoUs,EVALMETRIC
The expected average mAP should be around 36.5(%) as in Table 1 of our main paper.,mAP,EVALMETRIC
/pretrained/anet_tsp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.67 | 37.81 |  8.36 | 36.56 |   **[Optional] Reproducing Our Results with I3D Features**  * Download *anet_1.3_i3d.tar.gz* (`md5sum e649425954e0123401650312dd0d56a7`) from [this Google Drive Link](https://drive.google.com/file/d/16239kUT2Z-j6S6PXIT1b_31OJi35QW_o/view?,mAP,EVALMETRIC
/pretrained/anet_tsp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.67 | 37.81 |  8.36 | 36.56 |   **[Optional] Reproducing Our Results with I3D Features**  * Download *anet_1.3_i3d.tar.gz* (`md5sum e649425954e0123401650312dd0d56a7`) from [this Google Drive Link](https://drive.google.com/file/d/16239kUT2Z-j6S6PXIT1b_31OJi35QW_o/view?,tIoU,EVALMETRIC
The expected average mAP should be around 36.0(%).,mAP,EVALMETRIC
/pretrained/anet_i3d_reproduce/ ```  * The results (mAP at tIoUs) with I3D features should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.29 | 36.71 |  8.24 | 36.03 |  ## To Reproduce Our Results on EPIC Kitchens 100 **Download Features and Annotations** * Download *epic_kitchens.tar.gz* (`md5sum add9803756afd9a023bc9a9c547e0229`) from [this Box link](https://uwmadison.box.com/s/vdha47qnce6jhqktz9g4mq1gc40w82yj) or [this Google Drive Link](https://drive.google.com/file/d/1Z4U_dLuu6_cV5NBIrSzsSDOOj2Uar85X/view?,mAP,EVALMETRIC
/pretrained/anet_i3d_reproduce/ ```  * The results (mAP at tIoUs) with I3D features should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.29 | 36.71 |  8.24 | 36.03 |  ## To Reproduce Our Results on EPIC Kitchens 100 **Download Features and Annotations** * Download *epic_kitchens.tar.gz* (`md5sum add9803756afd9a023bc9a9c547e0229`) from [this Box link](https://uwmadison.box.com/s/vdha47qnce6jhqktz9g4mq1gc40w82yj) or [this Google Drive Link](https://drive.google.com/file/d/1Z4U_dLuu6_cV5NBIrSzsSDOOj2Uar85X/view?,tIoUs,EVALMETRIC
The expected average mAP should be around 23.4(%) as in Table 2 of our main paper.,mAP,EVALMETRIC
The expected average mAP should be around 21.9(%) as in Table 2 of our main paper.,mAP,EVALMETRIC
/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).,mAP,EVALMETRIC
/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).,tIoUs,EVALMETRIC
"The expected average mAP and Recall@1x, tIoU=0.5 should be around 22.0(%) and 40.0(%) respectively.",mAP,EVALMETRIC
"The expected average mAP and Recall@1x, tIoU=0.5 should be around 22.0(%) and 40.0(%) respectively.",Recall@1x,EVALMETRIC
"The expected average mAP and Recall@1x, tIoU=0.5 should be around 22.0(%) and 40.0(%) respectively.",tIoU,EVALMETRIC
/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,mAP,EVALMETRIC
/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,tIoUs,EVALMETRIC
BagMinHash2 is essentially always faster and is what we compare against. * DartMinHash: Optimized implementation following the pseudocode in the paper.  ### Performance timings  | id | L0   | log2_L1 | t    | ICWS    | FastICWS | ICWS_xxhash | BagMinHash1 | BagMinHash2 | DartMinHash | |----|------|---------|------|---------|----------|-------------|-------------|-------------|-------------| | 0  | 64   | 0.000   | 64   | 0.899   | 0.060    | 0.538       | 2.439       | 0.628       | 0.042       | | 1  | 1024 | 0.000   | 64   | 11.565  | 0.515    | 9.604       | 4.374       | 1.706       | 0.145       | | 2  | 64   | 0.000   | 1024 | 19.296  | 2.885    | 8.083       | 48.248      | 13.279      | 0.592       | | 3  | 1024 | 0.000   | 1024 | 187.661 | 12.643   | 120.135     | 79.775      | 16.586      | 0.824       | | 4  | 256  | 0.000   | 1    | 0.040   | 0.008    | 0.040       | 0.112       | 0.103       | 0.021       | | 5  | 256  | 0.000   | 256  | 14.645  | 0.939    | 7.716       | 13.687      | 3.270       | 0.187       | | 6  | 1024 | 0.000   | 256  | 45.239  | 2.703    | 30.127      | 18.175      | 4.296       | 0.274       | | 7  | 1024 | 64.000  | 256  | 46.717  | 2.720    | 30.122      | 18.241      | 4.250       | 2.632       | | 8  | 1024 | -64.000 | 256  | 47.677  | 2.719    | 30.117      | 18.096      | 4.192       | 2.333       |  ### Jaccard similarity estimates  | sim_j | t  | ICWS_xxhash | FastICWS | BagMinHash2 | DartMinHash | |-------|----|-------------|----------|-------------|-------------| | 0.500 | 1  | 1.000       | 1.000    | 0.000       | 1.000       | | 0.500 | 2  | 0.500       | 0.500    | 0.000       | 0.500       | | 0.500 | 3  | 0.333       | 0.333    | 0.000       | 0.333       | | 0.500 | 4  | 0.500       | 0.250    | 0.750       | 0.750       | | 0.500 | 5  | 0.000       | 0.400    | 0.600       | 0.200       | | 0.500 | 6  | 0.667       | 0.500    | 0.500       | 0.000       | | 0.500 | 7  | 0.571       | 0.714    | 0.429       | 0.429       | | 0.500 | 8  | 0.250       | 0.375    | 0.625       | 0.500       | | 0.500 | 9  | 0.889       | 0.222    | 0.556       | 0.444       | | 0.500 | 10 | 0.600       | 0.400    | 0.700       | 0.400       |  ## Tests We use Catch2 https://github.com/catchorg/Catch2 for unit testing.,Jaccard similarity,EVALMETRIC
- Number of examples evaluated and the **accuracy**     - Number of answers that the model has not been able to answer correctly (_no_pased_answers_)     - Number of draws in the majority voting (and how many resulted or not in a correct prediction) (_draws_)     - Number of times the model predicted each option (ABCD...),accuracy,EVALMETRIC
"Evaluation ----------  Evaluation of a trained model checkpoint can be done as follows:  ```shell python evaluate.py --config-yml /path/to/config.yml --load-pthpath /path/to/checkpoint.pth --split val --gpu-ids 0 ```  This will generate an EvalAI submission file, and report metrics from the [Visual Dialog paper][5] (Mean reciprocal rank, R@{1, 5, 10}, Mean rank), and Normalized Discounted Cumulative Gain (NDCG), introduced in the first Visual Dialog Challenge (in 2018).",Mean reciprocal rank,EVALMETRIC
"Evaluation ----------  Evaluation of a trained model checkpoint can be done as follows:  ```shell python evaluate.py --config-yml /path/to/config.yml --load-pthpath /path/to/checkpoint.pth --split val --gpu-ids 0 ```  This will generate an EvalAI submission file, and report metrics from the [Visual Dialog paper][5] (Mean reciprocal rank, R@{1, 5, 10}, Mean rank), and Normalized Discounted Cumulative Gain (NDCG), introduced in the first Visual Dialog Challenge (in 2018).",Mean rank,EVALMETRIC
"Evaluation ----------  Evaluation of a trained model checkpoint can be done as follows:  ```shell python evaluate.py --config-yml /path/to/config.yml --load-pthpath /path/to/checkpoint.pth --split val --gpu-ids 0 ```  This will generate an EvalAI submission file, and report metrics from the [Visual Dialog paper][5] (Mean reciprocal rank, R@{1, 5, 10}, Mean rank), and Normalized Discounted Cumulative Gain (NDCG), introduced in the first Visual Dialog Challenge (in 2018).",Normalized Discounted Cumulative Gain,EVALMETRIC
"Evaluation ----------  Evaluation of a trained model checkpoint can be done as follows:  ```shell python evaluate.py --config-yml /path/to/config.yml --load-pthpath /path/to/checkpoint.pth --split val --gpu-ids 0 ```  This will generate an EvalAI submission file, and report metrics from the [Visual Dialog paper][5] (Mean reciprocal rank, R@{1, 5, 10}, Mean rank), and Normalized Discounted Cumulative Gain (NDCG), introduced in the first Visual Dialog Challenge (in 2018).",NDCG,EVALMETRIC
"* validation.py         * Generate fake samples and calculate FID. * tools     * utils.py         * Functions and class for logger, make folders, averageMeter and add logs",FID,EVALMETRIC
"**Evaluation**ï¼We will run the submitted learning method on poisoned CIFAR-10 datasets by 10 backdoor attacks used in our paper, then test the Attack Sucess Rate (ASR) and Clean Accuracy (CA) of the final model.",Attack Sucess Rate,EVALMETRIC
"**Evaluation**ï¼We will run the submitted learning method on poisoned CIFAR-10 datasets by 10 backdoor attacks used in our paper, then test the Attack Sucess Rate (ASR) and Clean Accuracy (CA) of the final model.",ASR,EVALMETRIC
"**Evaluation**ï¼We will run the submitted learning method on poisoned CIFAR-10 datasets by 10 backdoor attacks used in our paper, then test the Attack Sucess Rate (ASR) and Clean Accuracy (CA) of the final model.",Clean Accuracy,EVALMETRIC
"**Evaluation**ï¼We will run the submitted learning method on poisoned CIFAR-10 datasets by 10 backdoor attacks used in our paper, then test the Attack Sucess Rate (ASR) and Clean Accuracy (CA) of the final model.",CA,EVALMETRIC
"| #     |           Paper            |    Venue     | Poisoned data | Architecture | Attack | ASR (%)| CA (%)| | ----- | :------------------------: | :----------: | :------------: | :----------: | :---------: | :-----------: | :----------: | | **1** | **[ABL]()** | NeurIPS 2021 |  *available* |    WRN-16-1    |   BadNets   |     3.04     |    86.11      | | **2** |                            |              |                |              |             |               |              | | **3** |                            |              |                |              |             |               |              | | **4** |                            |              |                |              |             |               |              | | **5** |                            |              |                |              |             |               |              | | **6** |                            |              |                |              |             |               |              | | **7** |                            |              |                |              |             |               |              | | **8** |                            |              |                |              |             |               |              |  ------  ## Verifying the unlearning effect of ABL with 1% isolated data:  ### An example with a pretrained model WRN-16-1, CIFAR-10, GridTrigger, target label 0, weights: `.",ASR,EVALMETRIC
"| #     |           Paper            |    Venue     | Poisoned data | Architecture | Attack | ASR (%)| CA (%)| | ----- | :------------------------: | :----------: | :------------: | :----------: | :---------: | :-----------: | :----------: | | **1** | **[ABL]()** | NeurIPS 2021 |  *available* |    WRN-16-1    |   BadNets   |     3.04     |    86.11      | | **2** |                            |              |                |              |             |               |              | | **3** |                            |              |                |              |             |               |              | | **4** |                            |              |                |              |             |               |              | | **5** |                            |              |                |              |             |               |              | | **6** |                            |              |                |              |             |               |              | | **7** |                            |              |                |              |             |               |              | | **8** |                            |              |                |              |             |               |              |  ------  ## Verifying the unlearning effect of ABL with 1% isolated data:  ### An example with a pretrained model WRN-16-1, CIFAR-10, GridTrigger, target label 0, weights: `.",CA,EVALMETRIC
It shows the ASR (bad acc) of drops from 99.99% to 0.48% with no obviouse drop of clean acc.,ASR,EVALMETRIC
It shows the ASR (bad acc) of drops from 99.99% to 0.48% with no obviouse drop of clean acc.,acc,EVALMETRIC
It shows the ASR (bad acc) of drops from 99.99% to 0.48% with no obviouse drop of clean acc.,acc,EVALMETRIC
/input --output-dir ${OUTDIR}_dev \      --config config/nl2prog.meta_2_0.001.rank.devconfig \     --meta_learning --test-model model_zoo/meta_sum/table_nl_prog-40  --production ``` * Run execution for developement set as follows:   ```   $ cp ${OUTDIR}_dev/test_top_1.log dev_top_1.log   $ python2 execute_dev.py      #Q2 (predition) result is wrong: 1254     #Q1 or Q2 fail to parse: 0     #Q1 (ground truth) exec to None: 20     #Q1 (ground truth) failed to execute: 0     Logical Form Accuracy: 0.631383269546     Execute Accuracy: 0.68277747403   ```   - Test set ``` $ mkdir -p ${OUTDIR}_test $ python run.py --input-dir .,Accuracy,EVALMETRIC
/input --output-dir ${OUTDIR}_dev \      --config config/nl2prog.meta_2_0.001.rank.devconfig \     --meta_learning --test-model model_zoo/meta_sum/table_nl_prog-40  --production ``` * Run execution for developement set as follows:   ```   $ cp ${OUTDIR}_dev/test_top_1.log dev_top_1.log   $ python2 execute_dev.py      #Q2 (predition) result is wrong: 1254     #Q1 or Q2 fail to parse: 0     #Q1 (ground truth) exec to None: 20     #Q1 (ground truth) failed to execute: 0     Logical Form Accuracy: 0.631383269546     Execute Accuracy: 0.68277747403   ```   - Test set ``` $ mkdir -p ${OUTDIR}_test $ python run.py --input-dir .,Accuracy,EVALMETRIC
$ python2 execute.py     #Q2 (predition) result is wrong: 2556     #Q1 or Q2 fail to parse: 0     #Q1 (ground truth) exec to None: 48     #Q1 (ground truth) failed to execute: 0     Logical Form Accuracy: 0.628073829775     Execute Accuracy: 0.680379563733   ```  - Baseline model on test set  ```  $ OUTDIR=output/base_sum  $ python run.py --input-dir .,Accuracy,EVALMETRIC
$ python2 execute.py     #Q2 (predition) result is wrong: 2556     #Q1 or Q2 fail to parse: 0     #Q1 (ground truth) exec to None: 48     #Q1 (ground truth) failed to execute: 0     Logical Form Accuracy: 0.628073829775     Execute Accuracy: 0.680379563733   ```  - Baseline model on test set  ```  $ OUTDIR=output/base_sum  $ python run.py --input-dir .,Accuracy,EVALMETRIC
$ python2 execute.py     #Q2 (predition) result is wrong: 2636     #Q1 or Q2 fail to parse: 0     #Q1 (ground truth) exec to None: 48     #Q1 (ground truth) failed to execute: 0     Logical Form Accuracy: 0.614592374009     Execute Accuracy: 0.668055314471   ```   # Pre-trained Models - Download [pretrained model checkpoints](https://1drv.ms/u/s!,Accuracy,EVALMETRIC
$ python2 execute.py     #Q2 (predition) result is wrong: 2636     #Q1 or Q2 fail to parse: 0     #Q1 (ground truth) exec to None: 48     #Q1 (ground truth) failed to execute: 0     Logical Form Accuracy: 0.614592374009     Execute Accuracy: 0.668055314471   ```   # Pre-trained Models - Download [pretrained model checkpoints](https://1drv.ms/u/s!,Accuracy,EVALMETRIC
**scibert_2stage_predictions.tsv** the classifier predictions to compute the prec@100 scores    2.,prec@100,EVALMETRIC
"**scibert_cv_predictions.tsv** the classifier predictions to compute precision, recall and F1 3.",precision,EVALMETRIC
"**scibert_cv_predictions.tsv** the classifier predictions to compute precision, recall and F1 3.",recall,EVALMETRIC
"**scibert_cv_predictions.tsv** the classifier predictions to compute precision, recall and F1 3.",F1,EVALMETRIC
"**classifier_p100.py** code to reproduce the prec@100 results in section 3, table 4 7.",prec@100,EVALMETRIC
"GPT4V, GPT4-turbo, CLIP, SSIM      â   âââ sample_rank_gen.py                       ## generate script for `EditRank_ori` and `EditRank`      â   âââ summary.json                             ## generated by `summary.py`      â   âââ summary_ori.json                         ## generated by `summary.py`      â   âââ summary.py                               ## generate script for `summary.json` and `summary.json`,                                                       #  describe metric scores for every models in every dimensions      â   âââ summary_model_type_avg_score.json        ## generated by `summary_model_type_avg_score.py`      â   âââ summary_model_type_avg_score.py          ## generate script for `summary_model_type_avg_score.json`,                                                       # describe metric scores for every editing models                                                       # in every dimensions      âââ readme.md    ```    # ð¤ How to evaluate with my own editing model    [Check how to evaluate with my own editing model](.",SSIM,EVALMETRIC
The most widely used metric for measuring the similarity between real and generated images has been the Fr&eacute;chet Inception Distance (FID) score.,Fr&eacute;chet Inception Distance,EVALMETRIC
The most widely used metric for measuring the similarity between real and generated images has been the Fr&eacute;chet Inception Distance (FID) score.,FID,EVALMETRIC
"In this paper, we show that even the latest version of the precision and recall (Kynk&auml;&auml;nniemi et al., 2019) metrics are not reliable yet.",precision,EVALMETRIC
"In this paper, we show that even the latest version of the precision and recall (Kynk&auml;&auml;nniemi et al., 2019) metrics are not reliable yet.",recall,EVALMETRIC
We propose **density and coverage** metrics that solve the above issues.,density,EVALMETRIC
We propose **density and coverage** metrics that solve the above issues.,coverage,EVALMETRIC
We analytically and experimentally show that density and coverage provide more interpretable and reliable signals for practitioners than the existing metrics.,density,EVALMETRIC
We analytically and experimentally show that density and coverage provide more interpretable and reliable signals for practitioners than the existing metrics.,coverage,EVALMETRIC
"Background  ### Precision and recall metrics  Precision and recall are defined below:  <a href=""https://www.codecogs.com/eqnedit.php?",Precision,EVALMETRIC
"Background  ### Precision and recall metrics  Precision and recall are defined below:  <a href=""https://www.codecogs.com/eqnedit.php?",recall,EVALMETRIC
"Background  ### Precision and recall metrics  Precision and recall are defined below:  <a href=""https://www.codecogs.com/eqnedit.php?",Precision,EVALMETRIC
"Background  ### Precision and recall metrics  Precision and recall are defined below:  <a href=""https://www.codecogs.com/eqnedit.php?",recall,EVALMETRIC
"latex=\fn_cm&space;\text{precision}:=\frac{1}{M}\sum_{j=1}^{M}1_{Y_j\in\text{manifold}(X_1,\cdots,X_N)}"" target=""_blank""><img src=""https://latex.codecogs.com/svg.latex?",precision,EVALMETRIC
"\fn_cm&space;\text{precision}:=\frac{1}{M}\sum_{j=1}^{M}1_{Y_j\in\text{manifold}(X_1,\cdots,X_N)}"" title=""\text{precision}:=\frac{1}{M}\sum_{j=1}^{M}1_{Y_j\in\text{manifold}(X_1,\cdots,X_N)}"" /></a>  <a href=""https://www.codecogs.com/eqnedit.php?",precision,EVALMETRIC
"\fn_cm&space;\text{precision}:=\frac{1}{M}\sum_{j=1}^{M}1_{Y_j\in\text{manifold}(X_1,\cdots,X_N)}"" title=""\text{precision}:=\frac{1}{M}\sum_{j=1}^{M}1_{Y_j\in\text{manifold}(X_1,\cdots,X_N)}"" /></a>  <a href=""https://www.codecogs.com/eqnedit.php?",precision,EVALMETRIC
"latex=\fn_cm&space;\text{recall}:=\frac{1}{N}\sum_{i=1}^{N}1_{X_i\in\text{manifold}(Y_1,\cdots,Y_M)}"" target=""_blank""><img src=""https://latex.codecogs.com/svg.latex?",recall,EVALMETRIC
"\fn_cm&space;\text{recall}:=\frac{1}{N}\sum_{i=1}^{N}1_{X_i\in\text{manifold}(Y_1,\cdots,Y_M)}"" title=""\text{recall}:=\frac{1}{N}\sum_{i=1}^{N}1_{X_i\in\text{manifold}(Y_1,\cdots,Y_M)}"" /></a>  where the manifold is the defined as  <a href=""https://www.codecogs.com/eqnedit.php?",recall,EVALMETRIC
"\fn_cm&space;\text{recall}:=\frac{1}{N}\sum_{i=1}^{N}1_{X_i\in\text{manifold}(Y_1,\cdots,Y_M)}"" title=""\text{recall}:=\frac{1}{N}\sum_{i=1}^{N}1_{X_i\in\text{manifold}(Y_1,\cdots,Y_M)}"" /></a>  where the manifold is the defined as  <a href=""https://www.codecogs.com/eqnedit.php?",recall,EVALMETRIC
"\inline&space;\fn_cm&space;\text{NND}_k(X_i)"" title=""\text{NND}_k(X_i)"" /></a> is the distance to the kth-nearest neighbour.   ### Density and coverage metrics  Density and coverage are defined below:  <a href=""https://www.codecogs.com/eqnedit.php?",Density,EVALMETRIC
"\inline&space;\fn_cm&space;\text{NND}_k(X_i)"" title=""\text{NND}_k(X_i)"" /></a> is the distance to the kth-nearest neighbour.   ### Density and coverage metrics  Density and coverage are defined below:  <a href=""https://www.codecogs.com/eqnedit.php?",coverage,EVALMETRIC
"\inline&space;\fn_cm&space;\text{NND}_k(X_i)"" title=""\text{NND}_k(X_i)"" /></a> is the distance to the kth-nearest neighbour.   ### Density and coverage metrics  Density and coverage are defined below:  <a href=""https://www.codecogs.com/eqnedit.php?",Density,EVALMETRIC
"\inline&space;\fn_cm&space;\text{NND}_k(X_i)"" title=""\text{NND}_k(X_i)"" /></a> is the distance to the kth-nearest neighbour.   ### Density and coverage metrics  Density and coverage are defined below:  <a href=""https://www.codecogs.com/eqnedit.php?",coverage,EVALMETRIC
"latex=\fn_cm&space;\text{density}:=\frac{1}{kM}\sum_{j=1}^{M}\sum_{i=1}^{N}1_{Y_j\in&space;B(X_i,\text{NND}_k(X_i))}"" target=""_blank""><img src=""https://latex.codecogs.com/svg.latex?",density,EVALMETRIC
"\fn_cm&space;\text{density}:=\frac{1}{kM}\sum_{j=1}^{M}\sum_{i=1}^{N}1_{Y_j\in&space;B(X_i,\text{NND}_k(X_i))}"" title=""\text{density}:=\frac{1}{kM}\sum_{j=1}^{M}\sum_{i=1}^{N}1_{Y_j\in B(X_i,\text{NND}_k(X_i))}"" /></a>  <a href=""https://www.codecogs.com/eqnedit.php?",density,EVALMETRIC
latex=\fn_cm&space;\text{coverage}:=\frac{1}{N}\sum_{i=1}^{N}1_{\exists\text{&space;}j\text{&space;s.t.,coverage,EVALMETRIC
\fn_cm&space;\text{coverage}:=\frac{1}{N}\sum_{i=1}^{N}1_{\exists\text{&space;}j\text{&space;s.t.,coverage,EVALMETRIC
"&space;}&space;Y_j\in&space;B(X_i,\text{NND}_k(X_i))}"" title=""\text{coverage}:=\frac{1}{N}\sum_{i=1}^{N}1_{\exists\text{ }j\text{ s.t. } Y_j\in B(X_i,\text{NND}_k(X_i))}"" /></a>   ### Why are DC better than PR?",DC,EVALMETRIC
"&space;}&space;Y_j\in&space;B(X_i,\text{NND}_k(X_i))}"" title=""\text{coverage}:=\frac{1}{N}\sum_{i=1}^{N}1_{\exists\text{ }j\text{ s.t. } Y_j\in B(X_i,\text{NND}_k(X_i))}"" /></a>   ### Why are DC better than PR?",PR,EVALMETRIC
"raw=true"" alt=""Precision versus density."" width=""500""/> </p>  **Precision versus Density.**  Because of the real outlier sample, the manifold is overestimated.",Precision,EVALMETRIC
"raw=true"" alt=""Precision versus density."" width=""500""/> </p>  **Precision versus Density.**  Because of the real outlier sample, the manifold is overestimated.",density,EVALMETRIC
"raw=true"" alt=""Precision versus density."" width=""500""/> </p>  **Precision versus Density.**  Because of the real outlier sample, the manifold is overestimated.",Precision,EVALMETRIC
"raw=true"" alt=""Precision versus density."" width=""500""/> </p>  **Precision versus Density.**  Because of the real outlier sample, the manifold is overestimated.",Density,EVALMETRIC
The problem of overestimating precision (100%) is resolved using the density estimate (60%).,precision,EVALMETRIC
The problem of overestimating precision (100%) is resolved using the density estimate (60%).,density,EVALMETRIC
"raw=true"" alt=""Recall versus coverage."" width=""600""/> </p>  **Recall versus Coverage.**  The real and fake samples are identical across left and right.",Recall,EVALMETRIC
"raw=true"" alt=""Recall versus coverage."" width=""600""/> </p>  **Recall versus Coverage.**  The real and fake samples are identical across left and right.",coverage,EVALMETRIC
"raw=true"" alt=""Recall versus coverage."" width=""600""/> </p>  **Recall versus Coverage.**  The real and fake samples are identical across left and right.",Recall,EVALMETRIC
"raw=true"" alt=""Recall versus coverage."" width=""600""/> </p>  **Recall versus Coverage.**  The real and fake samples are identical across left and right.",Coverage,EVALMETRIC
"In the figure above, while the fake samples are generally far from the modes in real samples, the recall measure is rewarded by the fact that real samples are contained in the overestimated fake manifold.   ## 2.",recall,EVALMETRIC
"We compute precision, recall, density, and coverage estimates below.",precision,EVALMETRIC
"We compute precision, recall, density, and coverage estimates below.",recall,EVALMETRIC
"We compute precision, recall, density, and coverage estimates below.",density,EVALMETRIC
"We compute precision, recall, density, and coverage estimates below.",coverage,EVALMETRIC
"```python {'precision': 0.4772,  'recall': 0.4705,  'density': 1.0555,  'coverage': 0.9735} ```  ## 3.",precision,EVALMETRIC
"```python {'precision': 0.4772,  'recall': 0.4705,  'density': 1.0555,  'coverage': 0.9735} ```  ## 3.",recall,EVALMETRIC
"```python {'precision': 0.4772,  'recall': 0.4705,  'density': 1.0555,  'coverage': 0.9735} ```  ## 3.",density,EVALMETRIC
"```python {'precision': 0.4772,  'recall': 0.4705,  'density': 1.0555,  'coverage': 0.9735} ```  ## 3.",coverage,EVALMETRIC
"/s3prl/downstream/speech_translation) by [Hsiang-Sheng Tsai](https://github.com/bearhsiang) ***(NTU)***, [Out-of-domain ASR](.",ASR,EVALMETRIC
[integration](file/S3PRL-integration.png)  You can start the journey of SSL with the following entry points:  - S3PRL: [A simple SUPERB downstream task](https://github.com/s3prl/s3prl/blob/main/s3prl/downstream/docs/superb.md#pr-phoneme-recognition) - ESPNet: [Levearging S3PRL for ASR](https://github.com/espnet/espnet/tree/master/egs2/librispeech/asr1#self-supervised-learning-features-hubert_large_ll60k-conformer-utt_mvn-with-transformer-lm)  ---  Feel free to use or modify our toolkit in your research.,ASR,EVALMETRIC
[integration](file/S3PRL-integration.png)  You can start the journey of SSL with the following entry points:  - S3PRL: [A simple SUPERB downstream task](https://github.com/s3prl/s3prl/blob/main/s3prl/downstream/docs/superb.md#pr-phoneme-recognition) - ESPNet: [Levearging S3PRL for ASR](https://github.com/espnet/espnet/tree/master/egs2/librispeech/asr1#self-supervised-learning-features-hubert_large_ll60k-conformer-utt_mvn-with-transformer-lm)  ---  Feel free to use or modify our toolkit in your research.,asr,EVALMETRIC
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",ASR,EVALMETRIC
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",ASR,EVALMETRIC
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",ASR,EVALMETRIC
/motivation.png) Figure 1: Example learning curves for predicting infiltration from chest x-rays assessed using area under the receiver operating characteristic (AUROC).,area under the receiver operating characteristic,EVALMETRIC
/motivation.png) Figure 1: Example learning curves for predicting infiltration from chest x-rays assessed using area under the receiver operating characteristic (AUROC).,AUROC,EVALMETRIC
"*  ## Our contributions  Our contributions are:  (1) a reusable GP-based accuracy probabilistic extrapolator (APEx-GP) that can match existing curve-fitting approaches in terms of error while providing additional uncertainty estimates, and   (2) a careful assessment of our proposed probabilistic extrapolations compared to ground truth on larger datasets across six medical classification tasks involving both 2D and 3D images across diverse modalities (x-ray, ultrasound, and CT) with various sample sizes.  ## Using our method  To use our Gaussian process to extrapolate classifier accuracy to larger datasets see `notebooks/demo.ipynb`.  ### Initializing our Gaussian process  ```python likelihood = gpytorch.likelihoods.GaussianLikelihood() # Note: If you want to use the Gaussian process with an arctan mean function use models.GPArctan() instead. model = models.GPPowerLaw(X_train, y_train, likelihood, epsilon_min=0.05, with_priors=True) ```  ### Extrapolating classifier accuracy  ```python with torch.no_grad(): predictions = likelihood(model(X_test)) loc = predictions.mean.numpy() scale = predictions.stddev.numpy() # Note: If you want to forecast with 20%-80% change lower and upper percentile. lower, upper = priors.truncated_normal_uncertainty(a=0.0, b=1.0, loc=loc, scale=scale, lower_percentile=0.025, upper_percentile=0.975)  ```  ## Citation  ```bibtex @inproceedings{harvey2023probabilistic,   author={Harvey, Ethan and Chen, Wansu and Kent, David M. and Hughes, Michael C.},   title={A Probabilistic Method to Predict Classifier Accuracy on Larger Datasets given Small Pilot Data},   booktitle={Machine Learning for Health (ML4H)},   year={2023} } ```  ## Reproducing results  To reproduce model performance at varying dataset sizes 1) download datasets (see `encode_images/README.md` and `label_images/README.md` for more details) and 2) fit classifiers to each dataset (see `src/finetune_2D.py` and `src/finetune_3D.py`).",accuracy,EVALMETRIC
"We investigate abstract datasets such as XOR, Spiral, and Circle and some well-known security-specific datasets for intrusion detection of simulated network traffic, using distributional shift detection measures including the Kolmogorov-Smirnov, Kuiper, Anderson-Darling, Wasserstein and mixed Wasserstein-Anderson-Darling measures.",Kolmogorov-Smirnov,EVALMETRIC
"We investigate abstract datasets such as XOR, Spiral, and Circle and some well-known security-specific datasets for intrusion detection of simulated network traffic, using distributional shift detection measures including the Kolmogorov-Smirnov, Kuiper, Anderson-Darling, Wasserstein and mixed Wasserstein-Anderson-Darling measures.",Kuiper,EVALMETRIC
"We investigate abstract datasets such as XOR, Spiral, and Circle and some well-known security-specific datasets for intrusion detection of simulated network traffic, using distributional shift detection measures including the Kolmogorov-Smirnov, Kuiper, Anderson-Darling, Wasserstein and mixed Wasserstein-Anderson-Darling measures.",Anderson-Darling,EVALMETRIC
"We investigate abstract datasets such as XOR, Spiral, and Circle and some well-known security-specific datasets for intrusion detection of simulated network traffic, using distributional shift detection measures including the Kolmogorov-Smirnov, Kuiper, Anderson-Darling, Wasserstein and mixed Wasserstein-Anderson-Darling measures.",Wasserstein,EVALMETRIC
"We investigate abstract datasets such as XOR, Spiral, and Circle and some well-known security-specific datasets for intrusion detection of simulated network traffic, using distributional shift detection measures including the Kolmogorov-Smirnov, Kuiper, Anderson-Darling, Wasserstein and mixed Wasserstein-Anderson-Darling measures.",mixed Wasserstein-Anderson-Darling,EVALMETRIC
"Thus, using a trusted dataset the classifier will be trained and its performance will be measured with existing KPIs.",KPI,EVALMETRIC
"So, it cannot be assured that the classifier can operate as accurate as of the training phase.",accurate,EVALMETRIC
The CDF-based statistical difference of each class in the training phase and application phase is used to estimate the accuracy.,accuracy,EVALMETRIC
"If the estimated accuracy and expected confidence difference was very low, the classifier results and accuracy can be trusted (In this example the autonomous car continues its operation), if the difference was low, the system can ask for more data and re-evaluation to make sure about the distance.",accuracy,EVALMETRIC
"If the estimated accuracy and expected confidence difference was very low, the classifier results and accuracy can be trusted (In this example the autonomous car continues its operation), if the difference was low, the system can ask for more data and re-evaluation to make sure about the distance.",accuracy,EVALMETRIC
"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>].",accuracy,EVALMETRIC
"**Most importantly, it highlights the main reported metrics under the key `eval_results`.**     * In the case of PaulGraham Passkey, it includes the accuracy of each depth-length combination, the average accuracy at each length (`background_len_wise_results`), and the overall accuracy (`overall_results`) in both exact and partial match fashion",accuracy,EVALMETRIC
# OpenAUC: Towards AUC-Oriented Open-Set Recognition This is a Pytorch implementation of our paper: [OpenAUC: Towards AUC-Oriented Open-Set Recognition](https://arxiv.org/abs/2210.13458).,OpenAUC,EVALMETRIC
# OpenAUC: Towards AUC-Oriented Open-Set Recognition This is a Pytorch implementation of our paper: [OpenAUC: Towards AUC-Oriented Open-Set Recognition](https://arxiv.org/abs/2210.13458).,OpenAUC,EVALMETRIC
"**If you only want to evaluate the model performance on OpenAUC,** please refer the file `utils/test_utils`.",OpenAUC,EVALMETRIC
And a more detailed instruction can be found in our [library](https://github.com/statusrank/XCurve/blob/master/example/example_ipynb/Metrics_for_AUTKC_OpenAUC.ipynb),OpenAUC,EVALMETRIC
"In this paper, a systematic analysis reveals that most existing metrics are essentially inconsistent with the aforementioned goal of OSR: (1) For metrics extended from close-set classification, such as Open-set F-score, Youden's index, and Normalized Accuracy, a poor open-set prediction can escape from a low performance score with a superior close-set prediction. (2) Novelty detection AUC, which measures the ranking performance between close-set and open-set samples, ignores the close-set performance.",Open-set F-score,EVALMETRIC
"In this paper, a systematic analysis reveals that most existing metrics are essentially inconsistent with the aforementioned goal of OSR: (1) For metrics extended from close-set classification, such as Open-set F-score, Youden's index, and Normalized Accuracy, a poor open-set prediction can escape from a low performance score with a superior close-set prediction. (2) Novelty detection AUC, which measures the ranking performance between close-set and open-set samples, ignores the close-set performance.",Youden's index,EVALMETRIC
"In this paper, a systematic analysis reveals that most existing metrics are essentially inconsistent with the aforementioned goal of OSR: (1) For metrics extended from close-set classification, such as Open-set F-score, Youden's index, and Normalized Accuracy, a poor open-set prediction can escape from a low performance score with a superior close-set prediction. (2) Novelty detection AUC, which measures the ranking performance between close-set and open-set samples, ignores the close-set performance.",Normalized Accuracy,EVALMETRIC
"In this paper, a systematic analysis reveals that most existing metrics are essentially inconsistent with the aforementioned goal of OSR: (1) For metrics extended from close-set classification, such as Open-set F-score, Youden's index, and Normalized Accuracy, a poor open-set prediction can escape from a low performance score with a superior close-set prediction. (2) Novelty detection AUC, which measures the ranking performance between close-set and open-set samples, ignores the close-set performance.",AUC,EVALMETRIC
"To fix these issues, we propose a novel metric named OpenAUC.",OpenAUC,EVALMETRIC
"Compared with existing metrics, OpenAUC enjoys a concise pairwise formulation that evaluates open-set performance and close-set performance in a coupling manner.",OpenAUC,EVALMETRIC
Further analysis shows that OpenAUC is free from the aforementioned inconsistency properties.,OpenAUC,EVALMETRIC
"Finally, an end-to-end learning method is proposed to minimize the OpenAUC risk, and the experimental results on popular benchmark datasets speak to its effectiveness.",OpenAUC,EVALMETRIC
"Note that here we follow the traditional assumption that $\forall c \in Y_k, \mathbb{P}[y = c \mid x]$ and $r(x) \propto 1 / \max_{k \in Y_k} f(x)_k$, which shows similar performances as those reported in our paper.   ## Citation  ``` @InProceedings{openauc,     title = {OpenAUC: Towards AUC-Oriented Open-Set Recognition},     author = {Zitai Wang and  Qianqian Xu and Zhiyong Yang and Yuan He and Xiaochun Cao and Qingming Huang},     booktitle = {Annual Conference on Neural Information Processing Systems},     year = {2022},     pages = {25033--25045} } ```",OpenAUC,EVALMETRIC
/ground.sh 0 val # Output the relation-aware spatio-temporal attention python generate_track_link.py # Generate relation-aware trajectories with Viterbi algorithm. python eval_ground.py # Evaluate the performance ``` You will get accuracy Acc_R: 24.58%.,accuracy,EVALMETRIC
/ground.sh 0 val # Output the relation-aware spatio-temporal attention python generate_track_link.py # Generate relation-aware trajectories with Viterbi algorithm. python eval_ground.py # Evaluate the performance ``` You will get accuracy Acc_R: 24.58%.,Acc_R,EVALMETRIC
```python import ComputationalHypergraphDiscovery.decision as decision ```  There are two leading indicators that we use to make decisions (See the [paper][paper_url] or the [blog post][blog_url] for more details): - **Signal-to-noise ratio**: The signal-to-noise ratio is a measure of how much the signal is stronger than the noise.,Signal-to-noise ratio,EVALMETRIC
```python import ComputationalHypergraphDiscovery.decision as decision ```  There are two leading indicators that we use to make decisions (See the [paper][paper_url] or the [blog post][blog_url] for more details): - **Signal-to-noise ratio**: The signal-to-noise ratio is a measure of how much the signal is stronger than the noise.,signal-to-noise ratio,EVALMETRIC
"We also use its complementary value, the noise-to-signal ratio.",noise-to-signal ratio,EVALMETRIC
- **Z_test**: The Z_test is a statistical test that allows to test if the signal-to-noise ratio we obtained is statistically significant.,Z_test,EVALMETRIC
- **Z_test**: The Z_test is a statistical test that allows to test if the signal-to-noise ratio we obtained is statistically significant.,Z_test,EVALMETRIC
- **Z_test**: The Z_test is a statistical test that allows to test if the signal-to-noise ratio we obtained is statistically significant.,signal-to-noise ratio,EVALMETRIC
"To do so, we compute the signal-to-noise ratio and the `Z_test` for each kernel.",signal-to-noise ratio,EVALMETRIC
"To do so, we compute the signal-to-noise ratio and the `Z_test` for each kernel.",Z_test,EVALMETRIC
"We get the following performances:  An example of output is the following dictionary (some keys have been hidden for clarity): ```python kernel_performances={   'linear': {     'noise-to-signal ratio': 0.45,     'Z_test': [0.98, 1.0]     },   'quadratic': {#see note above, this is not the quadratic kernel but linear+quadratic     'noise-to-signal ratio': 0.91,      'Z_test': [0.96, 1.0]     },    'gaussian': {#same comment as above     'noise-to-signal ratio': 0.33,      'Z_test': [0.74, 1.0]     } } ```  Several decisions can be made using this dictionary.",noise-to-signal ratio,EVALMETRIC
"We get the following performances:  An example of output is the following dictionary (some keys have been hidden for clarity): ```python kernel_performances={   'linear': {     'noise-to-signal ratio': 0.45,     'Z_test': [0.98, 1.0]     },   'quadratic': {#see note above, this is not the quadratic kernel but linear+quadratic     'noise-to-signal ratio': 0.91,      'Z_test': [0.96, 1.0]     },    'gaussian': {#same comment as above     'noise-to-signal ratio': 0.33,      'Z_test': [0.74, 1.0]     } } ```  Several decisions can be made using this dictionary.",Z_test,EVALMETRIC
"We get the following performances:  An example of output is the following dictionary (some keys have been hidden for clarity): ```python kernel_performances={   'linear': {     'noise-to-signal ratio': 0.45,     'Z_test': [0.98, 1.0]     },   'quadratic': {#see note above, this is not the quadratic kernel but linear+quadratic     'noise-to-signal ratio': 0.91,      'Z_test': [0.96, 1.0]     },    'gaussian': {#same comment as above     'noise-to-signal ratio': 0.33,      'Z_test': [0.74, 1.0]     } } ```  Several decisions can be made using this dictionary.",noise-to-signal ratio,EVALMETRIC
"We get the following performances:  An example of output is the following dictionary (some keys have been hidden for clarity): ```python kernel_performances={   'linear': {     'noise-to-signal ratio': 0.45,     'Z_test': [0.98, 1.0]     },   'quadratic': {#see note above, this is not the quadratic kernel but linear+quadratic     'noise-to-signal ratio': 0.91,      'Z_test': [0.96, 1.0]     },    'gaussian': {#same comment as above     'noise-to-signal ratio': 0.33,      'Z_test': [0.74, 1.0]     } } ```  Several decisions can be made using this dictionary.",Z_test,EVALMETRIC
"We get the following performances:  An example of output is the following dictionary (some keys have been hidden for clarity): ```python kernel_performances={   'linear': {     'noise-to-signal ratio': 0.45,     'Z_test': [0.98, 1.0]     },   'quadratic': {#see note above, this is not the quadratic kernel but linear+quadratic     'noise-to-signal ratio': 0.91,      'Z_test': [0.96, 1.0]     },    'gaussian': {#same comment as above     'noise-to-signal ratio': 0.33,      'Z_test': [0.74, 1.0]     } } ```  Several decisions can be made using this dictionary.",noise-to-signal ratio,EVALMETRIC
"We get the following performances:  An example of output is the following dictionary (some keys have been hidden for clarity): ```python kernel_performances={   'linear': {     'noise-to-signal ratio': 0.45,     'Z_test': [0.98, 1.0]     },   'quadratic': {#see note above, this is not the quadratic kernel but linear+quadratic     'noise-to-signal ratio': 0.91,      'Z_test': [0.96, 1.0]     },    'gaussian': {#same comment as above     'noise-to-signal ratio': 0.33,      'Z_test': [0.74, 1.0]     } } ```  Several decisions can be made using this dictionary.",Z_test,EVALMETRIC
"- Choose an amount of noise that is acceptable, and choose the simplest kernel that has a noise-to-signal ratio below this threshold",noise-to-signal ratio,EVALMETRIC
- Choose the kernel that has the lowest noise such that the noise is not in the `Z_test` interval (which would mean the noise ratio is not statistically significant),Z_test,EVALMETRIC
"To do so, we remove the ancestors one by one and compute the signal-to-noise ratio and the `Z_test` after each removal.",signal-to-noise ratio,EVALMETRIC
"To do so, we remove the ancestors one by one and compute the signal-to-noise ratio and the `Z_test` after each removal.",Z_test,EVALMETRIC
"The evolution of the signal-to-noise ratio and the `Z_test` gives the following graph:   <img style=""width:100%;"" alt=""Prototypical signal to noise ratio evolution"" src=""_images/signal_to_noise_ratio_evolution.png""></a>  As we remove ancestors, it becomes harder to explain the node from the remaining ancestors.",signal-to-noise ratio,EVALMETRIC
"The evolution of the signal-to-noise ratio and the `Z_test` gives the following graph:   <img style=""width:100%;"" alt=""Prototypical signal to noise ratio evolution"" src=""_images/signal_to_noise_ratio_evolution.png""></a>  As we remove ancestors, it becomes harder to explain the node from the remaining ancestors.",Z_test,EVALMETRIC
"Seeing the evolution of the signal-to-noise ratio and the `Z_test`, we must choose how many ancestors we wish to keep.",signal-to-noise ratio,EVALMETRIC
"Seeing the evolution of the signal-to-noise ratio and the `Z_test`, we must choose how many ancestors we wish to keep.",Z_test,EVALMETRIC
"We tested our method on various LLMs such as Vicuna, Falcon, and Mistral, achieving an Attack Success Rate (ASR) over 80%.",Attack Success Rate,EVALMETRIC
"We tested our method on various LLMs such as Vicuna, Falcon, and Mistral, achieving an Attack Success Rate (ASR) over 80%.",ASR,EVALMETRIC
"We also tested the model against Llama2-chat, the fine-tuned version of Llama2 designed to resist Jailbreak attacks, achieving good ASR with a suboptimal initial trigger seed.",ASR,EVALMETRIC
"Run the script from the command line by specifying the path to the instruction file and the authentication token:  ```bash python main.py data/instructions.csv [API_AUTH_TOKEN] ```  Replace `instructions.csv` with the path to your text file containing the instructions, and `API_AUTH_TOKEN` with the actual authentication token.  ### ð§   Supported Models  You can test the following models with QROA:  - **Llama2-chat** (`llama2_chat_hf`) - **Llama2** (`llama2_hf`) - **Vicuna** (`vicuna_hf`) - **Mistral** (`mistral_hf`) - **Falcon** (`falcon_hf`) - **OpenAI GPT** (`openai-0613`) - **Mistral Next** (`mistral`)  Simply change the `model` parameter in the `main` function to the desired model.  ### ð§ª  Demo and Testing Model Generation - **Notebook Demo:** Run `demo.ipynb` to see a demonstration of the process. - **Notebook Analysis Experiement:** Run `analysis.ipynb` to analyse results and calculate metrics value (ASR). - **Testing Model:** Generation: Execute `generate.py` to test the generation process on custom instructions and triggers.",ASR,EVALMETRIC
"/tools/dist_test.sh configs/mot/svm/svm_test.py 8 \    --eval bbox track --checkpoint svm_motsync_ch_mot17half.pth ```  ## Main Results ### MOT17 | Method | Dataset |                Train Data                | MOTA | HOTA | IDF1 |    URL    | | :---: | :---: |:----------------------------------------:|:----:|:----:|:----:|:---------:| | SVM | MOT17 | MOT17 half-train + crowdhuman + MOTSynth | 79.7 | 68.1 | 80.9 | [model](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm_mot17-half-val.pth) |  ## Citation If you find this work useful in your research, please consider citing: ``` @InProceedings{Zhao_2023_CVPR,     author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},     title     = {Streaming Video Model},     booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},     month     = {June},     year      = {2023},     pages     = {14602-14612} } ``` ## Acknowledgement Our code are built on top of [MMTracking](https://github.com/open-mmlab/mmtracking/) and [CLIP](https://github.com/openai/CLIP).",MOTA,EVALMETRIC
"/tools/dist_test.sh configs/mot/svm/svm_test.py 8 \    --eval bbox track --checkpoint svm_motsync_ch_mot17half.pth ```  ## Main Results ### MOT17 | Method | Dataset |                Train Data                | MOTA | HOTA | IDF1 |    URL    | | :---: | :---: |:----------------------------------------:|:----:|:----:|:----:|:---------:| | SVM | MOT17 | MOT17 half-train + crowdhuman + MOTSynth | 79.7 | 68.1 | 80.9 | [model](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm_mot17-half-val.pth) |  ## Citation If you find this work useful in your research, please consider citing: ``` @InProceedings{Zhao_2023_CVPR,     author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},     title     = {Streaming Video Model},     booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},     month     = {June},     year      = {2023},     pages     = {14602-14612} } ``` ## Acknowledgement Our code are built on top of [MMTracking](https://github.com/open-mmlab/mmtracking/) and [CLIP](https://github.com/openai/CLIP).",HOTA,EVALMETRIC
"/tools/dist_test.sh configs/mot/svm/svm_test.py 8 \    --eval bbox track --checkpoint svm_motsync_ch_mot17half.pth ```  ## Main Results ### MOT17 | Method | Dataset |                Train Data                | MOTA | HOTA | IDF1 |    URL    | | :---: | :---: |:----------------------------------------:|:----:|:----:|:----:|:---------:| | SVM | MOT17 | MOT17 half-train + crowdhuman + MOTSynth | 79.7 | 68.1 | 80.9 | [model](https://github.com/yuzhms/Streaming-Video-Model/releases/download/v1.0/svm_mot17-half-val.pth) |  ## Citation If you find this work useful in your research, please consider citing: ``` @InProceedings{Zhao_2023_CVPR,     author    = {Zhao, Yucheng and Luo, Chong and Tang, Chuanxin and Chen, Dongdong and Codella, Noel and Zha, Zheng-Jun},     title     = {Streaming Video Model},     booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},     month     = {June},     year      = {2023},     pages     = {14602-14612} } ``` ## Acknowledgement Our code are built on top of [MMTracking](https://github.com/open-mmlab/mmtracking/) and [CLIP](https://github.com/openai/CLIP).",IDF1,EVALMETRIC
The current Raspberry Pi HQ Camera + latest software combination provided a recall of 74.8% at 5 km/h and 50.5 % at 30 km/h.,recall,EVALMETRIC
Recall of up to 95.7% at 5 km/h was achieved by the global shutter Arducam AR0234.,Recall,EVALMETRIC
"Our best performing baseline achieves 74.8% R@100, which is promising for the feasibility of the task and indicates there is still room for improvement.",R@100,EVALMETRIC
"In this work, we analyze cross-lingual methods on three tasks in terms of their effectiveness (e.g., accuracy), development and deployment costs, as well as their latencies at inference time.",accuracy,EVALMETRIC
"Metrics are: Exact Match and F1-score for Q&A, Accuracy and F1-score for NLI and MRR@10 for Text Ranking",Exact Match,EVALMETRIC
"Metrics are: Exact Match and F1-score for Q&A, Accuracy and F1-score for NLI and MRR@10 for Text Ranking",F1-score,EVALMETRIC
"Metrics are: Exact Match and F1-score for Q&A, Accuracy and F1-score for NLI and MRR@10 for Text Ranking",Accuracy,EVALMETRIC
"Metrics are: Exact Match and F1-score for Q&A, Accuracy and F1-score for NLI and MRR@10 for Text Ranking",F1-score,EVALMETRIC
"Metrics are: Exact Match and F1-score for Q&A, Accuracy and F1-score for NLI and MRR@10 for Text Ranking",MRR@10,EVALMETRIC
| Model                           | Pre-train     | Fine-tune       | F1           | Accuracy    |  | ------------------------------- | ------------- | --------------- | ------------ | ----------- |  | mBERT (Souza et al,F1,EVALMETRIC
| Model                           | Pre-train     | Fine-tune       | F1           | Accuracy    |  | ------------------------------- | ------------- | --------------- | ------------ | ----------- |  | mBERT (Souza et al,Accuracy,EVALMETRIC
"An example is:  ```python from wildlifeml.training.evaluator import Evaluator  evaluator = Evaluator(     detector_file_path='<path_to_images_megadetector.json>',     label_file_path='<path_to_labels.csv>',     dataset=training_dataset,     num_classes=10,     conf_threshold=0.1,     empty_class_id=99 ) ```  For a trained model, which is contained in a `WildlifeTrainer`, the accuracy, precision, recall and f1 score is computed as:  ```python evaluator.evaluate(trainer) metrics = evaluator.compute_metrics() ```  If you wish to extract the predictions and ground-truth labels for all individual observations, use `evaluator.get_details()`.  ### 06: Active Learning  Apart from fitting a model in a fully supervised way, we offer an active learning pipeline.",accuracy,EVALMETRIC
"An example is:  ```python from wildlifeml.training.evaluator import Evaluator  evaluator = Evaluator(     detector_file_path='<path_to_images_megadetector.json>',     label_file_path='<path_to_labels.csv>',     dataset=training_dataset,     num_classes=10,     conf_threshold=0.1,     empty_class_id=99 ) ```  For a trained model, which is contained in a `WildlifeTrainer`, the accuracy, precision, recall and f1 score is computed as:  ```python evaluator.evaluate(trainer) metrics = evaluator.compute_metrics() ```  If you wish to extract the predictions and ground-truth labels for all individual observations, use `evaluator.get_details()`.  ### 06: Active Learning  Apart from fitting a model in a fully supervised way, we offer an active learning pipeline.",precision,EVALMETRIC
"An example is:  ```python from wildlifeml.training.evaluator import Evaluator  evaluator = Evaluator(     detector_file_path='<path_to_images_megadetector.json>',     label_file_path='<path_to_labels.csv>',     dataset=training_dataset,     num_classes=10,     conf_threshold=0.1,     empty_class_id=99 ) ```  For a trained model, which is contained in a `WildlifeTrainer`, the accuracy, precision, recall and f1 score is computed as:  ```python evaluator.evaluate(trainer) metrics = evaluator.compute_metrics() ```  If you wish to extract the predictions and ground-truth labels for all individual observations, use `evaluator.get_details()`.  ### 06: Active Learning  Apart from fitting a model in a fully supervised way, we offer an active learning pipeline.",recall,EVALMETRIC
"An example is:  ```python from wildlifeml.training.evaluator import Evaluator  evaluator = Evaluator(     detector_file_path='<path_to_images_megadetector.json>',     label_file_path='<path_to_labels.csv>',     dataset=training_dataset,     num_classes=10,     conf_threshold=0.1,     empty_class_id=99 ) ```  For a trained model, which is contained in a `WildlifeTrainer`, the accuracy, precision, recall and f1 score is computed as:  ```python evaluator.evaluate(trainer) metrics = evaluator.compute_metrics() ```  If you wish to extract the predictions and ground-truth labels for all individual observations, use `evaluator.get_details()`.  ### 06: Active Learning  Apart from fitting a model in a fully supervised way, we offer an active learning pipeline.",f1 score,EVALMETRIC
"The best performance in training the model (77.2% in accuracy) were obtained using a Convolutional Recurrent Network, which layers were organised as follows: <p align=""center""> <img src=""https://user-images.githubusercontent.com/44606182/162035266-953ff394-1cf3-407d-a3f8-a1d5a3f9f40c.png"" alt=""RCNN architecture"" width=""900""/> </p>  ### Melodic Features Extracting melodic features means horizontally investigating the music data, analysing what is functional to the melody.",accuracy,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,Accuracy-Numeric,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,Accuracy-Text,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,Bartscore,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,Bertscore,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,Bleu,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,Bleurt,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,CER,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,CHRF,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,COMET,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,F1-Numeric,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,F1-Text,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,METEOR,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,Precision-Numeric,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,Precision-Text,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,Prism,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,Recall-Numeric,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,Recall-Text,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,ROUGE,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,SacreBleu,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,Seqeval,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,Squad,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,TER,EVALMETRIC
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,WER,EVALMETRIC
"```python scorer = Jury(metrics=[""bleu"", ""meteor""]) scores = scorer(predictions, references) ```  #### Use of Metrics standalone  You can directly import metrics from `jury.metrics` as classes, and then instantiate and use as desired.",bleu,EVALMETRIC
"```python scorer = Jury(metrics=[""bleu"", ""meteor""]) scores = scorer(predictions, references) ```  #### Use of Metrics standalone  You can directly import metrics from `jury.metrics` as classes, and then instantiate and use as desired.",meteor,EVALMETRIC
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",Bleu,EVALMETRIC
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",bleu,EVALMETRIC
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",Bleu,EVALMETRIC
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",bleu,EVALMETRIC
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",Bleu,EVALMETRIC
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",bleu,EVALMETRIC
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",Bleu,EVALMETRIC
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",bleu,EVALMETRIC
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",Bleu,EVALMETRIC
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",bleu,EVALMETRIC
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",Bleu,EVALMETRIC
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",bleu,EVALMETRIC
"```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.",bleu,EVALMETRIC
"```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.",bleu,EVALMETRIC
"```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.",bleu,EVALMETRIC
"```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.",bleu,EVALMETRIC
"```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.",bleu,EVALMETRIC
"```json {   ""predictions"": ""/path/to/predictions.txt"",   ""references"": ""/path/to/references.txt"",   ""reduce_fn"": ""max"",   ""metrics"": [     ""bleu"",     ""meteor""   ] } ```  Then, you can call jury eval with `config` argument.",bleu,EVALMETRIC
"```json {   ""predictions"": ""/path/to/predictions.txt"",   ""references"": ""/path/to/references.txt"",   ""reduce_fn"": ""max"",   ""metrics"": [     ""bleu"",     ""meteor""   ] } ```  Then, you can call jury eval with `config` argument.",meteor,EVALMETRIC
"The security feature does not affect accuracy, however you can set `USE_DP_PRIVACY` to `False` if you want to see what the federated accuracy would be without differential privacy.",federated accuracy,EVALMETRIC
"A snippet of a sample output looks like: ``` Performance Metrics for client_agent2 on iteration 1  -------------------------------------------  Personal accuracy: 0.8283333333333334  Personal computation time: 0:00:01.194242  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:07.202375   Performance Metrics for client_agent0 on iteration 1  -------------------------------------------  Personal accuracy: 0.8216666666666667  Personal computation time: 0:00:01.198737  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:09.202375  ``` As you can see, the simulation prints out i) the personal accuracy: the accuracy that the client can obtain by itself on its own dataset, adding no DP noise.",Personal accuracy,EVALMETRIC
"A snippet of a sample output looks like: ``` Performance Metrics for client_agent2 on iteration 1  -------------------------------------------  Personal accuracy: 0.8283333333333334  Personal computation time: 0:00:01.194242  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:07.202375   Performance Metrics for client_agent0 on iteration 1  -------------------------------------------  Personal accuracy: 0.8216666666666667  Personal computation time: 0:00:01.198737  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:09.202375  ``` As you can see, the simulation prints out i) the personal accuracy: the accuracy that the client can obtain by itself on its own dataset, adding no DP noise.",Personal computation time,EVALMETRIC
"A snippet of a sample output looks like: ``` Performance Metrics for client_agent2 on iteration 1  -------------------------------------------  Personal accuracy: 0.8283333333333334  Personal computation time: 0:00:01.194242  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:07.202375   Performance Metrics for client_agent0 on iteration 1  -------------------------------------------  Personal accuracy: 0.8216666666666667  Personal computation time: 0:00:01.198737  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:09.202375  ``` As you can see, the simulation prints out i) the personal accuracy: the accuracy that the client can obtain by itself on its own dataset, adding no DP noise.",Federated accuracy,EVALMETRIC
"A snippet of a sample output looks like: ``` Performance Metrics for client_agent2 on iteration 1  -------------------------------------------  Personal accuracy: 0.8283333333333334  Personal computation time: 0:00:01.194242  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:07.202375   Performance Metrics for client_agent0 on iteration 1  -------------------------------------------  Personal accuracy: 0.8216666666666667  Personal computation time: 0:00:01.198737  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:09.202375  ``` As you can see, the simulation prints out i) the personal accuracy: the accuracy that the client can obtain by itself on its own dataset, adding no DP noise.",Simulated time to receive federated weights,EVALMETRIC
"A snippet of a sample output looks like: ``` Performance Metrics for client_agent2 on iteration 1  -------------------------------------------  Personal accuracy: 0.8283333333333334  Personal computation time: 0:00:01.194242  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:07.202375   Performance Metrics for client_agent0 on iteration 1  -------------------------------------------  Personal accuracy: 0.8216666666666667  Personal computation time: 0:00:01.198737  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:09.202375  ``` As you can see, the simulation prints out i) the personal accuracy: the accuracy that the client can obtain by itself on its own dataset, adding no DP noise.",Personal accuracy,EVALMETRIC
"A snippet of a sample output looks like: ``` Performance Metrics for client_agent2 on iteration 1  -------------------------------------------  Personal accuracy: 0.8283333333333334  Personal computation time: 0:00:01.194242  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:07.202375   Performance Metrics for client_agent0 on iteration 1  -------------------------------------------  Personal accuracy: 0.8216666666666667  Personal computation time: 0:00:01.198737  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:09.202375  ``` As you can see, the simulation prints out i) the personal accuracy: the accuracy that the client can obtain by itself on its own dataset, adding no DP noise.",Personal computation time,EVALMETRIC
"A snippet of a sample output looks like: ``` Performance Metrics for client_agent2 on iteration 1  -------------------------------------------  Personal accuracy: 0.8283333333333334  Personal computation time: 0:00:01.194242  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:07.202375   Performance Metrics for client_agent0 on iteration 1  -------------------------------------------  Personal accuracy: 0.8216666666666667  Personal computation time: 0:00:01.198737  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:09.202375  ``` As you can see, the simulation prints out i) the personal accuracy: the accuracy that the client can obtain by itself on its own dataset, adding no DP noise.",Federated accuracy,EVALMETRIC
"A snippet of a sample output looks like: ``` Performance Metrics for client_agent2 on iteration 1  -------------------------------------------  Personal accuracy: 0.8283333333333334  Personal computation time: 0:00:01.194242  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:07.202375   Performance Metrics for client_agent0 on iteration 1  -------------------------------------------  Personal accuracy: 0.8216666666666667  Personal computation time: 0:00:01.198737  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:09.202375  ``` As you can see, the simulation prints out i) the personal accuracy: the accuracy that the client can obtain by itself on its own dataset, adding no DP noise.",Simulated time to receive federated weights,EVALMETRIC
"A snippet of a sample output looks like: ``` Performance Metrics for client_agent2 on iteration 1  -------------------------------------------  Personal accuracy: 0.8283333333333334  Personal computation time: 0:00:01.194242  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:07.202375   Performance Metrics for client_agent0 on iteration 1  -------------------------------------------  Personal accuracy: 0.8216666666666667  Personal computation time: 0:00:01.198737  Federated accuracy: 0.8566666666666667  Simulated time to receive federated weights: 0:00:09.202375  ``` As you can see, the simulation prints out i) the personal accuracy: the accuracy that the client can obtain by itself on its own dataset, adding no DP noise.",personal accuracy,EVALMETRIC
"NOTE: this quantitiy does incorporate other client's data if you set the config.USING_CUMULATIVE flag to False, since that indicates to clients that they should start training on this iteration using the federated weights from the previous iteration since the datasets aren't cumulative. ii) the federated accuracy: the accuracy of the federated model which is the average of all the clients' personal weights + differentially private noise for that iteration.",federated accuracy,EVALMETRIC
"In particular, as one increases the amount of differentially private noise, the federated accuracy is expected to decrease.",federated accuracy,EVALMETRIC
"On the other hand, the personal accuracy will remain the same since it is assumed you don't add differentially private noise to your personal model since you are not sharing it.",personal accuracy,EVALMETRIC
"`Personal computation time` indicates how long your training took for that iteration while `Simulated time to receive federated weights` takes into account user-defined communication latencies between the clients and the server, as well as how long it took the other clients to compute their weights and the server to average them.  ## Authors  Vaikkunth Mugunthan* Anton Peraire* Lalana Kagal  ## License  This project is licensed under the MIT License   MIT License  Copyright (c) 2020 Vaikkunth, Anton, Lalana (PrivacyFL)  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",Personal computation time,EVALMETRIC
"`Personal computation time` indicates how long your training took for that iteration while `Simulated time to receive federated weights` takes into account user-defined communication latencies between the clients and the server, as well as how long it took the other clients to compute their weights and the server to average them.  ## Authors  Vaikkunth Mugunthan* Anton Peraire* Lalana Kagal  ## License  This project is licensed under the MIT License   MIT License  Copyright (c) 2020 Vaikkunth, Anton, Lalana (PrivacyFL)  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",Simulated time to receive federated weights,EVALMETRIC
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",AR,EVALMETRIC
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",Adjusted Random Index,EVALMETRIC
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",RI,EVALMETRIC
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",Random Index,EVALMETRIC
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",MI,EVALMETRIC
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .","""Mirkin's"" index",EVALMETRIC
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",HI,EVALMETRIC
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .","""Hubert's"" index",EVALMETRIC
"/datasets/data_matrix_1_S_Set.mat        2> Please enter number of processors:  16        3> Please set the value of alpha:  1        4> Please enter computing time limit (seconds):  40    ### (3) Result  The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index",AR,EVALMETRIC
"/datasets/data_matrix_1_S_Set.mat        2> Please enter number of processors:  16        3> Please set the value of alpha:  1        4> Please enter computing time limit (seconds):  40    ### (3) Result  The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index",Adjusted Random Index,EVALMETRIC
"/datasets/data_matrix_1_S_Set.mat        2> Please enter number of processors:  16        3> Please set the value of alpha:  1        4> Please enter computing time limit (seconds):  40    ### (3) Result  The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index",RI,EVALMETRIC
"/datasets/data_matrix_1_S_Set.mat        2> Please enter number of processors:  16        3> Please set the value of alpha:  1        4> Please enter computing time limit (seconds):  40    ### (3) Result  The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index",Random Index,EVALMETRIC
"/datasets/data_matrix_1_S_Set.mat        2> Please enter number of processors:  16        3> Please set the value of alpha:  1        4> Please enter computing time limit (seconds):  40    ### (3) Result  The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index",MI,EVALMETRIC
"/datasets/data_matrix_1_S_Set.mat        2> Please enter number of processors:  16        3> Please set the value of alpha:  1        4> Please enter computing time limit (seconds):  40    ### (3) Result  The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index","""Mirkin's"" index",EVALMETRIC
"/datasets/data_matrix_1_S_Set.mat        2> Please enter number of processors:  16        3> Please set the value of alpha:  1        4> Please enter computing time limit (seconds):  40    ### (3) Result  The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index",HI,EVALMETRIC
"/datasets/data_matrix_1_S_Set.mat        2> Please enter number of processors:  16        3> Please set the value of alpha:  1        4> Please enter computing time limit (seconds):  40    ### (3) Result  The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index","""Hubert's"" index",EVALMETRIC
"/docs/media/test_sketch.png"" alt=""test_sketch widgets"" width=""50%"">  ### Compare sketch performance Plots performance metrics (Mean Absolute Error, F1 Score, Classification accuracy, precision, and recall) for all of the selected sketches on the specified dataset. ``` msb.compare_sketches(sb) ```  <img src="".",Mean Absolute Error,EVALMETRIC
"/docs/media/test_sketch.png"" alt=""test_sketch widgets"" width=""50%"">  ### Compare sketch performance Plots performance metrics (Mean Absolute Error, F1 Score, Classification accuracy, precision, and recall) for all of the selected sketches on the specified dataset. ``` msb.compare_sketches(sb) ```  <img src="".",F1 Score,EVALMETRIC
"/docs/media/test_sketch.png"" alt=""test_sketch widgets"" width=""50%"">  ### Compare sketch performance Plots performance metrics (Mean Absolute Error, F1 Score, Classification accuracy, precision, and recall) for all of the selected sketches on the specified dataset. ``` msb.compare_sketches(sb) ```  <img src="".",Classification accuracy,EVALMETRIC
"/docs/media/test_sketch.png"" alt=""test_sketch widgets"" width=""50%"">  ### Compare sketch performance Plots performance metrics (Mean Absolute Error, F1 Score, Classification accuracy, precision, and recall) for all of the selected sketches on the specified dataset. ``` msb.compare_sketches(sb) ```  <img src="".",precision,EVALMETRIC
"/docs/media/test_sketch.png"" alt=""test_sketch widgets"" width=""50%"">  ### Compare sketch performance Plots performance metrics (Mean Absolute Error, F1 Score, Classification accuracy, precision, and recall) for all of the selected sketches on the specified dataset. ``` msb.compare_sketches(sb) ```  <img src="".",recall,EVALMETRIC
The RMSE is around 802 mm on the selected validation set for this model as reported in the paper.,RMSE,EVALMETRIC
"- `assign_clusters_to_tokens.py`: assign everyone to a cluster  Lemmatized, specifying resolution:   ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_lemmed --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  Assign only wiktionary words, lemmatized, specifying resolution:  ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_eval --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_eval --lemmatize True --wiki_eval True --resolution 0.5 ```  Sense NPMI - run `get_documentID_maps()` in `get_docID_to_group.py` - `word_sense.py`, for journals, fos, and wiktionary evaluation - `Wiktionary Validation.ipynb` is the notebook that contains Wiktionary evaluation results.",NPMI,EVALMETRIC
The log file can be found in `tb` folder.  ## Evaluate Recall **Save Ref-NMS proposals:** ``` PYTHONPATH=$PWD python tools/save_ref_nms_proposals.py --dataset refcoco --split-by unc --tid <tid> --m <loss_type> ``` `<loss_type>` can be either `att_vanilla` for binary XE loss or `att_rank` for rank loss.,Recall,EVALMETRIC
**Evaluate recall on referent object:** ``` PYTHONPATH=$PWD python tools/eval_proposal_hit_rate.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ``` `conf` parameter is the score threshold used to filter Ref-NMS proposals.,recall,EVALMETRIC
**Evaluate recall on critical objects:** ``` PYTHONPATH=$PWD python tools/eval_proposal_ctx_recall.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ```  ## Evaluate REG Performance Save MAttNet-style detection file: ``` PYTHONPATH=$PWD python tools/save_matt_dets.py --dataset refcoco --split-by unc --m <loss_type> --tid <tid> --conf <conf> ``` This script will save all the detection information needed for downstream REG evaluation to `output/matt_dets_<loss_type>_<tid>_<dataset>_<split_by>_<top_N>.json`.,recall,EVALMETRIC
**Evaluate recall on critical objects:** ``` PYTHONPATH=$PWD python tools/eval_proposal_ctx_recall.py --m <loss_type> --dataset refcoco --split-by unc --tid <tid> --conf <conf> ```  ## Evaluate REG Performance Save MAttNet-style detection file: ``` PYTHONPATH=$PWD python tools/save_matt_dets.py --dataset refcoco --split-by unc --m <loss_type> --tid <tid> --conf <conf> ``` This script will save all the detection information needed for downstream REG evaluation to `output/matt_dets_<loss_type>_<tid>_<dataset>_<split_by>_<top_N>.json`.,recall,EVALMETRIC
"Use the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` python run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.",Recall@100,EVALMETRIC
"Use the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` python run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.",Recall@100,EVALMETRIC
"Use the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` python run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.",LRAP,EVALMETRIC
"Use the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` python run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.",LRAP,EVALMETRIC
"Use the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` python run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.",passage-level Recall@100,EVALMETRIC
"Use the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` python run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.",passage-level Recall@100,EVALMETRIC
"Use the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` python run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.",Recall@k,EVALMETRIC
"Use the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` python run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.",passage-level Recall@k,EVALMETRIC
"Use the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` python run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.",LRAP,EVALMETRIC
"Use the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` python run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.",Label ranking average precision,EVALMETRIC
"Use the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` python run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.",label_ranking_average_precision_score,EVALMETRIC
"Use the above reader training scripts and set `--epochs` to be 0 for evaluation.  ### Reader Results  |   val F1  |  test F1  |  val Recall |  test Recall |  val Precision  |  test Precision  | |:-----------:|:-----------:|:-------------:|:--------------:|:-----------------:|:------------------:| |   87.32%  |   84.4%  |   90.23%    |    87.0%    |     84.6%      |      81.96%      |  ## GERBIL evaluation Our GERBIL evaluation steps follow [here](https://github.com/dalab/end2end_neural_el), specifically: 1.",F1,EVALMETRIC
"Use the above reader training scripts and set `--epochs` to be 0 for evaluation.  ### Reader Results  |   val F1  |  test F1  |  val Recall |  test Recall |  val Precision  |  test Precision  | |:-----------:|:-----------:|:-------------:|:--------------:|:-----------------:|:------------------:| |   87.32%  |   84.4%  |   90.23%    |    87.0%    |     84.6%      |      81.96%      |  ## GERBIL evaluation Our GERBIL evaluation steps follow [here](https://github.com/dalab/end2end_neural_el), specifically: 1.",F1,EVALMETRIC
"Use the above reader training scripts and set `--epochs` to be 0 for evaluation.  ### Reader Results  |   val F1  |  test F1  |  val Recall |  test Recall |  val Precision  |  test Precision  | |:-----------:|:-----------:|:-------------:|:--------------:|:-----------------:|:------------------:| |   87.32%  |   84.4%  |   90.23%    |    87.0%    |     84.6%      |      81.96%      |  ## GERBIL evaluation Our GERBIL evaluation steps follow [here](https://github.com/dalab/end2end_neural_el), specifically: 1.",Recall,EVALMETRIC
"Use the above reader training scripts and set `--epochs` to be 0 for evaluation.  ### Reader Results  |   val F1  |  test F1  |  val Recall |  test Recall |  val Precision  |  test Precision  | |:-----------:|:-----------:|:-------------:|:--------------:|:-----------------:|:------------------:| |   87.32%  |   84.4%  |   90.23%    |    87.0%    |     84.6%      |      81.96%      |  ## GERBIL evaluation Our GERBIL evaluation steps follow [here](https://github.com/dalab/end2end_neural_el), specifically: 1.",Recall,EVALMETRIC
"Use the above reader training scripts and set `--epochs` to be 0 for evaluation.  ### Reader Results  |   val F1  |  test F1  |  val Recall |  test Recall |  val Precision  |  test Precision  | |:-----------:|:-----------:|:-------------:|:--------------:|:-----------------:|:------------------:| |   87.32%  |   84.4%  |   90.23%    |    87.0%    |     84.6%      |      81.96%      |  ## GERBIL evaluation Our GERBIL evaluation steps follow [here](https://github.com/dalab/end2end_neural_el), specifically: 1.",Precision,EVALMETRIC
"Use the above reader training scripts and set `--epochs` to be 0 for evaluation.  ### Reader Results  |   val F1  |  test F1  |  val Recall |  test Recall |  val Precision  |  test Precision  | |:-----------:|:-----------:|:-------------:|:--------------:|:-----------------:|:------------------:| |   87.32%  |   84.4%  |   90.23%    |    87.0%    |     84.6%      |      81.96%      |  ## GERBIL evaluation Our GERBIL evaluation steps follow [here](https://github.com/dalab/end2end_neural_el), specifically: 1.",Precision,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",MAP@1,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Potential@1,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Precision@1,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",MAP@3,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",MAP@5,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",MAP@10,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Potential@3,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Potential@5,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Potential@10,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Accuracy@1@top_gold_1,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Accuracy@2@top_gold_1,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Accuracy@3@top_gold_1,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Potential@K,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",MAP@K,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Precision@K,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Recall@K,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Accuracy@K@top_gold_1,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",MAP@1,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Potential@1,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Precision@1,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",MAP@3,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",MAP@5,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",MAP@10,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Potential@3,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Potential@5,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Potential@10,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Accuracy@1@top_gold_1,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Accuracy@2@top_gold_1,EVALMETRIC
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",Accuracy@3@top_gold_1,EVALMETRIC
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | ðï¸   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ð¥   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ð¥   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,Perception,EVALMETRIC
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | ðï¸   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ð¥   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ð¥   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,Understanding,EVALMETRIC
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | ðï¸   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ð¥   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ð¥   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,Applying,EVALMETRIC
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | ðï¸   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ð¥   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ð¥   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,Analyzing,EVALMETRIC
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | ðï¸   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ð¥   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ð¥   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,Evaluation,EVALMETRIC
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | ðï¸   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ð¥   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ð¥   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,Creation,EVALMETRIC
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | ðï¸   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ð¥   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ð¥   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,Win Rates over LLaVA-v1.5-13B,EVALMETRIC
"Please find the example pipeline shown below.  ### Models + LSTM - lstm_luong_wmt_en_de + Transformer - transformer_iwslt_de_en + Dynamic Conv. - lightconv_iwslt_de_en  ### BPE ``` examples/translation/subword-nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code <iwslt14.tokenized.de-en/iwslt14.vocab.en> iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe ```  ### Preprocessing ``` TEXT=examples/translation/iwslt14.tokenized.de-en fairseq-preprocess --source-lang en --target-lang de \     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \     --destdir data-bin/iwslt14.tokenized.de-en \     --workers 20 ```  ### Training LSTM ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lstm_luong_wmt_en_de --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.2 --weight-decay 0.0 \     --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Transformer ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch transformer_iwslt_de_en --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Dynamic Conv. ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lightconv_iwslt_de_en \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ```  ### Evaluation BLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring bleu --remove-bpe --cpu >bleu.log 2>&1 & ``` ScareBLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring sacrebleu --remove-bpe --cpu >sacrebleu.log 2>&1 & ```  ## Authors * **Ning Shi** - mrshininnnnn@gmail.com  ## BibTex ``` @inproceedings{shi-etal-2022-revisit,     title = ""Revisit Systematic Generalization via Meaningful Learning"",     author = ""Shi, Ning  and       Wang, Boxin  and       Wang, Wei  and       Liu, Xiangyu  and       Lin, Zhouhan"",     booktitle = ""Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP"",     month = dec,     year = ""2022"",     address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.blackboxnlp-1.6"",     pages = ""62--79"",     abstract = ""Humans can systematically generalize to novel compositions of existing concepts.",BLEU,EVALMETRIC
"Please find the example pipeline shown below.  ### Models + LSTM - lstm_luong_wmt_en_de + Transformer - transformer_iwslt_de_en + Dynamic Conv. - lightconv_iwslt_de_en  ### BPE ``` examples/translation/subword-nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code <iwslt14.tokenized.de-en/iwslt14.vocab.en> iwslt14.tokenized.de-en/iwslt14.vocab.en.bpe ```  ### Preprocessing ``` TEXT=examples/translation/iwslt14.tokenized.de-en fairseq-preprocess --source-lang en --target-lang de \     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \     --destdir data-bin/iwslt14.tokenized.de-en \     --workers 20 ```  ### Training LSTM ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lstm_luong_wmt_en_de --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.2 --weight-decay 0.0 \     --encoder-dropout-out 0.2 --decoder-dropout-out 0.2 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Transformer ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch transformer_iwslt_de_en --share-decoder-input-output-embed \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ``` Dynamic Conv. ``` fairseq-train \     data-bin/iwslt14.tokenized.de-en \     -s en -t de \     --arch lightconv_iwslt_de_en \     --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \     --lr 0.001 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \     --dropout 0.1 --weight-decay 0.0 \     --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \     --max-tokens 32768 \     --fp16 --no-epoch-checkpoints >train.log 2>&1 & ```  ### Evaluation BLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring bleu --remove-bpe --cpu >bleu.log 2>&1 & ``` ScareBLEU ``` fairseq-generate data-bin/iwslt14.tokenized.de-en \     --path checkpoints/checkpoint_best.pt \     -s en -t de \     --batch-size 128 --beam 5 --lenpen 0.6 \     --scoring sacrebleu --remove-bpe --cpu >sacrebleu.log 2>&1 & ```  ## Authors * **Ning Shi** - mrshininnnnn@gmail.com  ## BibTex ``` @inproceedings{shi-etal-2022-revisit,     title = ""Revisit Systematic Generalization via Meaningful Learning"",     author = ""Shi, Ning  and       Wang, Boxin  and       Wang, Wei  and       Liu, Xiangyu  and       Lin, Zhouhan"",     booktitle = ""Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP"",     month = dec,     year = ""2022"",     address = ""Abu Dhabi, United Arab Emirates (Hybrid)"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.blackboxnlp-1.6"",     pages = ""62--79"",     abstract = ""Humans can systematically generalize to novel compositions of existing concepts.",ScareBLEU,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",NMI,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",mARP,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",Recall,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",NMI,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",mARP,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",Recall,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",NMI,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",mARP,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",Recall,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",NMI,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",mARP,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",Recall,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",NMI,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",mARP,EVALMETRIC
"ââââcabinet_final |   â   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",Recall,EVALMETRIC
[H-Scores](#hscores) 4.,H-Scores,EVALMETRIC
[H-Scores](#hscores) 4.,hscores,EVALMETRIC
"Highlighted features include:  - Easy to use: Users only need to call QPUs, run the main function, and get the scores. - Comprehensive scoring system: The toolkit generates a single score, termed H-Score, capturing all relevant QPU performance factors such as error rates, circuit compilation, and quantum error mitigation. - Standardized and comparable results: H-Scores ensure consistent and comparable evaluation across different platforms. - Focusing on real-world application: The toolkit is based on optimization algorithms to reflect actual application scenarios. - Multi-QPU support: Utility in multi-QPU resource management has been demonstrated.",H-Scores,EVALMETRIC
"<a name=""hscores""></a>  ## H-Scores  The following results were obtained on the built-in Q matrices.",H-Scores,EVALMETRIC
"/hamiltoniq/H_Scores/qubit_3.png"" alt=""n_qubits=3"" width=""700"" /></p>  ##### 4 qubits  <p align=center><img src="".",H_Scores,EVALMETRIC
"/hamiltoniq/H_Scores/qubit_4.png"" alt=""n_qubits=4"" width=""700"" /></p>  ##### 5 qubits  <p align=center><img src="".",H_Scores,EVALMETRIC
"/hamiltoniq/H_Scores/qubit_5.png"" alt=""n_qubits=4"" width=""700"" /></p>  ##### 6 qubits  <p align=center><img src="".",H_Scores,EVALMETRIC
"/hamiltoniq/H_Scores/qubit_6.png"" alt=""n_qubits=4"" width=""700"" /></p>  <a name=""architecture""></a>  ## Architecture  For more technical details please visit our [arXiv Paper](https://arxiv.org/abs/2404.13971).",H_Scores,EVALMETRIC
"The culmination of this rigorous workflow is the benchmarking result, which quantifies the performance of the QPU in terms of reliabilityârepresented by the H-Score and Execution Time.",H-Score,EVALMETRIC
"The culmination of this rigorous workflow is the benchmarking result, which quantifies the performance of the QPU in terms of reliabilityârepresented by the H-Score and Execution Time.",Execution Time,EVALMETRIC
"Additionally, the H-score can help manage computational resources in a Quantum-HPC system.",H-score,EVALMETRIC
The final H-Score is computed as the average of all individual scores.,H-Score,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,Recall@10,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,Recall@20,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,NDCG@10,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,NDCG@20,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,Recall@10,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,Recall@20,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,NDCG@10,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,NDCG@20,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,Recall@10,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,Recall@20,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,NDCG@10,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,NDCG@20,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,Recall@10,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,Recall@20,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,NDCG@10,EVALMETRIC
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,NDCG@20,EVALMETRIC
"Totally, the main experiment commands of DejaVu should output as follows: - FDG message, including the data paths, edge types, the number of nodes (failure units), the number of metrics, the metrics of each failure class. - Traning setup message: the faults used for training, validation and testing. - Model architecture: model parameters in each part, total params - Training process: the training/validation/testing loss and accuracy - Time Report. - command output one-line summary.  ### Example See https://github.com/NetManAIOps/DejaVu/issues/4  ## Datasets  The datasets A, B, C, D are public at : - https://www.dropbox.com/sh/ist4ojr03e2oeuw/AAD5NkpAFg1nOI2Ttug3h2qja?",accuracy,EVALMETRIC
"Then for two similar failures which occur at $v_1$ and $v_2$ respectively, their feature vectors are $(1, 0, 0, 0)$ and $(0, 1, 0, 0)$ respectively, which are dissimilar with respect to common similarity metrics (e.g., Manhattan or Euclidean).",Manhattan,EVALMETRIC
"Then for two similar failures which occur at $v_1$ and $v_2$ respectively, their feature vectors are $(1, 0, 0, 0)$ and $(0, 1, 0, 0)$ respectively, which are dissimilar with respect to common similarity metrics (e.g., Manhattan or Euclidean).",Euclidean,EVALMETRIC
