sentence,entity_text,entity_type
"Requirements ============  The code has been tested on `Ubuntu 18.04.2 LTS`, with `Python 2.7` , `Tensorflow 1.10` and `Numpy 1.15.1`.",Python 2.7,PROGLANG
"The command will be similar to this one:  ``` python test.py --arch=""lenet"" --type=""accuracy"" --batch=30000 --resume="".",python,PROGLANG
The generalizable properties and flexibility of our Holistic AI in Medicine (HAIM) framework could offer a promising pathway for future multimodal predictive systems in clinical and operational healthcare settings.  ## Code  The code uses Python3.6.9 and is separated into four sections:  0 - Software Package requirement  1 - Data Preprocessing.,Python3.6.9,PROGLANG
"If you use this code, please cite the paper using the bibtex reference below. ``` @inproceedings{tanl,     title={Structured Prediction as Translation between Augmented Natural Languages},     author={Giovanni Paolini and Ben Athiwaratkun and Jason Krone and Jie Ma and Alessandro Achille and Rishita Anubhai and Cicero Nogueira dos Santos and Bing Xiang and Stefano Soatto},     booktitle={9th International Conference on Learning Representations, {ICLR} 2021},     year={2021}, } ```   ## Requirements  - Python 3.6+ - PyTorch (tested with version 1.7.1) - Transformers (tested with version 4.0.0) - NetworkX (tested with version 2.5, only used in coreference resolution)  You can install all required Python packages with `pip install -r requirements.txt`   ## Datasets  By default, datasets are expected to be in `data/DATASET_NAME`.",Python,PROGLANG
"All relevant data was stored in a MariaDB database; an export of the table data is available [here](https://drive.google.com/drive/folders/1QGkh0MTT-lXiGh7ofzswA7042KVGEqq_).  ## Files/folders  **1-scrape**  - **Large-scale data collection (Python)** - The majority of event data was collected through Google's BigQuery service, using Python",Python,PROGLANG
"All relevant data was stored in a MariaDB database; an export of the table data is available [here](https://drive.google.com/drive/folders/1QGkh0MTT-lXiGh7ofzswA7042KVGEqq_).  ## Files/folders  **1-scrape**  - **Large-scale data collection (Python)** - The majority of event data was collected through Google's BigQuery service, using Python",Python,PROGLANG
- *usdValues.js*: Collect hourly USD pricing data from Coinbase and update each transaction with the most recent USD price.  **2-transform**  *algo.ipynb* contains the algorithm used to estimate the percentage of debt-financed collateral  **3-analyze**  *analysis.r* contains the R code used to create charts and tables for the final paper.,R,PROGLANG
"Also, please set 'objective' to 'binary', 'classification' or 'regression' based on your task.  ### Enable and specify GPU ```python import os os.environ['CUDA_VISIBLE_DEVICES'] = '0' os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' ```  ### Load Data ```python from sklearn.model_selection import train_test_split import openml  dataset = openml.datasets.get_dataset(40536) X, y, categorical_indicator, attribute_names = dataset.get_data(target=dataset.default_target_attribute) categorical_feature_indices = [idx for idx, idx_bool in enumerate(categorical_indicator) if idx_bool]  X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42) ```  ### Preprocessing, Hyperparameters and Training  GRANDE requires categorical features to be encoded appropriately.",python,PROGLANG
"Also, please set 'objective' to 'binary', 'classification' or 'regression' based on your task.  ### Enable and specify GPU ```python import os os.environ['CUDA_VISIBLE_DEVICES'] = '0' os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' ```  ### Load Data ```python from sklearn.model_selection import train_test_split import openml  dataset = openml.datasets.get_dataset(40536) X, y, categorical_indicator, attribute_names = dataset.get_data(target=dataset.default_target_attribute) categorical_feature_indices = [idx for idx, idx_bool in enumerate(categorical_indicator) if idx_bool]  X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42) ```  ### Preprocessing, Hyperparameters and Training  GRANDE requires categorical features to be encoded appropriately.",python,PROGLANG
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",python,PROGLANG
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",python,PROGLANG
# Installation  By using Pip (check the last release)  ```shell script pip install a2t ```  By clonning the repository  ```shell script git clone https://github.com/osainz59/Ask2Transformers.git cd Ask2Transformers pip install . ```  Or directly by ```shell script pip install git+https://github.com/osainz59/Ask2Transformers ```  <!,shell,PROGLANG
# Installation  By using Pip (check the last release)  ```shell script pip install a2t ```  By clonning the repository  ```shell script git clone https://github.com/osainz59/Ask2Transformers.git cd Ask2Transformers pip install . ```  Or directly by ```shell script pip install git+https://github.com/osainz59/Ask2Transformers ```  <!,shell,PROGLANG
# Installation  By using Pip (check the last release)  ```shell script pip install a2t ```  By clonning the repository  ```shell script git clone https://github.com/osainz59/Ask2Transformers.git cd Ask2Transformers pip install . ```  Or directly by ```shell script pip install git+https://github.com/osainz59/Ask2Transformers ```  <!,shell,PROGLANG
[Python 3.8](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/) [!,python,PROGLANG
"/fingpt)   + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**   ## Instruction Tuning Datasets and Models The datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>  [Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)      | Datasets | Train Rows |  Test Rows |Description  |   | --------- | ----------------- | ------------ | --------------------- |   | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |   | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |   | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|   | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|   | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|   | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|    Multi-task financial LLMs Models: ```python   demo_tasks = [       'Financial Sentiment Analysis',       'Financial Relation Extraction',       'Financial Headline Classification',       'Financial Named Entity Recognition',]   demo_inputs = [       ""Glaxo's ViiV Healthcare Signs China Manufacturing Deal With Desano"",       ""Apple Inc.",python,PROGLANG
"model=text-davinci-003)    Financial News + [Algorithmic Trading using Sentiment Analysis on News Articles](https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704) + [Accessing Historical Financial News Headlines with Python](https://python.plainenglish.io/access-historical-financial-news-headlines-with-python-be1b8faaea9f)  **PromptNet** Analogy to ImageNet and WordNet, it is critical to build a PromptNet",Python,PROGLANG
"model=text-davinci-003)    Financial News + [Algorithmic Trading using Sentiment Analysis on News Articles](https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704) + [Accessing Historical Financial News Headlines with Python](https://python.plainenglish.io/access-historical-financial-news-headlines-with-python-be1b8faaea9f)  **PromptNet** Analogy to ImageNet and WordNet, it is critical to build a PromptNet",python,PROGLANG
"model=text-davinci-003)    Financial News + [Algorithmic Trading using Sentiment Analysis on News Articles](https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704) + [Accessing Historical Financial News Headlines with Python](https://python.plainenglish.io/access-historical-financial-news-headlines-with-python-be1b8faaea9f)  **PromptNet** Analogy to ImageNet and WordNet, it is critical to build a PromptNet",python,PROGLANG
v=Nl2Cdbao5Ws)  + [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3) + [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper) + [OpenAI Examples Library](https://platform.openai.com/examples) + [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API. + [Exploring the Capabilities of the ChatGPT API: A Beginner’s Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f) + [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)  **Prompting programming**  ## ChatGPT relatives:   [A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.,python,PROGLANG
v=Nl2Cdbao5Ws)  + [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3) + [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper) + [OpenAI Examples Library](https://platform.openai.com/examples) + [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API. + [Exploring the Capabilities of the ChatGPT API: A Beginner’s Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f) + [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)  **Prompting programming**  ## ChatGPT relatives:   [A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.,shell,PROGLANG
"Further, you have to uncomment the respective reasoners in the `getReasoners()` function of the java file `org.unima.nheist.App`.",java,PROGLANG
"# DiHT ""Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training"" [[Paper](https://arxiv.org/abs/2301.02280)]  ## Installation  ```bash git clone https://github.com/facebookresearch/diht cd diht pip install -r requirements.txt pip install -e",bash,PROGLANG
"# To install as an editable package ```  You can use `pip install .` to install the codebase as a non-editable package.  ## Example  ```python import torch  from diht import model_zoo from PIL import Image   text_tokenizer, image_transform, model = model_zoo.load_model(     ""diht_vitl14_336px"", is_train=False )  image = Image.open(""infer_image.png"").convert(""RGB"") image = image_transform(image).unsqueeze(0) text_captions = [""a mountain"", ""a beach"", ""a desert""] text = text_tokenizer(text_captions)  with torch.no_grad():     image_features, text_features, logit_scale = model(image, text)     logits_per_image = logit_scale * image_features @ text_features.T     probs = logits_per_image.softmax(dim=-1).numpy()  print(f""text captions: {text_captions}"") print(f""text caption probs: {probs}"") ```  The above code snippet should output ```bash text captions: ['a mountain', 'a beach', 'a desert'] text caption probs: [[0.99370664 0.00514017 0.00115326]] ```  ## Running on GPU/CPU  By default the model runs on CPU, to run on GPU you can do `model = model.to(torch.device(""cuda""))`.",python,PROGLANG
"# To install as an editable package ```  You can use `pip install .` to install the codebase as a non-editable package.  ## Example  ```python import torch  from diht import model_zoo from PIL import Image   text_tokenizer, image_transform, model = model_zoo.load_model(     ""diht_vitl14_336px"", is_train=False )  image = Image.open(""infer_image.png"").convert(""RGB"") image = image_transform(image).unsqueeze(0) text_captions = [""a mountain"", ""a beach"", ""a desert""] text = text_tokenizer(text_captions)  with torch.no_grad():     image_features, text_features, logit_scale = model(image, text)     logits_per_image = logit_scale * image_features @ text_features.T     probs = logits_per_image.softmax(dim=-1).numpy()  print(f""text captions: {text_captions}"") print(f""text caption probs: {probs}"") ```  The above code snippet should output ```bash text captions: ['a mountain', 'a beach', 'a desert'] text caption probs: [[0.99370664 0.00514017 0.00115326]] ```  ## Running on GPU/CPU  By default the model runs on CPU, to run on GPU you can do `model = model.to(torch.device(""cuda""))`.",bash,PROGLANG
The image and text tensors will also have to be transferred accordingly.  ## Available Models  ```python import diht   print(diht.available_models()) ```  ## Example ImageNet-1K zero-shot evaluation  A simple image classification zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download ImageNet-1K dataset from the original website.,python,PROGLANG
Edit `IMAGENET_ROOT` in `example_imagenet_eval.py` to match the location on your machine. ``` python example_imagenet_eval.py ``` For DiHT-L/14@336 the output should look like: ``` ImageNet1K acc@1 for diht_vitl14_336px: 77.9 ```  ## Example retrieval zero-shot evaluation  A simple retrieval zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download COCO and Flickr30K datasets from the original websites.,python,PROGLANG
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",python,PROGLANG
"Alemi},         year={2019},         eprint={1905.00075},         archivePrefix={arXiv},         primaryClass={cs.IR}     }  ## Setup on Linux  Install the required system packages (or use an alternative Python distribution of your choice).",Python,PROGLANG
For Debian / Ubuntu / similar:      sudo apt install python3 python3-pip python3-virtualenv poppler-utils  Download the code and prepare the python environment:      git clone https://github.com/mattbierbaum/arxiv-public-datasets     cd arxiv-public-datasets      virtualenv venv     . venv/bin/activate      pip3 install -e .,python3,PROGLANG
For Debian / Ubuntu / similar:      sudo apt install python3 python3-pip python3-virtualenv poppler-utils  Download the code and prepare the python environment:      git clone https://github.com/mattbierbaum/arxiv-public-datasets     cd arxiv-public-datasets      virtualenv venv     . venv/bin/activate      pip3 install -e .,python3,PROGLANG
For Debian / Ubuntu / similar:      sudo apt install python3 python3-pip python3-virtualenv poppler-utils  Download the code and prepare the python environment:      git clone https://github.com/mattbierbaum/arxiv-public-datasets     cd arxiv-public-datasets      virtualenv venv     . venv/bin/activate      pip3 install -e .,python,PROGLANG
This code uses [python3.x](https://www.python.org/downloads/) (version 3.6 and higher) and requires the [Gurobi](https://www.gurobi.com/) solver.,python3.x,PROGLANG
"Change the hard code in `model_runs.py` to change the according target column - If results output file `-f file` is `None` the `models_run.py` calls automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder - `-e model_extras`, `-p priorities`, and `-f file` may be `None` input arguments, all others must hold a valid value  *** Call the `model_runs.py` `main` function within a python file as follows to generate a model ignorning our second objective,  ```python import model_runs  data_names = ['soybean_small', 'monk3', 'car', 'iris', 'climate'] heights = [3, 4, 5] models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 extras = ['max_features-25'] rand_seed = [13, 58, 94, None] tuning = None file = 'example_code_output.csv' consol_log = False model_runs.main(   [""-d"", data_names, ""-h"", heights, ""-m"", models, ""-t"", time_limit,    ""-e"", extras, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.main -d ['soybean-small','monk3','car','iris','climate'] -h [3,4,5] -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -e ['max_features-25'] -r [13, 58, 94, None] -c None -f 'test_results.csv' -l False ```  *** ## Bi-objective Modeling Call the `biobjective` function within a python file as follows to generate a model using the heirarchical modeling capabilities of Gurobi  ```python import model_runs  data_names = ['ionosphere', 'monk2', 'breat_cancer', 'climate'] height = 5 models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 rand_seed = [13, 58, 94, None] priorities = ['data','equal'] file = 'biobj_example.csv' consol_log = False model_runs.biobjective(   [""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", time_limit,    ""-p"", priorities, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.biobjective -d ['ionosphere', 'monk2', 'breat_cancer', 'climate'] -h 5 -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -p ['data','equal'] -r [13, 58, 94, None] -f 'biobj_example.csv' -l False ```  *** ## Pareto Frontier To generate the Pareto frontier call the `main` function in `pareto_runs.py` with the below parameters:   - d : str list, names of dataset files   - h : int, maximum depth of trained trees   - t : float, gurobi model time limit in s   - m : str list, models to use   - r : str list, random state(s) to use   - f : str, results output file .csv  A `.png` file for each dataset called by the user is generated and stored in `\results_figures\` folder  We assume `-f file` is located in the `\results_files` folder - If results output file `-f file` is `None` the `pareto` function automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder  You can generate pareto frontiers from within a python file as follows,  ```python import model_runs  height = 4 models = ['FlowOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] rand_states = [15, 78, 0] data_names = ['hayes_roth', 'house_votes_84'] file = 'pareto_example.csv' model_runs.pareto([""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", 3600, ""-r"", rand_states, ""-f"", file]) ```  To run from terminal do the following  ```bash python3 import model_runs; model_runs.pareto -d ['hayes_roth', 'house_votes_84'] -h 4 -m ['FOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] -t 3600 -r [15, 78, 0] -f 'pareto_example.csv' ``` - Note: `FlowOCT` must be the model name to generate the pareto frontier of FlowOCT ***  ## Models Functionality For understanding model functionality associated with integer and fractional separation procedures in **CUT1** and **CUT2** models, `-e model_extras` and `-c tuning` functionality please refer to the `USAGE.md` file.",python,PROGLANG
"Change the hard code in `model_runs.py` to change the according target column - If results output file `-f file` is `None` the `models_run.py` calls automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder - `-e model_extras`, `-p priorities`, and `-f file` may be `None` input arguments, all others must hold a valid value  *** Call the `model_runs.py` `main` function within a python file as follows to generate a model ignorning our second objective,  ```python import model_runs  data_names = ['soybean_small', 'monk3', 'car', 'iris', 'climate'] heights = [3, 4, 5] models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 extras = ['max_features-25'] rand_seed = [13, 58, 94, None] tuning = None file = 'example_code_output.csv' consol_log = False model_runs.main(   [""-d"", data_names, ""-h"", heights, ""-m"", models, ""-t"", time_limit,    ""-e"", extras, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.main -d ['soybean-small','monk3','car','iris','climate'] -h [3,4,5] -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -e ['max_features-25'] -r [13, 58, 94, None] -c None -f 'test_results.csv' -l False ```  *** ## Bi-objective Modeling Call the `biobjective` function within a python file as follows to generate a model using the heirarchical modeling capabilities of Gurobi  ```python import model_runs  data_names = ['ionosphere', 'monk2', 'breat_cancer', 'climate'] height = 5 models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 rand_seed = [13, 58, 94, None] priorities = ['data','equal'] file = 'biobj_example.csv' consol_log = False model_runs.biobjective(   [""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", time_limit,    ""-p"", priorities, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.biobjective -d ['ionosphere', 'monk2', 'breat_cancer', 'climate'] -h 5 -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -p ['data','equal'] -r [13, 58, 94, None] -f 'biobj_example.csv' -l False ```  *** ## Pareto Frontier To generate the Pareto frontier call the `main` function in `pareto_runs.py` with the below parameters:   - d : str list, names of dataset files   - h : int, maximum depth of trained trees   - t : float, gurobi model time limit in s   - m : str list, models to use   - r : str list, random state(s) to use   - f : str, results output file .csv  A `.png` file for each dataset called by the user is generated and stored in `\results_figures\` folder  We assume `-f file` is located in the `\results_files` folder - If results output file `-f file` is `None` the `pareto` function automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder  You can generate pareto frontiers from within a python file as follows,  ```python import model_runs  height = 4 models = ['FlowOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] rand_states = [15, 78, 0] data_names = ['hayes_roth', 'house_votes_84'] file = 'pareto_example.csv' model_runs.pareto([""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", 3600, ""-r"", rand_states, ""-f"", file]) ```  To run from terminal do the following  ```bash python3 import model_runs; model_runs.pareto -d ['hayes_roth', 'house_votes_84'] -h 4 -m ['FOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] -t 3600 -r [15, 78, 0] -f 'pareto_example.csv' ``` - Note: `FlowOCT` must be the model name to generate the pareto frontier of FlowOCT ***  ## Models Functionality For understanding model functionality associated with integer and fractional separation procedures in **CUT1** and **CUT2** models, `-e model_extras` and `-c tuning` functionality please refer to the `USAGE.md` file.",python,PROGLANG
"Change the hard code in `model_runs.py` to change the according target column - If results output file `-f file` is `None` the `models_run.py` calls automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder - `-e model_extras`, `-p priorities`, and `-f file` may be `None` input arguments, all others must hold a valid value  *** Call the `model_runs.py` `main` function within a python file as follows to generate a model ignorning our second objective,  ```python import model_runs  data_names = ['soybean_small', 'monk3', 'car', 'iris', 'climate'] heights = [3, 4, 5] models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 extras = ['max_features-25'] rand_seed = [13, 58, 94, None] tuning = None file = 'example_code_output.csv' consol_log = False model_runs.main(   [""-d"", data_names, ""-h"", heights, ""-m"", models, ""-t"", time_limit,    ""-e"", extras, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.main -d ['soybean-small','monk3','car','iris','climate'] -h [3,4,5] -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -e ['max_features-25'] -r [13, 58, 94, None] -c None -f 'test_results.csv' -l False ```  *** ## Bi-objective Modeling Call the `biobjective` function within a python file as follows to generate a model using the heirarchical modeling capabilities of Gurobi  ```python import model_runs  data_names = ['ionosphere', 'monk2', 'breat_cancer', 'climate'] height = 5 models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 rand_seed = [13, 58, 94, None] priorities = ['data','equal'] file = 'biobj_example.csv' consol_log = False model_runs.biobjective(   [""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", time_limit,    ""-p"", priorities, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.biobjective -d ['ionosphere', 'monk2', 'breat_cancer', 'climate'] -h 5 -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -p ['data','equal'] -r [13, 58, 94, None] -f 'biobj_example.csv' -l False ```  *** ## Pareto Frontier To generate the Pareto frontier call the `main` function in `pareto_runs.py` with the below parameters:   - d : str list, names of dataset files   - h : int, maximum depth of trained trees   - t : float, gurobi model time limit in s   - m : str list, models to use   - r : str list, random state(s) to use   - f : str, results output file .csv  A `.png` file for each dataset called by the user is generated and stored in `\results_figures\` folder  We assume `-f file` is located in the `\results_files` folder - If results output file `-f file` is `None` the `pareto` function automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder  You can generate pareto frontiers from within a python file as follows,  ```python import model_runs  height = 4 models = ['FlowOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] rand_states = [15, 78, 0] data_names = ['hayes_roth', 'house_votes_84'] file = 'pareto_example.csv' model_runs.pareto([""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", 3600, ""-r"", rand_states, ""-f"", file]) ```  To run from terminal do the following  ```bash python3 import model_runs; model_runs.pareto -d ['hayes_roth', 'house_votes_84'] -h 4 -m ['FOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] -t 3600 -r [15, 78, 0] -f 'pareto_example.csv' ``` - Note: `FlowOCT` must be the model name to generate the pareto frontier of FlowOCT ***  ## Models Functionality For understanding model functionality associated with integer and fractional separation procedures in **CUT1** and **CUT2** models, `-e model_extras` and `-c tuning` functionality please refer to the `USAGE.md` file.",python,PROGLANG
"Change the hard code in `model_runs.py` to change the according target column - If results output file `-f file` is `None` the `models_run.py` calls automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder - `-e model_extras`, `-p priorities`, and `-f file` may be `None` input arguments, all others must hold a valid value  *** Call the `model_runs.py` `main` function within a python file as follows to generate a model ignorning our second objective,  ```python import model_runs  data_names = ['soybean_small', 'monk3', 'car', 'iris', 'climate'] heights = [3, 4, 5] models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 extras = ['max_features-25'] rand_seed = [13, 58, 94, None] tuning = None file = 'example_code_output.csv' consol_log = False model_runs.main(   [""-d"", data_names, ""-h"", heights, ""-m"", models, ""-t"", time_limit,    ""-e"", extras, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.main -d ['soybean-small','monk3','car','iris','climate'] -h [3,4,5] -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -e ['max_features-25'] -r [13, 58, 94, None] -c None -f 'test_results.csv' -l False ```  *** ## Bi-objective Modeling Call the `biobjective` function within a python file as follows to generate a model using the heirarchical modeling capabilities of Gurobi  ```python import model_runs  data_names = ['ionosphere', 'monk2', 'breat_cancer', 'climate'] height = 5 models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 rand_seed = [13, 58, 94, None] priorities = ['data','equal'] file = 'biobj_example.csv' consol_log = False model_runs.biobjective(   [""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", time_limit,    ""-p"", priorities, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.biobjective -d ['ionosphere', 'monk2', 'breat_cancer', 'climate'] -h 5 -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -p ['data','equal'] -r [13, 58, 94, None] -f 'biobj_example.csv' -l False ```  *** ## Pareto Frontier To generate the Pareto frontier call the `main` function in `pareto_runs.py` with the below parameters:   - d : str list, names of dataset files   - h : int, maximum depth of trained trees   - t : float, gurobi model time limit in s   - m : str list, models to use   - r : str list, random state(s) to use   - f : str, results output file .csv  A `.png` file for each dataset called by the user is generated and stored in `\results_figures\` folder  We assume `-f file` is located in the `\results_files` folder - If results output file `-f file` is `None` the `pareto` function automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder  You can generate pareto frontiers from within a python file as follows,  ```python import model_runs  height = 4 models = ['FlowOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] rand_states = [15, 78, 0] data_names = ['hayes_roth', 'house_votes_84'] file = 'pareto_example.csv' model_runs.pareto([""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", 3600, ""-r"", rand_states, ""-f"", file]) ```  To run from terminal do the following  ```bash python3 import model_runs; model_runs.pareto -d ['hayes_roth', 'house_votes_84'] -h 4 -m ['FOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] -t 3600 -r [15, 78, 0] -f 'pareto_example.csv' ``` - Note: `FlowOCT` must be the model name to generate the pareto frontier of FlowOCT ***  ## Models Functionality For understanding model functionality associated with integer and fractional separation procedures in **CUT1** and **CUT2** models, `-e model_extras` and `-c tuning` functionality please refer to the `USAGE.md` file.",python,PROGLANG
"Change the hard code in `model_runs.py` to change the according target column - If results output file `-f file` is `None` the `models_run.py` calls automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder - `-e model_extras`, `-p priorities`, and `-f file` may be `None` input arguments, all others must hold a valid value  *** Call the `model_runs.py` `main` function within a python file as follows to generate a model ignorning our second objective,  ```python import model_runs  data_names = ['soybean_small', 'monk3', 'car', 'iris', 'climate'] heights = [3, 4, 5] models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 extras = ['max_features-25'] rand_seed = [13, 58, 94, None] tuning = None file = 'example_code_output.csv' consol_log = False model_runs.main(   [""-d"", data_names, ""-h"", heights, ""-m"", models, ""-t"", time_limit,    ""-e"", extras, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.main -d ['soybean-small','monk3','car','iris','climate'] -h [3,4,5] -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -e ['max_features-25'] -r [13, 58, 94, None] -c None -f 'test_results.csv' -l False ```  *** ## Bi-objective Modeling Call the `biobjective` function within a python file as follows to generate a model using the heirarchical modeling capabilities of Gurobi  ```python import model_runs  data_names = ['ionosphere', 'monk2', 'breat_cancer', 'climate'] height = 5 models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 rand_seed = [13, 58, 94, None] priorities = ['data','equal'] file = 'biobj_example.csv' consol_log = False model_runs.biobjective(   [""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", time_limit,    ""-p"", priorities, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.biobjective -d ['ionosphere', 'monk2', 'breat_cancer', 'climate'] -h 5 -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -p ['data','equal'] -r [13, 58, 94, None] -f 'biobj_example.csv' -l False ```  *** ## Pareto Frontier To generate the Pareto frontier call the `main` function in `pareto_runs.py` with the below parameters:   - d : str list, names of dataset files   - h : int, maximum depth of trained trees   - t : float, gurobi model time limit in s   - m : str list, models to use   - r : str list, random state(s) to use   - f : str, results output file .csv  A `.png` file for each dataset called by the user is generated and stored in `\results_figures\` folder  We assume `-f file` is located in the `\results_files` folder - If results output file `-f file` is `None` the `pareto` function automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder  You can generate pareto frontiers from within a python file as follows,  ```python import model_runs  height = 4 models = ['FlowOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] rand_states = [15, 78, 0] data_names = ['hayes_roth', 'house_votes_84'] file = 'pareto_example.csv' model_runs.pareto([""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", 3600, ""-r"", rand_states, ""-f"", file]) ```  To run from terminal do the following  ```bash python3 import model_runs; model_runs.pareto -d ['hayes_roth', 'house_votes_84'] -h 4 -m ['FOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] -t 3600 -r [15, 78, 0] -f 'pareto_example.csv' ``` - Note: `FlowOCT` must be the model name to generate the pareto frontier of FlowOCT ***  ## Models Functionality For understanding model functionality associated with integer and fractional separation procedures in **CUT1** and **CUT2** models, `-e model_extras` and `-c tuning` functionality please refer to the `USAGE.md` file.",python,PROGLANG
"Change the hard code in `model_runs.py` to change the according target column - If results output file `-f file` is `None` the `models_run.py` calls automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder - `-e model_extras`, `-p priorities`, and `-f file` may be `None` input arguments, all others must hold a valid value  *** Call the `model_runs.py` `main` function within a python file as follows to generate a model ignorning our second objective,  ```python import model_runs  data_names = ['soybean_small', 'monk3', 'car', 'iris', 'climate'] heights = [3, 4, 5] models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 extras = ['max_features-25'] rand_seed = [13, 58, 94, None] tuning = None file = 'example_code_output.csv' consol_log = False model_runs.main(   [""-d"", data_names, ""-h"", heights, ""-m"", models, ""-t"", time_limit,    ""-e"", extras, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.main -d ['soybean-small','monk3','car','iris','climate'] -h [3,4,5] -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -e ['max_features-25'] -r [13, 58, 94, None] -c None -f 'test_results.csv' -l False ```  *** ## Bi-objective Modeling Call the `biobjective` function within a python file as follows to generate a model using the heirarchical modeling capabilities of Gurobi  ```python import model_runs  data_names = ['ionosphere', 'monk2', 'breat_cancer', 'climate'] height = 5 models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 rand_seed = [13, 58, 94, None] priorities = ['data','equal'] file = 'biobj_example.csv' consol_log = False model_runs.biobjective(   [""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", time_limit,    ""-p"", priorities, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.biobjective -d ['ionosphere', 'monk2', 'breat_cancer', 'climate'] -h 5 -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -p ['data','equal'] -r [13, 58, 94, None] -f 'biobj_example.csv' -l False ```  *** ## Pareto Frontier To generate the Pareto frontier call the `main` function in `pareto_runs.py` with the below parameters:   - d : str list, names of dataset files   - h : int, maximum depth of trained trees   - t : float, gurobi model time limit in s   - m : str list, models to use   - r : str list, random state(s) to use   - f : str, results output file .csv  A `.png` file for each dataset called by the user is generated and stored in `\results_figures\` folder  We assume `-f file` is located in the `\results_files` folder - If results output file `-f file` is `None` the `pareto` function automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder  You can generate pareto frontiers from within a python file as follows,  ```python import model_runs  height = 4 models = ['FlowOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] rand_states = [15, 78, 0] data_names = ['hayes_roth', 'house_votes_84'] file = 'pareto_example.csv' model_runs.pareto([""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", 3600, ""-r"", rand_states, ""-f"", file]) ```  To run from terminal do the following  ```bash python3 import model_runs; model_runs.pareto -d ['hayes_roth', 'house_votes_84'] -h 4 -m ['FOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] -t 3600 -r [15, 78, 0] -f 'pareto_example.csv' ``` - Note: `FlowOCT` must be the model name to generate the pareto frontier of FlowOCT ***  ## Models Functionality For understanding model functionality associated with integer and fractional separation procedures in **CUT1** and **CUT2** models, `-e model_extras` and `-c tuning` functionality please refer to the `USAGE.md` file.",python,PROGLANG
We converted their data in `*.csv` files to work in Python.,Python,PROGLANG
Related Datasets ---------------- 'Proceedings of Higher Education Institutions: Mathematics' (PHEIM) dataset is available via [our SPARQL endpoint](http://lobachevskii-dml.ru:8890/sparql).,S,PROGLANG
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,C++11,PROGLANG
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,C++0x,PROGLANG
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,bash,PROGLANG
"Integration of this project with ROS is possible, but it requires compiling and installing the core project first and then linking it to your ROS logic.  ## Installation  Once you installed all the dependencies, you can download and install the project:  ```bash git clone https://github.com/m-dayani/EORB_SLAM.git cd EORB_SLAM  mkdir build cd build cmake .. make -j4 ```  To make life easier for you, I included a bash script (`build_eorb_slam.sh`) and a Docker file that contains all the necessary commands to download and install the required packages.  ## Usage  1.",bash,PROGLANG
"Integration of this project with ROS is possible, but it requires compiling and installing the core project first and then linking it to your ROS logic.  ## Installation  Once you installed all the dependencies, you can download and install the project:  ```bash git clone https://github.com/m-dayani/EORB_SLAM.git cd EORB_SLAM  mkdir build cd build cmake .. make -j4 ```  To make life easier for you, I included a bash script (`build_eorb_slam.sh`) and a Docker file that contains all the necessary commands to download and install the required packages.  ## Usage  1.",bash,PROGLANG
[python](https://img.shields.io/badge/-Python_3.7.9-blue?,python,PROGLANG
[python](https://img.shields.io/badge/-Python_3.7.9-blue?,Python_3.7.9,PROGLANG
logo=python&logoColor=white)](https://www.python.org/downloads/release/python-379/) [!,python,PROGLANG
logo=python&logoColor=white)](https://www.python.org/downloads/release/python-379/) [!,python,PROGLANG
logo=python&logoColor=white)](https://www.python.org/downloads/release/python-379/) [!,python,PROGLANG
"The code has been tested in the following environment: - Ubuntu 18.04.6 LTS - Python 3.8.5 - PyTorch 1.7.1 - CUDA 10.2, 11.2 and 11.4 - NVIDIA V100 32G - 64G RAM  <br>  ## 🏗️  Installation To install DeepViewAgg, simply run `.",Python 3.8.5,PROGLANG
```bash ├─ conf                    # All configurations live there ├─ notebooks               # Notebooks to get started with multimodal datasets and models ├─ eval.py                 # Eval script ├─ insall.sh               # Installation script for DeepViewAgg ├─ scripts                 # Some scripts to help manage the project ├─ torch_points3d     ├─ core                # Core components     ├─ datasets            # All code related to datasets     ├─ metrics             # All metrics and trackers     ├─ models              # All models     ├─ modules             # Basic modules that can be used in a modular way     ├─ utils               # Various utils     └─ visualization       # Visualization └─ train.py                # Main script to launch a training ```  Several changes were made to extend the original project to multimodal learning on point clouds with images.,bash,PROGLANG
"The `util/` directory contains some codes to parse specific datasets and generate queries.        ### Dataset Structure ---------------- The graphs and query/answer sets can be in any format but in order to use our train script and dataset utilities these should be in following format.        ### File Structure ---------------- Our training script assume that the data stored in the following format: ``` data/     dataset_name/         graph_{snapshot_id}.txt         graph_{snapshot_id}.txt         queries_{snapshot_id}.txt         queries_{snapshot_id}.txt ```  So for example for a a dataset named `foo` with 3 snapshots it would be like:  ``` data/     foo/         graph_1.txt         graph_2.txt         graph_3.txt         queries_1.txt         queries_2.txt         queries_3.txt         test_queries.txt         valid_queries.txt ```        ### Graph file and QueryFiles ---------------- The graph files must have the following format.  ``` node_id1 node_id2 .. .. ```  The query files must have the following format.   ``` q1_node1,q1_node2,... answer1_node1,answer1_node2,... q2_node1,q2_node2,... answer2_node1,answer2_node2 ```         ### Training the Models & Execution ---------------- Finally the models could be trained and evaluated with the following command.   ``` python3 train.py <DATASET_NAME> <MAX_VERTICES> <START_SNAPSHOT_ID> <END_SNAPSHOT_ID> <THRESHOLD> <HIDDEN_DIM_SIZE> <EPOCHS> <LEARNING_RATE> <REGULARIZATION> ```  For example: ``` python3 train.py football 200 1 4 0.4 64 100 0.001 0.00001 ```   ### Reference ----------------   ``` @inproceedings{CS-TGN, author = {Hashemi, Farnoosh and Behrouz, Ali and Rezaei Hajidehi, Milad}, title = {CS-TGN: Community Search via Temporal Graph Neural Networks}, year = {2023}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3543873.3587654}, doi = {10.1145/3543873.3587654}, booktitle = {Companion Proceedings of the Web Conference 2023}, numpages = {8}, location = {AUSTIN, TEXAS, USA}, series = {WWW '23} } ```",python,PROGLANG
[Python](https://img.shields.io/badge/python-3.10%5E-blue)](https://www.python.org/)  # StarCraft II Datasets  Library can be used to interface with datasets that were pre-processed with our pipeline as described in: - [SC2DatasetPreparator](https://github.com/Kaszanas/SC2DatasetPreparator)  Currently we have exposed PyTorch and PyTorch Lightning API.,Python,PROGLANG
[Python](https://img.shields.io/badge/python-3.10%5E-blue)](https://www.python.org/)  # StarCraft II Datasets  Library can be used to interface with datasets that were pre-processed with our pipeline as described in: - [SC2DatasetPreparator](https://github.com/Kaszanas/SC2DatasetPreparator)  Currently we have exposed PyTorch and PyTorch Lightning API.,python-3.10,PROGLANG
[Python](https://img.shields.io/badge/python-3.10%5E-blue)](https://www.python.org/)  # StarCraft II Datasets  Library can be used to interface with datasets that were pre-processed with our pipeline as described in: - [SC2DatasetPreparator](https://github.com/Kaszanas/SC2DatasetPreparator)  Currently we have exposed PyTorch and PyTorch Lightning API.,python,PROGLANG
Perform the following command:  ```bash $ pip install sc2_datasets ```  ## Usage  Basic example usage can be seen below.,bash,PROGLANG
"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir="".",python,PROGLANG
The library requires at least C++11.,C++11,PROGLANG
"It accompanies the paper ""Iterative Graph Reasoning"" and contains:  - 5 distinct rule-based scenarios - 200 training queries per scenario (low-data learning scenario) - 100 test queries per scenario - Weak annotations provided for each query  ## :new: Updates - [08/2024] [Arxiv paper](https://arxiv.org/abs/2408.16667) released. - [08/2024] RuleAlign dataset announced.  ## :gear: Evaluation  To run the evaluation on the RuleAlign dataset, use the following command:  ```shell python -m script.eval --model_name xxx ```  Replace `xxx` with the name of the model you want to evaluate.  ## :hugs: Citation If you find this dataset useful for your research, please kindly cite our paper:  ``` @misc{yu2024iterativegraphreasoning,       title={Iterative Graph Reasoning},        author={Fangyuan Yu},       year={2024},       eprint={2408.03615},       archivePrefix={arXiv},       primaryClass={cs.AI} } ```  ## :mailbox: Contact  For any questions or issues regarding the RuleEval dataset, please contact the corresponding author.",shell,PROGLANG
Run your desired experiment from `scripts/` by executing the script with Python 3.7 or higher.  ## Models We present three model architectures to predict early pass / fail student performance prediction.,Python 3.7,PROGLANG
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",bash,PROGLANG
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",python=3.7,PROGLANG
"usp=sharing) provided by [AlignPS](https://github.com/daodaofr/AlignPS).   #####  Train with a single GPU ```shell python tools/train.py ${CONFIG_FILE} --no-validate ``` - CONFIG_FILE about PSTR is in [configs/PSTR](configs/pstr)  #####  Test with a single GPU  ```shell PRW: sh run_test_prw.sh  CUHK: sh run_test_cuhk.sh   ```  - If you want to output the results of different models, please  change CONFIGPATH, MODELPATH, OUTPATH for diffferent models   ## Results  We provide some models with different backbones and results on PRW and CUHK-SYSU datasets, which have a little difference to CVPR version due to jitter",python,PROGLANG
```bash    docker pull lizytalk/dejavu    ``` 2.,bash,PROGLANG
Pull the code from GitHub    ```bash    git pull https://github.com/NetManAIOps/DejaVu.git DejaVu    ``` 3.,bash,PROGLANG
Start a Docker container with our image and enter its shell    ```bash    docker run -it --rm -v $(realpath DejaVu):/workspace lizytalk/dejavu bash    ``` 6.,bash,PROGLANG
"For instance, you can load XTR checkpoints as follows:  ```python ## Model Usage import tensorflow_hub as hub import tensorflow as tf import tensorflow_text as text  # Registers the ops.",python,PROGLANG
"```python import torch from tempnet import LLaMA_TempNet from transformers import AutoTokenizer, GenerationConfig  model_name = 'LLM-Opt/TempNet-LLaMA2-Chat-7B-v0.1'  tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=False) generation_config = GenerationConfig.from_pretrained(model_name) model = LLaMA_TempNet.from_pretrained(model_name, device_map=""auto"", torch_dtype=torch.float16)  inputs = 'How do you get water in the desert?'",python,PROGLANG
Please first install the following python packages. ``` torch==1.12.1 datasets evaluate accelerate tqdm ```  ## Implementation of DP fine-tuning  The transformers package in `dp_finetuning` enables DP training.,python,PROGLANG
"/dp_finetuning cd transformers pip install --editable . cd .. ```   ``` bash scripts/train_domain_classifier.sh 1.4 1 64 32 1e-3 ```  > The arguments are: noise_multiplier, clip, pergpu_bs, gradient accumulation steps, and learning rate.",bash,PROGLANG
"/pretraining cd transformers pip install --editable . cd .. ```    ``` bash scripts/pretraining.sh pretraining_data_40m.ds tiny 3e-4 1000000 32 8 1 ```  > The arguments are: pre-training data path, model size (tiny=5M), lr, pre-training steps, per-gpu-bs, num_gpus, gradient accumulation",bash,PROGLANG
Replace checkpoint-XXXX with your checkpoint.  ``` bash scripts/train_sst2.sh ..,bash,PROGLANG
"```python # examples question = ""Let $x,$ $y,$ and $z$ be positive real numbers such that $xyz(x + y + z) = 1.$  Find the minimum value of\n\\[(x + y)(y + z).\\]"" reasoning_steps = [""1.",python,PROGLANG
/codes/examples.py with your own content. python .,python,PROGLANG
Here is one annotated line     ```javascript     // .,javascript,PROGLANG
Here is one annotated line     ```javascript     // .,javascript,PROGLANG
Here is one annotated line   ```javascript   // .,javascript,PROGLANG
"/dataset/`  ### Code To reproduce our results in the meta-evaluations, run the following commands: ```bash python .",python,PROGLANG
/codes/mr-gsm8k_eval.py python .,python,PROGLANG
/codes/mr-math_eval.py --error_type invalid python .,python,PROGLANG
"<img src='images/f1.png' width=""600px""/>  ## Visualization  Visualization examples of the fitted TnT graph structures:  <img src='images/f3.png' width=""1000px""/>  - (Optional) install sknetwork to enable visualization: ```bash pip install scikit-network ```   ## Results \# S indicates the number of split nodes, which is an estimate of model complexity",bash,PROGLANG
"The changes in `goatools.obo_parser` are as follows:  ```python # line 132 elif line[:5] == ""def: "":     rec_curr.definition = line[5:]  # line 169 self.definition = """" ```  ### Environment for OntoProtein pre-training <span id=""environment-for-ontoprotein-pre-training""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / deepspeed 0.5.1/ lmdb /   ### Environment for protein-related tasks <span id=""environment-for-protein-related-tasks""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / lmdb / tape_proteins  Specially, in library `tape_proteins`, it only implements the calculation of metric `P@L` for the contact prediction task.",python,PROGLANG
"[[link]](https://ftp.ebi.ac.uk/pub/databases/GO/goa/old/UNIPROT/)  When download these raw data, you can excute following script to generate pre-training data:  ```bash python tools/gen_onto_protein_data.py ```  ### Downstream task data <span id=""downstream-task-data""></span> Our experiments involved with several protein-related downstream tasks.",bash,PROGLANG
We provide the script `bash script/run_pretrain.sh` to run pre-training.,bash,PROGLANG
"Also, you can utilize the running codes `run_downstream.py` , and write your shell files according to your need:  - `run_downstream.py`: support `{ss3, ss8, contact, remote_homology, fluorescence, stability}` tasks;  #### Training models  Running shell files: `bash script/run_{task}.sh`, and the contents of shell files are as follow:  ```shell bash run_main.sh \     --model model_data/ProtBertModel \     --output_file ss3-ProtBert \     --task_name ss3 \     --do_train True \     --epoch 5 \     --optimizer AdamW \     --per_device_batch_size 2 \     --gradient_accumulation_steps 8 \     --eval_step 100 \     --eval_batchsize 4 \     --warmup_ratio 0.08 \     --frozen_bert False ```  Arguments for the training and evalution script are as follows,  - `--task_name`: Specify which task to evaluate on, and now the script supports `{ss3, ss8, contact, remote_homology, fluorescence, stability}` tasks; - `--model`: The name or path of a protein pre-trained checkpoint. - `--output_file`: The path of the fine-tuned checkpoint saved. - `--do_train`: Specify if you want to finetune the pretrained model on downstream tasks. - `--epoch`: Epochs for training model. - `--optimizer`: The optimizer to use, e.g., `AdamW`. - `--per_device_batch_size`: Batch size per GPU. - `--gradient_accumulation_steps`: The number of gradient accumulation steps. - `--warmup_ratio`: Ratio of total training steps used for a linear warmup from 0 to `learning_rate`. - `--frozen_bert`: Specify if you want to frozen the encoder in the pretrained model.",bash,PROGLANG
"Also, you can utilize the running codes `run_downstream.py` , and write your shell files according to your need:  - `run_downstream.py`: support `{ss3, ss8, contact, remote_homology, fluorescence, stability}` tasks;  #### Training models  Running shell files: `bash script/run_{task}.sh`, and the contents of shell files are as follow:  ```shell bash run_main.sh \     --model model_data/ProtBertModel \     --output_file ss3-ProtBert \     --task_name ss3 \     --do_train True \     --epoch 5 \     --optimizer AdamW \     --per_device_batch_size 2 \     --gradient_accumulation_steps 8 \     --eval_step 100 \     --eval_batchsize 4 \     --warmup_ratio 0.08 \     --frozen_bert False ```  Arguments for the training and evalution script are as follows,  - `--task_name`: Specify which task to evaluate on, and now the script supports `{ss3, ss8, contact, remote_homology, fluorescence, stability}` tasks; - `--model`: The name or path of a protein pre-trained checkpoint. - `--output_file`: The path of the fine-tuned checkpoint saved. - `--do_train`: Specify if you want to finetune the pretrained model on downstream tasks. - `--epoch`: Epochs for training model. - `--optimizer`: The optimizer to use, e.g., `AdamW`. - `--per_device_batch_size`: Batch size per GPU. - `--gradient_accumulation_steps`: The number of gradient accumulation steps. - `--warmup_ratio`: Ratio of total training steps used for a linear warmup from 0 to `learning_rate`. - `--frozen_bert`: Specify if you want to frozen the encoder in the pretrained model.",shell,PROGLANG
This page contains Tulip plugins using [Tulip API](http://tulip.labri.fr/Documentation/current/doxygen/html/index.html) or [Tulip Python API](http://tulip.labri.fr/Documentation/current/tulip-python/html).,Python,PROGLANG
This page contains Tulip plugins using [Tulip API](http://tulip.labri.fr/Documentation/current/doxygen/html/index.html) or [Tulip Python API](http://tulip.labri.fr/Documentation/current/tulip-python/html).,python,PROGLANG
** This python plugin uses `collections.OrderedDict`.,python,PROGLANG
"For python version prior to 2.7, you need to install the package: `pip install ordereddict`  ### [Clique Percolation](https://github.com/fqueyroi/tulip_plugins/tree/master/Cliques)  An implementation of the [Clique Percolation Method](https://en.wikipedia.org/wiki/Clique_percolation_method) that finds an overlapping clustering of the graph.",python,PROGLANG
"Social Network Analysis and Mining, 4(1), 191.  ## [Minimum Spanning Tree](https://github.com/fqueyroi/tulip_plugins/tree/master/MinimumSpanningTree)  Compute the [Minimum Spanning Tree](https://en.wikipedia.org/wiki/Minimum_spanning_tree) of the graph (Python script) using a [Union-Find](https://en.wikipedia.org/wiki/Kruskal%27s_algorithm) data structure.   ## [Vertex Cycle Cover and Secret Santa!]",Python,PROGLANG
"Therefore, the Tulip Python plugin ""SecretSanta"" find a solution where the knowledge of a given assignment can not be used to infer another assignment.",Python,PROGLANG
"You can find an online version of our paper at arXiv: https://arxiv.org/abs/1904.10678  **If you use our method, please cite our paper.**  ---  ## Table of Contents  1. [ Dependencies, pre-requisites, and setting up the code ](#dependencies-pre-requisites-and-setting-up-the-code) 2. [ Reproduce the results of the paper ](#reproduce-the-results-of-the-paper) 3. [ Use the code with your own data ](#use-the-code-with-your-own-data) 4. [ Previous work ](#previous-work) 5. [ Acknowledgement ](#acknowledgement)  ---  ## Dependencies, pre-requisites, and setting up the code   In order to use our code, you have to firstly:   * use Python 3.x and install all the required packages listed at the [requirements file for PiP](https://github.com/dr-costas/undaw//blob/master/requirements.txt) or at the [requirements file for Anaconda](https://github.com/dr-costas/undaw//blob/master/conda_requirements.txt)  * download the data (the file ``AUDASC_features_labels.zip``) from  [!",Python 3.x,PROGLANG
"## MAP-Elites for Constrained Optimization  Python implementation of the [MAP Elites algorithm](https://arxiv.org/abs/1504.04909) (originally devised for unconstrained optimization), for constrained optimization problems.",Python,PROGLANG
"# Sparse Coding with Multi-Layer Decoders using Variance Regularization This is a PyTorch implementation for the setup described in  [Sparse Coding with Multi-Layer Decoders using Variance Regularization](https://arxiv.org/abs/2112.09214).   ### Requirements  - Python 3.7 - [PyTorch](https://pytorch.org/get-started/previous-versions/) 1.6.0 with torchvision 0.7.0 - Other dependencies: numpy, tensorboardX  ### Datasets  In our experiments, we use: - the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset.",Python 3.7,PROGLANG
"The columns contain: ```python COLUMNS_FALCON_DCI = [     'timestamp',    # unix timestamp in [s] as float, 1µs resolution     'sfn',          # system frame number     'subframe',     # subframe index {0,1,...,9}     'rnti',         # the addressed RNTI     'direction',    # 0 for uplink alloc., 1 for downlink alloc.",python,PROGLANG
CADET will solve this file easily:  ```bash $ .,bash,PROGLANG
With the command line argument `-c <filename>` CADET outputs this function as an [AIGER](fmv.jku.at/aiger/) circuit:    ```bash $ .,bash,PROGLANG
"For example, you can use the following command to minimize circuits with [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/):  ```bash $ abc -c ""read result.aig; dc2; write result.aig"" ```  After the minimization a circuit with a single AND-gate remains:  ```aiger aag 3 2 0 1 1 2 4 6 6 4 2 i0 1 i1 2 o0 3 ```  To view a human-readable version of the circuit as shown above you have to convert the AIGER binrary formag `.aig` to the AIGER ASCII format `.aag` using the tool `aigtoaig` available in the [AIGER toolset](http://fmv.jku.at/aiger/aiger-1.9.9.tar.gz).   ## Installing CADET   CADET can be built from source with both clang and gcc.",bash,PROGLANG
To compile the solver type:  ```bash $ .,bash,PROGLANG
"/configure && make ```  To make sure the solver works correctly execute the test suite:  ```bash $ make test ```  One of the test cases will timeout as part of the testsuite and a number of tests will return with the result UNKNOWN, which is intended.    ## Usage  The most common use case for the solver is to solve formulas specified as a [QDIMACS](http://www.qbflib.org/qdimacs.html) file.",bash,PROGLANG
```bash $ .,bash,PROGLANG
/cadet file.qdimacs ```  You can also pipe QDIMACS into the solver:  ```bash $ cat file.qdimacs | .,bash,PROGLANG
"For example, type:  ```bash $ .",bash,PROGLANG
"We apply our approach to generate sentence analogies from sentence embeddings.  ## Quickstart  You can quickly install Vec2Sent using pip:  ```shell pip install ""vec2sent @ git+https://github.com/maruker/vec2sent.git"" ```  There are three entry points to **generate** and **evaluate** sentences, and to perform **arithmetic** in the vector space.  ### Vector Arithmetic  ```shell vec2sent_arithmetic -s infersent -c maruker/vec2sent-infersent ```  ```text Please enter sentence a (Or nothing if done):his name is robert Please enter sentence b (Or nothing if done):he is a doctor Please enter sentence c (Or nothing if done):her name is julia Please enter sentence d (Or nothing if done): Please enter an arithmetic expression (e.g.",shell,PROGLANG
"(a + b) * c / 2):b-a+c  she is a doctor ```  ### Sentence Generation  For example, generate outputs using the hierarchical sentence embedding  ```shell vec2sent_generate -s hier -c maruker/vec2sent-hier -d data/test.en.2008 -o hier.txt ```  ### Evaluation  The outputs from the previous step can now be evaluated.",shell,PROGLANG
"For example, the following command computes the bleu score  ```shell vec2sent_evaluate --metric BLEU --file hier.txt ```  The following metrics are available  | Parameter | Explanation                                                                       | |-----------|-----------------------------------------------------------------------------------| | ID        | Fraction of all sentences where the output is identical to the input              | | PERM      | Fraction of all output sentences that can be formed as a permutation of the input | | ID_PERM   | Fraction of all permuations that are identical to the input                       | | BLEU      | Document BLEU score                                                               | | MOVER     | Average Mover Score between input and output sentences                            |   > [!",shell,PROGLANG
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",shell,PROGLANG
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",shell,PROGLANG
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",shell,PROGLANG
Only `python3` is supported.,python3,PROGLANG
python vinfo/experiment.py example/roberta768-upos-layer5-example-cpu.yaml  1.,python,PROGLANG
"Intended to minimize the amount of experiment logic code needed to swap out new `Probe`, `Loss`, or `Model` classes when extending the repository, all python classes defined in the codebase are actually constructed with the `yaml` loading process.",python,PROGLANG
"[Auto-rewrite](figs/autorewrite.png)  ## Setup  ### Install dependencies  Please install all dependency packages using the following command: ```bash pip install -r requirements.txt ```  ### Download the datasets  Our experiments use [QuAC dataset](https://quac.ai) for passages and conversations, and the test set of [CANARD dataset](https://sites.google.com/view/qanta/projects/canard) for context-independent questions in `Auto-Replace`.  ## Evaluating existing models  We provide our implementations for the four models that we used in our paper: BERT, [GraphFlow](https://www.ijcai.org/Proceedings/2020/171), [HAM](https://dl.acm.org/doi/abs/10.1145/3357384.3357905), [ExCorD](https://aclanthology.org/2021.acl-long.478/).",bash,PROGLANG
```bash # Run Training python run_quac_train.py \   --type bert \   --model_name_or_path bert-base-uncased \   --do_train \   --output_dir ${directory_to_save_model} \   --overwrite_output_dir \   --train_file ${path_to_quac_train_file} \   --train_batch_size 8 \   --gradient_accumulation_steps 4 \   --max_seq_length 512 \   --learning_rate 3e-5 \   --history_len 2 \   --warmup_proportion 0.1 \   --max_grad_norm -1 \   --weight_decay 0.01 \   --rationale_beta 0 \ # important for BERT  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type bert \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --match_metric f1 \   --add_background \   --skip_entity \   --rewrite \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### GraphFlow We did not find an uploaded model checkpoint so we trained our own using [their training script](https://github.com/hugochan/GraphFlow).,bash,PROGLANG
"```bash  # Download Stanford CoreNLP package wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip unzip stanford-corenlp-latest.zip rm -f stanford-corenlp-latest.zip  # Start StanfordCoreNLP server java -mx4g -cp ""${directory_to_standford_corenlp_package}"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 &  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \     --type graphflow \     --predict_file ${path-to-annotated-dev-json-file} \     --output_dir ${directory-to-model-checkpoint} \     --saved_vocab_file ${directory-to-saved-model-vocab} \     --pretrained ${directory-to-model-checkpoint} \     --write_dir /n/fs/scratch/huihanl/unified/graphflow/write \     --match_metric f1 \     --add_background \     --skip_entity \     --rewrite \     --fix_vocab_embed \     --f_qem \     --f_pos \     --f_ner \     --use_ques_marker \     --use_gnn \     --temporal_gnn \     --use_bert \     --use_bert_weight \     --shuffle \     --out_predictions \     --predict_raw_text \     --out_pred_in_folder \     --optimizer adamax \     --start_i ${index_of_first_passage_to_eval} \     --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### HAM The orgininal model checkpoint can be downloaded from [CodaLab](https://worksheets.codalab.org/rest/bundles/0x5c08cb0fb90c4afd8a2811bb63023cce/contents/blob/)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type ham \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --do_lower_case \   --history_len 6 \   --match_metric f1 \   --add_background \   --skip_entity \   --replace \   --init_checkpoint ${directory-to-model-checkpoint}/model_52000.ckpt \   --bert_config_file ${directory-to-pretrained-bert-large-uncased}/bert_config.json \   --vocab_file ${directory-to-model-checkpoint}/vocab.txt \   --MTL_mu 0.8 \   --MTL_lambda 0.1 \   --mtl_input reduce_mean \   --max_answer_length 40 \   --max_considered_history_turns 4 \   --bert_hidden 1024 \   --fine_grained_attention \   --better_hae \   --MTL \   --use_history_answer_marker \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### ExCorD The original model checkpoint can be downloaded from [their repo](https://drive.google.com/file/d/1Xf0-XUvGi7jgiAAdA5BQLk7p5ikc_wOl/view?",bash,PROGLANG
"```bash  # Download Stanford CoreNLP package wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip unzip stanford-corenlp-latest.zip rm -f stanford-corenlp-latest.zip  # Start StanfordCoreNLP server java -mx4g -cp ""${directory_to_standford_corenlp_package}"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 &  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \     --type graphflow \     --predict_file ${path-to-annotated-dev-json-file} \     --output_dir ${directory-to-model-checkpoint} \     --saved_vocab_file ${directory-to-saved-model-vocab} \     --pretrained ${directory-to-model-checkpoint} \     --write_dir /n/fs/scratch/huihanl/unified/graphflow/write \     --match_metric f1 \     --add_background \     --skip_entity \     --rewrite \     --fix_vocab_embed \     --f_qem \     --f_pos \     --f_ner \     --use_ques_marker \     --use_gnn \     --temporal_gnn \     --use_bert \     --use_bert_weight \     --shuffle \     --out_predictions \     --predict_raw_text \     --out_pred_in_folder \     --optimizer adamax \     --start_i ${index_of_first_passage_to_eval} \     --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### HAM The orgininal model checkpoint can be downloaded from [CodaLab](https://worksheets.codalab.org/rest/bundles/0x5c08cb0fb90c4afd8a2811bb63023cce/contents/blob/)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type ham \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --do_lower_case \   --history_len 6 \   --match_metric f1 \   --add_background \   --skip_entity \   --replace \   --init_checkpoint ${directory-to-model-checkpoint}/model_52000.ckpt \   --bert_config_file ${directory-to-pretrained-bert-large-uncased}/bert_config.json \   --vocab_file ${directory-to-model-checkpoint}/vocab.txt \   --MTL_mu 0.8 \   --MTL_lambda 0.1 \   --mtl_input reduce_mean \   --max_answer_length 40 \   --max_considered_history_turns 4 \   --bert_hidden 1024 \   --fine_grained_attention \   --better_hae \   --MTL \   --use_history_answer_marker \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### ExCorD The original model checkpoint can be downloaded from [their repo](https://drive.google.com/file/d/1Xf0-XUvGi7jgiAAdA5BQLk7p5ikc_wOl/view?",java,PROGLANG
usp=sharing)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type excord \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --match_metric f1 \   --add_background \   --skip_entity \   --rewrite \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```  ## Evaluating your own model One can follow our existing implementations for the four models to implement evaluation for their own models.,bash,PROGLANG
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",Python,PROGLANG
"/utility_evaluation/', in which the intermediate results and figures that shown in the paper are put inline.   ### Privacy  Privacy risk analysis were packaged into Python source files.",Python,PROGLANG
We provide the bash scripts used for our main experiments in the `bash_scripts` directory.,bash,PROGLANG
"```bash git clone https://github.com/SuHoHan95/VISOLO.git cd VISOLO pip install -e . pip install -r requirements.txt pip install git+https://github.com/youtubevos/cocoapi.git#""egg=pycocotools&subdirectory=PythonAPI"" ```  2.",bash,PROGLANG
usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.,bash,PROGLANG
usp=sharing)) ```bash cp coco_to_ytvis2019.json /path_to_coco_dataset/annotations cd projects/VISOLO mkdir -p datasets/coco ln -s /path_to_coco_dataset/annotations datasets/coco/annotations ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017 ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017 ```  YTVIS 2019 ```bash mkdir -p datasets/ytvis_2019 ln -s /path_to_ytvis_2019_dataset/* datasets/ytvis_2019 ``` we expect ytvis_2019 folder to be like ``` └── ytvis_2019     ├── train     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── valid     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── test     │   ├── Annotations     │   ├── JPEGImages     │   └── meta.json     ├── train.json     ├── valid.json     └── test.json ```  3.,bash,PROGLANG
* Training using 4 GPUS(TESLA V100-PCIE-32GB) * Pre-training on COCO dataset ```bash python train_net.py --num-gpus 4 --config-file .,bash,PROGLANG
usp=sharing)) ```bash python train_net.py --num-gpus 4 --config-file .,bash,PROGLANG
Evaluating on YTVIS 2019 ```bash python train_net.py --eval-only --num-gpus 1 --config-file .,bash,PROGLANG
"Fast R-CNN  - trains state-of-the-art models, like VGG16, 9x faster than traditional R-CNN and 3x faster than SPPnet,  - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time,  - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet,  - and is written in Python and C++/Caffe.",Python,PROGLANG
"Fast R-CNN  - trains state-of-the-art models, like VGG16, 9x faster than traditional R-CNN and 3x faster than SPPnet,  - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time,  - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet,  - and is written in Python and C++/Caffe.",C++,PROGLANG
Requirements for `Caffe` and `pycaffe` (see: [Caffe installation instructions](http://caffe.berkeleyvision.org/installation.html))    **Note:** Caffe *must* be built with support for Python layers!,Python,PROGLANG
"```make   # In your Makefile.config, make sure to have this line uncommented   WITH_PYTHON_LAYER := 1   ```    You can download my [Makefile.config](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/Makefile.config) for reference. 2.",PYTHON,PROGLANG
"Python packages you might not have: `cython`, `python-opencv`, `easydict` 3.",cython,PROGLANG
Clone the Fast R-CNN repository   ```Shell   # Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/fast-rcnn.git   ```    2.,Shell,PROGLANG
"We'll call the directory that you cloned Fast R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*        **Note 1:** If you didn't clone Fast R-CNN with the `--recursive` flag, then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `fast-rcnn` branch (or equivalent detached state).",Shell,PROGLANG
Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```      4.,Cython,PROGLANG
Build the Cython modules     ```Shell     cd $FRCN_ROOT/lib     make     ```      4.,Shell,PROGLANG
"Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     # Now follow the Caffe installation instructions here:     #   http://caffe.berkeleyvision.org/installation.html      # If you're experienced with Caffe and have all of the requirements installed     # and your Makefile.config in place, then simply do:     make -j8 && make pycaffe     ```      5.",Shell,PROGLANG
**Python**  To run the demo ```Shell cd $FRCN_ROOT .,Python,PROGLANG
"**MATLAB**  There's also a *basic* MATLAB demo, though it's missing some minor bells and whistles compared to the Python version.",Python,PROGLANG
```Shell cd $FRCN_ROOT/matlab matlab # wait for matlab to start,Shell,PROGLANG
```Shell cd $FRCN_ROOT/matlab matlab # wait for matlab to start,matlab,PROGLANG
"# At the matlab prompt, run the script: >> fast_rcnn_demo ```  Fast R-CNN training is implemented in Python only, but test-time detection functionality also exists in MATLAB.",Python,PROGLANG
See `matlab/fast_rcnn_demo.m` and `matlab/fast_rcnn_im_detect.m` for details.,matlab,PROGLANG
See `matlab/fast_rcnn_demo.m` and `matlab/fast_rcnn_im_detect.m` for details.,matlab,PROGLANG
"Selective Search: [original matlab code](http://disi.unitn.it/~uijlings/MyHomepage/index.php#page=projects1), [python wrapper](https://github.com/sergeyk/selective_search_ijcv_with_python) 2.",matlab,PROGLANG
"Selective Search: [original matlab code](http://disi.unitn.it/~uijlings/MyHomepage/index.php#page=projects1), [python wrapper](https://github.com/sergeyk/selective_search_ijcv_with_python) 2.",python,PROGLANG
"Selective Search: [original matlab code](http://disi.unitn.it/~uijlings/MyHomepage/index.php#page=projects1), [python wrapper](https://github.com/sergeyk/selective_search_ijcv_with_python) 2.",python,PROGLANG
EdgeBoxes: [matlab code](https://github.com/pdollar/edges) 3.,matlab,PROGLANG
GOP and LPO: [python code](http://www.philkr.net/) 4.,python,PROGLANG
MCG: [matlab code](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/) 5.,matlab,PROGLANG
Create symlinks for the PASCAL VOC dataset   ```Shell     cd $FRCN_ROOT/data     ln -s $VOCdevkit VOCdevkit2007     ```     Using symlinks is a good idea because you will likely want to share the same PASCAL dataset installation between multiple projects. 5.,Shell,PROGLANG
```Shell cd $FRCN_ROOT .,Shell,PROGLANG
```Shell cd $FRCN_ROOT .,Shell,PROGLANG
"For example, train a VGG16 network on VOC 2007 trainval:  ```Shell .",Shell,PROGLANG
"For example, test the VGG 16 network on VOC 2007 test:  ```Shell .",Shell,PROGLANG
**Compress** a Fast R-CNN model using truncated SVD on the fully-connected layers:  ```Shell .,Shell,PROGLANG
"The following steps are necessary to reproduce the results:   First, create a new conda environment and install poetry.  ``` conda create -n GISTEmbed python=3.10  conda activate GISTEmbed  pip install poetry ```  Next, clone the repository and install the dependencies.  ``` git clone https://github.com/avsolatorio/GISTEmbed.git  cd GISTEmbed  poetry install ```  To reduce the likelihood of encountering issues and unexpected training runs, we set up a convention that would validate the intended parameters and configurations.",python,PROGLANG
"# Efficient and performance-portable vector software  [//]: # (placeholder, do not remove)  Highway is a C++ library that provides portable SIMD/vector intrinsics.",C++,PROGLANG
Highway makes SIMD/vector programming practical and workable according to these guiding principles:  **Does what you expect**: Highway is a C++ library with carefully-chosen functions that map well to CPU instructions without extensive compiler transformations.,C++,PROGLANG
Highway only requires C++11 and supports four families of compilers.,C++11,PROGLANG
"We recommend these resources for getting started:  -   [SIMD for C++ Developers](http://const.me/articles/simd/simd.pdf) -   [Algorithms for Modern Hardware](https://en.algorithmica.org/hpc/) -   [Optimizing software in C++](https://agner.org/optimize/optimizing_cpp.pdf) -   [Improving performance with SIMD intrinsics in three use cases](https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/)  ## Examples  Online demos using Compiler Explorer:  -   [multiple targets with dynamic dispatch](https://gcc.godbolt.org/z/KM3ben7ET)     (more complicated, but flexible and uses best available SIMD) -   [single target using -m flags](https://gcc.godbolt.org/z/rGnjMevKG)     (simpler, but requires/only uses the instruction set enabled by compiler     flags)  We observe that Highway is referenced in the following open source projects, found via sourcegraph.com.",C++,PROGLANG
"We recommend these resources for getting started:  -   [SIMD for C++ Developers](http://const.me/articles/simd/simd.pdf) -   [Algorithms for Modern Hardware](https://en.algorithmica.org/hpc/) -   [Optimizing software in C++](https://agner.org/optimize/optimizing_cpp.pdf) -   [Improving performance with SIMD intrinsics in three use cases](https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/)  ## Examples  Online demos using Compiler Explorer:  -   [multiple targets with dynamic dispatch](https://gcc.godbolt.org/z/KM3ben7ET)     (more complicated, but flexible and uses best available SIMD) -   [single target using -m flags](https://gcc.godbolt.org/z/rGnjMevKG)     (simpler, but requires/only uses the instruction set enabled by compiler     flags)  We observe that Highway is referenced in the following open source projects, found via sourcegraph.com.",C++,PROGLANG
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",C++,PROGLANG
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",C++11,PROGLANG
"You can avoid this by setting the `HWY_SYSTEM_GTEST` CMake variable to ON and installing gtest separately:  ```bash sudo apt install libgtest-dev ```  Alternatively, you can define `HWY_TEST_STANDALONE=1` and remove all occurrences of `gtest_main` in each BUILD file, then tests avoid the dependency on GUnit.",bash,PROGLANG
"To build Highway as a shared or static library (depending on BUILD_SHARED_LIBS), the standard CMake workflow can be used:  ```bash mkdir -p build && cd build cmake .. make -j && make test ```  Or you can run `run_tests.sh` (`run_tests.bat` on Windows).",bash,PROGLANG
"This     takes care of the loop and remainder handling and you simply define a     generic lambda function (C++14) or functor which receives the current vector     from the input/output array, plus optionally vectors from up to two extra     input arrays, and returns the value to write to the input/output array.",C++14,PROGLANG
```shell python .,shell,PROGLANG
/configs/thumos_i3d.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.,shell,PROGLANG
```shell python .,shell,PROGLANG
```shell python .,shell,PROGLANG
```shell python .,shell,PROGLANG
/configs/anet_tsp.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.,shell,PROGLANG
```shell python .,shell,PROGLANG
```shell python .,shell,PROGLANG
```shell python .,shell,PROGLANG
```shell python .,shell,PROGLANG
"/pretrained*), and run ```shell python .",shell,PROGLANG
"│ └───data/ │    └───epic_kitchens/ │    │  └───annotations │    │  └───features    │    └───... | └───libs │ │   ... ```  **Training and Evaluation** * On EPIC Kitchens, we train separate models for nouns and verbs. * To train our ActionFormer on verbs with SlowFast features, use ```shell python .",shell,PROGLANG
"/configs/epic_slowfast_verb.yaml --output reproduce ``` * To train our ActionFormer on nouns with SlowFast features, use ```shell python .",shell,PROGLANG
```shell python .,shell,PROGLANG
```shell python .,shell,PROGLANG
```shell python .,shell,PROGLANG
```shell python .,shell,PROGLANG
```shell python .,shell,PROGLANG
/configs/ego4d_omnivore_egovlp.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.,shell,PROGLANG
```shell python .,shell,PROGLANG
```shell python .,shell,PROGLANG
"Then, execute the following script:  ``` python prompt_engine/run.py configs/your_config.YAML ```  ### Configure an execution To configure an execution of Medprompt or Self-Consistency CoT ensembling, a configuration file must be created.",python,PROGLANG
[python](https://img.shields.io/badge/Python-3.10-3776AB.svg?,python,PROGLANG
[python](https://img.shields.io/badge/Python-3.10-3776AB.svg?,Python-3.10,PROGLANG
style=flat&logo=python&logoColor=white)](https://www.python.org) [!,python,PROGLANG
style=flat&logo=python&logoColor=white)](https://www.python.org) [!,python,PROGLANG
"```python import pandas as pd from reachml import ActionSet, ReachableSet, ReachableDatabase from reachml.constraints import OneHotEncoding, DirectionalLinkage  # feature matrix with 3 points X = pd.DataFrame(     {         ""age"": [32, 19, 52],         ""marital_status"": [1, 0, 0],         ""years_since_last_default"": [5, 0, 21],         ""job_type_a"": [0, 1, 1], # categorical feature with one-hot encoding         ""job_type_b"": [1, 0, 0],         ""job_type_c"": [0, 0, 0],     } )  # Create an action set action_set = ActionSet(X)  # `ActionSet` infers the type and bounds on each feature from `X`.",python,PROGLANG
"SLURM commands used for model training take this form: ```bash sbatch --job-name=frac_100u --nodes=1 --time=24:00:00 -p gpua100 --gres=gpu:1 --mem-per-cpu=32G --cpus-per-task=8     --output=frac_100u.out     --error=frac_100u.err     --wrap=""python model_training/finetune.py       --train_data_path data/fractial_code/fractial_code_100_train.jsonl      --output_dir models/fractial_code_100       --micro_batch_size 32       --num_epochs 2       --cutoff_len 512       --val_data_path data/fractial_code/fractial_code_1000_validation.jsonl       --test_data_path data/fractial_code/fractial_code_only_test.jsonl"" ``` and are partially available in the `data/fractial_mixes` folder.  ### Evaluation  Evaluation contains all code (called by scripts in `scripts/result_aggregation`) to compute prediction scores for all scorers.",bash,PROGLANG
"The paper is available at https://arxiv.org/abs/2303.17646  # Instructions for running Phase1-Cosearch  ## Running the Co-search Run the Code using  ``` python Phase1_VGG16_backbone.py with the variables described below. ``` ## Variable Description   ``` --lr: Learning Rate --hw_params: Tuple of #PEs per Tile, #Crossbars per PE, #Tiles, Crossbar Size --target_latency: Target Latency --target_area: Target Area Constraint (mm^2) --epochs: Total Number of Search Epochs --wt_prec: Weight Precision --cellbit: Number of bits per NVM device --area_tolerance: The uncerainity in the on-chip area tolerated in the searched model.  ``` # Instructions for XPertSim C++ Evaluation  ## Step 1: Create the Network_custom.csv file  The Network_custom.csv file contains the DNN information required for evaluation.",C++,PROGLANG
"## Installation  The code has been tested with Python 3.8, CUDA 11.3, pytorch 1.10.1 and pytorch-lighting 1.4.8.",Python 3.8,PROGLANG
raw=true)  ## Code The code has been written in Python using the Pytorch framework.,Python,PROGLANG
"Indexes for RDF data --------------------  This is the C++ library used for the experiments in the paper [*Compressed Indexes for Fast Search of Semantic Data*](http://pages.di.unipi.it/pibiri/papers/TKDE20.pdf) [1], by Raffaele Perego, Giulio Ermanno Pibiri and Rossano Venturini.",C++,PROGLANG
"Preparing the data for indexing <a name=""preparing""></a> -------------------------------  The folder `scripts` contains all the python3 scripts needed to prepare the datasets for indexing.",python3,PROGLANG
"```C++ struct iterator; void build(compact_vector::builder const& from,            compact_vector::builder const& pointers); inline uint64_t access(uint64_t pos) const; inline uint64_t access(range const& r, uint64_t pos); iterator begin() const; iterator end() const; iterator at(range const& r, uint64_t pos) const; uint64_t find(range const& r, uint64_t id); uint64_t size() const; size_t bytes() const; void save(std::ostream& os) const; void load(std::istream& is); ```   Authors <a name=""authors""></a> ------- * [Giulio Ermanno Pibiri](http://pages.di.unipi.it/pibiri/), <giulio.ermanno.pibiri@isti.cnr.it>   References <a name=""references""></a> ------- * [1] Raffaele Perego, Giulio Ermanno Pibiri and Rossano Venturini.",C++,PROGLANG
"```bash # Team-DAB-DETR and Team-DN-DETR # multi-gpu python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --resume /path/to/your/checkpoint \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --matcher team \   --q_splits 65 20 15 \   --eval  # single-gpu python main.py \   --coco_path /path/to/your/COCODIR \   --resume /path/to/your/checkpoint \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --matcher team \   --q_splits 65 20 15 \   --eval  # --------------------------------------------  # Team-DINO # You need to write config and .sh files in advance. # multi-gpu bash scripts/DINO_4scale_1stage_team_r50_e12_eval.sh /path/to/your/COCODIR /path/to/your/output/dir /path/to/your/checkpoint ```  ### Training  In default, we divide the queries into three groups, with the proportions of 65%, 20%, and 15%, corresponding to the relative scales of (0, 0.2], (0.2, 0.4], and (0.4, 1], respectively.",bash,PROGLANG
The code used to interface with SemanticScholar uses the unofficial [semanticscholar](https://github.com/danielnsilva/semanticscholar) python library.,python,PROGLANG
Install Anaconda or Miniconda distribution based on Python3+ from their [downloads' site][1]. 2.,Python3+,PROGLANG
Clone this repository and create an environment:  ```shell git clone https://www.github.com/yuleiniu/rva conda create -n visdial-ch python=3.6  # activate the environment and install all dependencies conda activate visdial-ch cd rva/ pip install -r requirements.txt  # install this codebase as a package in development version python setup.py develop ```   Download Data -------------  1.,shell,PROGLANG
Clone this repository and create an environment:  ```shell git clone https://www.github.com/yuleiniu/rva conda create -n visdial-ch python=3.6  # activate the environment and install all dependencies conda activate visdial-ch cd rva/ pip install -r requirements.txt  # install this codebase as a package in development version python setup.py develop ```   Download Data -------------  1.,python=3.6,PROGLANG
```shell python .,shell,PROGLANG
/data/extract_features_detectron.py --image-root /path/to/Flickr/VisualDialog_test2018 --save-path /path/to/feature --split test # Bottom-up features of 36 proposals from images of test split. ```  Initializing GloVe Word Embeddings -------------- Simply run  ```shell python data/init_glove.py ```   Training --------  Train the model provided in this repository as:  ```shell python train.py --config-yml configs/rva.yml --gpu-ids 0 # provide more ids for multi-GPU execution other args... ```  ### Saving model checkpoints  This script will save model checkpoints at every epoch as per path specified by `--save-dirpath`.,shell,PROGLANG
"Evaluation ----------  Evaluation of a trained model checkpoint can be done as follows:  ```shell python evaluate.py --config-yml /path/to/config.yml --load-pthpath /path/to/checkpoint.pth --split val --gpu-ids 0 ```  This will generate an EvalAI submission file, and report metrics from the [Visual Dialog paper][5] (Mean reciprocal rank, R@{1, 5, 10}, Mean rank), and Normalized Discounted Cumulative Gain (NDCG), introduced in the first Visual Dialog Challenge (in 2018).",shell,PROGLANG
"style=flat-square)](https://makeapullrequest.com)  :fire::fire: This is a collection of awesome articles about Implicit Neural Representation networks in medical imaging:fire::fire:  :loudspeaker: Our review paper published on arXiv: [Implicit Neural Representation in Medical Imaging: A Comparative Survey](https://arxiv.org/abs/2307.16142) :heart:  #### Citation  ```python @inproceedings{molaei2023implicit,   title={Implicit neural representation in medical imaging: A comparative survey},   author={Molaei, Amirali and Aminimehr, Amirhossein and Tavakoli, Armin and Kazerouni, Amirhossein and Azad, Bobby and Azad, Reza and Merhof, Dorit},   booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},   pages={2381--2391},   year={2023} } ```  # Introduction   Implicitly representing image signals has gained popularity in recent years for a broad range of medical imaging applications.",python,PROGLANG
"-- TABLE OF CONTENTS --> <ol>   <li>     <a href=""#about-the-project"">About The Project</a>   </li>   <li><a href=""#citing-abides"">Citing ABIDES</a></li>   <li>     <a href=""#getting-started"">Getting Started</a>     <ul>       <li><a href=""#installation"">Installation</a></li>     </ul>   </li>   <li>     <a href=""#usage-regular"">Usage (regular)</a>     <ul>       <li><a href=""#using-a-python-script"">Using a Python Script</a></li>       <li><a href=""#using-the-abides-command"">Using the `abides` Command</a></li>     </ul>   </li>   <li><a href=""#usage-gym"">Usage (Gym)</a></li>   <li><a href=""#default-available-markets-configurations"">Default Available Markets Configurations</a></li>   <li><a href=""#contributing"">Contributing</a></li>   <li><a href=""#license"">License</a></li>   <li><a href=""#acknowledgments"">Acknowledgments</a></li> </ol>  <!",python,PROGLANG
"-- TABLE OF CONTENTS --> <ol>   <li>     <a href=""#about-the-project"">About The Project</a>   </li>   <li><a href=""#citing-abides"">Citing ABIDES</a></li>   <li>     <a href=""#getting-started"">Getting Started</a>     <ul>       <li><a href=""#installation"">Installation</a></li>     </ul>   </li>   <li>     <a href=""#usage-regular"">Usage (regular)</a>     <ul>       <li><a href=""#using-a-python-script"">Using a Python Script</a></li>       <li><a href=""#using-the-abides-command"">Using the `abides` Command</a></li>     </ul>   </li>   <li><a href=""#usage-gym"">Usage (Gym)</a></li>   <li><a href=""#default-available-markets-configurations"">Default Available Markets Configurations</a></li>   <li><a href=""#contributing"">Contributing</a></li>   <li><a href=""#license"">License</a></li>   <li><a href=""#acknowledgments"">Acknowledgments</a></li> </ol>  <!",Python,PROGLANG
"Download the ABIDES source code, either directly from GitHub or with git:      ```bash     git clone https://github.com/jpmorganchase/abides-jpmc-public     ```      **Note:** The latest stable version is contained within the `main` branch.  2.",bash,PROGLANG
"-- USAGE EXAMPLES --> ## Usage (regular) Regular ABIDES simulations can be run either directly in python or through the command line  _For more examples, please refer to the [Documentation](https://example.com)_  ### Using a Python Script:  ```python from abides_markets.configs import rmsc04 from abides_core import abides  config_state = rmsc04.build_config(seed = 0, end_time = '10:00:00') end_state = abides.run(config_state) ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ### Using the abides Command:  The config can be loaded and the simulation run using the `abides` command in the terminal (from directory root):  ``` $ abides abides-markets/abides_markets/configs/rmsc04.py --end_time ""10:00:00"" ```  The first argument is a path to a valid ABIDES configuration file.",python,PROGLANG
"-- USAGE EXAMPLES --> ## Usage (regular) Regular ABIDES simulations can be run either directly in python or through the command line  _For more examples, please refer to the [Documentation](https://example.com)_  ### Using a Python Script:  ```python from abides_markets.configs import rmsc04 from abides_core import abides  config_state = rmsc04.build_config(seed = 0, end_time = '10:00:00') end_state = abides.run(config_state) ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ### Using the abides Command:  The config can be loaded and the simulation run using the `abides` command in the terminal (from directory root):  ``` $ abides abides-markets/abides_markets/configs/rmsc04.py --end_time ""10:00:00"" ```  The first argument is a path to a valid ABIDES configuration file.",Python,PROGLANG
"-- USAGE EXAMPLES --> ## Usage (regular) Regular ABIDES simulations can be run either directly in python or through the command line  _For more examples, please refer to the [Documentation](https://example.com)_  ### Using a Python Script:  ```python from abides_markets.configs import rmsc04 from abides_core import abides  config_state = rmsc04.build_config(seed = 0, end_time = '10:00:00') end_state = abides.run(config_state) ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ### Using the abides Command:  The config can be loaded and the simulation run using the `abides` command in the terminal (from directory root):  ``` $ abides abides-markets/abides_markets/configs/rmsc04.py --end_time ""10:00:00"" ```  The first argument is a path to a valid ABIDES configuration file.",python,PROGLANG
"```python import gym import abides_gym  env = gym.make(     ""markets-daily_investor-v0"",     background_config=""rmsc04"", )  env.seed(0) initial_state = env.reset() for i in range(5):     state, reward, done, info = env.step(0) ```  ## Default Available Markets Configurations  ABIDES currently has the following available background Market Simulation Configuration:  * RMSC03: 1 Exchange Agent, 1 POV Market Maker Agent, 100 Value Agents, 25 Momentum Agents, 5000 Noise Agents   * RMSC04: 1 Exchange Agent, 2 Market Maker Agents, 102 Value Agents, 12 Momentum Agents, 1000  Noise Agents  <p align=""right"">(<a href=""#top"">back to top</a>)</p>  <!",python,PROGLANG
"The models can be loaded with the following snippet after putting [ciwfiwgan](ciwfiwgan) on your path (all following code also assumes the module is on the path):  ```python import torch from infowavegan import WaveGANGenerator  device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")  G = WaveGANGenerator(slice_len=2**16) G.load_state_dict(torch.load(""model.pt"", map_location=device)) G.to(device) ```  ### Compute resources used for model training   The model was trained across approximately 3 days on a single Nvidia 1080Ti (11 GB GPU memory) on a cluster-based instance running on Intel Xeon E5-2623.     ## Analysis   ### The click detector  The [click detector module](https://github.com/andleb/Approaching-an-unknown-communication-system/blob/main/clickDetector.py) outputs the *inter-click intervals* (ICIs) and the overall number of clicks in the provided generated raw audio sample.",python,PROGLANG
"Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data._  ### Requirement  __Library__ ``` pip install -r requirements.txt  * pytorch==1.1.0 or 1.2.0   * tqdm   * opencv-python   * scipy   * sklearn * matplotlib   * pillow   * tensorboardX  ```  __Dataset__   * This repo. utilizes the variant of ""ImageFolder"". * Download : [AFHQ (StarGANv2)](https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?",python,PROGLANG
- Based on Perez's SNN implementation: [SDNN_python](https://github.com/npvoid/SDNN_python)  - **Major Update: Image Reconstruction**   - `reconstruct/`: Contains (I hope easy to follow) notebooks for image reconstruction experiments,python,PROGLANG
[Pytorch 1.10](https://img.shields.io/badge/pytorch-1.2.0-DodgerBlue.svg?,Pytorch 1.10,PROGLANG
"```python Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss 0,82.77777777777777,99.9888888888889,0.9145596397187975,0.0007119161817762587 Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss 1,82.97777777777777,47.13333333333333,0.9546798907385932,4.189897534688313 Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss 2,82.46666666666667,5.766666666666667,1.034722186088562,15.361101960923937 Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss 3,82.15555555555555,1.5222222222222221,1.0855470676422119,22.175255742390952 Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss 4,82.0111111111111,0.7111111111111111,1.1183592330084906,26.754894670274524 Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss 5,81.86666666666666,0.4777777777777778,1.1441074348025853,30.429284422132703 ```  The unlearned model will be saved to `'weight/ABL_results/<model_name>.tar'`  Please read `quick_unlearning_demo.py` to adjust the default parameters for your experiment.  ---  ## More defense results on BadNets model trained with Data Augmentation   ```python [Logs for our ABL against Badnet Attacks]  ----------- Model Fine-tuning -------------- epoch: 40  lr: 0.0100 Epoch[41]:[200/774] loss:0.1456(0.1240)  prec@1:98.44(95.84)  prec@5:98.44(99.96) Epoch[41]:[400/774] loss:0.0553(0.1080)  prec@1:98.44(96.38)  prec@5:100.00(99.97) Epoch[41]:[600/774] loss:0.0693(0.1015)  prec@1:96.88(96.63)  prec@5:100.00(99.97) [Clean] Prec@1: 92.23, Loss: 0.2408 [Bad] Prec@1: 100.00, Loss: 0.0001 epoch: 41  lr: 0.0100 Epoch[42]:[200/774] loss:0.0532(0.0653)  prec@1:98.44(97.89)  prec@5:100.00(100.00) Epoch[42]:[400/774] loss:0.0534(0.0659)  prec@1:98.44(97.76)  prec@5:100.00(100.00) Epoch[42]:[600/774] loss:0.0514(0.0659)  prec@1:96.88(97.76)  prec@5:100.00(99.99) [Clean] Prec@1: 92.60, Loss: 0.2390 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 42  lr: 0.0100 Epoch[43]:[200/774] loss:0.0054(0.0499)  prec@1:100.00(98.33)  prec@5:100.00(99.99) Epoch[43]:[400/774] loss:0.0429(0.0525)  prec@1:98.44(98.21)  prec@5:100.00(99.99) Epoch[43]:[600/774] loss:0.0448(0.0537)  prec@1:98.44(98.19)  prec@5:100.00(99.99) [Clean] Prec@1: 92.52, Loss: 0.2409 [Bad] Prec@1: 100.00, Loss: 0.0001 epoch: 43  lr: 0.0100 Epoch[44]:[200/774] loss:0.0253(0.0472)  prec@1:98.44(98.41)  prec@5:100.00(99.99) Epoch[44]:[400/774] loss:0.0104(0.0463)  prec@1:100.00(98.43)  prec@5:100.00(99.99) Epoch[44]:[600/774] loss:0.0200(0.0452)  prec@1:100.00(98.46)  prec@5:100.00(99.99) [Clean] Prec@1: 92.60, Loss: 0.2459 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 44  lr: 0.0100 Epoch[45]:[200/774] loss:0.0510(0.0385)  prec@1:98.44(98.79)  prec@5:100.00(99.99) Epoch[45]:[400/774] loss:0.0244(0.0381)  prec@1:98.44(98.82)  prec@5:100.00(100.00) Epoch[45]:[600/774] loss:0.0203(0.0391)  prec@1:100.00(98.83)  prec@5:100.00(99.99) [Clean] Prec@1: 92.81, Loss: 0.2484 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 45  lr: 0.0100 Epoch[46]:[200/774] loss:0.0110(0.0374)  prec@1:100.00(98.75)  prec@5:100.00(99.99) Epoch[46]:[400/774] loss:0.0204(0.0371)  prec@1:98.44(98.79)  prec@5:100.00(99.99) Epoch[46]:[600/774] loss:0.0183(0.0369)  prec@1:100.00(98.76)  prec@5:100.00(99.99) [Clean] Prec@1: 92.99, Loss: 0.2495 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 46  lr: 0.0100 Epoch[47]:[200/774] loss:0.0452(0.0315)  prec@1:98.44(98.97)  prec@5:100.00(100.00) Epoch[47]:[400/774] loss:0.0315(0.0310)  prec@1:98.44(98.98)  prec@5:100.00(100.00) Epoch[47]:[600/774] loss:0.0298(0.0303)  prec@1:100.00(99.01)  prec@5:100.00(100.00) [Clean] Prec@1: 92.82, Loss: 0.2563 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 47  lr: 0.0100 Epoch[48]:[200/774] loss:0.0397(0.0269)  prec@1:98.44(99.12)  prec@5:100.00(100.00) Epoch[48]:[400/774] loss:0.0617(0.0262)  prec@1:98.44(99.16)  prec@5:100.00(100.00) Epoch[48]:[600/774] loss:0.0630(0.0270)  prec@1:98.44(99.16)  prec@5:100.00(100.00) [Clean] Prec@1: 92.81, Loss: 0.2678 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 48  lr: 0.0100 Epoch[49]:[200/774] loss:0.0251(0.0267)  prec@1:100.00(99.15)  prec@5:100.00(100.00) Epoch[49]:[400/774] loss:0.0298(0.0262)  prec@1:98.44(99.14)  prec@5:100.00(100.00) Epoch[49]:[600/774] loss:0.0384(0.0258)  prec@1:98.44(99.15)  prec@5:100.00(100.00) [Clean] Prec@1: 93.09, Loss: 0.2586 [Bad] Prec@1: 100.00, Loss: 0.0002 epoch: 49  lr: 0.0100 Epoch[50]:[200/774] loss:0.0359(0.0203)  prec@1:98.44(99.30)  prec@5:100.00(100.00) Epoch[50]:[400/774] loss:0.0062(0.0214)  prec@1:100.00(99.27)  prec@5:100.00(100.00) Epoch[50]:[600/774] loss:0.0418(0.0222)  prec@1:98.44(99.25)  prec@5:100.00(100.00) [Clean] Prec@1: 93.03, Loss: 0.2626 [Bad] Prec@1: 100.00, Loss: 0.0001 epoch: 50  lr: 0.0100 Epoch[51]:[200/774] loss:0.0040(0.0222)  prec@1:100.00(99.27)  prec@5:100.00(100.00) Epoch[51]:[400/774] loss:0.0269(0.0236)  prec@1:98.44(99.21)  prec@5:100.00(100.00) Epoch[51]:[600/774] loss:0.0219(0.0234)  prec@1:100.00(99.23)  prec@5:100.00(100.00) [Clean] Prec@1: 93.19, Loss: 0.2604 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 51  lr: 0.0100 Epoch[52]:[200/774] loss:0.0154(0.0201)  prec@1:98.44(99.34)  prec@5:100.00(100.00) Epoch[52]:[400/774] loss:0.0328(0.0200)  prec@1:98.44(99.38)  prec@5:100.00(100.00) Epoch[52]:[600/774] loss:0.0220(0.0204)  prec@1:98.44(99.36)  prec@5:100.00(100.00) [Clean] Prec@1: 93.27, Loss: 0.2652 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 52  lr: 0.0100 Epoch[53]:[200/774] loss:0.0090(0.0194)  prec@1:100.00(99.39)  prec@5:100.00(100.00) Epoch[53]:[400/774] loss:0.0019(0.0195)  prec@1:100.00(99.41)  prec@5:100.00(100.00) Epoch[53]:[600/774] loss:0.0402(0.0190)  prec@1:98.44(99.45)  prec@5:100.00(100.00) [Clean] Prec@1: 93.04, Loss: 0.2735 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 53  lr: 0.0100 Epoch[54]:[200/774] loss:0.0154(0.0186)  prec@1:100.00(99.38)  prec@5:100.00(100.00) Epoch[54]:[400/774] loss:0.0124(0.0182)  prec@1:100.00(99.40)  prec@5:100.00(100.00) Epoch[54]:[600/774] loss:0.0144(0.0181)  prec@1:100.00(99.45)  prec@5:100.00(100.00) [Clean] Prec@1: 93.17, Loss: 0.2693 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 54  lr: 0.0100 Epoch[55]:[200/774] loss:0.0119(0.0168)  prec@1:100.00(99.43)  prec@5:100.00(100.00) Epoch[55]:[400/774] loss:0.0228(0.0170)  prec@1:98.44(99.42)  prec@5:100.00(100.00) Epoch[55]:[600/774] loss:0.0096(0.0164)  prec@1:100.00(99.47)  prec@5:100.00(100.00) [Clean] Prec@1: 92.84, Loss: 0.2786 [Bad] Prec@1: 100.00, Loss: 0.0001 epoch: 55  lr: 0.0100 Epoch[56]:[200/774] loss:0.0307(0.0146)  prec@1:98.44(99.51)  prec@5:100.00(100.00) Epoch[56]:[400/774] loss:0.0065(0.0149)  prec@1:100.00(99.52)  prec@5:100.00(100.00) Epoch[56]:[600/774] loss:0.0348(0.0155)  prec@1:98.44(99.50)  prec@5:100.00(100.00) [Clean] Prec@1: 93.12, Loss: 0.2794 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 56  lr: 0.0100 Epoch[57]:[200/774] loss:0.0014(0.0134)  prec@1:100.00(99.59)  prec@5:100.00(100.00) Epoch[57]:[400/774] loss:0.0060(0.0133)  prec@1:100.00(99.59)  prec@5:100.00(100.00) Epoch[57]:[600/774] loss:0.0400(0.0133)  prec@1:95.31(99.61)  prec@5:100.00(100.00) [Clean] Prec@1: 93.13, Loss: 0.2819 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 57  lr: 0.0100 Epoch[58]:[200/774] loss:0.0062(0.0122)  prec@1:100.00(99.60)  prec@5:100.00(100.00) Epoch[58]:[400/774] loss:0.0065(0.0134)  prec@1:100.00(99.56)  prec@5:100.00(100.00) Epoch[58]:[600/774] loss:0.0198(0.0134)  prec@1:100.00(99.59)  prec@5:100.00(100.00) [Clean] Prec@1: 93.11, Loss: 0.2795 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 58  lr: 0.0100 Epoch[59]:[200/774] loss:0.0053(0.0094)  prec@1:100.00(99.73)  prec@5:100.00(100.00) Epoch[59]:[400/774] loss:0.0064(0.0105)  prec@1:100.00(99.70)  prec@5:100.00(100.00) Epoch[59]:[600/774] loss:0.0068(0.0112)  prec@1:100.00(99.67)  prec@5:100.00(100.00) [Clean] Prec@1: 93.04, Loss: 0.2900 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 59  lr: 0.0100 Epoch[60]:[200/774] loss:0.0039(0.0147)  prec@1:100.00(99.55)  prec@5:100.00(99.99) Epoch[60]:[400/774] loss:0.0399(0.0142)  prec@1:96.88(99.58)  prec@5:100.00(100.00) Epoch[60]:[600/774] loss:0.0030(0.0134)  prec@1:100.00(99.59)  prec@5:100.00(100.00) [Clean] Prec@1: 93.24, Loss: 0.2905 [Bad] Prec@1: 100.00, Loss: 0.0000  ----------- Model unlearning -------------- epoch: 0  lr: 0.0005 [Clean] Prec@1: 93.24, Loss: 0.2905 [Bad] Prec@1: 100.00, Loss: 0.0000 testing the ascended model......",python,PROGLANG
"Please read `backdoor_unlearning.py` and `config.py` and adjust the parameters for your experiment.  ----  ## Links to External Repos  #### Attacks  **CL:** Clean-label backdoor attacks  - [Paper](https://people.csail.mit.edu/madry/lab/cleanlabel.pdf) - [pytorch implementation](https://github.com/hkunzhe/label_consistent_attacks_pytorch)  **SIG:** A New Backdoor Attack in CNNS by Training Set Corruption Without Label Poisoning  - [Paper](https://ieeexplore.ieee.org/document/8802997/footnotes)  ```python ## reference code def plant_sin_trigger(img, delta=20, f=6, debug=False):     """"""     Implement paper:     > Barni, M., Kallas, K., & Tondi, B. (2019)",python,PROGLANG
"# Introduction This is PointSQL, the source codes of [Natural Language to Structured Query Generation via Meta-Learning](https://arxiv.org/abs/1803.02400)  and [Pointing Out SQL Queries From Text](https://www.microsoft.com/en-us/research/publication/pointing-sql-queries-text) from Microsoft Research.",SQL,PROGLANG
"# Introduction This is PointSQL, the source codes of [Natural Language to Structured Query Generation via Meta-Learning](https://arxiv.org/abs/1803.02400)  and [Pointing Out SQL Queries From Text](https://www.microsoft.com/en-us/research/publication/pointing-sql-queries-text) from Microsoft Research.",sql,PROGLANG
"+ Meta + Sum loss: `model_zoo/meta_sum`   + Base Sum loss: `model_zoo/base_sum`   # Requirements  - Tensorflow 1.4 - python 3.6 - [Stanza](https://github.com/stanfordnlp/stanza)   # Citation  If you use the code in your paper, then please cite it as:    ``` @inproceedings{pshuang2018PT-MAML,   author    = {Po{-}Sen Huang and                Chenglong Wang and                Rishabh Singh and                Wen-tau Yih and                Xiaodong He},   title     = {Natural Language to Structured Query Generation via Meta-Learning},   booktitle = {NAACL},   year      = {2018}, } ```   ``` @inproceedings{2018executionguided,   author    = {Chenglong Wang and                Po{-}Sen Huang and                Alex Polozov and                Marc Brockschmidt and                 Rishabh Singh},   title = ""{Execution-Guided Neural Program Decoding}"",   booktitle = {ICML workshop on Neural Abstract Machines & Program Induction v2 (NAMPI)},   year = {2018} } ```   and   ``` @techreport{chenglong,   author = {Wang, Chenglong and Brockschmidt, Marc and Singh, Rishabh},   title = {Pointing Out {SQL} Queries From Text},   number = {MSR-TR-2017-45},   year = {2017},   month = {November},   url = {https://www.microsoft.com/en-us/research/publication/pointing-sql-queries-text/}, } ```    # Contributing  This project welcomes contributions and suggestions.",python 3.6,PROGLANG
"+ Meta + Sum loss: `model_zoo/meta_sum`   + Base Sum loss: `model_zoo/base_sum`   # Requirements  - Tensorflow 1.4 - python 3.6 - [Stanza](https://github.com/stanfordnlp/stanza)   # Citation  If you use the code in your paper, then please cite it as:    ``` @inproceedings{pshuang2018PT-MAML,   author    = {Po{-}Sen Huang and                Chenglong Wang and                Rishabh Singh and                Wen-tau Yih and                Xiaodong He},   title     = {Natural Language to Structured Query Generation via Meta-Learning},   booktitle = {NAACL},   year      = {2018}, } ```   ``` @inproceedings{2018executionguided,   author    = {Chenglong Wang and                Po{-}Sen Huang and                Alex Polozov and                Marc Brockschmidt and                 Rishabh Singh},   title = ""{Execution-Guided Neural Program Decoding}"",   booktitle = {ICML workshop on Neural Abstract Machines & Program Induction v2 (NAMPI)},   year = {2018} } ```   and   ``` @techreport{chenglong,   author = {Wang, Chenglong and Brockschmidt, Marc and Singh, Rishabh},   title = {Pointing Out {SQL} Queries From Text},   number = {MSR-TR-2017-45},   year = {2017},   month = {November},   url = {https://www.microsoft.com/en-us/research/publication/pointing-sql-queries-text/}, } ```    # Contributing  This project welcomes contributions and suggestions.",SQL,PROGLANG
"+ Meta + Sum loss: `model_zoo/meta_sum`   + Base Sum loss: `model_zoo/base_sum`   # Requirements  - Tensorflow 1.4 - python 3.6 - [Stanza](https://github.com/stanfordnlp/stanza)   # Citation  If you use the code in your paper, then please cite it as:    ``` @inproceedings{pshuang2018PT-MAML,   author    = {Po{-}Sen Huang and                Chenglong Wang and                Rishabh Singh and                Wen-tau Yih and                Xiaodong He},   title     = {Natural Language to Structured Query Generation via Meta-Learning},   booktitle = {NAACL},   year      = {2018}, } ```   ``` @inproceedings{2018executionguided,   author    = {Chenglong Wang and                Po{-}Sen Huang and                Alex Polozov and                Marc Brockschmidt and                 Rishabh Singh},   title = ""{Execution-Guided Neural Program Decoding}"",   booktitle = {ICML workshop on Neural Abstract Machines & Program Induction v2 (NAMPI)},   year = {2018} } ```   and   ``` @techreport{chenglong,   author = {Wang, Chenglong and Brockschmidt, Marc and Singh, Rishabh},   title = {Pointing Out {SQL} Queries From Text},   number = {MSR-TR-2017-45},   year = {2017},   month = {November},   url = {https://www.microsoft.com/en-us/research/publication/pointing-sql-queries-text/}, } ```    # Contributing  This project welcomes contributions and suggestions.",sql,PROGLANG
"Springer, 2020, pp. 48–56. doi: 10.1007/978-3-030- 57855-8_4. url: https://doi.org/10.1007/978-3-030-57855-8_4.  ``` @inproceedings{Monnin2020,   author    = {Pierre Monnin and                Miguel Couceiro and                Amedeo Napoli and                Adrien Coulet},   editor    = {Mehwish Alam and                Tanya Braun and                Bruno Yun},   title     = {Knowledge-Based Matching of n-ary Tuples},   booktitle = {Ontologies and Concepts in Mind and Machine - 25th International Conference                on Conceptual Structures, {ICCS} 2020, Bolzano, Italy, September 18-20,                2020, Proceedings},   series    = {Lecture Notes in Computer Science},   volume    = {12277},   pages     = {48--56},   publisher = {Springer},   year      = {2020},   url       = {https://doi.org/10.1007/978-3-030-57855-8_4},   doi       = {10.1007/978-3-030-57855-8_4}, } ```  ## Execution  ### ``batch`` mode  Executes reconciliation rules on every pair of relationships in the triplestore.  #### Execution (without Docker)  ```bash tcn3r --configuration conf.json -o output.ttl --simlimit SL --complimit CL --dimensionlimit DL --max-rows MR -t threads ```  where:  * *conf.json*: is the configuration file needed to configure the scripts -- see below * *output.ttl*: is the path to the output TTL file where the generated links between relationships will be stored * *SL*: Minimum similarity on non-empty aggregated dimensions to consider relations as related (< 0 to disable) * *CL*: Minimum number of non-empty comparable aggregated dimensions to consider relations as related (< 0 to disable) * *DL*: Minimum number of non-empty aggregated dimensions to apply simlimit or complimit (< 0 to disable) * *MR*: Max number of rows the SPARQL endpoint can return for a query * *threads*: number of threads to use when comparing relations (e.g., 8)  #### Execution (in Docker)  You can use the target ``run`` of the provided Makefile that calls the Docker image with:  ```bash docker run --rm $(MAPUSER) -v ${PWD}/data:/data $(INAME):$(VERSION) --configuration /data/conf.json.example -o /data/output.ttl --simlimit 0.8 --complimit 2 --dimensionlimit 2 --max-rows 10000 --explain false -t 4 ```  The ``data`` subdirectory of the current directory is shared with the Docker container as ``/data``.",bash,PROGLANG
"Springer, 2020, pp. 48–56. doi: 10.1007/978-3-030- 57855-8_4. url: https://doi.org/10.1007/978-3-030-57855-8_4.  ``` @inproceedings{Monnin2020,   author    = {Pierre Monnin and                Miguel Couceiro and                Amedeo Napoli and                Adrien Coulet},   editor    = {Mehwish Alam and                Tanya Braun and                Bruno Yun},   title     = {Knowledge-Based Matching of n-ary Tuples},   booktitle = {Ontologies and Concepts in Mind and Machine - 25th International Conference                on Conceptual Structures, {ICCS} 2020, Bolzano, Italy, September 18-20,                2020, Proceedings},   series    = {Lecture Notes in Computer Science},   volume    = {12277},   pages     = {48--56},   publisher = {Springer},   year      = {2020},   url       = {https://doi.org/10.1007/978-3-030-57855-8_4},   doi       = {10.1007/978-3-030-57855-8_4}, } ```  ## Execution  ### ``batch`` mode  Executes reconciliation rules on every pair of relationships in the triplestore.  #### Execution (without Docker)  ```bash tcn3r --configuration conf.json -o output.ttl --simlimit SL --complimit CL --dimensionlimit DL --max-rows MR -t threads ```  where:  * *conf.json*: is the configuration file needed to configure the scripts -- see below * *output.ttl*: is the path to the output TTL file where the generated links between relationships will be stored * *SL*: Minimum similarity on non-empty aggregated dimensions to consider relations as related (< 0 to disable) * *CL*: Minimum number of non-empty comparable aggregated dimensions to consider relations as related (< 0 to disable) * *DL*: Minimum number of non-empty aggregated dimensions to apply simlimit or complimit (< 0 to disable) * *MR*: Max number of rows the SPARQL endpoint can return for a query * *threads*: number of threads to use when comparing relations (e.g., 8)  #### Execution (in Docker)  You can use the target ``run`` of the provided Makefile that calls the Docker image with:  ```bash docker run --rm $(MAPUSER) -v ${PWD}/data:/data $(INAME):$(VERSION) --configuration /data/conf.json.example -o /data/output.ttl --simlimit 0.8 --complimit 2 --dimensionlimit 2 --max-rows 10000 --explain false -t 4 ```  The ``data`` subdirectory of the current directory is shared with the Docker container as ``/data``.",SPARQL,PROGLANG
"Springer, 2020, pp. 48–56. doi: 10.1007/978-3-030- 57855-8_4. url: https://doi.org/10.1007/978-3-030-57855-8_4.  ``` @inproceedings{Monnin2020,   author    = {Pierre Monnin and                Miguel Couceiro and                Amedeo Napoli and                Adrien Coulet},   editor    = {Mehwish Alam and                Tanya Braun and                Bruno Yun},   title     = {Knowledge-Based Matching of n-ary Tuples},   booktitle = {Ontologies and Concepts in Mind and Machine - 25th International Conference                on Conceptual Structures, {ICCS} 2020, Bolzano, Italy, September 18-20,                2020, Proceedings},   series    = {Lecture Notes in Computer Science},   volume    = {12277},   pages     = {48--56},   publisher = {Springer},   year      = {2020},   url       = {https://doi.org/10.1007/978-3-030-57855-8_4},   doi       = {10.1007/978-3-030-57855-8_4}, } ```  ## Execution  ### ``batch`` mode  Executes reconciliation rules on every pair of relationships in the triplestore.  #### Execution (without Docker)  ```bash tcn3r --configuration conf.json -o output.ttl --simlimit SL --complimit CL --dimensionlimit DL --max-rows MR -t threads ```  where:  * *conf.json*: is the configuration file needed to configure the scripts -- see below * *output.ttl*: is the path to the output TTL file where the generated links between relationships will be stored * *SL*: Minimum similarity on non-empty aggregated dimensions to consider relations as related (< 0 to disable) * *CL*: Minimum number of non-empty comparable aggregated dimensions to consider relations as related (< 0 to disable) * *DL*: Minimum number of non-empty aggregated dimensions to apply simlimit or complimit (< 0 to disable) * *MR*: Max number of rows the SPARQL endpoint can return for a query * *threads*: number of threads to use when comparing relations (e.g., 8)  #### Execution (in Docker)  You can use the target ``run`` of the provided Makefile that calls the Docker image with:  ```bash docker run --rm $(MAPUSER) -v ${PWD}/data:/data $(INAME):$(VERSION) --configuration /data/conf.json.example -o /data/output.ttl --simlimit 0.8 --complimit 2 --dimensionlimit 2 --max-rows 10000 --explain false -t 4 ```  The ``data`` subdirectory of the current directory is shared with the Docker container as ``/data``.",bash,PROGLANG
"``simlimit`` is set to 0.8, ``complimit`` to 2 and ``dimensionlimit`` is set to 2. 4 threads will be used.  ### ``explain`` mode  #### Execution (without Docker)  ```bash tcn3r --configuration conf.json -o output.ttl --simlimit SL --complimit CL --dimensionlimit DL --max-rows MR --explain true ```  where:  * *conf.json*: is the configuration file needed to configure the scripts -- see below * *output.txt*: is the path to the output text file where explanations of links between relationships will be stored * *SL*: Minimum similarity on non-empty aggregated dimensions to consider relations as related (< 0 to disable) * *CL*: Minimum number of non-empty comparable aggregated dimensions to consider relations as related (< 0 to disable) * *DL*: Minimum number of non-empty aggregated dimensions to apply simlimit or complimit (< 0 to disable) * *MR*: Max number of rows the SPARQL endpoint can return for a query  URIs of relations to compare will be asked interactively.  #### Execution (in Docker)  Not available.  ## Input  ### Configuration JSON file  A configuration JSON file is needed to configure the scripts.",SPARQL,PROGLANG
"It should contains:  ```json {     ""server-address"": ""http://pgxlod.loria.fr/sparql"",     ""url-json-conf-attribute"": ""format"",     ""url-json-conf-value"": ""application/sparql-results+json"",     ""url-default-graph-attribute"": ""default-graph-uri"",     ""url-default-graph-value"": ""http://pgxlod.loria.fr/"",     ""url-query-attribute"": ""query"",     ""timeout"": 10000000,     ""relation-types"": [         ""http://pgxo.loria.fr/PharmacogenomicRelationship""     ],     ""dimensions"": {         ""GeneticFactor"": {             ""ind-types"": [                 ""http://pgxo.loria.fr/GeneticFactor""             ],             ""rel2ind-predicates"": [                 ""http://pgxo.loria.fr/isAssociatedWith"",                 ""http://pgxo.loria.fr/isNotAssociatedWith""             ],             ""ind2dep-predicates"": [             ],             ""preorder"": ""Individuals"",             ""ind-leq-predicates"": [                 ""http://purl.obolibrary.org/obo/BFO_0000050""             ],             ""ind-geq-predicates"": [                 ""http://purl.obolibrary.org/obo/BFO_0000051""             ]         },         ""Drug"": {             ""ind-types"": [                 ""http://pgxo.loria.fr/Drug""             ],             ""rel2ind-predicates"": [                 ""http://pgxo.loria.fr/isAssociatedWith"",                 ""http://pgxo.loria.fr/isNotAssociatedWith""             ],             ""ind2dep-predicates"": [             ],             ""preorder"": ""Annotations"",             ""ann-base-uris"": [                 ""http://purl.obolibrary.org/obo/CHEBI_"",                 ""http://bio2rdf.org/chebi:"",                 ""http://identifiers.org/chebi/"",                 ""http://purl.bioontology.org/ontology/UATC/"",                 ""http://bio2rdf.org/atc:"",                 ""http://identifiers.org/atc/""             ],             ""ind2ann-predicates"": [                 ""http://www.w3.org/1999/02/22-rdf-syntax-ns#type""             ],             ""ann-leq-predicates"": [                 ""http://www.w3.org/2000/01/rdf-schema#subClassOf""             ],             ""ann-geq-predicates"": [             ]         },         ""Phenotype"": {             ""ind-types"": [                 ""http://pgxo.loria.fr/Phenotype""             ],             ""rel2ind-predicates"": [                 ""http://pgxo.loria.fr/isAssociatedWith"",                 ""http://pgxo.loria.fr/isNotAssociatedWith""             ],             ""ind2dep-predicates"": [                 ""http://purl.obolibrary.org/obo/RO_0002502""             ],             ""preorder"": ""Annotations"",             ""ann-base-uris"": [                 ""http://purl.bioontology.org/ontology/MESH/"",                 ""http://bio2rdf.org/mesh:"",                 ""http://identifiers.org/mesh/""             ],             ""ind2ann-predicates"": [                 ""http://www.w3.org/1999/02/22-rdf-syntax-ns#type""             ],             ""ann-leq-predicates"": [                 ""http://www.w3.org/2000/01/rdf-schema#subClassOf""             ],             ""ann-geq-predicates"": [             ]         }     },     ""output-pred-equal"": ""http://www.w3.org/2002/07/owl#sameAs"",     ""output-pred-equiv"": ""http://www.w3.org/2004/02/skos/core#closeMatch"",     ""output-pred-leq"": ""http://www.w3.org/2004/02/skos/core#broadMatch"",     ""output-pred-geq"": ""http://www.w3.org/2004/02/skos/core#narrowMatch"",     ""output-pred-comparable"": ""http://www.w3.org/2004/02/skos/core#relatedMatch"",     ""output-pred-dependency-related"": ""http://www.w3.org/2004/02/skos/core#related"" } ```  with:  * _server-address_: address of the SPARQL endpoint to query * _url-json-conf-attribute_: URL attribute to use to get JSON results * _url-json-conf-value_: value of the _url-json-conf-attribute_ to get JSON results * _url-default-graph-attribute_: URL attribute to use to define the default graph * _url-default-graph-value_: value of _url-default-graph-attribute_ to define the default graph * _url-query-attribute_: URL attribute to use to define the query * _timeout_: timeout value for HTTP requests * _relation-types_: URIs of classes whose instances are relationships to reconcile * _dimensions_: dictionary of dimensions.",SPARQL,PROGLANG
Please [disable TF2](https://www.tensorflow.org/api_docs/python/tf/compat/v1/disable_v2_behavior) behaviors by adding the line `tf.compat.v1.disable_v2_behavior()` after the line of importing TF in all the files,python,PROGLANG
[The Java Runtime Environment (JRE) and the Java Development Kit (JDK)](https://www.java.com/en/download/):   ```  apt-get install default-jre default-jdk  ```  3.,Java,PROGLANG
Check out the source code from the Git repository:  ```bash  git clone https://github.com/arminmoin/ML-Quadrat/  ```    2.,bash,PROGLANG
"Install ML2 using the Apache Maven:  ```bash  cd ML-Quadrat  mvn clean install -DskipTests  cd ML2/language  mvn clean install -DskipTests  cd ../..  ```    Note that the -DskipTests option lets us skip running the tests, thus saving more time.",bash,PROGLANG
"If you want to see a more detailed output to debug, use the option -X:    ```bash  mvn clean install -DskipTests -X  ```    Moreover, if you want to use Maven in the offline mode, e.g., in the case that your machine is behind a firewall that prohibits the Internet access, you should use the option -o, but, remember that you would need to first copy the .m2 directory, which includes the Maven cache from another computer behind the firewall (on which you have already cached ´the required dependencies) to this computer.",bash,PROGLANG
```bash  mvn clean install -DskipTests -o  ```    ### How to find a sample model instance?,bash,PROGLANG
"There exist a number of sample model instances with the .thingml extension at this location: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml    Let's choose [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) for this quick tutorial, and generate, e.g., the Python and Java source code out of it using the Python_Java model-to-code transformation (a.k.a. code generator or ""compiler"").",Python,PROGLANG
"There exist a number of sample model instances with the .thingml extension at this location: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml    Let's choose [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) for this quick tutorial, and generate, e.g., the Python and Java source code out of it using the Python_Java model-to-code transformation (a.k.a. code generator or ""compiler"").",Java,PROGLANG
"There exist a number of sample model instances with the .thingml extension at this location: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml    Let's choose [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) for this quick tutorial, and generate, e.g., the Python and Java source code out of it using the Python_Java model-to-code transformation (a.k.a. code generator or ""compiler"").",Python,PROGLANG
"There exist a number of sample model instances with the .thingml extension at this location: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml    Let's choose [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) for this quick tutorial, and generate, e.g., the Python and Java source code out of it using the Python_Java model-to-code transformation (a.k.a. code generator or ""compiler"").",Java,PROGLANG
Run the following commands in the Linux terminal:  ```bash  cd ML2/compilers/registry/target  java -jar mlquadrat.compilers.registry-2.0.0-SNAPSHOT-jar-with-dependencies.jar -c auto -s ../../..,bash,PROGLANG
"For instance, in this case, the generated code is in Java and Python.",Java,PROGLANG
"For instance, in this case, the generated code is in Java and Python.",Python,PROGLANG
The Java and the Python parts are already seamlessly integrated.,Java,PROGLANG
The Java and the Python parts are already seamlessly integrated.,Python,PROGLANG
"Hence, running the generated IoT service will be also very easy with just one command (java -jar ...).",java,PROGLANG
"Install the latest version of [Anaconda for Python 3.x](https://docs.anaconda.com/anaconda/install/linux/), e.g., as follows (use ```sudo su``` to run the chmod command as root and then ```exit``` in order to get back to the non-root user):    ```bash  wget https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh  sudo su  chmod u+x .",Python 3.x,PROGLANG
"Install the latest version of [Anaconda for Python 3.x](https://docs.anaconda.com/anaconda/install/linux/), e.g., as follows (use ```sudo su``` to run the chmod command as root and then ```exit``` in order to get back to the non-root user):    ```bash  wget https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh  sudo su  chmod u+x .",bash,PROGLANG
"Assuming, you have downloaded and installed Anaconda in /home/user:    ```bash   export PATH=$PATH:/home/user/anaconda3/condabin/  ```  To make it permanent, use a text editor, such as vim, append that to the .bashrc file (replace user with your username), and run source as follows:  ```bash   vim /home/user/.bashrc  source /home/user/.bashrc  ```    Then, please follow the steps below, in order to create a conda virtual environment and install the latest versions of the required Python libraries.",bash,PROGLANG
"Assuming, you have downloaded and installed Anaconda in /home/user:    ```bash   export PATH=$PATH:/home/user/anaconda3/condabin/  ```  To make it permanent, use a text editor, such as vim, append that to the .bashrc file (replace user with your username), and run source as follows:  ```bash   vim /home/user/.bashrc  source /home/user/.bashrc  ```    Then, please follow the steps below, in order to create a conda virtual environment and install the latest versions of the required Python libraries.",bash,PROGLANG
"Assuming, you have downloaded and installed Anaconda in /home/user:    ```bash   export PATH=$PATH:/home/user/anaconda3/condabin/  ```  To make it permanent, use a text editor, such as vim, append that to the .bashrc file (replace user with your username), and run source as follows:  ```bash   vim /home/user/.bashrc  source /home/user/.bashrc  ```    Then, please follow the steps below, in order to create a conda virtual environment and install the latest versions of the required Python libraries.",Python,PROGLANG
"They are necessary for running the generated code, if the generated code includes Python code for Data Analytics and Machine Learning (DAML).",Python,PROGLANG
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,bash,PROGLANG
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,python=3.8,PROGLANG
"The code that we generated for the sample model instance through the [quick (15 mins) tutorial](#user-doc-quick) above, namely [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml), can be built easily using the Apache Maven (the generated code resides in the /home/user/Generated_ML2_Demo_PingPong directory):    ```bash  cd /home/user/Generated_ML2_Demo_PingPong  cd python_java/  mvn clean install  ```  As mentioned, if you want to see a more detailed output to debug, use the option -X:    ```bash  mvn clean install -X  ```    Moreover, as stated before, if you want to use Maven in the offline mode, e.g., in the case that your machine is behind a firewall that prohibits the Internet access, you should use the option -o, but, remember that you would need to first copy the .m2 directory, which includes the Maven cache from another computer behind the firewall (on which you have already cached ´the required dependencies) to this computer.",bash,PROGLANG
"The code that we generated for the sample model instance through the [quick (15 mins) tutorial](#user-doc-quick) above, namely [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml), can be built easily using the Apache Maven (the generated code resides in the /home/user/Generated_ML2_Demo_PingPong directory):    ```bash  cd /home/user/Generated_ML2_Demo_PingPong  cd python_java/  mvn clean install  ```  As mentioned, if you want to see a more detailed output to debug, use the option -X:    ```bash  mvn clean install -X  ```    Moreover, as stated before, if you want to use Maven in the offline mode, e.g., in the case that your machine is behind a firewall that prohibits the Internet access, you should use the option -o, but, remember that you would need to first copy the .m2 directory, which includes the Maven cache from another computer behind the firewall (on which you have already cached ´the required dependencies) to this computer.",bash,PROGLANG
```bash  mvn clean install -o  ```    The target directory is created/updated.,bash,PROGLANG
"We should copy this Comma-Sparated Values (CSV) file into the correct path, in this case at target/data (the data sub-directory must be created):    ```bash  cd target/  mkdir data  cp /home/user/ML-Quadrat/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv data/  ```  Now, in order to run the generated application / IoT service:    ```bash  java -jar SmartPingPongCfg-1.0.0-jar-with-dependencies.jar  ```  After running the service, you will see the output in the terminal.",bash,PROGLANG
"We should copy this Comma-Sparated Values (CSV) file into the correct path, in this case at target/data (the data sub-directory must be created):    ```bash  cd target/  mkdir data  cp /home/user/ML-Quadrat/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv data/  ```  Now, in order to run the generated application / IoT service:    ```bash  java -jar SmartPingPongCfg-1.0.0-jar-with-dependencies.jar  ```  After running the service, you will see the output in the terminal.",bash,PROGLANG
"**Note:** If you try to generate the code on one machine and run the generated code on another machine, you might experience troubles, due to the absolute paths, e.g., in the case of the python_java code generator, in the generated Python scripts under src/python-scripts.",python,PROGLANG
"**Note:** If you try to generate the code on one machine and run the generated code on another machine, you might experience troubles, due to the absolute paths, e.g., in the case of the python_java code generator, in the generated Python scripts under src/python-scripts.",java,PROGLANG
"**Note:** If you try to generate the code on one machine and run the generated code on another machine, you might experience troubles, due to the absolute paths, e.g., in the case of the python_java code generator, in the generated Python scripts under src/python-scripts.",Python,PROGLANG
"**Note:** If you try to generate the code on one machine and run the generated code on another machine, you might experience troubles, due to the absolute paths, e.g., in the case of the python_java code generator, in the generated Python scripts under src/python-scripts.",python,PROGLANG
"Once you import the projects into the workspace in the Eclipse IDE, you will get notified about a number of missing extensions, such as the Maven support in Eclipse (m2e) and the Eclipse Enterprise Java and Web Developers Tools, which can be installed through the Eclipse Marketplace.",Java,PROGLANG
"Last but not least, the **semantics** are implemented both via the Xtext framework (in Java and Xtend) on the modeling langauge layer and also through the model-to-code transformations (i.e., code generators, also known as ""compilers"").",Java,PROGLANG
"Last but not least, the **semantics** are implemented both via the Xtext framework (in Java and Xtend) on the modeling langauge layer and also through the model-to-code transformations (i.e., code generators, also known as ""compilers"").",Xtend,PROGLANG
The provided examples [ML2_Demo_NIALM_PSM_Java.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_NIALM_PSM_Java.thingml) and [ML2_Demo_NIALM_PIM.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_NIALM_PIM.thingml) illustrate this.,Java,PROGLANG
The provided examples [ML2_Demo_NIALM_PSM_Java.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_NIALM_PSM_Java.thingml) and [ML2_Demo_NIALM_PIM.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_NIALM_PIM.thingml) illustrate this.,Java,PROGLANG
"For instance, the provided example [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) demonstrates how the String, Boolean<1> and Int32<4> datatypes are supposed to be mapped to the ""platform""-specific datatypes, such as String <-> char * for the C code generation, but String <-> String for the Java and the Javascript code generation.",C,PROGLANG
"For instance, the provided example [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) demonstrates how the String, Boolean<1> and Int32<4> datatypes are supposed to be mapped to the ""platform""-specific datatypes, such as String <-> char * for the C code generation, but String <-> String for the Java and the Javascript code generation.",Java,PROGLANG
"For instance, the provided example [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) demonstrates how the String, Boolean<1> and Int32<4> datatypes are supposed to be mapped to the ""platform""-specific datatypes, such as String <-> char * for the C code generation, but String <-> String for the Java and the Javascript code generation.",Javascript,PROGLANG
"Supporting more DAML libraries/frameworks, such as [PyTorch](https://pytorch.org) (Python) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) (Java) is currently in progress.",Python,PROGLANG
"Supporting more DAML libraries/frameworks, such as [PyTorch](https://pytorch.org) (Python) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) (Java) is currently in progress.",Java,PROGLANG
"(iv) **//:** Similar to Java, in order to comment out a line (i.e., disable or inactivate it) or write any comment, a double slash can be added to the beginning of the line, e.g., see ""//@dalib ""keras-tensorflow"" {"" above.",Java,PROGLANG
"Note that the serialized objects (e.g., the trained ML models) are stored in a directory, called ""pickles"" under src/python-scripts/ (in the case of the python_java code generator) or ""objects"" under src/ (in the case of the pure java code generator).",python,PROGLANG
"Note that the serialized objects (e.g., the trained ML models) are stored in a directory, called ""pickles"" under src/python-scripts/ (in the case of the python_java code generator) or ""objects"" under src/ (in the case of the pure java code generator).",python,PROGLANG
"Note that the serialized objects (e.g., the trained ML models) are stored in a directory, called ""pickles"" under src/python-scripts/ (in the case of the python_java code generator) or ""objects"" under src/ (in the case of the pure java code generator).",java,PROGLANG
"Note that the serialized objects (e.g., the trained ML models) are stored in a directory, called ""pickles"" under src/python-scripts/ (in the case of the python_java code generator) or ""objects"" under src/ (in the case of the pure java code generator).",java,PROGLANG
"The majority of them have the Python Pickle or the Java Byte Stream types, respectively.",Python,PROGLANG
"The majority of them have the Python Pickle or the Java Byte Stream types, respectively.",Java,PROGLANG
"The syntax must match the syntax of the target programming language, e.g., Python.",Python,PROGLANG
"However, in the blackbox-ML mode, if there exist categorical labels that had been transformed to one-hot-encoding via the Label Encoder in Python (Scikit-Learn), the path to the serialized Label Encoder object must be provided here.",Python,PROGLANG
The prepared data will be serialized and saved as pickle (in the case of the python_java code generator) or byte stream (in the case of the pure java code generator) on the disk to be fed later to the training script (see below).,python,PROGLANG
The prepared data will be serialized and saved as pickle (in the case of the python_java code generator) or byte stream (in the case of the pure java code generator) on the disk to be fed later to the training script (see below).,java,PROGLANG
"As stated before, the format is typically the Python Pickle or the Java Byte Stream.",Python,PROGLANG
"As stated before, the format is typically the Python Pickle or the Java Byte Stream.",Java,PROGLANG
"However, the PSM instances, e.g., [ML2_Demo_NIALM_PSM_Java.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_NIALM_PSM_Java.thingml) must contain the configuration.",Java,PROGLANG
"However, the PSM instances, e.g., [ML2_Demo_NIALM_PSM_Java.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_NIALM_PSM_Java.thingml) must contain the configuration.",Java,PROGLANG
"Here is the sample configuration of the above-mentioned example [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml):    ```  configuration ML2_Demo_PingPong_Cfg @compiler ""python_java"" {        instance pingClient : PingClient       instance pingServer : PingServer       instance pingPongDataAnalytics : PingPongDataAnalytics       connector pingClient.ping_service => pingServer.ping_service       connector pingServer.da_service => pingPongDataAnalytics.da_service      }  ```    The configuration subsection has typically 3 parts:    1.",python,PROGLANG
"Here is the sample configuration of the above-mentioned example [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml):    ```  configuration ML2_Demo_PingPong_Cfg @compiler ""python_java"" {        instance pingClient : PingClient       instance pingServer : PingServer       instance pingPongDataAnalytics : PingPongDataAnalytics       connector pingClient.ping_service => pingServer.ping_service       connector pingServer.da_service => pingPongDataAnalytics.da_service      }  ```    The configuration subsection has typically 3 parts:    1.",java,PROGLANG
"Optionally, it has the annotation @compiler, determining the particular model-to-code transformation (code generator/""compiler"") that needs to be deployed, e.g., @compiler ""python_java"" for the Python and Java code generator, or @compiler ""java"" for the pure Java code generator.",python,PROGLANG
"Optionally, it has the annotation @compiler, determining the particular model-to-code transformation (code generator/""compiler"") that needs to be deployed, e.g., @compiler ""python_java"" for the Python and Java code generator, or @compiler ""java"" for the pure Java code generator.",java,PROGLANG
"Optionally, it has the annotation @compiler, determining the particular model-to-code transformation (code generator/""compiler"") that needs to be deployed, e.g., @compiler ""python_java"" for the Python and Java code generator, or @compiler ""java"" for the pure Java code generator.",Python,PROGLANG
"Optionally, it has the annotation @compiler, determining the particular model-to-code transformation (code generator/""compiler"") that needs to be deployed, e.g., @compiler ""python_java"" for the Python and Java code generator, or @compiler ""java"" for the pure Java code generator.",Java,PROGLANG
"Optionally, it has the annotation @compiler, determining the particular model-to-code transformation (code generator/""compiler"") that needs to be deployed, e.g., @compiler ""python_java"" for the Python and Java code generator, or @compiler ""java"" for the pure Java code generator.",java,PROGLANG
"Optionally, it has the annotation @compiler, determining the particular model-to-code transformation (code generator/""compiler"") that needs to be deployed, e.g., @compiler ""python_java"" for the Python and Java code generator, or @compiler ""java"" for the pure Java code generator.",Java,PROGLANG
"Otherwise, if it is not specified in the configuration, the practitioner may later use the ""-c"" option in the code generation to determine the target model-to-code transformation (code generator/""compiler"") that shall be deployed, e.g., ""-c python_java"" for the Python and Java code generator, or ""-c java"" for the pure Java code generator.",python,PROGLANG
"Otherwise, if it is not specified in the configuration, the practitioner may later use the ""-c"" option in the code generation to determine the target model-to-code transformation (code generator/""compiler"") that shall be deployed, e.g., ""-c python_java"" for the Python and Java code generator, or ""-c java"" for the pure Java code generator.",java,PROGLANG
"Otherwise, if it is not specified in the configuration, the practitioner may later use the ""-c"" option in the code generation to determine the target model-to-code transformation (code generator/""compiler"") that shall be deployed, e.g., ""-c python_java"" for the Python and Java code generator, or ""-c java"" for the pure Java code generator.",Python,PROGLANG
"Otherwise, if it is not specified in the configuration, the practitioner may later use the ""-c"" option in the code generation to determine the target model-to-code transformation (code generator/""compiler"") that shall be deployed, e.g., ""-c python_java"" for the Python and Java code generator, or ""-c java"" for the pure Java code generator.",Java,PROGLANG
"Otherwise, if it is not specified in the configuration, the practitioner may later use the ""-c"" option in the code generation to determine the target model-to-code transformation (code generator/""compiler"") that shall be deployed, e.g., ""-c python_java"" for the Python and Java code generator, or ""-c java"" for the pure Java code generator.",java,PROGLANG
"Otherwise, if it is not specified in the configuration, the practitioner may later use the ""-c"" option in the code generation to determine the target model-to-code transformation (code generator/""compiler"") that shall be deployed, e.g., ""-c python_java"" for the Python and Java code generator, or ""-c java"" for the pure Java code generator.",Java,PROGLANG
"Currently, only the [Python and Java (""python_java"") code generator](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/compilers/python_java) supports DAML out-of-the-box.",Python,PROGLANG
"Currently, only the [Python and Java (""python_java"") code generator](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/compilers/python_java) supports DAML out-of-the-box.",Java,PROGLANG
"Currently, only the [Python and Java (""python_java"") code generator](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/compilers/python_java) supports DAML out-of-the-box.",python,PROGLANG
"Currently, only the [Python and Java (""python_java"") code generator](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/compilers/python_java) supports DAML out-of-the-box.",java,PROGLANG
This code generator generates Python and Java code.,Python,PROGLANG
This code generator generates Python and Java code.,Java,PROGLANG
"The Python code is responsible for the DAML functions, whereas the rest is implemented in Java.",Python,PROGLANG
"The Python code is responsible for the DAML functions, whereas the rest is implemented in Java.",Java,PROGLANG
The Python and Java codes are already seamlessly integrated using the Java Process Builder API.,Python,PROGLANG
The Python and Java codes are already seamlessly integrated using the Java Process Builder API.,Java,PROGLANG
The Python and Java codes are already seamlessly integrated using the Java Process Builder API.,Java,PROGLANG
"However, enabling more code generators to support DAML out-of-the-box, such as the [pure Java (""java"") code generator](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/compilers/java) is currently in progress.",Java,PROGLANG
"However, enabling more code generators to support DAML out-of-the-box, such as the [pure Java (""java"") code generator](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/compilers/java) is currently in progress.",java,PROGLANG
"For example, if you have already set @dalib ""scikit-learn"" or @dalib ""keras-tensorflow"", then you must choose the python_java code generator, not the pure Java code generator, as Scikit-learn/Keras are Python libraries.",python,PROGLANG
"For example, if you have already set @dalib ""scikit-learn"" or @dalib ""keras-tensorflow"", then you must choose the python_java code generator, not the pure Java code generator, as Scikit-learn/Keras are Python libraries.",java,PROGLANG
"For example, if you have already set @dalib ""scikit-learn"" or @dalib ""keras-tensorflow"", then you must choose the python_java code generator, not the pure Java code generator, as Scikit-learn/Keras are Python libraries.",Python,PROGLANG
"Similarly, if you have already set @dalib ""weka"", then you must choose the pure Java code generator here.",Java,PROGLANG
"(iii) The semantics realized through the Xtext framework, e.g., in the Java/Xtend classes at [ML2/language/thingml/src/org/thingml](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/language/thingml/src/org/thingml).",Java,PROGLANG
"(iii) The semantics realized through the Xtext framework, e.g., in the Java/Xtend classes at [ML2/language/thingml/src/org/thingml](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/language/thingml/src/org/thingml).",Xtend,PROGLANG
"After any modifications, please build the entire project again using Maven in the terminal as follows:    ```bash  cd ML-Quadrat  mvn clean install -X  cd ML2/language  mvn clean install -X  ```  The -X option is optional and enables the debugging mode, thus resulting in a more detailed output.",bash,PROGLANG
"This way, the customized textual model editor will also work properly.    ### Contributing to the Concrete Syntax of the DSML  If you want to introduce new keywords and let the features of the textual model editor, such as syntax highlighting work for them, please add the keywords to the [ThingMLAntlrTokenToAttributeIdMapper Java class](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/language/thingml.ui/src/org/thingml/xtext/ui/ThingMLAntlrTokenToAttributeIdMapper.java).    ### Contributing to the Semantics of the DSML at the Modeling Layer  If you want to adapt or extend the model checking constraints, validation rules, etc., please check out the Java/Xtend classes here: [ML2/language/thingml/src/org/thingml](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/language/thingml/src/org/thingml).    ### Contributing to the Semantics of the DSML at the Model-to-Code Transformations Layer  Please check out the existing model-to-code transformations (code generators/""compilers"") at [ML2/compilers](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/compilers).",Java,PROGLANG
"This way, the customized textual model editor will also work properly.    ### Contributing to the Concrete Syntax of the DSML  If you want to introduce new keywords and let the features of the textual model editor, such as syntax highlighting work for them, please add the keywords to the [ThingMLAntlrTokenToAttributeIdMapper Java class](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/language/thingml.ui/src/org/thingml/xtext/ui/ThingMLAntlrTokenToAttributeIdMapper.java).    ### Contributing to the Semantics of the DSML at the Modeling Layer  If you want to adapt or extend the model checking constraints, validation rules, etc., please check out the Java/Xtend classes here: [ML2/language/thingml/src/org/thingml](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/language/thingml/src/org/thingml).    ### Contributing to the Semantics of the DSML at the Model-to-Code Transformations Layer  Please check out the existing model-to-code transformations (code generators/""compilers"") at [ML2/compilers](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/compilers).",Java,PROGLANG
"This way, the customized textual model editor will also work properly.    ### Contributing to the Concrete Syntax of the DSML  If you want to introduce new keywords and let the features of the textual model editor, such as syntax highlighting work for them, please add the keywords to the [ThingMLAntlrTokenToAttributeIdMapper Java class](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/language/thingml.ui/src/org/thingml/xtext/ui/ThingMLAntlrTokenToAttributeIdMapper.java).    ### Contributing to the Semantics of the DSML at the Modeling Layer  If you want to adapt or extend the model checking constraints, validation rules, etc., please check out the Java/Xtend classes here: [ML2/language/thingml/src/org/thingml](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/language/thingml/src/org/thingml).    ### Contributing to the Semantics of the DSML at the Model-to-Code Transformations Layer  Please check out the existing model-to-code transformations (code generators/""compilers"") at [ML2/compilers](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/compilers).",Xtend,PROGLANG
Let's say you want to adapt/extend the Python and Java code generator (python_java).,Python,PROGLANG
Let's say you want to adapt/extend the Python and Java code generator (python_java).,Java,PROGLANG
Let's say you want to adapt/extend the Python and Java code generator (python_java).,python,PROGLANG
Let's say you want to adapt/extend the Python and Java code generator (python_java).,java,PROGLANG
"**Note:** You may add any new Java packages that must be included among the imported Java packages in the generated Java code to the Helper Java class, e.g., in the case of the python_java code generator to the [PythonJavaHelper Java class](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/compilers/python_java/src/main/java/org/thingml/compilers/python_java/PythonJavaHelper.java).",Java,PROGLANG
"**Note:** You may add any new Java packages that must be included among the imported Java packages in the generated Java code to the Helper Java class, e.g., in the case of the python_java code generator to the [PythonJavaHelper Java class](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/compilers/python_java/src/main/java/org/thingml/compilers/python_java/PythonJavaHelper.java).",Java,PROGLANG
"**Note:** You may add any new Java packages that must be included among the imported Java packages in the generated Java code to the Helper Java class, e.g., in the case of the python_java code generator to the [PythonJavaHelper Java class](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/compilers/python_java/src/main/java/org/thingml/compilers/python_java/PythonJavaHelper.java).",Java,PROGLANG
"**Note:** You may add any new Java packages that must be included among the imported Java packages in the generated Java code to the Helper Java class, e.g., in the case of the python_java code generator to the [PythonJavaHelper Java class](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/compilers/python_java/src/main/java/org/thingml/compilers/python_java/PythonJavaHelper.java).",Java,PROGLANG
"**Note:** You may add any new Java packages that must be included among the imported Java packages in the generated Java code to the Helper Java class, e.g., in the case of the python_java code generator to the [PythonJavaHelper Java class](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/compilers/python_java/src/main/java/org/thingml/compilers/python_java/PythonJavaHelper.java).",python,PROGLANG
"**Note:** You may add any new Java packages that must be included among the imported Java packages in the generated Java code to the Helper Java class, e.g., in the case of the python_java code generator to the [PythonJavaHelper Java class](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/compilers/python_java/src/main/java/org/thingml/compilers/python_java/PythonJavaHelper.java).",java,PROGLANG
"**Note:** You may add any new Java packages that must be included among the imported Java packages in the generated Java code to the Helper Java class, e.g., in the case of the python_java code generator to the [PythonJavaHelper Java class](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/compilers/python_java/src/main/java/org/thingml/compilers/python_java/PythonJavaHelper.java).",Java,PROGLANG
"After any modifications, please build the corresponding projects again using Maven in the terminal as follows (e.g., for the python_java case):    ```bash  cd ML-Quadrat/ML2/compilers/python_java  mvn clean install -X  cd ML-Quadrat/ML2/compilers/registry  mvn clean install -X  ```  The -X option is optional and enables the debugging mode, thus resulting in a more detailed output.",python,PROGLANG
"After any modifications, please build the corresponding projects again using Maven in the terminal as follows (e.g., for the python_java case):    ```bash  cd ML-Quadrat/ML2/compilers/python_java  mvn clean install -X  cd ML-Quadrat/ML2/compilers/registry  mvn clean install -X  ```  The -X option is optional and enables the debugging mode, thus resulting in a more detailed output.",java,PROGLANG
"After any modifications, please build the corresponding projects again using Maven in the terminal as follows (e.g., for the python_java case):    ```bash  cd ML-Quadrat/ML2/compilers/python_java  mvn clean install -X  cd ML-Quadrat/ML2/compilers/registry  mvn clean install -X  ```  The -X option is optional and enables the debugging mode, thus resulting in a more detailed output.",bash,PROGLANG
The main conclusion is that Hybrid Self-Attention NEAT can eliminate the restriction of the original NEAT.,Hybrid Self-Attention NEAT,PROGLANG
"_NOTE: The original implementation of self-attention for atari-games, and the NEAT algorithm can be found here:<br/>_  Neuroevolution of Self-Interpretable Agents: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionAgent <br/> Neat Package: https://github.com/CodeReclaimers/neat-python <br/> Pure python library for the NEAT and other variations: https://github.com/ukuleleplayer/pureples  ### Execution  #### To use this work on your researches or projects you need: * Python 3.7 * Python packages listed in `requirements.txt`  _NOTE: The following commands are based on Ubuntu 20.04_ ###  #### To install Python: _First, check if you already have it installed or not_. ~~~~ python3 --version ~~~~ _If you don't have python 3.7 in your computer you can use the code below_: ~~~~ sudo add-apt-repository ppa:deadsnakes/ppa sudo apt-get update sudo apt-get install python3.7 sudo apt install python3.7-distutils ~~~~ ###  _NOTE: To create a virtual environment, you can use the following link:_ <br/> Creation of virtual environment: https://docs.python.org/3.7/library/venv.html  #### To install packages via pip install: ~~~~ python3.7 -m pip install -r requirements.txt ~~~~ ###  #### To run this project on Ubuntu server: _You need to uncomment the following lines in_ `experiments/configs/configs.py` ~~~~ _display = pyvirtualdisplay.Display(visible=False, size=(1400, 900)) _display.start() ~~~~  _And also install some system dependencies as well_ ~~~~ apt-get install -y xvfb x11-utils ~~~~ ###  #### To train the model: * First, check the configuration you need.",python,PROGLANG
"_NOTE: The original implementation of self-attention for atari-games, and the NEAT algorithm can be found here:<br/>_  Neuroevolution of Self-Interpretable Agents: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionAgent <br/> Neat Package: https://github.com/CodeReclaimers/neat-python <br/> Pure python library for the NEAT and other variations: https://github.com/ukuleleplayer/pureples  ### Execution  #### To use this work on your researches or projects you need: * Python 3.7 * Python packages listed in `requirements.txt`  _NOTE: The following commands are based on Ubuntu 20.04_ ###  #### To install Python: _First, check if you already have it installed or not_. ~~~~ python3 --version ~~~~ _If you don't have python 3.7 in your computer you can use the code below_: ~~~~ sudo add-apt-repository ppa:deadsnakes/ppa sudo apt-get update sudo apt-get install python3.7 sudo apt install python3.7-distutils ~~~~ ###  _NOTE: To create a virtual environment, you can use the following link:_ <br/> Creation of virtual environment: https://docs.python.org/3.7/library/venv.html  #### To install packages via pip install: ~~~~ python3.7 -m pip install -r requirements.txt ~~~~ ###  #### To run this project on Ubuntu server: _You need to uncomment the following lines in_ `experiments/configs/configs.py` ~~~~ _display = pyvirtualdisplay.Display(visible=False, size=(1400, 900)) _display.start() ~~~~  _And also install some system dependencies as well_ ~~~~ apt-get install -y xvfb x11-utils ~~~~ ###  #### To train the model: * First, check the configuration you need.",Python,PROGLANG
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",python,PROGLANG
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",python,PROGLANG
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",python,PROGLANG
"# Soft-QMIX  This repo is heavily based on the [pymarl](https://github.com/benellis3/pymarl2)  ## Installation  please follow the installation guide in the original repo [pymarl](https://github.com/benellis3/pymarl2)  ## Run  To run the code, you can use the following command:  ```bash bash .",bash,PROGLANG
"Note that you might have to update c++ compiler and/or paths, e.g.: ```sh conda install cxx-compiler -c conda-forge export CPATH=/usr/local/cuda-12/targets/x86_64-linux/include:$CPATH export LD_LIBRARY_PATH=/usr/local/cuda-12/targets/x86_64-linux/lib:$LD_LIBRARY_PATH export PATH=/usr/local/cuda-12/bin:$PATH ```  ### How to use You can import the activation layer and use it within your network.",c++,PROGLANG
/LREC2020/ablation.py)  | Python version | Dependencies                         | | -------------- | ------------------------------------ | | \>= Python3.5  | [pandas](https://pandas.pydata.org/) |  This script generates the files necessary to perform the ablation study described in the paper.,Python,PROGLANG
[Python application](https://github.com/slideflow/slideflow/actions/workflows/python-app.yml/badge.svg?,Python,PROGLANG
[Python application](https://github.com/slideflow/slideflow/actions/workflows/python-app.yml/badge.svg?,python,PROGLANG
topic=v12100-installing-cplex-optimization-studio) 20.1.0 with [Python API](https://www.ibm.com/docs/en/icos/12.10.0?,Python,PROGLANG
"The fastest way to get started is to use one of our preconfigured projects, which will automatically download slides from the Genomic Data Commons:  ```python import slideflow as sf  P = sf.create_project(     root='/project/destination',     cfg=sf.project.LungAdenoSquam(),     download=True ) ```  After the slides have been downloaded and verified, you can skip to [Extract tiles from slides](#extract-tiles-from-slides).",python,PROGLANG
"Alternatively, to create a new custom project, supply the location of patient-level annotations (CSV), slides, and a destination for TFRecords to be saved:  ```python import slideflow as sf P = sf.create_project(   '/project/path',   annotations=""/patient/annotations.csv"",   slides=""/slides/directory"",   tfrecords=""/tfrecords/directory"" ) ```  Ensure that the annotations file has a `slide` column for each annotation entry with the filename (without extension) of the corresponding slide.  ### Extract tiles from slides  Next, whole-slide images are segmented into smaller image tiles and saved in `*.tfrecords` format.",python,PROGLANG
"[Extract tiles](https://slideflow.dev/slide_processing) from slides at a given magnification (width in microns size) and resolution (width in pixels) using `sf.Project.extract_tiles()`:  ```python P.extract_tiles(   tile_px=299,  # Tile size, in pixels   tile_um=302   # Tile size, in microns ) ```  If slides are on a network drive or a spinning HDD, tile extraction can be accelerated by buffering slides to a SSD or ramdisk:  ```python P.extract_tiles(   ...,   buffer=""/mnt/ramdisk"" ) ```  ### Training models  Once tiles are extracted, models can be [trained](https://slideflow.dev/training).",python,PROGLANG
"[Extract tiles](https://slideflow.dev/slide_processing) from slides at a given magnification (width in microns size) and resolution (width in pixels) using `sf.Project.extract_tiles()`:  ```python P.extract_tiles(   tile_px=299,  # Tile size, in pixels   tile_um=302   # Tile size, in microns ) ```  If slides are on a network drive or a spinning HDD, tile extraction can be accelerated by buffering slides to a SSD or ramdisk:  ```python P.extract_tiles(   ...,   buffer=""/mnt/ramdisk"" ) ```  ### Training models  Once tiles are extracted, models can be [trained](https://slideflow.dev/training).",python,PROGLANG
"Start by configuring a set of [hyperparameters](https://slideflow.dev/model#modelparams):  ```python params = sf.ModelParams(   tile_px=299,   tile_um=302,   batch_size=32,   model='xception',   learning_rate=0.0001,   ... ) ```  Models can then be trained using these parameters.",python,PROGLANG
"For example, to train models in cross-validation to predict the outcome `'category1'` as stored in the project annotations file:  ```python P.train(   'category1',   params=params,   save_predictions=True,   multi_gpu=True ) ```  ### Evaluation, heatmaps, mosaic maps, and more  Slideflow includes a host of additional tools, including model [evaluation and prediction](https://slideflow.dev/evaluation), [heatmaps](https://slideflow.dev/evaluation#heatmaps), analysis of [layer activations](https://slideflow.dev/posthoc), [mosaic maps](https://slideflow.dev/posthoc#mosaic-maps), and more.",python,PROGLANG
[PyPI download month](https://img.shields.io/pypi/dm/prdc.svg)](https://pypi.python.org/pypi/prdc/) [!,python,PROGLANG
"[PyPI license](https://img.shields.io/pypi/l/prdc.svg)](https://pypi.python.org/pypi/prdc/)  ## Reliable Fidelity and Diversity Metrics for Generative Models (ICML 2020)  [Paper: Reliable Fidelity and Diversity Metrics for Generative Models](https://arxiv.org/abs/2002.09797)  Muhammad Ferjad Naeem <sup>1,3*</sup>, Seong Joon Oh<sup>2*</sup>, Yunjey Choi<sup>1</sup>,  Youngjung Uh<sup>1</sup>, Jaejun Yoo<sup>1,4</sup>    <sub>**Work done at Clova AI Research**</sub>  <sub>\* Equal contribution</sub> <sup>1</sup> <sub>Clova AI Research, NAVER Corp.",python,PROGLANG
"```python import numpy as np from prdc import compute_prdc   num_real_samples = num_fake_samples = 10000 feature_dim = 1000 nearest_k = 5 real_features = np.random.normal(loc=0.0, scale=1.0,                                  size=[num_real_samples, feature_dim])  fake_features = np.random.normal(loc=0.0, scale=1.0,                                  size=[num_fake_samples, feature_dim])  metrics = compute_prdc(real_features=real_features,                        fake_features=fake_features,                        nearest_k=nearest_k)  print(metrics) ``` Above test code will result in the following estimates (may fluctuate due to randomness).",python,PROGLANG
"```python {'precision': 0.4772,  'recall': 0.4705,  'density': 1.0555,  'coverage': 0.9735} ```  ## 3.",python,PROGLANG
Install the S3PRL package:  ```sh pip install s3prl ```  2.,sh,PROGLANG
"Use it to extract representations for your own audio:  ```python import torch from s3prl.nn import S3PRLUpstream  model = S3PRLUpstream(""hubert"") model.eval()  with torch.no_grad():     wavs = torch.randn(2, 16000 * 2)     wavs_len = torch.LongTensor([16000 * 1, 16000 * 2])     all_hs, all_hs_len = model(wavs, wavs_len)  for hs, hs_len in zip(all_hs, all_hs_len):     assert isinstance(hs, torch.FloatTensor)     assert isinstance(hs_len, torch.LongTensor)      batch_size, max_seq_len, hidden_size = hs.shape     assert hs_len.dim() == 1 ```  ---  With this modularization, we have achieved close integration with the general speech processing toolkit [ESPNet](https://github.com/espnet/espnet), enabling the use of SSL models for a broader range of speech processing tasks and corpora to achieve state-of-the-art (SOTA) results (kudos to the [ESPNet Team](https://www.wavlab.org/open_source)):  !",python,PROGLANG
"[](multi-area-model_5faa0e9c.png) ***Example `beNNch` output (Figure 5C of Albers et al., 2022): Strong-scaling performance of the [multi-area model](https://github.com/INM-6/multi-area-model) simulated with the neuronal network simulator [NEST](https://www.nest-simulator.org) on JURECA-DC.** The left graph shows the absolute wall-clock time measured with Python-level timers for both network construction and state propagation.",Python,PROGLANG
"If set-up problems persist after following both approaches, don't hesitate to get into contact via the [Community discussion](https://github.com/INM-6/beNNch#community-discussion).   ### Initialization  - Download git submodules:  ```bash git submodule init ```   + _optional: if you want to change the url of any of the submodules (requires `git v2.25.0`):_     `git submodule set-url -- <submodule> <new_url>`  ```bash git submodule update --remote ```  - Install benchplot as Python module:  ```bash pip install -e plot --user ```    ### Software dependencies  - [git annex](https://git-annex.branchable.com)   + can e.g. be installed via  ```bash wget 'http://downloads.kitenet.net/git-annex/linux/current/git-annex-standalone-amd64.tar.gz' tar -xzf git-annex-standalone-amd64.tar.gz export PATH=$PATH:<install_path>/git-annex.linux ``` - [JUBE](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)   _Note that  JUBE version 2.4.2 or later is necessary_   - [Builder](https://github.com/INM-6/Builder)   + see Builder documentation for installation guide  - Python 3.X  ### Models  For the following network models, there is currently a NEST implementation in the *models* submodule and corresponding JUBE benchmark script in the `benchmarks/` folder:  - **Multi-Area Model**    - `multi-area-model_2` for usage with NEST 2",bash,PROGLANG
"If set-up problems persist after following both approaches, don't hesitate to get into contact via the [Community discussion](https://github.com/INM-6/beNNch#community-discussion).   ### Initialization  - Download git submodules:  ```bash git submodule init ```   + _optional: if you want to change the url of any of the submodules (requires `git v2.25.0`):_     `git submodule set-url -- <submodule> <new_url>`  ```bash git submodule update --remote ```  - Install benchplot as Python module:  ```bash pip install -e plot --user ```    ### Software dependencies  - [git annex](https://git-annex.branchable.com)   + can e.g. be installed via  ```bash wget 'http://downloads.kitenet.net/git-annex/linux/current/git-annex-standalone-amd64.tar.gz' tar -xzf git-annex-standalone-amd64.tar.gz export PATH=$PATH:<install_path>/git-annex.linux ``` - [JUBE](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)   _Note that  JUBE version 2.4.2 or later is necessary_   - [Builder](https://github.com/INM-6/Builder)   + see Builder documentation for installation guide  - Python 3.X  ### Models  For the following network models, there is currently a NEST implementation in the *models* submodule and corresponding JUBE benchmark script in the `benchmarks/` folder:  - **Multi-Area Model**    - `multi-area-model_2` for usage with NEST 2",Python,PROGLANG
"If set-up problems persist after following both approaches, don't hesitate to get into contact via the [Community discussion](https://github.com/INM-6/beNNch#community-discussion).   ### Initialization  - Download git submodules:  ```bash git submodule init ```   + _optional: if you want to change the url of any of the submodules (requires `git v2.25.0`):_     `git submodule set-url -- <submodule> <new_url>`  ```bash git submodule update --remote ```  - Install benchplot as Python module:  ```bash pip install -e plot --user ```    ### Software dependencies  - [git annex](https://git-annex.branchable.com)   + can e.g. be installed via  ```bash wget 'http://downloads.kitenet.net/git-annex/linux/current/git-annex-standalone-amd64.tar.gz' tar -xzf git-annex-standalone-amd64.tar.gz export PATH=$PATH:<install_path>/git-annex.linux ``` - [JUBE](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)   _Note that  JUBE version 2.4.2 or later is necessary_   - [Builder](https://github.com/INM-6/Builder)   + see Builder documentation for installation guide  - Python 3.X  ### Models  For the following network models, there is currently a NEST implementation in the *models* submodule and corresponding JUBE benchmark script in the `benchmarks/` folder:  - **Multi-Area Model**    - `multi-area-model_2` for usage with NEST 2",bash,PROGLANG
"If set-up problems persist after following both approaches, don't hesitate to get into contact via the [Community discussion](https://github.com/INM-6/beNNch#community-discussion).   ### Initialization  - Download git submodules:  ```bash git submodule init ```   + _optional: if you want to change the url of any of the submodules (requires `git v2.25.0`):_     `git submodule set-url -- <submodule> <new_url>`  ```bash git submodule update --remote ```  - Install benchplot as Python module:  ```bash pip install -e plot --user ```    ### Software dependencies  - [git annex](https://git-annex.branchable.com)   + can e.g. be installed via  ```bash wget 'http://downloads.kitenet.net/git-annex/linux/current/git-annex-standalone-amd64.tar.gz' tar -xzf git-annex-standalone-amd64.tar.gz export PATH=$PATH:<install_path>/git-annex.linux ``` - [JUBE](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)   _Note that  JUBE version 2.4.2 or later is necessary_   - [Builder](https://github.com/INM-6/Builder)   + see Builder documentation for installation guide  - Python 3.X  ### Models  For the following network models, there is currently a NEST implementation in the *models* submodule and corresponding JUBE benchmark script in the `benchmarks/` folder:  - **Multi-Area Model**    - `multi-area-model_2` for usage with NEST 2",Python 3.X,PROGLANG
_Specific to NEST benchmarking: don't forget to include `-Dwith-detailed-timers=ON` in the `CMAKEFLAGS` if you want to have access to C++ level timers._   ### Run benchmarks  The JUBE benchmarking scripts can be found in `benchmarks/`.,C++,PROGLANG
"`beNNch` provides defaults for plotting timers across `nodes` and `threads`, but alternatives can be readily implemented by adding to `analysis/plot_helpers.py`. - the path to the JUBE output (usually the same as the `outpath` of the `<benchmark>` in `benchmarks/<model>`)  To start the analysis, execute ```bash cd results ``` - _optional: initialize for the first time_   + `git pull origin main`   + `git checkout main`   + `git annex init`   + `git annex sync` ```bash python ..",bash,PROGLANG
"`beNNch` provides defaults for plotting timers across `nodes` and `threads`, but alternatives can be readily implemented by adding to `analysis/plot_helpers.py`. - the path to the JUBE output (usually the same as the `outpath` of the `<benchmark>` in `benchmarks/<model>`)  To start the analysis, execute ```bash cd results ``` - _optional: initialize for the first time_   + `git pull origin main`   + `git checkout main`   + `git annex init`   + `git annex sync` ```bash python ..",bash,PROGLANG
"For sharing, upload the results to the central repository via ```bash git annex sync ```  ### Get remote benchmark results  ```bash cd results ``` - _optional: add a new remote_    + `git add remote <name> <location>`, e.g.",bash,PROGLANG
"For sharing, upload the results to the central repository via ```bash git annex sync ```  ### Get remote benchmark results  ```bash cd results ``` - _optional: add a new remote_    + `git add remote <name> <location>`, e.g.",bash,PROGLANG
"To ""go back"" a view, execute ```bash git annex vpop ``` After choosing which benchmarks to display via filtering above and ordering them via `<differing_metadata>`, you can create a flip book of all plots with ```bash python ..",bash,PROGLANG
"To ""go back"" a view, execute ```bash git annex vpop ``` After choosing which benchmarks to display via filtering above and ordering them via `<differing_metadata>`, you can create a flip book of all plots with ```bash python ..",bash,PROGLANG
"Use `models/Potjans_2014/run_bm_microcircuit.py` for the former and `config/templates/microcircuit_config_template.yaml` as a reference.  #### Output  As current releases of NEST (including 2.14.1, 2.20.2 and 3.0+) include timers on the C++ level for measuring the simulation performance, the model only needs to output this information in a way compliant with `beNNch`.",C++,PROGLANG
"Note that this also provides the optional functionality to include python level timers as well as memory information.  ## How to cite beNNch  Please cite our paper:   > Albers J, Pronold J, Kurth AC, Vennemo SB, Haghighi Mood K, Patronis A, Terhorst D, Jordan J, Kunkel S, Tetzlaff T, Diesmann M and Senk J (2022) A Modular Workflow for Performance Benchmarking of Neuronal Network Simulations.",python,PROGLANG
- The codes ported from original python codes are included.,python,PROGLANG
"- scr.m, cr_subsolver.m, subsamp_tr.m, tr_subsolver.m: Python codes are originally created by [J.",Python,PROGLANG
"Intuitively speaking, a fault is a dormant error, for example, an erroneous line of the Java code that has not been activated in the execution because some  if-conditions have not been satisfied yet.",Java,PROGLANG
"- **Implementation errors:** They are introduced by the   humans responsible for coding the smart contract,   for example, in Solidity language",Solidity,PROGLANG
"For example, smart contracts implemented  in Solidity language are known to be at risk of being afflicted by reentrancy, unchecked send, integer overflow, and other language related errors.",Solidity,PROGLANG
"Consequently, the contract ends abnormally.   ### What can Epromela do with smart contracts?",Epromela,PROGLANG
Epromela is a language for writing models that can be model checked by the Spin model checker.,Epromela,PROGLANG
We have built it by extending the standard Promela language with primitives that help developers build models of smart contracts using constructs that are natural to smart contracts.,Promela,PROGLANG
We use the promela tool for   * *model checking:* we verify the logical correctness of the contract at design time and  * *generation of execution sequences* we generate excution sequences     (test cases) with the tool for testing for conformance the     actual implementation.    ### Model checking   Spin can verify the logical consistency of the model  epromela of the smart contract against correctness   properties written   in LTL (Linear Temporal Logics) formulae.,epromela,PROGLANG
<br />  ### Generation of execution sequences (test cases) for conformance testing  Spin can be instructed to generate all the execution  sequences encoded in the epromela model of the  smart contract.,epromela,PROGLANG
The basic idea is to use Spin as a  generator of counterexamples produced against   LTL formulae.  ### Business Event Generator of epromela At the heart of epromela lies a Business Event Generator (BEG) that is responsible for  generating the events that correspond to the execution of contractual operations.,epromela,PROGLANG
"""> </p>   ### Epromela model of the contract example The smart contract oriented primitives provided by epromela  help developers build models that describe the behaivour of contractual interactions in an intuitive manner.",Epromela,PROGLANG
"""> </p>   ### Epromela model of the contract example The smart contract oriented primitives provided by epromela  help developers build models that describe the behaivour of contractual interactions in an intuitive manner.",epromela,PROGLANG
*IS_O* is an illustrative example of the contract oriented primitives that epromela offers.,epromela,PROGLANG
"/figures/epromelaModelOfBuyerStoreContractOutputTO.png""     width=""400"" title=""epromela model of the buyer-store example with                       output TO",epromela,PROGLANG
You will notice that the rules (included in the *rules.h* file) are slightly different in comparison with the rules of the previous model.   ### Alternative epromela model of the contract example The execution model of operations shown above is not unique.,epromela,PROGLANG
The epromela model includes four rules and is shown in the figure.,epromela,PROGLANG
"/figures/epromelaModelOfBuyerStoreContractInitTO.png""     width=""400"" title=""epromela model of the buyer-store example with                       init  TO",epromela,PROGLANG
"As in the model discussed earlier, a run of this model will mechanically produce all (three) the execution sequences encoded in the model, namely:  ``` - payto - pay -> ack - pay -> ackto ``` Where *payto* and *ackto* stand for expiration of time out to pay  and expiration of time out to ack, respectively.    ### Execution model of bilateral operations with potential exceptions  A salient feature of epromela is that it can model exceptions on the basis of the following execution model: <br/>  Contractual operations are bilateral in the sense that their execution requires the interaction of the two business partners.",epromela,PROGLANG
Installation  The directions for installation of epromela are documented in the [UserGuide_v1.2.pdf](.,epromela,PROGLANG
The reader is expected to have some basic knowledge of  [SPIN and Promela](http://spinroot.com/spin/whatispin.html) and a local computer to dowload and deploy them,Promela,PROGLANG
"Abdelsadiq developed the macros as part of his    PhD dissertation (2012) at University of Newcastle, UK. *  Ionnis Sfyrakis from University of Newcastle, UK    (Ioannis.Sfyrakis@newcastle.ac.uk) implemented     the Python parsers to extract execution sequences during his    Masters degree (2012) at Newcastle. *  [Carlos Molina-Jimenez](https://www.cl.cam.ac.uk/~cm770/ ""MyWebPage"")     from **The Department of     Computer Science and Technology (Computer Laboratory),     University of Cambridge**     (Carlos.Molina@cl.cam.ac.uk) implemented shell scripts to     mechanise the extraction of execution sequences.",Python,PROGLANG
"*  ## Our contributions  Our contributions are:  (1) a reusable GP-based accuracy probabilistic extrapolator (APEx-GP) that can match existing curve-fitting approaches in terms of error while providing additional uncertainty estimates, and   (2) a careful assessment of our proposed probabilistic extrapolations compared to ground truth on larger datasets across six medical classification tasks involving both 2D and 3D images across diverse modalities (x-ray, ultrasound, and CT) with various sample sizes.  ## Using our method  To use our Gaussian process to extrapolate classifier accuracy to larger datasets see `notebooks/demo.ipynb`.  ### Initializing our Gaussian process  ```python likelihood = gpytorch.likelihoods.GaussianLikelihood() # Note: If you want to use the Gaussian process with an arctan mean function use models.GPArctan() instead. model = models.GPPowerLaw(X_train, y_train, likelihood, epsilon_min=0.05, with_priors=True) ```  ### Extrapolating classifier accuracy  ```python with torch.no_grad(): predictions = likelihood(model(X_test)) loc = predictions.mean.numpy() scale = predictions.stddev.numpy() # Note: If you want to forecast with 20%-80% change lower and upper percentile. lower, upper = priors.truncated_normal_uncertainty(a=0.0, b=1.0, loc=loc, scale=scale, lower_percentile=0.025, upper_percentile=0.975)  ```  ## Citation  ```bibtex @inproceedings{harvey2023probabilistic,   author={Harvey, Ethan and Chen, Wansu and Kent, David M. and Hughes, Michael C.},   title={A Probabilistic Method to Predict Classifier Accuracy on Larger Datasets given Small Pilot Data},   booktitle={Machine Learning for Health (ML4H)},   year={2023} } ```  ## Reproducing results  To reproduce model performance at varying dataset sizes 1) download datasets (see `encode_images/README.md` and `label_images/README.md` for more details) and 2) fit classifiers to each dataset (see `src/finetune_2D.py` and `src/finetune_3D.py`).",python,PROGLANG
"*  ## Our contributions  Our contributions are:  (1) a reusable GP-based accuracy probabilistic extrapolator (APEx-GP) that can match existing curve-fitting approaches in terms of error while providing additional uncertainty estimates, and   (2) a careful assessment of our proposed probabilistic extrapolations compared to ground truth on larger datasets across six medical classification tasks involving both 2D and 3D images across diverse modalities (x-ray, ultrasound, and CT) with various sample sizes.  ## Using our method  To use our Gaussian process to extrapolate classifier accuracy to larger datasets see `notebooks/demo.ipynb`.  ### Initializing our Gaussian process  ```python likelihood = gpytorch.likelihoods.GaussianLikelihood() # Note: If you want to use the Gaussian process with an arctan mean function use models.GPArctan() instead. model = models.GPPowerLaw(X_train, y_train, likelihood, epsilon_min=0.05, with_priors=True) ```  ### Extrapolating classifier accuracy  ```python with torch.no_grad(): predictions = likelihood(model(X_test)) loc = predictions.mean.numpy() scale = predictions.stddev.numpy() # Note: If you want to forecast with 20%-80% change lower and upper percentile. lower, upper = priors.truncated_normal_uncertainty(a=0.0, b=1.0, loc=loc, scale=scale, lower_percentile=0.025, upper_percentile=0.975)  ```  ## Citation  ```bibtex @inproceedings{harvey2023probabilistic,   author={Harvey, Ethan and Chen, Wansu and Kent, David M. and Hughes, Michael C.},   title={A Probabilistic Method to Predict Classifier Accuracy on Larger Datasets given Small Pilot Data},   booktitle={Machine Learning for Health (ML4H)},   year={2023} } ```  ## Reproducing results  To reproduce model performance at varying dataset sizes 1) download datasets (see `encode_images/README.md` and `label_images/README.md` for more details) and 2) fit classifiers to each dataset (see `src/finetune_2D.py` and `src/finetune_3D.py`).",python,PROGLANG
"Create a new virtual environment with Python 3.10, e.g., ``` conda create --name prompt-oirl python==3.10 conda activate prompt-oirl cd Prompt-OIRL ``` 3.",Python 3.10,PROGLANG
"Create a new virtual environment with Python 3.10, e.g., ``` conda create --name prompt-oirl python==3.10 conda activate prompt-oirl cd Prompt-OIRL ``` 3.",python==3.10,PROGLANG
"<p align=""left""> </p>   <a href=""https://opensource.org/licenses/MIT""><img src=""https://img.shields.io/badge/License-MIT-yellow.svg"" alt=""License: MIT"">   <a href=""https://standardjs.com""><img src=""https://img.shields.io/badge/code_style-standard-brightgreen.svg"" alt=""Standard - \Python Style Guide""></a> [!",Python,PROGLANG
"In one of our <a href = ""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/Explainable_AI"">examples for security dataset</a>, we showed how SafeML can be used as XAI.",MATLAB,PROGLANG
"--Similar to the idea of <a href = ""https://github.com/slundberg/shap"">SHAP XAI Python Package</a>, the SafeML idea can be used for <a href ""https://en.wikipedia.org/wiki/Explainable_artificial_intelligence"">AI explainability and interpretability</a>.",Python,PROGLANG
"Application of the SafeML in AI explainability and interpretability</figcaption>  </p> -->  ## Case Studies <ul>   <li><b>Security Datasets</b></li> <p align=""justify""> A Intrusion Detection Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2017.html"">CICIDS2017</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",MATLAB,PROGLANG
"Application of the SafeML in AI explainability and interpretability</figcaption>  </p> -->  ## Case Studies <ul>   <li><b>Security Datasets</b></li> <p align=""justify""> A Intrusion Detection Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2017.html"">CICIDS2017</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</li></p> <p align=""justify""> Tor-nonTor Dataset (<a href=""https://www.unb.ca/cic/datasets/tor.html"">ISCXTor2016</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</li></p> <p align=""justify""> Tor-nonTor Dataset (<a href=""https://www.unb.ca/cic/datasets/tor.html"">ISCXTor2016</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</p> <p align=""justify""> DDoS Attach Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2018.html"">CSE-CIC-IDS2018</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</p> <p align=""justify""> DDoS Attach Dataset (<a href=""https://www.unb.ca/cic/datasets/ids-2018.html"">CSE-CIC-IDS2018</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</p> <p align=""justify""> NSL version of KDD Security Dataset  (<a href=""https://www.unb.ca/cic/datasets/nsl.html"">NSL-KDD</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/NSL_KDD_Security_Dataset"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</p> <p align=""justify""> NSL version of KDD Security Dataset  (<a href=""https://www.unb.ca/cic/datasets/nsl.html"">NSL-KDD</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/NSL_KDD_Security_Dataset"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</p> <p align=""justify""> DDoS Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ddos-2019.html"">CICDDoS2019</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/CICDDoS2019_Security_Dataset"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</p> <p align=""justify""> DDoS Evaluation Dataset (<a href=""https://www.unb.ca/cic/datasets/ddos-2019.html"">CICDDoS2019</a>) [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB/CICDDoS2019_Security_Dataset"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</p> <p align=""justify""> CIC DoS Dataset (<a href=""https://www.unb.ca/cic/datasets/dos-dataset.html"">CICDoS2017</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</p> <p align=""justify""> CIC DoS Dataset (<a href=""https://www.unb.ca/cic/datasets/dos-dataset.html"">CICDoS2017</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</p>  <p align=""justify""> VPN-nonVPN Dataset (<a href=""https://www.unb.ca/cic/datasets/vpn.html"">ISCXVPN2016</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</p>  <p align=""justify""> VPN-nonVPN Dataset (<a href=""https://www.unb.ca/cic/datasets/vpn.html"">ISCXVPN2016</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</p>  <p align=""justify""> Botnet Dataset (<a href=""https://www.unb.ca/cic/datasets/botnet.html"">BotNet2014</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</p>  <p align=""justify""> Botnet Dataset (<a href=""https://www.unb.ca/cic/datasets/botnet.html"">BotNet2014</a>) [Will be available in <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_MATLAB"">MATLAB implementation</a>].",MATLAB,PROGLANG
"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",R,PROGLANG
"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",R,PROGLANG
"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",R,PROGLANG
"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",R,PROGLANG
"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",R,PROGLANG
"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",R,PROGLANG
"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",R,PROGLANG
"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>].",Python,PROGLANG
"section=gtsrb&subsection=dataset""><b>GTSRB - German Traffic Sign Recognition Benchmark</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_Python/Examples/German_Traffic_Sign_Recognition%20(GTSR)/GTSRB_CNN_SafeML_v1.ipynb"">Python implementation</a>][Available on <a href = ""https://www.kaggle.com/kooaslansefat/cnn-97-accuracy-plus-safety-monitoring-safeml"">Kaggle</a>].",Python,PROGLANG
"</li>   <li><a href = ""https://www.cs.toronto.edu/~kriz/cifar.html""><b>CIFAR 10/100 Datasets</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_MATLAB/CIFAR_10_Dataset/README.md"">MATLAB implementation</a>]</li>   <li>More datasets will be tested.",MATLAB,PROGLANG
"Within this Python package, we provide three components,  1.",Python,PROGLANG
"Each (datasets,     jax learners, and pytorch learners) have their own `requirements.txt` that     you are welcome to install with `pip` and a Python version above 3.8",Python,PROGLANG
"Code paths  The code is structured as follows:  ```bash |--- dm_nevis/ |    |--- benchmarker/ |    |--- datasets_storage/ |    |--- streams/ |--- experiments_jax/ |    |--- launch.py |    |--- experiment.py |    |--- configs/ |    |--- learners/ |    |--- metrics/ |    |--- environment/ |    |--- training/ |--- experiments_torch/ |    |--- launch.py |    |--- experiment.py |    |--- configs/ |    |--- learners/ |    |--- metrics/ |    |--- environment/ |    |--- training/ ```  `dm_nevis/` is the library of the benchmark, containing the `benchmarker/` library, which implements the evaluation protocol used in the [paper].",bash,PROGLANG
"# sql4ml  Sql4ml is a Haskell project that takes as input SQL code which defines the objective/cost/loss function of a supervised machine learning model (ML), as well as a set of parameters, and generates TensorFlow (Python API) code that trains this model.",Haskell,PROGLANG
"# sql4ml  Sql4ml is a Haskell project that takes as input SQL code which defines the objective/cost/loss function of a supervised machine learning model (ML), as well as a set of parameters, and generates TensorFlow (Python API) code that trains this model.",SQL,PROGLANG
"# sql4ml  Sql4ml is a Haskell project that takes as input SQL code which defines the objective/cost/loss function of a supervised machine learning model (ML), as well as a set of parameters, and generates TensorFlow (Python API) code that trains this model.",Python,PROGLANG
To compile it you need to install the Haskell Platform (https://www.haskell.org/platform/).,Haskell,PROGLANG
"See also the section on installation on how to build dependent projects.  ## Installation  Sql4ml uses the open source project queryparser (https://github.com/uber/queryparser), also in Haskell.",Haskell,PROGLANG
File main.hs containts two examples on how to translate SQL to TensorFlow code end-to-end.,SQL,PROGLANG
"After installing MySQL, you can run the SQL scripts in the directory /db_setups to create two toy databases, one based on the Boston Housing dataset (https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) and the other based on the Iris dataset (https://archive.ics.uci.edu/ml/datasets/iris), which you can find in the directory /data.  ## Use  In file main.hs you can find two examples on how to call function endToEndTranslate, which takes the following input parameters:  * A list of SQL queries defining the objective function of a supervised ML model.",SQL,PROGLANG
"After installing MySQL, you can run the SQL scripts in the directory /db_setups to create two toy databases, one based on the Boston Housing dataset (https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) and the other based on the Iris dataset (https://archive.ics.uci.edu/ml/datasets/iris), which you can find in the directory /data.  ## Use  In file main.hs you can find two examples on how to call function endToEndTranslate, which takes the following input parameters:  * A list of SQL queries defining the objective function of a supervised ML model.",SQL,PROGLANG
"You can find two examples of SQL code defining Linear and Logistic Regression in directory working_examples. * A list of names of the tables storing the features of the model. * A list of columns in the aforementioned tables that store names of features. * A list of lists, each of which has the actual names of features stored in each table. * The name of the table storing the training observations. * The name of the table storing the labels/targets of training observations. * The name of the view computing the objective function of the ML model. * The username of a user to connect to the database where training data are stored. * The password of the same user. * The name of the database storing training data. * A list of names of the tables storing the weights of the model. * A map whose elements consist of the name of a weight table and its dimenions as a list. * The number of gradient descent iterations. * The learning rate used in gradient descent.",SQL,PROGLANG
"Currently sql4ml supports the translation of SQL create view queries of the following form,      CREATE VIEW $(name) AS     SELECT $(columns), $(numericExpr)     FROM $(tables)     WHERE $(joinElement)     GROUP BY $(groupingElement)  and generate an equivalent TensorFlow expression as below:      $(name) = $(translateNumericExpr(numericExpr))  For examples, check SQL files in the directory /working_examples.",SQL,PROGLANG
"Currently sql4ml supports the translation of SQL create view queries of the following form,      CREATE VIEW $(name) AS     SELECT $(columns), $(numericExpr)     FROM $(tables)     WHERE $(joinElement)     GROUP BY $(groupingElement)  and generate an equivalent TensorFlow expression as below:      $(name) = $(translateNumericExpr(numericExpr))  For examples, check SQL files in the directory /working_examples.",SQL,PROGLANG
/main logistic //To generate TensorFlow code for the Logistic Regression model in working_examples/logistic_regression.sql  The files with the generated TensorFlow/Python code can be executed like any other TensorFlow program.,Python,PROGLANG
"You can also try to translate individual SQL queries by loading the sql4ml_translator module in ghci and type:      translateToTensorFlowCommand (L.pack ""CREATE VIEW squaredErrors AS SELECT POW(errors.errorValue, 2) AS squaredErrorValue, errors.observationID AS observationID FROM errors;"") [""features""] [""weights""] [[""f1"", ""f2""]]  where      translateToTensorFlowCommand :: L.Text -> [String] -> [String] -> [[String]] -> String     translateToTensorFlowCommand sql_statement feature_tables variable_tables feature_names  and * sql_statement: a SQL create view query in type Text * feature_tables: a list of names of the tables storing the features of the model. * variable_tables: a list of names of the tables storing the weights of the model. * feature_names: a list of lists, each of which has the actual names of features stored in each table.",SQL,PROGLANG
"You can also try to translate individual SQL queries by loading the sql4ml_translator module in ghci and type:      translateToTensorFlowCommand (L.pack ""CREATE VIEW squaredErrors AS SELECT POW(errors.errorValue, 2) AS squaredErrorValue, errors.observationID AS observationID FROM errors;"") [""features""] [""weights""] [[""f1"", ""f2""]]  where      translateToTensorFlowCommand :: L.Text -> [String] -> [String] -> [[String]] -> String     translateToTensorFlowCommand sql_statement feature_tables variable_tables feature_names  and * sql_statement: a SQL create view query in type Text * feature_tables: a list of names of the tables storing the features of the model. * variable_tables: a list of names of the tables storing the weights of the model. * feature_names: a list of lists, each of which has the actual names of features stored in each table.",SQL,PROGLANG
"[](https://github.com/doc-doc/vRGV/blob/master/model.png) ## Notes Fix issue on unstable result [2021/10/07].  ## Environment  Anaconda 3, python 3.6.5, pytorch 0.4.1 (Higher version is OK once feature is ready) and cuda >= 9.0.",python 3.6.5,PROGLANG
"For others libs, please refer to the file requirements.txt.  ## Install Please create an env for this project using anaconda3 (should install [anaconda](https://docs.anaconda.com/anaconda/install/linux/) first) ``` >conda create -n envname python=3.6.5 # Create >conda activate envname # Enter >pip install -r requirements.txt # Install the provided libs >sh vRGV/lib/make.sh # Set the environment for detection, make sure you have nvcc ``` ## Data Preparation Please download the data [here](https://drive.google.com/file/d/1qNJ3jBPPoi0BPkvLqooS66czvCxsib1M/view?",python=3.6.5,PROGLANG
[Python 3.11.4](https://img.shields.io/badge/python-3.11.4-blue.svg)](https://www.python.org/downloads/release/python-3114/) [!,Python 3.11.4,PROGLANG
[Python 3.11.4](https://img.shields.io/badge/python-3.11.4-blue.svg)](https://www.python.org/downloads/release/python-3114/) [!,python-3.11.4,PROGLANG
[Python 3.11.4](https://img.shields.io/badge/python-3.11.4-blue.svg)](https://www.python.org/downloads/release/python-3114/) [!,python,PROGLANG
[Python 3.11.4](https://img.shields.io/badge/python-3.11.4-blue.svg)](https://www.python.org/downloads/release/python-3114/) [!,python-3114,PROGLANG
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",Python 3,PROGLANG
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",bash,PROGLANG
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",bash,PROGLANG
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",bash,PROGLANG
```python import ComputationalHypergraphDiscovery as CHD import pandas as pd df=pd.read_csv('https://github.com/TheoBourdais/ComputationalHypergraphDiscovery/blob/8c3fed6dfe58a01cb73e469579ff7703b0681f7e/examples/Sachs/SachsData.csv?,python,PROGLANG
"Here is an example:   ```python import ComputationalHypergraphDiscovery as CHD import numpy as np X=np.random.rand(10,100) node_names=[f'node_{i}' for i in range(10)] graph_discovery = CHD.GraphDiscovery(X,node_names) ```  > **Note**: This shows how to create a `GraphDiscovery` object from a numpy array.",python,PROGLANG
"You can use the code below to load it:  If you have a Pandas dataframe, you can use the `from_dataframe` method (see the method's docstring for more details):  ```python import ComputationalHypergraphDiscovery as CHD import pandas as pd df=pd.read_csv('https://github.com/TheoBourdais/ComputationalHypergraphDiscovery/blob/8c3fed6dfe58a01cb73e469579ff7703b0681f7e/examples/Sachs/SachsData.csv?",python,PROGLANG
"Here is an example: ```python graph_discovery.fit() ``` If you wish to discover the ancestors of a specific node, you can do so by specifying the `targets` parameter: ```python graph_discovery.fit(targets=['$Raf$']) ```  Once the model is fitted, you can access all the results of the graph discovery using the `.G` attribute.",python,PROGLANG
"Here is an example: ```python graph_discovery.fit() ``` If you wish to discover the ancestors of a specific node, you can do so by specifying the `targets` parameter: ```python graph_discovery.fit(targets=['$Raf$']) ```  Once the model is fitted, you can access all the results of the graph discovery using the `.G` attribute.",python,PROGLANG
"For example, you can plot the graph using the `.plot_graph()` method: ```python graph_discovery.plot_graph() ```  >The `.plot_graph()` method allows for some customization of the resulting plot.",python,PROGLANG
"To manipulate kernels, import the modes module: ```python import ComputationalHypergraphDiscovery.Modes as Modes ```   The `ModeKernel` interface defines our kernel modes.",python,PROGLANG
```python import ComputationalHypergraphDiscovery.decision as decision ```  There are two leading indicators that we use to make decisions (See the [paper][paper_url] or the [blog post][blog_url] for more details): - **Signal-to-noise ratio**: The signal-to-noise ratio is a measure of how much the signal is stronger than the noise.,python,PROGLANG
"We get the following performances:  An example of output is the following dictionary (some keys have been hidden for clarity): ```python kernel_performances={   'linear': {     'noise-to-signal ratio': 0.45,     'Z_test': [0.98, 1.0]     },   'quadratic': {#see note above, this is not the quadratic kernel but linear+quadratic     'noise-to-signal ratio': 0.91,      'Z_test': [0.96, 1.0]     },    'gaussian': {#same comment as above     'noise-to-signal ratio': 0.33,      'Z_test': [0.74, 1.0]     } } ```  Several decisions can be made using this dictionary.",python,PROGLANG
"For example, with the Sachs dataset, we can define the following clusters: ```python clusters=[     ['$PKC$','$P38$','$Jnk$'],     ['$Erk$','$Akt$','$PKA$'],     ['$Raf$','$Mek$'],     ['$Plcg$','$PIP2$','$PIP3$'] ] graph_discovery = CHD.GraphDiscovery.from_dataframe(df,clusters=clusters) ```  #### After creating a `GraphDiscovery` object, for the second run  If you already have a `GraphDiscovery` object named `graph_discovery` on which you have run the algorithm, you can choose the clusters based on the results and apply them to get a new `GraphDiscovery` object: ```python graph_discovery2=graph_discovery.prepare_new_graph_with_clusters(clusters) ```  ### Using clusters Once the clusters have been defined, you can use the `GraphDiscovery` object as usual.",python,PROGLANG
"```python import numpy as np import ComputationalHypergraphDiscovery as CHD from ComputationalHypergraphDiscovery.Modes import *  # Load the data data = np.loadtxt('data/Sachs.txt', delimiter='\t') # Normalize the data data = (data - np.mean(data, axis=0)) / np.std(data, axis=0) node_names=np.array(['$Raf$','$Mek$','$Plcg$','$PIP2$','$PIP3$','$Erk$','$Akt$','$PKA$','$PKC$','$P38$','$Jnk$'])  # Define the kernel kernel = [0.1*LinearMode(), 0.01*QuadraticMode()] # Set up CHD graph_discovery = CHD.GraphDiscovery(data,node_names,kernel) # Perform CHD graph_discovery.fit() graph_discovery.plot_graph()  # Refine the graph with clusters clusters=[     ['$PKC$','$P38$','$Jnk$'],     ['$Erk$','$Akt$','$PKA$'],     ['$Raf$','$Mek$'],     ['$Plcg$','$PIP2$','$PIP3$'] ] graph_discovery2=graph_discovery.prepare_new_graph_with_clusters(clusters) graph_discovery2.fit() graph_discovery2.plot_graph() ``` This code recovers the following graph:  <img style=""width:100%;"" alt=""Resulting graph Sachs example"" src=""_images/sachs_cluster_plot.png""></a>  ### Possible edges  As a practitioner, you may have some knowledge about the data that you wish to use to inform the graph discovery process.",python,PROGLANG
Clone the repository:      ```bash     git clone https://github.com/qroa/qroa.git     cd qroa     ```  2.,bash,PROGLANG
"Install the required packages:      ```bash     pip install -r requirements.txt     ```  ## 🚀  Usage   ### ⚔️ Running the Attack  To run the script, you need to provide the path to the input file containing the instructions.",bash,PROGLANG
"Run the script from the command line by specifying the path to the instruction file and the authentication token:  ```bash python main.py data/instructions.csv [API_AUTH_TOKEN] ```  Replace `instructions.csv` with the path to your text file containing the instructions, and `API_AUTH_TOKEN` with the actual authentication token.  ### 🧠  Supported Models  You can test the following models with QROA:  - **Llama2-chat** (`llama2_chat_hf`) - **Llama2** (`llama2_hf`) - **Vicuna** (`vicuna_hf`) - **Mistral** (`mistral_hf`) - **Falcon** (`falcon_hf`) - **OpenAI GPT** (`openai-0613`) - **Mistral Next** (`mistral`)  Simply change the `model` parameter in the `main` function to the desired model.  ### 🧪  Demo and Testing Model Generation - **Notebook Demo:** Run `demo.ipynb` to see a demonstration of the process. - **Notebook Analysis Experiement:** Run `analysis.ipynb` to analyse results and calculate metrics value (ASR). - **Testing Model:** Generation: Execute `generate.py` to test the generation process on custom instructions and triggers.",bash,PROGLANG
"This script can be run from the command line as follows:  ```bash python generate.py -auth_token [API_AUTH_TOKEN] -instruction [THE INSTRUCTION HERE] -suffix [THE SUFFIX HERE] ```  Where:   - **auth_token:** Authentication token required for accessing the model. - **instruction:** The specific instruction you want the model to follow. - **suffix:** The adversarial trigger that, when appended to the instruction, causes the LLM to obey the instruction.  ### 📁  Output Files The following output files are generated during the execution of the script:  Generated and validated triggers are saved in JSON format:  - **Generated Triggers:** `.",bash,PROGLANG
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",shell,PROGLANG
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",python=3.7,PROGLANG
We provide scripts to do this:  ```shell # crowdhuman python .,shell,PROGLANG
You can download them from [here](https://github.com/openai/CLIP/tree/main) and put them under the `pretrain` directory.  ### Training and Evaluation  Training on single node ```shell bash .,shell,PROGLANG
/pretrain/ViT-B-16.pt ``` Evaluation on MOT17 half validation set ```shell bash .,shell,PROGLANG
We're installing packages specific to the OWL.  ### Step 2 - Installing packages We now need to install the Python libraries that let the OWL work.,Python,PROGLANG
"```python owl = Owl(config_file='config/ENTER_YOUR_CONFIG_FILE_HERE.ini') ```  These are the various system, data collection and detection settings that can be changed.",python,PROGLANG
Once you have a functioning setup the process to update is simple with a single bash script.,bash,PROGLANG
"Mixture Proportion Estimation and PU Learning: A Modern Approach. arxiv preprint  arXiv:2111.00980.  ``` @inproceedings{garg2021PUlearning,     title={Mixture Proportion Estimation and {PU} Learning: A Modern Approach},     author={Garg, Saurabh and Wu, Yifan and Smola, Alex and Balakrishnan, Sivaraman and Lipton, Zachary},     year={2021},     booktitle={Advances in Neural Information Processing Systems (NeurIPS)}  } ```  ## Requirements  The code is written in Python and uses [PyTorch](https://pytorch.org/).",Python,PROGLANG
"# Thermometer: Towards Universal Calibration for Large Language Models #### This repository contains official implementation of the paper [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819).  ## Requirements ```bash pip install torch==2.2.1 transformers==4.28.1 evaluate==0.4.1 tqdm pandas ``` ## File Structure #### *src* folder includes all the source code for the experiments. - ### *configs* folder:      includes all the information of training configurations and model hyper-parameter. - ### *data* folder:     includes the dataloader to load and process the datasets. - ### other code files:   - *process_mrqa.py* pre-process the raw data in free-form QA datasets MRQA;    - *extract_features.py* aims to extract labels, features, and logits from pretrained LLMs;   - *train_thermometer.py* and *eval_thermometer.py* contain the main function to train Thermometer,   and the functions to evaluate calibration performance of trained Thermometer, respectively.   ## Usage #### We provide the scripts to help reproduce the results of our paper. - ### Step-1: extract the features and logits from pre-trained LLMs,     ```     exract.sh     ``` - ### Step-2: train Thermometer model,     ```     train.sh     ``` - ### Step-3: evaluate calibration of trained Thermometer model,     ```     eval.sh     ``` - ### Free-form QA task requires an additional step to pre-process the raw data, i.e., append the LLM's response to the prompts,     ```     mrqa.sh     ``` - ### Choose different types of LLMs,     ```     --model_type decoder_only --model_name Llama-2-7b-chat-hf     --model_type encoder_decoder --model_name flan-t5-xl     ```  ## Citation #### If you find this repository helpful for your research, please consider citing our paper,  ``` @InProceedings{pmlr-v235-shen24c,   title = {Thermometer: Towards Universal Calibration for Large Language Models},   author =  {Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W. and Ghosh, Soumya},   booktitle = {Proceedings of the 41st International Conference on Machine Learning},   pages = {44687--44711},   year =  {2024},   editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},   volume =  {235},   series =  {Proceedings of Machine Learning Research},   month = {21--27 Jul},   publisher = {PMLR} } ```",bash,PROGLANG
"```python import datasets hagrid = datasets.load_dataset(""miracl/hagrid"", split=""train"") print(hagrid[0]) ```  |  Split | #Q | #A       | #Informativeness  | #Attribuatability       | |:-----|:------:|:-------:|:------:|:-------:| | Train     | 1,922 | 3,214 | 3,214 | 754 | | Dev     | 716 | 1,318 | 1,157 | 826 |   ## Baselines (Coming soon!)",python,PROGLANG
p=dc-shadownet-single-image-hard-and-soft-1)  ## Prerequisites ``` git clone https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal.git cd DC-ShadowNet-Hard-and-Soft-Shadow-Removal/ conda create -n shadow python=3.7 conda activate shadow conda install pytorch=1.10.2 torchvision torchaudio cudatoolkit=11.3 -c pytorch python3 -m pip install -r requirements.txt ```  ## Datasets 1.,python=3.7,PROGLANG
"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",python,PROGLANG
"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",python,PROGLANG
"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",python,PROGLANG
"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",python,PROGLANG
"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",python,PROGLANG
pwd=lrss) | |-----|-----|-----|-----|-----|-----|-----|-----|   Option 1 [MATLAB](./0_Shadow-Free_Chromaticity_matlab): [inputs](./0_Shadow-Free_Chromaticity_matlab/input/) and [results](./0_Shadow-Free_Chromaticity_matlab/sfchroma/) ``` 0_Shadow-Free_Chromaticity_matlab/physics_all.m ```  Option 2 [Python](.,Python,PROGLANG
"/_Shadow-Free_Chromaticity_python): [inputs](./0_Shadow-Free_Chromaticity_python/input/) and [results](./0_Shadow-Free_Chromaticity_python/sfchroma/) ``` cd 0_Shadow-Free_Chromaticity_python python physics_all.py ```  <p align=""left"">   <img width=450"" src=""teaser/chromaticity.png""> </p>   ## Train with Shadow-Robust Feature Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_pecp_loss True ```  Get the following Figure 5 in the main paper, VGG feature visualization [results](.",python,PROGLANG
"/_Shadow-Free_Chromaticity_python): [inputs](./0_Shadow-Free_Chromaticity_python/input/) and [results](./0_Shadow-Free_Chromaticity_python/sfchroma/) ``` cd 0_Shadow-Free_Chromaticity_python python physics_all.py ```  <p align=""left"">   <img width=450"" src=""teaser/chromaticity.png""> </p>   ## Train with Shadow-Robust Feature Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_pecp_loss True ```  Get the following Figure 5 in the main paper, VGG feature visualization [results](.",python,PROGLANG
"/_Shadow-Free_Chromaticity_python): [inputs](./0_Shadow-Free_Chromaticity_python/input/) and [results](./0_Shadow-Free_Chromaticity_python/sfchroma/) ``` cd 0_Shadow-Free_Chromaticity_python python physics_all.py ```  <p align=""left"">   <img width=450"" src=""teaser/chromaticity.png""> </p>   ## Train with Shadow-Robust Feature Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_pecp_loss True ```  Get the following Figure 5 in the main paper, VGG feature visualization [results](.",python,PROGLANG
"/feature_release/results_VGGfeatures/shadow_VGGfeatures/22/214/visual_179_0.1125.jpg) ``` cd feature_release python test_VGGfeatures.py ``` <p align=""left"">   <img width=350"" src=""teaser/feature_map.png""> </p>    ## Evaluation The root mean squared error (RMSE) [evaluation code](https://drive.google.com/file/d/1-lG8nAJbWajAC4xopx7hGPKbuwYRw4x-/view) used by all methods (including ours) computes mean absolute error (MAE).   ### 1.",python,PROGLANG
[python](https://img.shields.io/pypi/pyversions/pfhedge.svg)](https://pypi.org/project/pfhedge) [!,python,PROGLANG
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",sh,PROGLANG
"You can use a wide variety of frameworks for OWL, RDF and the SPARQL graph query language to access and query the ontology.",RDF,PROGLANG
"You can use a wide variety of frameworks for OWL, RDF and the SPARQL graph query language to access and query the ontology.",SPARQL,PROGLANG
Example: [Google Colab notebook demonstrating SPARQL queries with ITO](https://colab.research.google.com/drive/1g3gDgakBcmAfIi4opXX99KXB7yALK66S?,SPARQL,PROGLANG
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",python,PROGLANG
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",python=3.6,PROGLANG
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",scipy,PROGLANG
Install EfficientNet implementation ```bash cd efficientNet python setup.py develop ``` d.,python,PROGLANG
"If your folder structure is different, you may need to change the corresponding paths in config files.  ``` EvPSNet ├── mmdet ├── tools ├── configs └── data     └── cityscapes         ├── annotations         ├── train         ├── val         ├── stuffthingmaps         ├── cityscapes_panoptic_val.json         └── cityscapes_panoptic_val ``` The cityscapes annotations have to be converted into the aforementioned format using `tools/convert_datasets/cityscapes.py`: ```shell python tools/convert_cityscapes.py ROOT_DIRECTORY_OF_CITYSCAPES .",shell,PROGLANG
[Python: 3.8+](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/) [!,Python: 3.8+,PROGLANG
[Python: 3.8+](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/) [!,Python-3.8,PROGLANG
[Python: 3.8+](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/) [!,python,PROGLANG
"We shall update this README accordingly.   #### Machine, Tools and Requirements  Machine specification:    * `Machine: Dell 5810`   * `RAM: 64GB`   * `Processor: Intel 16-Core Xeon (3.10GHz)`   * `GPU: NVIDIA P4000 (8GB VRAM)`   * `OS: Ubuntu Linux 16.04.6 LTS (64bit)`  Tools used in this work:    * `Editor: vi/vim`   * `Execution: Linux-Shell`   * `MATLAB R2018b (64bit)`   * `Prolog compiler: YAP 6.2.2`   * `ILP Engine: Aleph`   * `CUDA 10.1.105`   * `Python 3.7.6`  Python Libraries:     * `torch 1.4.0`   * `torch_geometric 1.2.1`   * `numpy 1.18.1`   #### Source Data  There are 73 problems obtained from NCI.",Python 3.7.6,PROGLANG
"We shall update this README accordingly.   #### Machine, Tools and Requirements  Machine specification:    * `Machine: Dell 5810`   * `RAM: 64GB`   * `Processor: Intel 16-Core Xeon (3.10GHz)`   * `GPU: NVIDIA P4000 (8GB VRAM)`   * `OS: Ubuntu Linux 16.04.6 LTS (64bit)`  Tools used in this work:    * `Editor: vi/vim`   * `Execution: Linux-Shell`   * `MATLAB R2018b (64bit)`   * `Prolog compiler: YAP 6.2.2`   * `ILP Engine: Aleph`   * `CUDA 10.1.105`   * `Python 3.7.6`  Python Libraries:     * `torch 1.4.0`   * `torch_geometric 1.2.1`   * `numpy 1.18.1`   #### Source Data  There are 73 problems obtained from NCI.",Python,PROGLANG
"/data/test_split`.   #### Shell scripts  The script ""run.bash"" is one-shot execution of all methods",Shell,PROGLANG
` bash createressum.bash --help `   **added 2:** Added a Python script to load a (trained) saved model and print the structure  ` python modelsummary.py `  **added 3:** Code to load the saved models for deployment.,Python,PROGLANG
"It is accessible as a python object:  ```python from wildlifeml import MegaDetector  md = MegaDetector(batch_size=32, confidence_threshold=0.1) md.predict_directory(directory='<directory>', output_file='<output_file>') ```  `<directory>` should be a directory that contains images, where bounding boxes should be predicted.",python,PROGLANG
"It is accessible as a python object:  ```python from wildlifeml import MegaDetector  md = MegaDetector(batch_size=32, confidence_threshold=0.1) md.predict_directory(directory='<directory>', output_file='<output_file>') ```  `<directory>` should be a directory that contains images, where bounding boxes should be predicted.",python,PROGLANG
"We also offer a CLI option:  ```shell wildlifeml-get-bbox -d <directory> ```  Access help over the `--help` flag.  #### The MD index file  By default, the MD saves its results in `images_megadetector.json`.",shell,PROGLANG
```python from wildlifeml.data import BBoxMapper mapper = BBoxMapper(detector_file_path='<path_to_images_megadetector.json>') key_map = mapper.get_keymap() ```  ### 03: Creating a Wildlife Dataset  #### Initializing a Dataset  Our dataset builds on the Keras `Sequence` utility and thus supports multi-threaded loading during training.,python,PROGLANG
"This corresponds to a list that contains the identifiers provided by the MD file, e.g:  ```python train_keys = ['img_id1.xx_001', 'img_id1.xx_002', 'img_id2.xx_001'] ```  Using a key based initialization, one can easily derive cross validation splits, test sets or other variants based on one MD file.",python,PROGLANG
"All in all, an instantiation of `WildlifeDataset` is done like:  ```python from wildlifeml.data import WildlifeDataset  wd = WildlifeDataset(     keys=train_keys,     image_dir='<path_to_image_dir>',     detector_file_path='<path_to_images_megadetector.json>',     batch_size=8,     bbox_map=key_map,     label_file_path='<path_to_labels.csv>', ) ```  `do_cropping` is by default set to `True` and leads to the image being directly cropped to their respective bounding boxes when being loaded in the data process.  ### 04: Training a classifier  `wildlife-ml` contains the `WildlifeTrainer` that is an interface for directly training a classifier on the MD processed wildlife data.",python,PROGLANG
"Thus, a minimal training initialization is for example:  ```python from tensorflow.keras.losses import SparseCategoricalCrossentropy from tensorflow.keras.optimizers import Adam  from wildlifeml.training.trainer import WildlifeTrainer  trainer = WildlifeTrainer(     batch_size=8,     loss_func=SparseCategoricalCrossentropy(),     num_classes=10,     transfer_epochs=10,     finetune_epochs=10,     transfer_optimizer=Adam(),     finetune_optimizer=Adam(),     finetune_layers=3 ) ```  The training itself is triggered over the `fit` function and providing a training and validation `WildlifeDataset`.",python,PROGLANG
"```python trainer.fit(train_dataset, val_dataset) ```  ### 05: Evaluating a model  Due to our cascaded MD approach.",python,PROGLANG
"An example is:  ```python from wildlifeml.training.evaluator import Evaluator  evaluator = Evaluator(     detector_file_path='<path_to_images_megadetector.json>',     label_file_path='<path_to_labels.csv>',     dataset=training_dataset,     num_classes=10,     conf_threshold=0.1,     empty_class_id=99 ) ```  For a trained model, which is contained in a `WildlifeTrainer`, the accuracy, precision, recall and f1 score is computed as:  ```python evaluator.evaluate(trainer) metrics = evaluator.compute_metrics() ```  If you wish to extract the predictions and ground-truth labels for all individual observations, use `evaluator.get_details()`.  ### 06: Active Learning  Apart from fitting a model in a fully supervised way, we offer an active learning pipeline.",python,PROGLANG
"An example is:  ```python from wildlifeml.training.evaluator import Evaluator  evaluator = Evaluator(     detector_file_path='<path_to_images_megadetector.json>',     label_file_path='<path_to_labels.csv>',     dataset=training_dataset,     num_classes=10,     conf_threshold=0.1,     empty_class_id=99 ) ```  For a trained model, which is contained in a `WildlifeTrainer`, the accuracy, precision, recall and f1 score is computed as:  ```python evaluator.evaluate(trainer) metrics = evaluator.compute_metrics() ```  If you wish to extract the predictions and ground-truth labels for all individual observations, use `evaluator.get_details()`.  ### 06: Active Learning  Apart from fitting a model in a fully supervised way, we offer an active learning pipeline.",python,PROGLANG
"An example is:  ```python from wildlifeml import ActiveLearner  a_learner = ActiveLearner(     trainer=trainer,     pool_dataset=train_dataset,     label_file_path='<path_to_labels.csv>',     conf_threshold=0.1,     empty_class_id=10,     test_dataset=test_dataset ) ```  Execution of the active learning loop is triggered over:  ```python a_learner.run() ```  After `.run()` has finished, the directory `active-wildlife` will contain a directory `images` and a file `active_labels.csv`.",python,PROGLANG
"An example is:  ```python from wildlifeml import ActiveLearner  a_learner = ActiveLearner(     trainer=trainer,     pool_dataset=train_dataset,     label_file_path='<path_to_labels.csv>',     conf_threshold=0.1,     empty_class_id=10,     test_dataset=test_dataset ) ```  Execution of the active learning loop is triggered over:  ```python a_learner.run() ```  After `.run()` has finished, the directory `active-wildlife` will contain a directory `images` and a file `active_labels.csv`.",python,PROGLANG
"```python from jury import Jury  scorer = Jury() predictions = [     [""the cat is on the mat"", ""There is cat playing on the mat""],      [""Look!",python,PROGLANG
"```python scorer = Jury(metrics=[""bleu"", ""meteor""]) scores = scorer(predictions, references) ```  #### Use of Metrics standalone  You can directly import metrics from `jury.metrics` as classes, and then instantiate and use as desired.",python,PROGLANG
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",python,PROGLANG
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",python,PROGLANG
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",python,PROGLANG
"```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.",python,PROGLANG
"```python from jury.metrics import MetricForTask  class CustomMetric(MetricForTask):     def _compute_single_pred_single_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_single_pred_multi_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_multi_pred_multi_ref(             self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError ```  For more details, have a look at base metric implementation [jury.metrics.Metric](.",python,PROGLANG
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",bash,PROGLANG
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",python=3.6,PROGLANG
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",Shell,PROGLANG
- Download the annotations of the images ```Shell .,Shell,PROGLANG
- Download the global image features and region features ```Shell .,Shell,PROGLANG
- Download the pretrained models ```Shell .,Shell,PROGLANG
"The results will appear in `DrillDown/logs` Please note that, when finetuning the supervisedly pretrained DrillDown model, e.g. runing the script ```Shell .",Shell,PROGLANG
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",python 2.7+,PROGLANG
"Most of the methods are available in the form of R package, and in the ""interface to comparison methods"" folders, we provide interface programs (written in R) to use these datasets with the available R packages for comparison.",R,PROGLANG
"Most of the methods are available in the form of R package, and in the ""interface to comparison methods"" folders, we provide interface programs (written in R) to use these datasets with the available R packages for comparison.",R,PROGLANG
"Most of the methods are available in the form of R package, and in the ""interface to comparison methods"" folders, we provide interface programs (written in R) to use these datasets with the available R packages for comparison.",R,PROGLANG
Please install the related R pacakges before using the interface programs.,R,PROGLANG
"It has not yet been tested in a wide variety of development setups, but see our [sample Colab notebook](https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) for a working example.  ## Installation First, you'll need to install this [package](https://pypi.org/project/model-sketch-book/) using PyPI. ``` pip install model_sketch_book ```  ## 1: Basic setup ### Imports Then import the package into a Python notebook (i.e., a Jupyter or Colab notebook). ``` import model_sketch_book as msb ```  ### Set up the sketchbook **Data**.",Python,PROGLANG
"and execute the following command:  `source Test/test.sh /path/to/directory_with_saved_model/ $num_samples /path/to/dataset/ /path/to/directory_with_ground_truth_for_selected_validation_files/`  (You might have to recompile the C files for testing, provided by KITTI, if your architecture is different from mine)  ## Results  Comparision with state-of-the-art:  !",C,PROGLANG
"Generated with `conda env export > environment.yml`     │     ├── setup.py           <- Makes project pip installable (pip install -e .) so src can be imported     ├── src                <- Python modules with classes and functions that will be orchestrated in the notebooks.         ├── data           <- Python modules - Generators, preprocess and postprocess functions         ├── models         <- Python modules - TF.keras 2.X Model and Layer definition         ├── utils          <- Python modules - Metrics, losses, prediction, evaluation code, TF-callbacks and io-utils         └── visualization  <- Python modules - Plot functions for the evaluation and data description  ## Dataset For this work a multi-centric heterogeneous cine-SSFPs CMR TOF data set from the [German Competence Network for Congenital Heart Defects](https://www.kompetenznetz-ahf.de/en/about-us/competence-network/)) was used (study identifier: NCT00266188, title: Non-invasive Imaging and Exercise Tolerance Tests in Post-repair Tetralogy of Fallot -Intervention and Course in Patients Over 8 Years Old).",Python,PROGLANG
"Generated with `conda env export > environment.yml`     │     ├── setup.py           <- Makes project pip installable (pip install -e .) so src can be imported     ├── src                <- Python modules with classes and functions that will be orchestrated in the notebooks.         ├── data           <- Python modules - Generators, preprocess and postprocess functions         ├── models         <- Python modules - TF.keras 2.X Model and Layer definition         ├── utils          <- Python modules - Metrics, losses, prediction, evaluation code, TF-callbacks and io-utils         └── visualization  <- Python modules - Plot functions for the evaluation and data description  ## Dataset For this work a multi-centric heterogeneous cine-SSFPs CMR TOF data set from the [German Competence Network for Congenital Heart Defects](https://www.kompetenznetz-ahf.de/en/about-us/competence-network/)) was used (study identifier: NCT00266188, title: Non-invasive Imaging and Exercise Tolerance Tests in Post-repair Tetralogy of Fallot -Intervention and Course in Patients Over 8 Years Old).",Python,PROGLANG
"Generated with `conda env export > environment.yml`     │     ├── setup.py           <- Makes project pip installable (pip install -e .) so src can be imported     ├── src                <- Python modules with classes and functions that will be orchestrated in the notebooks.         ├── data           <- Python modules - Generators, preprocess and postprocess functions         ├── models         <- Python modules - TF.keras 2.X Model and Layer definition         ├── utils          <- Python modules - Metrics, losses, prediction, evaluation code, TF-callbacks and io-utils         └── visualization  <- Python modules - Plot functions for the evaluation and data description  ## Dataset For this work a multi-centric heterogeneous cine-SSFPs CMR TOF data set from the [German Competence Network for Congenital Heart Defects](https://www.kompetenznetz-ahf.de/en/about-us/competence-network/)) was used (study identifier: NCT00266188, title: Non-invasive Imaging and Exercise Tolerance Tests in Post-repair Tetralogy of Fallot -Intervention and Course in Patients Over 8 Years Old).",Python,PROGLANG
"Generated with `conda env export > environment.yml`     │     ├── setup.py           <- Makes project pip installable (pip install -e .) so src can be imported     ├── src                <- Python modules with classes and functions that will be orchestrated in the notebooks.         ├── data           <- Python modules - Generators, preprocess and postprocess functions         ├── models         <- Python modules - TF.keras 2.X Model and Layer definition         ├── utils          <- Python modules - Metrics, losses, prediction, evaluation code, TF-callbacks and io-utils         └── visualization  <- Python modules - Plot functions for the evaluation and data description  ## Dataset For this work a multi-centric heterogeneous cine-SSFPs CMR TOF data set from the [German Competence Network for Congenital Heart Defects](https://www.kompetenznetz-ahf.de/en/about-us/competence-network/)) was used (study identifier: NCT00266188, title: Non-invasive Imaging and Exercise Tolerance Tests in Post-repair Tetralogy of Fallot -Intervention and Course in Patients Over 8 Years Old).",Python,PROGLANG
"Generated with `conda env export > environment.yml`     │     ├── setup.py           <- Makes project pip installable (pip install -e .) so src can be imported     ├── src                <- Python modules with classes and functions that will be orchestrated in the notebooks.         ├── data           <- Python modules - Generators, preprocess and postprocess functions         ├── models         <- Python modules - TF.keras 2.X Model and Layer definition         ├── utils          <- Python modules - Metrics, losses, prediction, evaluation code, TF-callbacks and io-utils         └── visualization  <- Python modules - Plot functions for the evaluation and data description  ## Dataset For this work a multi-centric heterogeneous cine-SSFPs CMR TOF data set from the [German Competence Network for Congenital Heart Defects](https://www.kompetenznetz-ahf.de/en/about-us/competence-network/)) was used (study identifier: NCT00266188, title: Non-invasive Imaging and Exercise Tolerance Tests in Post-repair Tetralogy of Fallot -Intervention and Course in Patients Over 8 Years Old).",Python,PROGLANG
"|Field | Type | Description| ---|---|--- `id` | string | ID of the article `date` | string | date of publication (`YYYY-MM-DD`) `source` | string | name of the source `title` | string | article's headline `content` | string | article's body text `author` | string | author who signed the article `published` | string | date time string as provided by source `published_utc` | integer | unix timestamp of publication `collection_utc` | integer | unix timestamp of collection date  ### Aggregated labels  We provide aggregated labels based on Media Bias/Fact Check reports, classifying each source as:  * _Reliable_ - class 0 * _Mixed_ - class 1 * _Unreliable_ - class 2  These labels can be found in `labels.csv`  __Note__: the labels used in this aggregation were collected from Media Bias/Fact Check on Mar 20, 2020.   ## Examples ###  load-sqlite3.py  * How to load the data from the Sqlite3 database using SQL queries",SQL,PROGLANG
"Python packages might missing: cython, opencv-python >= 3.2.0, easydict.",Python,PROGLANG
"Python packages might missing: cython, opencv-python >= 3.2.0, easydict.",python,PROGLANG
"If `pip` is set up on your system, those packages should be able to be fetched and installed by running  ```  pip install Cython  pip install opencv-python==3.2.0.6  pip install easydict==1.6  pip install hickle  ``` 3.",python,PROGLANG
"***Build from source (alternative way)***   3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by  ```  git clone --recursive https://github.com/dmlc/mxnet.git  git checkout 998378a  git submodule init  git submodule update  ```  3.3 Copy channel operators in `$(FCIS_ROOT)/fcis/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by  ```  cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/     ```  3.4 Compile MXNet  ```  cd ${MXNET_ROOT}  make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1  ```  3.5 Install the MXNet Python binding by    ***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***  ```  cd python  sudo python setup.py install  ```  3.6 For advanced users, you may put your Python packge into `.",Python,PROGLANG
"***Build from source (alternative way)***   3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by  ```  git clone --recursive https://github.com/dmlc/mxnet.git  git checkout 998378a  git submodule init  git submodule update  ```  3.3 Copy channel operators in `$(FCIS_ROOT)/fcis/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by  ```  cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/     ```  3.4 Compile MXNet  ```  cd ${MXNET_ROOT}  make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1  ```  3.5 Install the MXNet Python binding by    ***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***  ```  cd python  sudo python setup.py install  ```  3.6 For advanced users, you may put your Python packge into `.",python,PROGLANG
"***Build from source (alternative way)***   3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by  ```  git clone --recursive https://github.com/dmlc/mxnet.git  git checkout 998378a  git submodule init  git submodule update  ```  3.3 Copy channel operators in `$(FCIS_ROOT)/fcis/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by  ```  cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/     ```  3.4 Compile MXNet  ```  cd ${MXNET_ROOT}  make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1  ```  3.5 Install the MXNet Python binding by    ***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***  ```  cd python  sudo python setup.py install  ```  3.6 For advanced users, you may put your Python packge into `.",python,PROGLANG
"***Build from source (alternative way)***   3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by  ```  git clone --recursive https://github.com/dmlc/mxnet.git  git checkout 998378a  git submodule init  git submodule update  ```  3.3 Copy channel operators in `$(FCIS_ROOT)/fcis/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by  ```  cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/     ```  3.4 Compile MXNet  ```  cd ${MXNET_ROOT}  make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1  ```  3.5 Install the MXNet Python binding by    ***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***  ```  cd python  sudo python setup.py install  ```  3.6 For advanced users, you may put your Python packge into `.",Python,PROGLANG
"To perform experiments, run the python scripts with the corresponding config file as input.",python,PROGLANG
"Run `bash prepare_sense_input.sh 2>&1 | tee temp.log` to do the next three scripts:   - `write_mask_preds/wsi_vocab.py`: determine vocabulary of words to perform WSI - `val_data_process/process_wiktionary.py`: get wiktionary definitions for vocabulary words - `write_mask_preds/wsi_preprocessing.py`: input preparation, also copy vocab file into output folder  Then, run the following script on S2ORC and Wikipedia:   - `write_mask_preds/write_mask_preds.py`: write replacements   We recommend splitting input files into numbered parts and running the script on ranges of file numbers.",bash,PROGLANG
"style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)  To reproduce the results please use Python 3.9, PyTorch version 2.1.2, Cuda 12.1, PyG version 2.3.0.",Python 3.9,PROGLANG
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",python=3.9,PROGLANG
The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,python,PROGLANG
"```python python train.py --config-name=config_binding_hetero        # heterogenous model, top performing model in the paper ```  For training on [SLURM](https://www.schedmd.com/) cluster with [submitit](https://github.com/facebookincubator/submitit)  used the `conf/hydra/meluxina.yaml` as blueprint.",python,PROGLANG
"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and Günter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",python,PROGLANG
"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and Günter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",python,PROGLANG
# EntQA  This repo provides the code for our ICLR 2022 paper [EntQA: Entitly Linking as Question Answering](https://arxiv.org/pdf/2110.02369.pdf)  ## Setup  ``` conda create --name entqa python=3.8 conda activate entqa pip install -r requirements.txt conda install -c pytorch faiss-gpu cudatoolkit=11.0  ```  ## Download data & preprocess All the preprocessed data can be downloaded [here](https://drive.google.com/drive/folders/1DQvfjKOuOoUE3YcYrg2GIvODaOEZXMdH?,python=3.8,PROGLANG
"# Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites  ## About  The Forschungszentrum Jülich Machine Learning Library  It is currently being developed and maintained at the [Applied Machine Learning](https://www.fz-juelich.de/en/inm/inm-7/research-groups/applied-machine-learning-aml) group at [Forschungszentrum Juelich](https://www.fz-juelich.de/en), Germany.   ## Overview  **PrettYharmonize** is a Python package developed to address data leakage in the harmonization of biomedical datasets with site-specific variability, particularly under scenarios where class balance differs across data collection sites.",Python,PROGLANG
"**Clone the repository:**     ```bash    git clone https://github.com/juaml/PrettYharmonize.git    cd PrettYharmonize   ## Citation ```bibtex If you use PrettYharmonize in your work, please cite the following: @article{nieto2024impact,   title={Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites},   author={Nieto, Nicol{\'a}s and Eickhoff, Simon B and Jung, Christian and Reuter, Martin and Diers, Kersten and Kelm, Malte and Lichtenberg, Artur and Raimondo, Federico and Patil, Kaustubh R},   journal={arXiv preprint arXiv:2410.19643},   year={2024} } ```  ## Licensing  preattyharmonize is released under the AGPL v3 license:  preattyharmonize, FZJuelich AML machine learning library.",bash,PROGLANG
<br/> doi: 10.3389/frai.2022.991242  Preprint available at ArXiV: [https://arxiv.org/abs/2209.05301](https://arxiv.org/abs/2209.05301)   ## License  The python scripts follow [AGPL 3.0v license](LICENSE).,python,PROGLANG
[Python 3.9+](https://img.shields.io/badge/Python-3.9+-lightblue) !,Python 3.9+,PROGLANG
[Python 3.9+](https://img.shields.io/badge/Python-3.9+-lightblue) !,Python-3.9+,PROGLANG
"Ashley, Vincent Herrmann, Zachary Friggstad, and Jürgen Schmidhuber.   ## Installation  This project is implemented in [Python](https://www.python.org/) and uses models learned with [PyTorch](https://pytorch.org).",Python,PROGLANG
"Ashley, Vincent Herrmann, Zachary Friggstad, and Jürgen Schmidhuber.   ## Installation  This project is implemented in [Python](https://www.python.org/) and uses models learned with [PyTorch](https://pytorch.org).",python,PROGLANG
"Before installation, first, ensure you have a recent version of Python and pip, then clone the repository using the `--sparse` option: ```bash git clone --sparse git@github.com:dylanashley/story-distiller.git ```  Afterwards, install the Python dependencies using pip: ```bash pip install -r requirements.txt ```  To use the full suite of file types supported by the tool, it is also necessary to separately install the [ffmpeg](https://ffmpeg.org/) tool by following the instructions for your operating system provided on [their website](https://ffmpeg.org/download.html).",bash,PROGLANG
"Before installation, first, ensure you have a recent version of Python and pip, then clone the repository using the `--sparse` option: ```bash git clone --sparse git@github.com:dylanashley/story-distiller.git ```  Afterwards, install the Python dependencies using pip: ```bash pip install -r requirements.txt ```  To use the full suite of file types supported by the tool, it is also necessary to separately install the [ffmpeg](https://ffmpeg.org/) tool by following the instructions for your operating system provided on [their website](https://ffmpeg.org/download.html).",Python,PROGLANG
"Before installation, first, ensure you have a recent version of Python and pip, then clone the repository using the `--sparse` option: ```bash git clone --sparse git@github.com:dylanashley/story-distiller.git ```  Afterwards, install the Python dependencies using pip: ```bash pip install -r requirements.txt ```  To use the full suite of file types supported by the tool, it is also necessary to separately install the [ffmpeg](https://ffmpeg.org/) tool by following the instructions for your operating system provided on [their website](https://ffmpeg.org/download.html).",bash,PROGLANG
"You can now run the tool by directly executing the `__main__.py` file, or—if you're using Linux or macOS—you can use the makefile to compile and then install an executable Python zip archive: ```bash make all sudo make install ```   ## Command-line Tool Usage  To run the program, execute it while passing the audio files as command-line arguments: ```bash sdistil files [files ...] >> playlist.txt ```  [librosa](https://librosa.org/doc/latest/index.html) is used to process the audio files, so most common audio file types are supported.",Python,PROGLANG
"Note that to obtain the preprocessed data and the learned PyTorch models and template curves, you will have to clone the repository without using the `--sparse` option: ```bash git clone git@github.com:dylanashley/story-distiller.git ```  At this time, the raw data used to train the movie frame extractor is not possible to release due to copyright issues.",bash,PROGLANG
"Tested in Docker, image = ```pytorch/pytorch:0.4_cuda9_cudnn7```.  ## General setup   Execute inside ```scripts/```:  ##### Create directories that aren't part of the Git repo (checkpoints/, outputs/):  ``` bash setup_dirs.sh ```  ##### Install python packages:  ``` bash install_python_pkgs.sh ```  ##### The default parameters for Tensorboard(x?)",python,PROGLANG
"Tested in Docker, image = ```pytorch/pytorch:0.4_cuda9_cudnn7```.  ## General setup   Execute inside ```scripts/```:  ##### Create directories that aren't part of the Git repo (checkpoints/, outputs/):  ``` bash setup_dirs.sh ```  ##### Install python packages:  ``` bash install_python_pkgs.sh ```  ##### The default parameters for Tensorboard(x?)",python,PROGLANG
"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g.",Python 3.10.x.,PROGLANG
"The python scripts included in this repository  should be able to read files from and write files to the resorce-folder.  ### 2) Preload MONDO ontology Run the [onto/load.py](onto/load.py) script: in the resource-folder a MONDO ontology pickle object file  exploited to store the contents of the ontology to support ontological knowledge infusion, will be  created.  ### 3) Generate synthetic definitions of MONDO ontology concepts - Add your OpenAI access credentials to prompt GPT-3.5-turbo in the module [synth_data_gen/constants.py](synth_data_gen/constants.py).",python,PROGLANG
"```shell cd /path/to/your/directory git clone https://github.com/FelixXu35/hamiltoniq.git cd hamiltoniq pip install -e . ```  ### Bechmark a backend  Simply copy and run the following Python code:  ```python from hamiltoniq.bechmark import Toniq  toniq = Toniq() backend = <your_backend> n_qubits = <your_prefered_number_of_qubits> n_layers = <your_prefered_number_of_layers> n_cores = <number_of_cores_in_your_PC>  score = toniq.simulator_run(backend=backend, n_qubits=n_qubits, n_layers=n_layers, n_cores=n_cores) ```  An example is given in [this notebook](.",shell,PROGLANG
"```shell cd /path/to/your/directory git clone https://github.com/FelixXu35/hamiltoniq.git cd hamiltoniq pip install -e . ```  ### Bechmark a backend  Simply copy and run the following Python code:  ```python from hamiltoniq.bechmark import Toniq  toniq = Toniq() backend = <your_backend> n_qubits = <your_prefered_number_of_qubits> n_layers = <your_prefered_number_of_layers> n_cores = <number_of_cores_in_your_PC>  score = toniq.simulator_run(backend=backend, n_qubits=n_qubits, n_layers=n_layers, n_cores=n_cores) ```  An example is given in [this notebook](.",Python,PROGLANG
"```shell cd /path/to/your/directory git clone https://github.com/FelixXu35/hamiltoniq.git cd hamiltoniq pip install -e . ```  ### Bechmark a backend  Simply copy and run the following Python code:  ```python from hamiltoniq.bechmark import Toniq  toniq = Toniq() backend = <your_backend> n_qubits = <your_prefered_number_of_qubits> n_layers = <your_prefered_number_of_layers> n_cores = <number_of_cores_in_your_PC>  score = toniq.simulator_run(backend=backend, n_qubits=n_qubits, n_layers=n_layers, n_cores=n_cores) ```  An example is given in [this notebook](.",python,PROGLANG
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",bash,PROGLANG
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",bash,PROGLANG
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",bash,PROGLANG
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",bash,PROGLANG
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",bash,PROGLANG
"The experiments in the paper are done on Python snippets, but the code preprocessing currently also supports java, javascript, and bash.",Python,PROGLANG
"The experiments in the paper are done on Python snippets, but the code preprocessing currently also supports java, javascript, and bash.",java,PROGLANG
"The experiments in the paper are done on Python snippets, but the code preprocessing currently also supports java, javascript, and bash.",javascript,PROGLANG
"The experiments in the paper are done on Python snippets, but the code preprocessing currently also supports java, javascript, and bash.",bash,PROGLANG
"Install the tree-sitter parsers (for preprocessing the code snippets): e.g., `codesearch install_parsers python java` or simply `codesearch install_parsers` to install parsers for all supported languages.",python,PROGLANG
"Install the tree-sitter parsers (for preprocessing the code snippets): e.g., `codesearch install_parsers python java` or simply `codesearch install_parsers` to install parsers for all supported languages.",java,PROGLANG
"The pretrained models also expose a consistent interface to embed snippets and queries:  #### Example: Query a snippet collection with a pretrained embedding model  ```python from codesearch.utils import load_model from codesearch.embedding_retrieval import EmbeddingRetrievalModel  query = ""plot a bar chart"" snippets = [{                           # a dummy snippet collection with 1 snippet     ""id"": ""1"",     ""description"": ""Hello world"",     ""code"": ""print('hello world')"",     ""language"": ""python""     }]  embedding_model = load_model(""use-embedder-pacs"") retrieval_model = EmbeddingRetrievalModel(embedding_model) retrieval_model.add_snippets(snippets) retrieval_model.query(query) ```  #### Example: Embed snippets or queries with a pre-trained embedding model  ```python from codesearch.utils import load_model  model_name = ""use-embedder-pacs"" queries = [""plot a bar chart""] snippets = [{     ""description"": ""Hello world"",     ""code"": ""print('hello world')"",     ""language"": ""python""     }]  embedding_model = load_model(model_name) query_embs = embedding_model.embed_queries(queries) snippet_embs = embedding_model.embed_snippets(snippets) ```  ### Available models  Below you find a table with the pretrained models.",python,PROGLANG
"The pretrained models also expose a consistent interface to embed snippets and queries:  #### Example: Query a snippet collection with a pretrained embedding model  ```python from codesearch.utils import load_model from codesearch.embedding_retrieval import EmbeddingRetrievalModel  query = ""plot a bar chart"" snippets = [{                           # a dummy snippet collection with 1 snippet     ""id"": ""1"",     ""description"": ""Hello world"",     ""code"": ""print('hello world')"",     ""language"": ""python""     }]  embedding_model = load_model(""use-embedder-pacs"") retrieval_model = EmbeddingRetrievalModel(embedding_model) retrieval_model.add_snippets(snippets) retrieval_model.query(query) ```  #### Example: Embed snippets or queries with a pre-trained embedding model  ```python from codesearch.utils import load_model  model_name = ""use-embedder-pacs"" queries = [""plot a bar chart""] snippets = [{     ""description"": ""Hello world"",     ""code"": ""print('hello world')"",     ""language"": ""python""     }]  embedding_model = load_model(model_name) query_embs = embedding_model.embed_queries(queries) snippet_embs = embedding_model.embed_snippets(snippets) ```  ### Available models  Below you find a table with the pretrained models.",python,PROGLANG
"The pretrained models also expose a consistent interface to embed snippets and queries:  #### Example: Query a snippet collection with a pretrained embedding model  ```python from codesearch.utils import load_model from codesearch.embedding_retrieval import EmbeddingRetrievalModel  query = ""plot a bar chart"" snippets = [{                           # a dummy snippet collection with 1 snippet     ""id"": ""1"",     ""description"": ""Hello world"",     ""code"": ""print('hello world')"",     ""language"": ""python""     }]  embedding_model = load_model(""use-embedder-pacs"") retrieval_model = EmbeddingRetrievalModel(embedding_model) retrieval_model.add_snippets(snippets) retrieval_model.query(query) ```  #### Example: Embed snippets or queries with a pre-trained embedding model  ```python from codesearch.utils import load_model  model_name = ""use-embedder-pacs"" queries = [""plot a bar chart""] snippets = [{     ""description"": ""Hello world"",     ""code"": ""print('hello world')"",     ""language"": ""python""     }]  embedding_model = load_model(model_name) query_embs = embedding_model.embed_queries(queries) snippet_embs = embedding_model.embed_snippets(snippets) ```  ### Available models  Below you find a table with the pretrained models.",python,PROGLANG
"The pretrained models also expose a consistent interface to embed snippets and queries:  #### Example: Query a snippet collection with a pretrained embedding model  ```python from codesearch.utils import load_model from codesearch.embedding_retrieval import EmbeddingRetrievalModel  query = ""plot a bar chart"" snippets = [{                           # a dummy snippet collection with 1 snippet     ""id"": ""1"",     ""description"": ""Hello world"",     ""code"": ""print('hello world')"",     ""language"": ""python""     }]  embedding_model = load_model(""use-embedder-pacs"") retrieval_model = EmbeddingRetrievalModel(embedding_model) retrieval_model.add_snippets(snippets) retrieval_model.query(query) ```  #### Example: Embed snippets or queries with a pre-trained embedding model  ```python from codesearch.utils import load_model  model_name = ""use-embedder-pacs"" queries = [""plot a bar chart""] snippets = [{     ""description"": ""Hello world"",     ""code"": ""print('hello world')"",     ""language"": ""python""     }]  embedding_model = load_model(model_name) query_embs = embedding_model.embed_queries(queries) snippet_embs = embedding_model.embed_snippets(snippets) ```  ### Available models  Below you find a table with the pretrained models.",python,PROGLANG
"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded.",python,PROGLANG
"Stack Overflow dumps can be found here: https://archive.org/details/stackexchange, [LICENSE](https://creativecommons.org/licenses/by-sa/4.0/)                                                             | | staqc-py-cleaned                     | Derived from the Python StaQC snippets (additional cleaning was done as decribed in the paper).",Python,PROGLANG
"**Note**: not all of these snippets have descriptions |  ### Evaluation data Evaluation datasets link queries to relevant snippets in one of the above snippet collections.   #### Example: load an evaluation dataset ```python from codesearch.data import load_eval_dataset queries, query2ids = load_eval_dataset(""so-ds-feb20-valid"") ```  #### Available evaluation datasets | name                           | description                                                                     | |--------------------------------|---------------------------------------------------------------------------------| | so-ds-feb20-{valid\|test}      | Queries paired to relevant snippets in the so-ds-feb20 snippet collection",python,PROGLANG
"Note that this only makes sense to evaluate code-only models (i.e., models that do not use the description field).  #### Example: load a snippet collection as evaluation data ```python queries, query2ids = load_eval_dataset(""codesearchnet-python-valid"") ```   ### Training data  The different models we implement use different kinds of training data.",python,PROGLANG
"To download and load the title pairs from Stack Overflow duplicate posts run:  ```python from codesearch.data import load_train_dataset duplicate_records = load_train_dataset(""so-duplicates-pacs-train"") ```  These duplicate records have been filtered to ensure that there is no overlap with the `so-ds-feb20` and `staqc-py` evaluation datasets.",python,PROGLANG
"To download a text file with Stack Overflow post titles tagged with Python (used for the TNBOW baseline) run:   ```python from codesearch.data import load_train_dataset filename = load_train_dataset(""so-python-question-titles-feb20"") ```  ## Demo notebook   You can run the demo notebook `nbs/demo/demo.ipynb` to quickly try out any of the pretrained models on one of the snippet collections.  ## Benchmark on PACS  To replicate the results of our paper or evaluate your own model on the PACS benchmark, have a look at `nbs/evaluate.ipynb` and `codesearch/benchmark.ipynb`.",Python,PROGLANG
"To download a text file with Stack Overflow post titles tagged with Python (used for the TNBOW baseline) run:   ```python from codesearch.data import load_train_dataset filename = load_train_dataset(""so-python-question-titles-feb20"") ```  ## Demo notebook   You can run the demo notebook `nbs/demo/demo.ipynb` to quickly try out any of the pretrained models on one of the snippet collections.  ## Benchmark on PACS  To replicate the results of our paper or evaluate your own model on the PACS benchmark, have a look at `nbs/evaluate.ipynb` and `codesearch/benchmark.ipynb`.",python,PROGLANG
"A custom embedding model class should implement the `embed_snippets` and `embed_queries` functions (similar to `codesearch/tuse/tuse_embedder.py`, `codesearch/tnbow/tnbow_embedder.py`, `codesearch/ncs/ncs_embedder.py` etc.).  #### Example: Benchmark a model on PACS  ```python from codesearch.benchmark import benchmark_on_pacs  benchmark_on_pacs(     model_path=model_path, # one of the pretrained model names or a path to a model that can be loaded with `codesearch.utils.load_model`     output_dir=output_dir ) ```",python,PROGLANG
"[Multilevel Splitting](https://repository-images.githubusercontent.com/243241472/797da100-5891-11ea-857f-0cca52af9239 ""Multilevel Splitting"")   ## Setup Clone the repository and run `setup.py` ```bash git clone https://github.com/alpiges/LinConGauss.git ~/LinConGauss cd ~/LinConGauss python setup.py install ```  ## Usage For usage, please refer to the tutorials in the `notebook` section.  ## How to cite If you are using `LinConGauss` for your research, consider citing the [paper](https://arxiv.org/abs/1910.09328)  ``` @inproceedings{GessnerKH2020,     title     = {Integrals over Gaussians under Linear Domain Constraints},     author    = {Alexandra Gessner and Oindrila Kanjilal and Philipp Hennig},     booktitle = {Proceedings of Machine Learning Research},     publisher = {PMLR},     year      = {2020},     url       = {https://arxiv.org/abs/1910.09328} } ```",bash,PROGLANG
```bash    docker pull lizytalk/dejavu    ``` 2.,bash,PROGLANG
Pull the code from GitHub    ```bash    git pull https://github.com/NetManAIOps/DejaVu.git DejaVu    ``` 3.,bash,PROGLANG
Start a Docker container with our image and enter its shell    ```bash    docker run -it --rm -v $(realpath DejaVu):/workspace lizytalk/dejavu bash    ``` 6.,bash,PROGLANG
Note that the pickle files are not compatible in different Python and Pandas versions.,Python,PROGLANG
Extract and place the `WIDER` folder in the same directory as the `EXTD` and `EResFD` folders.  ## Dependencies  A requirements.txt file is provided with all the necessary python dependencies.,python,PROGLANG
"Additionally, the code was developed using Python 3.11.7, CUDA 11.4 and Ubuntu 20.04.06 LTS.",CUDA 11.4,PROGLANG
"To ensure compatibility and proper functionality of the pruning scripts, please install the specific versions of the python packages listed in the requirements.txt file, using the following command:  ```bash pip install -r requirements.txt ```  ## Running the Scripts for Pruning a Face Detector  The pruning script executes the model pruning process as outlined in Section 4.2 of our paper.",bash,PROGLANG
"E.g., for pruning with the Geometric Median (FPGM) algorithm the EResFD model:   ```bash   python fpgm.py --pruning_rate 0.1 --pruned_eres '.",bash,PROGLANG
style=flat)](https://github.com/mmp2/megaman/blob/master/LICENSE)  ``megaman`` is a scalable manifold learning package implemented in python.,python,PROGLANG
It has a front-end API designed to be familiar to [scikit-learn](http://scikit-learn.org/) but harnesses the C++ Fast Library for Approximate Nearest Neighbors (FLANN) and the Sparse Symmetric Positive Definite (SSPD) solver Locally Optimal Block Precodition Gradient (LOBPCG) method to scale manifold learning algorithms to large data sets.,C++,PROGLANG
"Package documentation can be found at http://mmp2.github.io/megaman/  If you use our software please cite the following JMLR paper:  McQueen, Meila, VanderPlas, & Zhang, ""Megaman: Scalable Manifold Learning in Python"", Journal of Machine Learning Research, Vol 17 no. 14, 2016. http://jmlr.org/papers/v17/16-109.html  You can also find our arXiv paper at http://arxiv.org/abs/1603.02763  ## Examples  - [Tutorial Notebook]( https://github.com/mmp2/megaman/blob/master/examples/megaman_tutorial.ipynb)  ## Installation and Examples in Google Colab  Below it's a tutorial to install megaman on Google Colab, through Conda environment.",Python,PROGLANG
"-- The easiest way to install ``megaman`` and its dependencies is with [conda](http://conda.pydata.org/miniconda.html), the cross-platform package manager for the scientific Python ecosystem.",Python,PROGLANG
"To install megaman and its dependencies, run  ``` $ conda install megaman --channel=conda-forge ```  Currently builds are available for OSX and Linux, on Python 2.7, 3.4, and 3.5.",Python 2.7,PROGLANG
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",python,PROGLANG
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",C++,PROGLANG
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",shell,PROGLANG
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",shell,PROGLANG
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",shell,PROGLANG
"NetCov is written in Python and can be used in concert with [pybatfish](https://pybatfish.readthedocs.io/en/latest/notebooks/interacting.html), Batfish's Python API.",Python,PROGLANG
"NetCov is written in Python and can be used in concert with [pybatfish](https://pybatfish.readthedocs.io/en/latest/notebooks/interacting.html), Batfish's Python API.",Python,PROGLANG
A virtual environment and Python version 3.7 is recommended.,Python version 3.7,PROGLANG
"For import, replace pybatfish client session with the one provided by NetCov: ```python #from pybatfish.client.session import Session from netcov import NetCovSession as Session ``` 2.",python,PROGLANG
"To save the coverage report to file, you can customize logger by: ```python import logging fh = logging.FileHandler('cov.log') logging.getLogger('netcov').addHandler(fh) ```  ## References ``` @inproceedings {netcov-nsdi-2023,   author = {Xieyang Xu and Weixin Deng and Ryan Beckett and Ratul Mahajan and David Walker},   title = {Test Coverage for Network Configurations},   booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},   year = {2023},   isbn = {978-1-939133-33-5},   address = {Boston, MA},   pages = {1717--1732},   url = {https://www.usenix.org/conference/nsdi23/presentation/xu},   publisher = {USENIX Association},   month = apr, }```",python,PROGLANG
Clone the repository     ```bash     git clone https://github.com/lojzezust/SLR     cd SLR     ``` 2.,bash,PROGLANG
Install the requirements     ```bash     pip install -r requirements.txt     ``` 3.,bash,PROGLANG
```bash     pip install -e .     ``` 4.,bash,PROGLANG
```bash     python tools/prepare_data.py     ```     The preparation script performs the following operations:     1.,bash,PROGLANG
"```bash chmod +x tools/train_slr.sh tools/train_slr.sh ```  The script contains the following variables, that can be changed to achieve the desired results",bash,PROGLANG
"Used for saving logs and weights. - `BATCH_SIZE`: Batch size per gpu. - `WARMUP_EPOCHS`: Number of epochs for the warm-up phase. - `FINETUNE_EPOCHS`: Number of epochs for the fine-tuning phase. - `NUM_ITER`: Number of iterations of the SLR pseudo label estimation and fine-tuning.  ### Individual training steps  Individual steps of the SLR pipeline can also be executed separately, with the following python scripts.  #### Step I: Feature warm-up  Train an initial model on partial labels generated from weak annotations and IMU.",python,PROGLANG
"```bash export CUDA_VISIBLE_DEVICES=0,1 python tools/train.py warmup \ --architecture wasr_resnet101_imu \ --model-name wasr_slr_warmup \ --batch-size 4 ```  > [!",bash,PROGLANG
"```bash export CUDA_VISIBLE_DEVICES=0,1 python tools/generate_pseudo_labels.py \ --architecture wasr_resnet101_imu \ --weights-file output/logs/wasr_slr_warmup/version_0/checkpoints/last.ckpt \ --output-dir output/pseudo_labels/wasr_slr_warmup_v0 ```  This creates the pseudo-labels and stores them into `output/pseudo_labels/wasr_slr_warmup_v0`",bash,PROGLANG
"```bash export CUDA_VISIBLE_DEVICES=0,1 python tools/train.py finetune \ --architecture wasr_resnet101_imu \ --model-name wasr_slr \ --batch-size 4 \ --pretrained-weights output/logs/wasr_slr_warmup/version_0/checkpoints/last.ckpt \ --mask-dir output/pseudo_labels/wasr_slr_warmup_v0 ```  > [!",bash,PROGLANG
"```bash export CUDA_VISIBLE_DEVICES=0,1 python tools/general_inference.py \ --architecture wasr_resnet101 \ --weights-file output/logs/wasr_slr_v2_it1/version_0/checkpoints/last.ckpt \ --image-dir data/example_dir \ --output-dir output/predictions/test_predictions ```  Additionally, `--imu-dir` can be used to supply a directory with corresponding IMU horizon masks.",bash,PROGLANG
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",python,PROGLANG
"View example usage and sample document matches here: [`examples/demo-contextualsentence-multim.ipynb`](https://github.com/allenai/aspire/blob/main/examples/demo-contextualsentence-multim.ipynb)  ##### `SPECTER-CoCite`  The `SPECTER-CoCite` bi-encoder model can be used via the `transformers` library as:  ```python from transformers import AutoModel, AutoTokenizer aspire_bienc = AutoModel.from_pretrained('allenai/aspire-biencoder-compsci-spec') aspire_tok = AutoTokenizer.from_pretrained('allenai/aspire-biencoder-compsci-spec') title = ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific ""         ""Document Similarity"" abstract = ""We present a new scientific document similarity model based on matching ""            ""fine-grained aspects of texts."" d=[title + aspire_tok.sep_token + abstract] inputs = aspire_tok(d, padding=True, truncation=True, return_tensors=""pt"", max_length=512) result = aspire_bienc(**inputs) clsrep = result.last_hidden_state[:, 0, :] ```  However, note that the Hugging Face models don't have a set of additional scalar-mix parameters to compute a learned weighted sum of the representations from different layers of the transformer encoder.",python,PROGLANG
"Obtain the model zip files:  - [`aspire-biencoder-biomed-scib-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-scib-full.zip) - [`aspire-biencoder-biomed-spec-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-spec-full.zip) - [`aspire-biencoder-compsci-spec-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip)  ```bash wget -O aspire-biencoder-compsci-spec-full.zip https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip unzip aspire-biencoder-compsci-spec-full.zip ```  Now it may be used as:  ```python  import os, json, codecs, torch from transformers import AutoTokenizer from examples.ex_aspire_bienc import AspireBiEnc  # Directory where zipped model was downloaded and unzipped. model_path = '.",bash,PROGLANG
"Obtain the model zip files:  - [`aspire-biencoder-biomed-scib-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-scib-full.zip) - [`aspire-biencoder-biomed-spec-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-biomed-spec-full.zip) - [`aspire-biencoder-compsci-spec-full`](https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip)  ```bash wget -O aspire-biencoder-compsci-spec-full.zip https://ai2-s2-research.s3.us-west-2.amazonaws.com/aspire/aspire-biencoder-compsci-spec-full.zip unzip aspire-biencoder-compsci-spec-full.zip ```  Now it may be used as:  ```python  import os, json, codecs, torch from transformers import AutoTokenizer from examples.ex_aspire_bienc import AspireBiEnc  # Directory where zipped model was downloaded and unzipped. model_path = '.",python,PROGLANG
