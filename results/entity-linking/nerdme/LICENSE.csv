sentence,entity_text,entity_type
"License =========  MIT License  Copyright (c) 2019 Massimiliano Patacchiola, Patrick Fox-Roberts, Edward Rosten  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",MIT License,LICENSE
"`t5-base`) - `do_train` (bool): whether to run training (default is False) - `do_eval` (bool): whether to run evaluation on the `dev` set (default is False) - `do_predict` (bool): whether to run evaluation on the `test` set (default is False) - `train_split` (str): comma-separated list of data splits for training (default is `train`) - `num_train_epochs` (int): number of train epochs - `learning_rate` (float): initial learning rate (default is 5e-4) - `train_subset` (float > 0 and <=1): portion of training data to effectively use during training (default is 1, i.e., use all training data) - `per_device_train_batch_size` (int): batch size per GPU during training (default is 8) - `per_device_eval_batch_size` (int): batch size during evaluation (default is 8; only one GPU is used for evaluation) - `max_seq_length` (int): maximum input sequence length after tokenization; longer sequences are truncated - `max_output_seq_length` (int): maximum output sequence length (default is `max_seq_length`) - `max_seq_length_eval` (int): maximum input sequence length for evaluation (default is `max_seq_length`) - `max_output_seq_length_eval` (int): maximum output sequence length for evaluation (default is `max_output_seq_length` or `max_seq_length_eval` or `max_seq_length`) - `episodes` (str): episodes to run (default is `0`; an interval can be specified, such as `1-4`; the episode number is used as the random seed) - `num_beams` (int): number of beams for beam search during generation (default is 1) - `multitask` (bool): if True, the name of the dataset is prepended to each input sentence (default is False)  See [arguments.py](arguments.py) and [transformers.TrainingArguments](https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py) for additional config arguments.   ## Fine-tuned multi-task model  The weights of our multi-task model (released under the [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/)) can be downloaded here: https://tanl.s3.amazonaws.com/tanl-multitask.zip  Extract the zip file in the `experiments/` directory.",CC BY 4.0,LICENSE
The results differ slightly from what is reported in the paper due to small code changes.   ## Licenses  The code of this repository is released under the [Apache 2.0 license](LICENSE).,Apache 2.0,LICENSE
The weights of the [fine-tuned multi-task model](#fine-tuned-multi-task-model) are released under the [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/).,CC BY 4.0,LICENSE
"v=x4dIx9VYQoM) --->   ## Citing FinGPT ``` @article{yang2023fingpt,   title={FinGPT: Open-Source Financial Large Language Models},   author={Yang, Hongyang and Liu, Xiao-Yang and Wang, Christina Dan},   journal={FinLLM Symposium at IJCAI 2023},   year={2023} } @article{zhang2023instructfingpt,       title={Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models},        author={Boyu Zhang and Hongyang Yang and Xiao-Yang Liu},       journal={FinLLM Symposium at IJCAI 2023},       year={2023} } @article{zhang2023fingptrag,   title={Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models},   author={Zhang, Boyu and Yang, Hongyang and Zhou, tianyu and Babar, Ali and Liu, Xiao-Yang},  journal = {ACM International Conference on AI in Finance (ICAIF)},   year={2023} }  @article{wang2023fingptbenchmark,   title={FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets},   author={Wang, Neng and Yang, Hongyang and Wang, Christina Dan},   journal={NeurIPS Workshop on Instruction Tuning and Instruction Following},   year={2023} } @article{2023finnlp,   title={Data-centric FinGPT: Democratizing Internet-scale Data for Financial Large Language Models},   author={Liu, Xiao-Yang and Wang, Guoxuan and Yang, Hongyang and Zha, Daochen},   journal={NeurIPS Workshop on Instruction Tuning and Instruction Following},   year={2023} }  ```  <div align=""center""> <a href=""https://finllm.github.io/workshop/#/fcb"" target=""_blank""> <img align=""center"" src=figs/fingpt_best_presentation.png width=""65%""> </div>   ## LICENSE  MIT License  **Disclaimer: We are sharing codes for academic purposes under the MIT education license.",MIT License,LICENSE
"/scripts/deploy_baseline.sh [GPU_ID] [DATASET] [LEVEL] ---- GPU_ID: required, 0-based int,  DATASET: required, 'CUB_200_2011' or 'Butterfly200' or 'Vegfru' LEVEL: require,      CUB_200_2011: LEVEL is chosen in ['order', 'family', 'genus', 'class']     Butterfly200: LEVEL is chosen in ['family', 'subfamily', 'genus', 'species']     Vegfru: LEVEL is chosen in ['sup', 'sub'] ```  # License The code is released under the SYSU License (refer to the LICENSE file for details).",SYSU License,LICENSE
The content in this repository is licensed under the MIT license.,MIT,LICENSE
"This repo started as a fork of [Ph0bi0/T_DEEP](https://github.com/Ph0bi0/T_DEEP), their repo, which contains the source code and the dataset of their paper.  ## Citation  If you use the code or data in an academic context, please cite the following work:  ```bibtex @article{LaRocque2024BorealTC,     title={Proprioception Is All You Need: Terrain Classification for Boreal Forests},     author={Damien LaRocque and William Guimont-Martin and David-Alexandre Duclos and Philippe GiguÃ¨re and FranÃ§ois Pomerleau},     journal={arXiv preprint arXiv:2403.16877},     year={2024},     eprint={2403.16877},     archivePrefix={arXiv},     primaryClass={cs.RO} } ```  ## License  This project is licensed under a [MIT](LICENSE) license.",MIT,LICENSE
"Related Mashups ---------------- [Ontology-based math formula search](https://github.com/CLLKazan/MathSearch)  Links to External LOD Datasets ------------------------------ * [A mapping to DBpedia](https://github.com/CLLKazan/OntoMathPro/blob/master/external.links.dbpedia.nt) * [A mapping to ScienceWISE](https://github.com/CLLKazan/OntoMathPro/blob/master/external.links.sciencewise.nt)    License ---------------------  Licensed under the Apache License, Version 2.0: [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)","Apache License, Version 2.0",LICENSE
[license](https://img.shields.io/badge/License-BSD+MIT-green.svg?,BSD,LICENSE
[license](https://img.shields.io/badge/License-BSD+MIT-green.svg?,MIT,LICENSE
"Run training in docker container     ```     docker run --gpus all -v ""$(pwd)/data/processed:/data/processed"" -v ""$(pwd)/log:/log"" -v ""$(pwd)/exp_files/train_final_model.py:/exp_file.py"" -e WANDB_API_KEY=<your-api-key> --shm-size=500m train_model     ```   ## Project structure  The directory structure of the project looks like this:  ```txt  â”œâ”€â”€ Makefile             <- Makefile with convenience commands like `make data` â”œâ”€â”€ README.md            <- The top-level README for developers using this project. â”œâ”€â”€ data â”‚   â”œâ”€â”€ processed        <- The final, canonical data sets for modeling. â”‚   â””â”€â”€ raw              <- The original, immutable data dump. â”‚ â”œâ”€â”€ exp_files            <- files to define the experiment configuration | â”œâ”€â”€ models               <- checkpoint models â”‚ â”œâ”€â”€ pyproject.toml       <- Project configuration file â”‚ â”œâ”€â”€ requirements.txt     <- The requirements file for reproducing the analysis environment | â”œâ”€â”€ requirements_dev.txt <- The requirements file for reproducing the analysis environment â”‚ â”œâ”€â”€ tests                <- Test files â”‚ â”œâ”€â”€ rumexleaves_centernet  <- Source code for use in this project. â”‚ â”œâ”€â”€ submodules          <- relevant submodules are stored here â”‚ â””â”€â”€ LICENSE              <- MIT License ``` Created using [mlops_template](https://github.com/SkafteNicki/mlops_template), a [cookiecutter template](https://github.com/cookiecutter/cookiecutter) for getting started with Machine Learning Operations (MLOps).  ## Citation  If you find this work useful in your research, please cite: ``` @article{RumexLeaves-CenterNet, author = {GÃ¼ldenring, Ronja and Andersen, Rasmus Eckholdt and Nalpantidis, Lazaros}, title = {Zoom in on the Plant: Fine-grained Analysis of Leaf, Stem and Vein Instances}, journal = {IEEE Robotics and Automation Letters (RA-L)}, year = {2024} } ```  ## References Our code is partially based on the following code bases. * [CenterNet](https://github.com/xingyizhou/CenterNet) * [YOLOX](https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX) * [Deformabel Convolutions v2 (pytorch)](https://github.com/developer0hye/PyTorch-Deformable-Convolution-v2)",MIT License,LICENSE
"It is licensed under the terms of the GNU General Public License v3.0 license.  ## Cite  ### [Dataset Description Article](https://www.researchgate.net/publication/373767449_SC2EGSet_StarCraft_II_Esport_Replay_and_Game-state_Dataset)  To cite the article that introduces [SC2ReSet](https://doi.org/10.5281/zenodo.5575796) and [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) use this:  ```bibtex @article{BiaÅ‚ecki2023,   author   = {Bia{\l}ecki, Andrzej               and Jakubowska, Natalia               and Dobrowolski, Pawe{\l}               and Bia{\l}ecki, Piotr               and Krupi{\'{n}}ski, Leszek               and Szczap, Andrzej               and Bia{\l}ecki, Robert               and Gajewski, Jan},   title    = {SC2EGSet: StarCraft II Esport Replay and Game-state Dataset},   journal  = {Scientific Data},   year     = {2023},   month    = {Sep},   day      = {08},   volume   = {10},   number   = {1},   pages    = {600},   issn     = {2052-4463},   doi      = {10.1038/s41597-023-02510-7},   url      = {https://doi.org/10.1038/s41597-023-02510-7} } ```",GNU General Public License v3.0,LICENSE
License  DSM is released under a [GPLv3 license](https://github.com/jzubizarreta/dsm/blob/master/License-gpl.txt).,GPLv3,LICENSE
In: Proceedings of the 2022 ACM Conference on Learning at Scale (L@S 2022).  ```  ## License This code is free software: you can redistribute it and/or modify it under the terms of the [MIT License](LICENSE).,MIT License,LICENSE
See the [MIT License](LICENSE) for details.,MIT License,LICENSE
"XTR is also available in [Huggingface](https://huggingface.co/google/xtr-base-en) thanks to [Mujeen Sung](https://github.com/mjeensung).  ## Citing this work  ```bibtex @article{lee2024rethinking,   title={Rethinking the role of token retrieval in multi-vector retrieval},   author={Lee, Jinhyuk and Dai, Zhuyun and Duddu, Sai Meher Karthik and Lei, Tao and Naim, Iftekhar and Chang, Ming-Wei and Zhao, Vincent},   journal={Advances in Neural Information Processing Systems},   volume={36},   year={2024} } ```  ## License and disclaimer  Copyright 2024 DeepMind Technologies Limited  All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license.",Apache License,LICENSE
"XTR is also available in [Huggingface](https://huggingface.co/google/xtr-base-en) thanks to [Mujeen Sung](https://github.com/mjeensung).  ## Citing this work  ```bibtex @article{lee2024rethinking,   title={Rethinking the role of token retrieval in multi-vector retrieval},   author={Lee, Jinhyuk and Dai, Zhuyun and Duddu, Sai Meher Karthik and Lei, Tao and Naim, Iftekhar and Chang, Ming-Wei and Zhao, Vincent},   journal={Advances in Neural Information Processing Systems},   volume={36},   year={2024} } ```  ## License and disclaimer  Copyright 2024 DeepMind Technologies Limited  All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license.",Apache 2.0,LICENSE
"XTR is also available in [Huggingface](https://huggingface.co/google/xtr-base-en) thanks to [Mujeen Sung](https://github.com/mjeensung).  ## Citing this work  ```bibtex @article{lee2024rethinking,   title={Rethinking the role of token retrieval in multi-vector retrieval},   author={Lee, Jinhyuk and Dai, Zhuyun and Duddu, Sai Meher Karthik and Lei, Tao and Naim, Iftekhar and Chang, Ming-Wei and Zhao, Vincent},   journal={Advances in Neural Information Processing Systems},   volume={36},   year={2024} } ```  ## License and disclaimer  Copyright 2024 DeepMind Technologies Limited  All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license.",Apache 2.0 license,LICENSE
You may obtain a copy of the Apache 2.0 license at: https://www.apache.org/licenses/LICENSE-2.0  All other materials are licensed under the Creative Commons Attribution 4.0 International License (CC-BY).,Apache 2.0,LICENSE
You may obtain a copy of the Apache 2.0 license at: https://www.apache.org/licenses/LICENSE-2.0  All other materials are licensed under the Creative Commons Attribution 4.0 International License (CC-BY).,Creative Commons Attribution 4.0 International License,LICENSE
You may obtain a copy of the Apache 2.0 license at: https://www.apache.org/licenses/LICENSE-2.0  All other materials are licensed under the Creative Commons Attribution 4.0 International License (CC-BY).,CC-BY,LICENSE
"You may obtain a copy of the CC-BY license at: https://creativecommons.org/licenses/by/4.0/legalcode  Unless required by applicable law or agreed to in writing, all software and materials distributed here under the Apache 2.0 or CC-BY licenses are distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",CC-BY,LICENSE
"You may obtain a copy of the CC-BY license at: https://creativecommons.org/licenses/by/4.0/legalcode  Unless required by applicable law or agreed to in writing, all software and materials distributed here under the Apache 2.0 or CC-BY licenses are distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",Apache 2.0,LICENSE
"You may obtain a copy of the CC-BY license at: https://creativecommons.org/licenses/by/4.0/legalcode  Unless required by applicable law or agreed to in writing, all software and materials distributed here under the Apache 2.0 or CC-BY licenses are distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",CC-BY,LICENSE
"- ðŸ› ï¸  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ðŸ¤—  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ðŸ¤—  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",Apache License 2.0,LICENSE
"usp=sharing).  ## License  This work is under the [Apache 2.0 license](LICENSE).  ## Citation  If you find this work helpful, please kindly consider citing our paper:  ```bibtex @article{xu2023frnet,     title = {FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation},     author = {Xu, Xiang and Kong, Lingdong and Shuai, Hui and Liu, Qingshan},     journal = {arXiv preprint arXiv:2312.04484},     year = {2023} } ```  ## Acknowledgements  This work is developed based on the [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) codebase",Apache 2.0,LICENSE
"The resulting *Ontologies* for these datasets can be downloaded and explored here: - **WIDER Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/b1c2f92b-4b69-46fc-9282-16acc7a1c9aa/download/wider.tar.gz) | [explore](https://tibhannover.github.io/VisE/WIDER/index.html) - **SocEID Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/a8373c98-32a8-408c-b8e9-51e6b1e01777/download/soceid.tar.gz) | [explore](https://tibhannover.github.io/VisE/SocEID/index.html) - **RED Ontology**: [download](https://data.uni-hannover.de/dataset/3afb333d-230f-4829-91bb-d4dd41bfdcfa/resource/d0f5cd8b-7c3e-4055-9810-f9cba2b69a33/download/red.tar.gz) | [explore](https://tibhannover.github.io/VisE/RED/index.html)  ## Supplemental Material  Detailed information on the sampling strategy to gather event images, statistics for the training and testing datasets presented in Section 3.3, and results using different inference strategies (Section 4.2.3) are available in the [vise_supplemental.pdf](vise_supplemental.pdf).    ## LICENSE  This work is published under the GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007.",GNU GENERAL PUBLIC LICENSE Version 3,LICENSE
"v=Va_aZYxRu3U)  ## License FALCON is released under the [AGPLv3 license](LICENSE).   ## Key Features * Reliable real-time monitoring public LTE cells * Monitoring up to 20 MHz bandwidth * FDD only * Supported DCI formats: 0/1A, 1, 1B, 1C, 2, 2A, 2B * Suitable for short-term and long-term monitoring with non-ideal radio conditions * Qt-based and OpenGL-accelerated GUI for visualization of allocated resource blocks, spectrogram and cell-specific performance metrics (throughput, resource utilization, user activity, etc.). * Synchronized recorder with integrated support for network probing by an auxiliary modem  Check the [changelog](CHANGELOG.md) for recently introduced updates.  ### Planned Features * TDD * Support for DCI with Carrier Indicator Field (CIF) * Multithreaded DCI search * Visualization of System Information Blocks (SIB)  ## Installation  Installation has been verified on the following operating systems:  * Ubuntu 18.04.x LTS (Bionic Beaver) * Ubuntu 20.04.x LTS (Focal Fossa) * Archlinux [!",AGPLv3 license,LICENSE
"[9] Demirel, Emir and Ahlb{\""a}ck, Sven and Dixon, Simon, ""Automatic lyrics transcription using dilated convolutional neural networks with self-attention""In IEEE - IJCNN 2020  ### Important Notice: This work is licensed under Creative Commons - Attribution-NonCommercial-ShareAlike 4.0 International, which means that the reusers can copy, distribute, remix, transform and build upon the material in any media providing the appropriate credits to this repository and to be used for non-commercial purposes.",Creative Commons - Attribution-NonCommercial-ShareAlike 4.0 International,LICENSE
Read more on the compatibility with SBML-FBCv2 `here <https://opencobra.github.io/cobratoolbox/docs/notes.html>`__.  .. end-binaries-marker  Disclaimer ----------  *The software provided by the openCOBRA Project is distributed under the GNU GPLv3 or later.,GNU GPLv3,LICENSE
"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     # --model__cache_dir /vector_cache # if you use my docker image ```  It takes about 4 hours in a single V100 with mixed precision training.  ## How to cite  ``` @inproceedings{chun2021pcme,     title={Probabilistic Embeddings for Cross-Modal Retrieval},     author={Chun, Sanghyuk and Oh, Seong Joon and De Rezende, Rafael Sampaio and Kalantidis, Yannis and Larlus, Diane},     year={2021},     booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, } ```  I would like to suggest citing [ECCV Caption](https://github.com/naver-ai/eccv-caption) and [PCME++](https://github.com/naver-ai/pcmepp), too. ``` @inproceedings{chun2022eccv_caption,     title={ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO},      author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},     year={2022},     booktitle={European Conference on Computer Vision (ECCV)}, }  @inproceedings{chun2024pcmepp,     title={Improved Probabilistic Image-Text Representations},     author={Chun, Sanghyuk},     year={2024},     booktitle={International Conference on Learning Representations (ICLR)}, } ```  ## License  ``` MIT License  Copyright (c) 2021-present NAVER Corp.",MIT License,LICENSE
| Attribute}  | Human AUC | BERT AUC | |------------ |-----------|----------| |Antagonistic |0.71       |  0.82    |  |Condescending| 0.72      |    0.78  | |Dismissive   | 0.68      | 0.82     | |Generalisation | 0.73    | 0.74     | |Hostile       |0.76      |0.84      | |Sarcastic     |0.72      |0.64      | |Unhealthy     | 0.62     | 0.69   |   Code for  this analysis is in `AUC_analysis.ipynb`.  _____   This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. https://creativecommons.org/licenses/by-nc-sa/4.0/,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License,LICENSE
"You will likely need to customize them, along with the launcher, to your compute environment.  ### Step 3: Aggregating Results  We provide sample code for creating aggregate results for an experiment in `notebooks/AggResults.ipynb`.    ## License This source code is released under the MIT license, included [here](LICENSE).",MIT,LICENSE
usp=sharing) |  ## Video Comparisons The overall flow of our VISOLO and the comparison of different VIS methods on the YouTube-VIS 2019 dataset are provided at https://youtu.be/j33H7vcJ2uU  ## License  VISOLO is released under the [Apache 2.0 license](LICENSE).,Apache 2.0,LICENSE
"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions.",Attribution-ShareAlike 4.0 International,LICENSE
"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions.",CC BY-SA 4.0,LICENSE
[](imgs/cmu_logo2.png)  ## License  This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.  ## Acknowledgments  This work was supported in part by U.S.,MIT License,LICENSE
"Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083).  ### License  Fast R-CNN is released under the MIT License (refer to the LICENSE file for details).  ### Citing Fast R-CNN  If you find Fast R-CNN useful in your research, please consider citing:      @article{girshick15fastrcnn,         Author = {Ross Girshick},         Title = {Fast R-CNN},         Journal = {arXiv preprint arXiv:1504.08083},         Year = {2015}     }      ### Contents 1.",MIT License,LICENSE
[License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://github.com/qiangbo1222/HierDiff/blob/main/LICENSE)  [!,MIT,LICENSE
Run the following scripts to create your own dataset.  ``` cd dataset #create data for pretraining python clean_all_reaction_for_pretrain.py  #create data for generative model training python create_dataset.py  ```  ### Training ``` python trainer_pretrainig_graph_pl.py python trainer_set_vae.py ```    ## License  This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.,MIT License,LICENSE
"[Documentation](https://google.github.io/highway/en/master/)  Previously licensed under Apache 2, now dual-licensed as Apache 2 / BSD-3.  ## Why  We are passionate about high-performance software.",Apache 2,LICENSE
"[Documentation](https://google.github.io/highway/en/master/)  Previously licensed under Apache 2, now dual-licensed as Apache 2 / BSD-3.  ## Why  We are passionate about high-performance software.",Apache 2,LICENSE
"[Documentation](https://google.github.io/highway/en/master/)  Previously licensed under Apache 2, now dual-licensed as Apache 2 / BSD-3.  ## Why  We are passionate about high-performance software.",BSD-3,LICENSE
[License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [!,MIT,LICENSE
[License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [!,MIT,LICENSE
[License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [!,MIT,LICENSE
"[license](https://img.shields.io/badge/License-Apache--2.0-blue)](LICENSE)  > Authors: Tian Qiu, Linyun Zhou, Wenxiang Xu, Lechao Cheng, Zunlei Feng, Mingli Song   > Affiliation: Zhejiang University   > Paper Link: [[arXiv]](https://arxiv.org/abs/2302.07116) / [[IEEE ICIP]](https://ieeexplore.ieee.org/document/10222890)  ## Abstract  Recent proposed DETR variants have made tremendous progress in various scenarios due to their streamlined processes and remarkable performance.",License-Apache--2.0,LICENSE
"Ni, Heung-Yeung Shum     International Conference on Learning Representations (ICLR) 2023      [[Paper]](https://arxiv.org/abs/2203.03605) [[Code]](https://github.com/IDEACVR/DINO)  ## License  Team DETR is released under the Apache 2.0 license.",Apache 2.0,LICENSE
"Licensed under the Apache License, Version 2.0 (the ""License""); you may not use these files except in compliance with the License.","Apache License, Version 2.0",LICENSE
[License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT) [!,MIT,LICENSE
[License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT) [!,MIT,LICENSE
"-- LICENSE --> ## License Distributed under the BSD 3-Clause ""New"" or ""Revised"" License.",BSD 3-Clause,LICENSE
"The analysis of those outcomes and the generation of the results was then performed locally.  ## Citations  If you use this repository in your research, please refer to the [citation file](https://github.com/andleb/Approaching-an-unknown-communication-system/blob/main/CITATION.cff) (for the code) and [the paper](https://arxiv.org/abs/2303.10931) in general.  ## License  All content in this repository is licensed under the [BSD 3-Clause License](LICENSE).",BSD 3-Clause License,LICENSE
The network is trained under fully unsupervised manner._**   ## License TUNIT is distributed under MIT unless the header specifies another license.   ``` Copyright (c) 2020-present NAVER Corp.,MIT,LICENSE
"IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORTd OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. ```  The pretrained models is covered by Creative Commons BY-NC 4.0 license by NAVER Corporation.",Creative Commons BY-NC 4.0,LICENSE
[License CC BY-NC](https://img.shields.io/badge/license-CC_BY--NC-DodgerBlue.svg?,License CC BY-NC,LICENSE
[License CC BY-NC](https://img.shields.io/badge/license-CC_BY--NC-DodgerBlue.svg?,CC_BY--NC,LICENSE
"Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution.",Contributor License Agreement,LICENSE
"Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution.",CLA,LICENSE
"When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment).",CLA,LICENSE
You will only need to do this once across all repos using our CLA.,CLA,LICENSE
"Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 2022, [download](https://arxiv.org/abs/2206.07360).*  ```bib @inproceedings{hafid2022scitweets,   title={SciTweets-A Dataset and Annotation Framework for Detecting Scientific Online Discourse},   author={Hafid, Salim and Schellhammer, Sebastian and Bringay, Sandra and Todorov, Konstantin and Dietze, Stefan},   booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},   pages={3988--3992},   year={2022} } ```  ## Licensing This dataset is published under CC BY 4.0 license.",CC BY 4.0,LICENSE
"Similar to ThingML/HEADS, ML2 is built using the [Eclipse Modeling Framework (EMF)](https://www.eclipse.org/modeling/emf/), as well as the [Xtext framework](https://www.eclipse.org/Xtext/), and is released under the terms of the Apache 2.0 permissive open source license.",Apache 2.0,LICENSE
"style=flat"" /></a> <a href=""https://github.com/chengtan9907/OpenSTL/blob/master/LICENSE"" alt=""license"">     <img src=""https://img.shields.io/badge/license-Apache--2.0-%23002FA7"" /></a> <!",Apache--2.0,LICENSE
"<div align=""center"">  | Moving MNIST | Moving FMNIST |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/mmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_fashionmnist_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Moving MNIST-CIFAR | KittiCaltech | | :---: | :---: | |  <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/moving_mnist_cifar_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kitticaltech_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | KTH | Human 3.6M |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/kth20_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-video/human_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Traffic - in flow | Traffic - out flow | | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_in_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-traffic/taxibj_out_flow_ConvLSTM.gif' height=""auto"" width=""260"" ></div> |  | Weather - Temperature | Weather - Humidity | |  :---: |  :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_temperature_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_humidity_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div>|  | Weather - Latitude Wind | Weather - Cloud Cover |  | :---: | :---: | | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_wind_latitude_ConvLSTM.gif' height=""auto"" width=""360"" ></div> | <div align=center><img src='https://github.com/chengtan9907/OpenSTL/releases/download/vis-weather-5-625/weather_cloud_cover_5_625_ConvLSTM.gif' height=""auto"" width=""360"" ></div> |  | BAIR Robot Pushing | Kinetics-400 |  | :---: | :---: | | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872182-4f31928d-2ebc-4407-b2d4-1fe4a8da5837.gif' height=""auto"" width=""260"" ></div> | <div align=center><img src='https://github-production-user-asset-6210df.s3.amazonaws.com/44519745/257872560-00775edf-5773-478c-8836-f7aec461e209.gif' height=""auto"" width=""260"" ></div> |  </div>  ## License  This project is released under the [Apache 2.0 license](LICENSE).",Apache 2.0,LICENSE
"```python import torch from ditac import DiTAC  act = DiTAC() x = torch.randn(8,32,20,20).to('cuda')  act(x) ```  You can recreate Fig. 1 using the sample code: ```sh python run_regression.py ```  **Other experiments' code will be uploaded soon**  ## License This project is released under the MIT license.",MIT,LICENSE
/NUBes-guias-de-anotacion.pdf) are licensed under the Creative Commons Attribution-ShareAlike 3.0 Spain  License.,Creative Commons Attribution-ShareAlike 3.0 Spain  License,LICENSE
"See our [full documentation](https://slideflow.dev) for more details and tutorials.  ## ðŸ“š  Publications  Slideflow has been used by:  - [Dolezal et al](https://www.nature.com/articles/s41379-020-00724-3), _Modern Pathology_, 2020 - [Rosenberg et al](https://ascopubs.org/doi/10.1200/JCO.2020.38.15_suppl.e23529), _Journal of Clinical Oncology_ [abstract], 2020 - [Howard et al](https://www.nature.com/articles/s41467-021-24698-1), _Nature Communications_, 2021 - [Dolezal et al](https://www.nature.com/articles/s41467-022-34025-x) _Nature Communications_, 2022 - [Storozuk et al](https://www.nature.com/articles/s41379-022-01039-1.pdf), _Modern Pathology_ [abstract], 2022 - [Partin et al](https://doi.org/10.3389/fmed.2023.1058919) _Front Med_, 2022 - [Dolezal et al](https://ascopubs.org/doi/abs/10.1200/JCO.2022.40.16_suppl.8549) _Journal of Clinical Oncology_ [abstract], 2022 - [Dolezal et al](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9792820/) _Mediastinum_ [abstract], 2022 - [Howard et al](https://www.nature.com/articles/s41523-023-00530-5) _npj Breast Cancer_, 2023 - [Dolezal et al](https://www.nature.com/articles/s41698-023-00399-4) _npj Precision Oncology_, 2023 - [Hieromnimon et al](https://doi.org/10.1101/2023.03.22.533810) [bioRxiv], 2023 - [Carrillo-Perez et al](https://doi.org/10.1186/s40644-023-00586-3) _Cancer Imaging_, 2023  ## ðŸ”“  License This code is made available under the Apache-2.0 license.  ## ðŸ”—  Reference If you find our work useful for your research, or if you use parts of this code, please consider citing as follows:  Dolezal, J.M., Kochanny, S., Dyer, E. et al.",Apache-2.0,LICENSE
"[PyPI license](https://img.shields.io/pypi/l/prdc.svg)](https://pypi.python.org/pypi/prdc/)  ## Reliable Fidelity and Diversity Metrics for Generative Models (ICML 2020)  [Paper: Reliable Fidelity and Diversity Metrics for Generative Models](https://arxiv.org/abs/2002.09797)  Muhammad Ferjad Naeem <sup>1,3*</sup>, Seong Joon Oh<sup>2*</sup>, Yunjey Choi<sup>1</sup>,  Youngjung Uh<sup>1</sup>, Jaejun Yoo<sup>1,4</sup>    <sub>**Work done at Clova AI Research**</sub>  <sub>\* Equal contribution</sub> <sup>1</sup> <sub>Clova AI Research, NAVER Corp.",PyPI license,LICENSE
"/LICENSE.txt""><img alt=""Apache License 2.0"" src=""https://raw.githubusercontent.com/s3prl/s3prl/main/file/license.svg"" /></a>     <a href=""https://creativecommons.org/licenses/by-nc/4.0/""><img alt=""CC_BY_NC License"" src=""https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg"" /></a>     <a href=""https://github.com/s3prl/s3prl/actions/workflows/ci.yml""><img alt=""CI"" src=""https://github.com/s3prl/s3prl/actions/workflows/ci.yml/badge.svg?",Apache License 2.0,LICENSE
"/LICENSE.txt""><img alt=""Apache License 2.0"" src=""https://raw.githubusercontent.com/s3prl/s3prl/main/file/license.svg"" /></a>     <a href=""https://creativecommons.org/licenses/by-nc/4.0/""><img alt=""CC_BY_NC License"" src=""https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg"" /></a>     <a href=""https://github.com/s3prl/s3prl/actions/workflows/ci.yml""><img alt=""CI"" src=""https://github.com/s3prl/s3prl/actions/workflows/ci.yml/badge.svg?",CC_BY_NC License,LICENSE
"Liu. * [ESPnet](https://github.com/espnet/espnet), Shinji Watanabe * [speech-representations](https://github.com/awslabs/speech-representations), aws lab * [PASE](https://github.com/santi-pdp/pase), Santiago Pascual and Mirco Ravanelli * [LibriMix](https://github.com/JorisCos/LibriMix), Joris Cosentino and Manuel Pariente  ## License  The majority of S3PRL Toolkit is licensed under the Apache License version 2.0, however all the files authored by Facebook, Inc.",Apache License version 2.0,LICENSE
"(which have explicit copyright statement on the top) are licensed under CC-BY-NC.  ## Used by <details><summary>List of papers that used our toolkit (Feel free to add your own paper by making a pull request)</summary><p>  ### Self-Supervised Pretraining  + [Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders (Liu et al., 2020)](https://arxiv.org/abs/1910.12638)   ```   @article{mockingjay,      title={Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders},      ISBN={9781509066315},      url={http://dx.doi.org/10.1109/ICASSP40776.2020.9054458},      DOI={10.1109/icassp40776.2020.9054458},      journal={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},      publisher={IEEE},      author={Liu, Andy T. and Yang, Shu-wen and Chi, Po-Han and Hsu, Po-chun and Lee, Hung-yi},      year={2020},      month={May}   }   ``` + [TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech (Liu et al., 2020)](https://arxiv.org/abs/2007.06028)   ```   @misc{tera,       title={TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech},       author={Andy T.",CC-BY-NC,LICENSE
"The code files are released under the open source BSD 2-clause license.    ## Citation  If you use the code (partial or all) in your research, please cite the following manuscript: ```diff Ge, Shufei, Shijia Wang, and Lloyd Elliott.",BSD 2-clause,LICENSE
Statistics and Computing 33.1 (2023): 13. ```  [Link to the manuscript](https://doi.org/10.1007/s11222-022-10170-7) ## LICENSES  Shape-Modeling-with-Spline-Partitions v1.0.,Shape-Modeling-with-Spline-Partitions v1.0. Copyright (c) 2021,LICENSE
Copyright (c) 2021.,Shape-Modeling-with-Spline-Partitions v1.0. Copyright (c) 2021,LICENSE
"Copyright (C) 2021 Forschungszentrum Juelich GmbH, INM-6  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.",GNU General Public License,LICENSE
See the GNU General Public License for more details.,GNU General Public License,LICENSE
You should have received a copy of the GNU General Public License along with this program.,GNU General Public License,LICENSE
SPDX-License-Identifier: GPL-3.0-or-later -->  # beNNch  Computational efficiency is essential to simulate complex neuronal networks and study long-term effects such as learning.,GPL-3.0,LICENSE
"Text string in the view         hierarchy.  ## Citation  If you use or discuss this dataset in your work, please cite our paper:  ``` @misc{bai2021uibert,       title={UIBert: Learning Generic Multimodal Representations for UI Understanding},       author={Chongyang Bai and Xiaoxue Zang and Ying Xu and Srinivas Sunkara and Abhinav Rastogi and Jindong Chen and Blaise Aguera y Arcas},       year={2021},       eprint={2107.13731},       archivePrefix={arXiv},       primaryClass={cs.CV} } ```  ## License  Datasets are licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).  ## Contact us  If you have a technical question regarding the dataset or publication, please create an issue in this repository.",CC BY 4.0,LICENSE
/evaluation/qa/qa_gpt-3.5-turbo_result.json --category all ```  ## License  HaluEval uses [MIT License](.,MIT License,LICENSE
[MIT license](http://img.shields.io/badge/license-MIT-brightgreen.svg)](http://opensource.org/licenses/MIT) [!,MIT,LICENSE
"The team includes [Seongmin Lee](http://www.seongmin.xyz), [Benjamin Hoover](https://bhoov.com), [Hendrik Strobelt](http://hendrik.strobelt.com), [Jay Wang](https://zijie.wang), [ShengYun (Anthony) Peng](https://shengyun-peng.github.io), [Austin Wright](https://www.austinpwright.com), [Kevin Li](https://www.linkedin.com/in/kevinyli/), [Haekyu Park](https://haekyu.github.io/), [Alex Yang](https://alexanderyang.me/), and [Polo Chau](http://www.cc.gatech.edu/~dchau/).  ## Citation  ```bibTeX @article{lee2024diffusion,   title = {{D}iffusion {E}xplainer: {V}isual {E}xplanation for {T}ext-to-image {S}table {D}iffusion},   shorttitle = {Diffusion Explainer},   author = {Lee, Seongmin and Hoover, Benjamin and Strobelt, Hendrik and Wang, Zijie J and Peng, ShengYun and Wright, Austin and Li, Kevin and Park, Haekyu and Yang, Haoyang and Chau, Duen Horng},   journal={IEEE VIS},   year={2024} } ```  ## License The software is available under the [MIT License](https://github.com/poloclub/diffusion-explainer/blob/main/LICENSE).  ## Contact If you have any questions, feel free to [open an issue](https://github.com/poloclub/diffusion-explainer/issues/new/choose) or contact [Seongmin Lee](http://www.seongmin.xyz/).",MIT License,LICENSE
"Licence The contraval tool is released under the Apache License,  Version 2.0 which is available from Apacheâ€™s web pages.","Apache License,  Version 2.0",LICENSE
"<p align=""left""> </p>   <a href=""https://opensource.org/licenses/MIT""><img src=""https://img.shields.io/badge/License-MIT-yellow.svg"" alt=""License: MIT"">   <a href=""https://standardjs.com""><img src=""https://img.shields.io/badge/code_style-standard-brightgreen.svg"" alt=""Standard - \Python Style Guide""></a> [!",MIT,LICENSE
"<p align=""left""> </p>   <a href=""https://opensource.org/licenses/MIT""><img src=""https://img.shields.io/badge/License-MIT-yellow.svg"" alt=""License: MIT"">   <a href=""https://standardjs.com""><img src=""https://img.shields.io/badge/code_style-standard-brightgreen.svg"" alt=""Standard - \Python Style Guide""></a> [!",License-MIT,LICENSE
"<p align=""left""> </p>   <a href=""https://opensource.org/licenses/MIT""><img src=""https://img.shields.io/badge/License-MIT-yellow.svg"" alt=""License: MIT"">   <a href=""https://standardjs.com""><img src=""https://img.shields.io/badge/code_style-standard-brightgreen.svg"" alt=""Standard - \Python Style Guide""></a> [!",License: MIT,LICENSE
"</p>  ## License This framework is available under an MIT License.  ## Acknowledgments <p align=""justify"">We  would  like  to  thank  <a href = ""https://www.edfenergy.com/about"">EDF Energy R&D UK Centre</a>, <a href = ""https://aura-innovation.co.uk/"">AURA Innovation Centre</a> and <a href = ""https://www.hull.ac.uk/"">University of Hull</a> for their support.",MIT License,LICENSE
"# AltEntities (Alternative Entities) dataset: Resolving Indirect Referring Expressions for Entity Selection  ## COPYRIGHT NOTICE  This is the work of *Mohammad Javad Hosseini*, *Filip Radlinski*, *Silvia Pareti*, and *Annie Louis* from Google LLC, made available under the CC-BY SA 3.0 License.",CC-BY SA 3.0 License,LICENSE
"Le and Thang Luong and Golnaz Ghiasi},   booktitle={European Conference on Computer Vision},   year={2024},   organization={Springer} } ```  ## License and disclaimer  Copyright 2024 DeepMind Technologies Limited  All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license.",Copyright 2024 DeepMind Technologies Limited,LICENSE
"Le and Thang Luong and Golnaz Ghiasi},   booktitle={European Conference on Computer Vision},   year={2024},   organization={Springer} } ```  ## License and disclaimer  Copyright 2024 DeepMind Technologies Limited  All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license.","Apache License, Version 2.0",LICENSE
"Le and Thang Luong and Golnaz Ghiasi},   booktitle={European Conference on Computer Vision},   year={2024},   organization={Springer} } ```  ## License and disclaimer  Copyright 2024 DeepMind Technologies Limited  All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license.",Apache 2.0,LICENSE
"Le and Thang Luong and Golnaz Ghiasi},   booktitle={European Conference on Computer Vision},   year={2024},   organization={Springer} } ```  ## License and disclaimer  Copyright 2024 DeepMind Technologies Limited  All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license.",Apache 2.0,LICENSE
You may obtain a copy of the Apache 2.0 license at: https://www.apache.org/licenses/LICENSE-2.0  All other materials are licensed under the Creative Commons Attribution 4.0 International License (CC-BY).,Apache 2.0,LICENSE
You may obtain a copy of the Apache 2.0 license at: https://www.apache.org/licenses/LICENSE-2.0  All other materials are licensed under the Creative Commons Attribution 4.0 International License (CC-BY).,Creative Commons Attribution 4.0 International License,LICENSE
You may obtain a copy of the Apache 2.0 license at: https://www.apache.org/licenses/LICENSE-2.0  All other materials are licensed under the Creative Commons Attribution 4.0 International License (CC-BY).,CC-BY,LICENSE
You may obtain a copy of the CC-BY license at: https://creativecommons.org/licenses/by/4.0/legalcode  Image URLs are from the [Open Images Dataset v7](https://storage.googleapis.com/openimages/web/factsfigures_v7.html#publications) and [Midjourney Showcase](https://www.midjourney.com/showcase).,CC-BY,LICENSE
"Unless required by applicable law or agreed to in writing, all software and materials distributed here under the Apache 2.0 or CC-BY licenses are  distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",Apache 2.0,LICENSE
"Unless required by applicable law or agreed to in writing, all software and materials distributed here under the Apache 2.0 or CC-BY licenses are  distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",CC-BY,LICENSE
"[](https://media.giphy.com/media/ZE4vFIjfm1BHXP7w0R/giphy.gif)|  ## Citation  ``` @inproceedings{xiao2020visual,   title={Visual Relation Grounding in Videos},   author={Xiao, Junbin and Shang, Xindi and Yang, Xun and Tang, Sheng and Chua, Tat-Seng},   booktitle={European Conference on Computer Vision},   pages={447--464},   year={2020},   organization={Springer} } ```  ## License  NUS Â© [NExT++](https://nextcenter.org/)",NUS Â© [NExT++],LICENSE
[License: Apache 2.0](https://img.shields.io/badge/License-Apache_2.0-yellow.svg)](https://opensource.org/license/apache-2-0/) [!,Apache 2.0,LICENSE
[License: Apache 2.0](https://img.shields.io/badge/License-Apache_2.0-yellow.svg)](https://opensource.org/license/apache-2-0/) [!,Apache_2.0,LICENSE
[License: Apache 2.0](https://img.shields.io/badge/License-Apache_2.0-yellow.svg)](https://opensource.org/license/apache-2-0/) [!,apache-2-0,LICENSE
"Assembly and use of OWL is entirely at your own risk and the license expressly states there is no warranty.  ``` MIT License  Copyright (c) 2020 Guy Coleman  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",MIT License,LICENSE
We provide paper results in paper_results folder and the corresponding code for plots in plot_helper.   ## License This repository is licensed under the terms of the [Apache-2.0 License](LICENSE).  ## Questions?,Apache-2.0 License,LICENSE
"## Contact  If you have any questions, feel free to email us (project.miracl [at] gmail.com) or start a Github issue under this repository.   ## License  This work is licensed under the Apache 2 license.",Apache 2,LICENSE
<br> One trick used in `networks.py` is to change `out = self.UpBlock2(x)` to [out = (self.UpBlock2(x)+input).tanh()](https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/blob/35aaf00625f039f9f331d261355945c5eb1a8806/networks.py#L103) to learn a residual.  ## License The code and models in this repository are licensed under the MIT License for academic and other non-commercial uses.,MIT License,LICENSE
"â€žA curated, ontology-based, large-scale knowledge graph of artificial intelligence tasks and benchmarksâ€œ. arXiv:2110.01434 [cs], October 2021. http://arxiv.org/abs/2110.01434  ### Licensing  The ontology file and JSON extract files are distributed under a [CC-BY-SA 3.0 AT](https://creativecommons.org/licenses/by-sa/3.0/at/) license.",CC-BY-SA 3.0 AT,LICENSE
Papers With Code is licensed under the CC-BY-SA license.,CC-BY-SA,LICENSE
The EDAM ontology is licensed under a CC-BY-SA license.,CC-BY-SA,LICENSE
"We especially thank the authors of: - [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS) - [mmdetection](https://github.com/open-mmlab/mmdetection) - [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) - [seamseg](https://github.com/mapillary/seamseg.git)  ## Contacts * [Kshitij Sirohi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Sajad Marvi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Daniel BÃ¼scher](http://www2.informatik.uni-freiburg.de/~buescher/)  ## License For academic usage, the code is released under the [GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html) license.",GPLv3,LICENSE
"We especially thank the authors of: - [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS) - [mmdetection](https://github.com/open-mmlab/mmdetection) - [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) - [seamseg](https://github.com/mapillary/seamseg.git)  ## Contacts * [Kshitij Sirohi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Sajad Marvi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Daniel BÃ¼scher](http://www2.informatik.uni-freiburg.de/~buescher/)  ## License For academic usage, the code is released under the [GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html) license.",gpl-3.0,LICENSE
[License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [!,MIT,LICENSE
[License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [!,MIT,LICENSE
[License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [!,MIT,LICENSE
"By the specificity of the domain and addressed task, BSARD presents a unique challenge problem for future research on legal information retrieval.  ## Documentation  Detailed documentation on the dataset and how to reproduce the main experimental results can be found [here](docs/README.md).  ## Citation  For attribution in academic contexts, please cite this work as:  ```latex @inproceedings{louis2022statutory,   title = {A Statutory Article Retrieval Dataset in French},   author = {Louis, Antoine and Spanakis, Gerasimos},   booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},   month = may,   year = {2022},   address = {Dublin, Ireland},   publisher = {Association for Computational Linguistics},   url = {https://aclanthology.org/2022.acl-long.468},   pages = {6789--6803}, } ```  ## License  This repository is MIT-licensed.",MIT,LICENSE
"The sixth and last movement is the chorale entitled *""Wie bin ich doch so herzlich froh""*.   ## Citations  ``` @article{poltronieri2022HaMSEOntology,           author    = {Andrea Poltronieri and                        Aldo Gangemi},           title     = {The HaMSE Ontology: Using Semantic Technologies to support Music Representation                        Interoperability and Musicological Analysis},           journal   = {CoRR},           volume    = {abs/2202.05817},           year      = {2022},           url       = {https://arxiv.org/abs/2202.05817},           eprinttype = {arXiv},           eprint    = {2202.05817},           timestamp = {Fri, 18 Feb 2022 12:23:53 +0100},           biburl    = {https://dblp.org/rec/journals/corr/abs-2202-05817.bib},           bibsource = {dblp computer science bibliography, https://dblp.org} }  ```  A more advanced version of this work can be found in:  ``` @inproceedings{poltronieri2021musicnoteontology,           title={The Music Note Ontology},           author={Poltronieri, Andrea and Gangemi, Aldo},           booktitle={Proceedings of the 12th Workshop on Ontology Design and Patterns (WOP 2021), Online, October 24, 2021.},           journal={CEUR-WS},           editor={Hammar, Karl and Shimizu, Cogan and KÃ¼Ã§Ã¼k McGinty, Hande and Asprino, Luigi and Carriero, Valentina Anita},           year={2021},           month={11} } ```  ## License MIT License  Copyright (c) 2021 Andrea Poltronieri  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",MIT License,LICENSE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",MIT,LICENSE
"assignees=&labels=&projects=&template=new-metric.md&title=) | | All other issues and questions | [General Issues](https://github.com/obss/jury/issues/new)                                                            |  ## <div align=""center""> License </div>  Licensed under the [MIT](LICENSE) License.",MIT,LICENSE
"/experiments/scripts/train_drill_down_3x128_reinforce.sh ``` the default pretrained model is `DrillDown/caches/region_ckpts/vg_f128_i3_sl_ckpt.pkl`.    ## Citing  If you find our paper/code useful, please consider citing:   @InProceedings{drilldown,     author={Fuwen Tan and Paola Cascante-Bonilla and Xiaoxiao Guo and Hui Wu and Song Feng and Vicente Ordonez},     title={Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries},     booktitle = {Neural Information Processing Systems (NeurIPS)},     month = {December},     year = {2019}     }        # License  This project is licensed under the [MIT license](https://opensource.org/licenses/MIT):  Copyright (c) 2019 University of Virginia, Fuwen Tan, Paola Cascante-Bonilla, Xiaoxiao Guo, Hui Wu, Song Feng, Vicente Ordonez.",MIT,LICENSE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",Creative Commons Attribution 3.0 Unported License,LICENSE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",MIT License,LICENSE
"`Personal computation time` indicates how long your training took for that iteration while `Simulated time to receive federated weights` takes into account user-defined communication latencies between the clients and the server, as well as how long it took the other clients to compute their weights and the server to average them.  ## Authors  Vaikkunth Mugunthan* Anton Peraire* Lalana Kagal  ## License  This project is licensed under the MIT License   MIT License  Copyright (c) 2020 Vaikkunth, Anton, Lalana (PrivacyFL)  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",MIT License,LICENSE
"`Personal computation time` indicates how long your training took for that iteration while `Simulated time to receive federated weights` takes into account user-defined communication latencies between the clients and the server, as well as how long it took the other clients to compute their weights and the server to average them.  ## Authors  Vaikkunth Mugunthan* Anton Peraire* Lalana Kagal  ## License  This project is licensed under the MIT License   MIT License  Copyright (c) 2020 Vaikkunth, Anton, Lalana (PrivacyFL)  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",MIT License,LICENSE
It is licensed under the terms of the MIT license.  ## Credits  `model_sketch_book` was created with [`cookiecutter`](https://cookiecutter.readthedocs.io/en/latest/) and the `py-pkgs-cookiecutter` [template](https://github.com/py-pkgs/py-pkgs-cookiecutter).,MIT,LICENSE
"Licensed under an MIT license.  ### Citing FCIS  If you find FCIS useful in your research, please consider citing: ``` @inproceedings{li2016fully,   Author = {Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji and Yichen Wei}   Title = {Fully Convolutional Instance-aware Semantic Segmentation},   Conference = {CVPR},   year = {2017} } ```  ### Main Results  |                                 | training data  | testing data | mAP^r@0.5 | mAP^r@0.7 | time   | |:---------------------------------:|:----------------:|:--------------:|:-----------:|:-----------:|:--------:| | FCIS, ResNet-v1-101             | VOC 2012 train | VOC 2012 val | 66.0      | 51.9      |   0.23s    |  |                                 | <sub>training data</sub> | <sub>testing data</sub>  | <sub>mAP^r</sub>  | <sub>mAP^r@0.5</sub> | <sub>mAP^r@0.75</sub>| <sub>mAP^r@S</sub> | <sub>mAP^r@M</sub> | <sub>mAP^r@L</sub> | |:---------------------------------:|:---------------:|:---------------:|:------:|:---------:|:---------:|:-------:|:-------:|:-------:| | <sub>FCIS, ResNet-v1-101, OHEM </sub> | <sub>coco trainval35k</sub> | <sub>coco minival</sub> | 29.2 | 50.8 | 29.7 | 7.9 | 31.4 | 51.1 | | <sub>FCIS, ResNet-v1-101, OHEM </sub> | <sub>coco trainval35k</sub> | <sub>coco test-dev</sub>| 29.6 | 51.4 | 30.2 | 8.0 | 31.0 | 49.7 |  *Running time is counted on a single Maxwell Titan X GPU (mini-batch size is 1 in inference).*  ### Requirements: Software  1.",MIT,LICENSE
"# Words as Gatekeepers  License: [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/)  **Authors**: Li Lucy, Jesse Dodge, David Bamman, Katherine A.",CC BY-NC 4.0,LICENSE
"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and GÃ¼nter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",MIT,LICENSE
"**Clone the repository:**     ```bash    git clone https://github.com/juaml/PrettYharmonize.git    cd PrettYharmonize   ## Citation ```bibtex If you use PrettYharmonize in your work, please cite the following: @article{nieto2024impact,   title={Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites},   author={Nieto, Nicol{\'a}s and Eickhoff, Simon B and Jung, Christian and Reuter, Martin and Diers, Kersten and Kelm, Malte and Lichtenberg, Artur and Raimondo, Federico and Patil, Kaustubh R},   journal={arXiv preprint arXiv:2410.19643},   year={2024} } ```  ## Licensing  preattyharmonize is released under the AGPL v3 license:  preattyharmonize, FZJuelich AML machine learning library.",AGPL v3,LICENSE
"This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or any later version.",GNU Affero General Public License,LICENSE
See the GNU Affero General Public License for more details.,GNU Affero General Public License,LICENSE
You should have received a copy of the GNU Affero General Public License along with this program.,GNU Affero General Public License,LICENSE
<br/> doi: 10.3389/frai.2022.991242  Preprint available at ArXiV: [https://arxiv.org/abs/2209.05301](https://arxiv.org/abs/2209.05301)   ## License  The python scripts follow [AGPL 3.0v license](LICENSE).,AGPL 3.0v license,LICENSE
The datasets (under the /datasets directory) are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License [CC-BY-NC-SA-4.0](CC-BY-NC-SA-4.0).  ## Contact https://taln.upf.edu/pages/tsar2022-st/#contact,Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License,LICENSE
The datasets (under the /datasets directory) are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License [CC-BY-NC-SA-4.0](CC-BY-NC-SA-4.0).  ## Contact https://taln.upf.edu/pages/tsar2022-st/#contact,CC-BY-NC-SA-4.0,LICENSE
The datasets (under the /datasets directory) are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License [CC-BY-NC-SA-4.0](CC-BY-NC-SA-4.0).  ## Contact https://taln.upf.edu/pages/tsar2022-st/#contact,CC-BY-NC-SA-4.0,LICENSE
"Furthermore, we are not liable for any damage, loss or expense of any kind arising from or relating to your use of the resources shared in this code repository or the data collected, regardless of whether such liability is based in tort, contract or otherwise.  ## License The code is released under the **BSD-3 License** (see `LICENSE.txt` for details).   ## Usage  #### 1.",BSD-3 License,LICENSE
We would also like to thank Luke Clifton for his assistance and expertise in fitting the DMPC data.  ## License Distributed under the GPL-3.0 License.,GPL-3.0 License,LICENSE
"We release this code under the MIT licence in the hope that it will prove useful to others working on EO and timeseries large observation models.  ## install  Dependencies:  - `pip install -r requirements.txt`  ## results  Our EarthPT-700M model is able to predict future satellite passes well into the future, and also learns semantically meaningful information about the timeseries that it is fed:  <p align=""center"">     <img src=""assets/3d.gif"" alt=""embeddings"" width=""400""/> </p>  You can find a plot with less angular momentum and further results in our paper [here](https://arxiv.org/abs/2309.07207).  ## pretrained weights  You can find our weights for all the EarthPT models on [HuggingFace](https://doi.org/10.57967/hf/1598) ðŸ¤— .  ## citation  If you find EarthPT useful in your work please do drop us a cite:  ```bibtex @article{ref_smith2023,     author = {Smith, M.",MIT,LICENSE
[License: MIT](https://img.shields.io/badge/license-MIT-blue.svg?,MIT,LICENSE
[License: MIT](https://img.shields.io/badge/license-MIT-blue.svg?,MIT,LICENSE
style=flat-square)](https://opensource.org/licenses/MIT) [!,MIT,LICENSE
Please consider citing them as well if you use our code.  ## License  The source files in this repository are released under the [Apache 2.0](LICENSE.txt) license.,Apache 2.0,LICENSE
"These are also released under the Apache 2.0 license, the text of which can be seen in the LICENSE file on their repository.",Apache 2.0,LICENSE
"In order to run one of the examples, the Graph2Vec snippet:  ```sh $ cd examples/whole_graph_embedding/ $ python graph2vec_example.py ```  --------------------------------------------------------------------------------  **Running tests**  From the project's root-level directory:  ```sh $ pytest ```  --------------------------------------------------------------------------------  **License**  - [GNU General Public License v3.0](https://github.com/benedekrozemberczki/karateclub/blob/master/LICENSE)",GNU General Public License v3.0,LICENSE
[license](http://img.shields.io/badge/license-BSD-blue.svg?,BSD,LICENSE
