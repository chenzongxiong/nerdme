sentence,entity_text,entity_type
"<br><br> <span style=""white-space:nowrap""><img src=""figures/example_data.png"" alt=""Diverse examplary synthetic data samples."" align=""middle"" width=""100%"" /><br> <span style=""white-space:nowrap""><img src=""figures/multi-channel.png"" alt=""Synthetic multi-channel data sample."" align=""middle"" width=""32.7%"" /><img src=""figures/overlapping_cells.png"" alt=""Synthetic data sample of overlapping cells."" align=""middle"" width=""32.7%"" /><img src=""figures/timeseries.gif"" alt=""Synthetic timeseries data sample."" align=""middle"" width=""32.7%"" /></span></span><br> <em>Exemplary synthetic samples from our experiments</em><br><br><br>   If you are using code or data, please cite the following work: ``` @article{eschweiler2024celldiffusion,   title={Denoising diffusion probabilistic models for generation of realistic fully-annotated microscopy image datasets},   author={Eschweiler, Dennis and Yilmaz, R{\""u}veyda and Baumann, Matisse and Laube, Ina and Roy, Rijo and Jose, Abin and Br{\""u}ckner, Daniel and Stegmaier, Johannes},   journal={PLOS Computational Biology},   volume={20},   number={2},   pages={e1011890},   year={2024} } ``` <br><br><br> We provide Jupyter Notebooks that give an overview of how to preprocess your data, train and apply the image generation process.",Jupyter Notebooks,SOFTWARE
"Requirements ============  The code has been tested on `Ubuntu 18.04.2 LTS`, with `Python 2.7` , `Tensorflow 1.10` and `Numpy 1.15.1`.",Ubuntu 18.04.2 LTS,SOFTWARE
"Requirements ============  The code has been tested on `Ubuntu 18.04.2 LTS`, with `Python 2.7` , `Tensorflow 1.10` and `Numpy 1.15.1`.",Tensorflow 1.10,SOFTWARE
"Requirements ============  The code has been tested on `Ubuntu 18.04.2 LTS`, with `Python 2.7` , `Tensorflow 1.10` and `Numpy 1.15.1`.",Numpy 1.15.1,SOFTWARE
In the test phase `OpenCV 3.4.3.18` and `Scikit-image 0.14.2` have been used to manipulate and visualize the samples.,OpenCV 3.4.3.18,SOFTWARE
In the test phase `OpenCV 3.4.3.18` and `Scikit-image 0.14.2` have been used to manipulate and visualize the samples.,Scikit-image 0.14.2,SOFTWARE
[conda](https://docs.conda.io/en/latest/)) to setup all the packages.,conda,SOFTWARE
To better visualize this README offline the use of `grip` is suggested:  ``` pip install grip grip .,grip,SOFTWARE
To better visualize this README offline the use of `grip` is suggested:  ``` pip install grip grip .,pip,SOFTWARE
To better visualize this README offline the use of `grip` is suggested:  ``` pip install grip grip .,grip,SOFTWARE
To better visualize this README offline the use of `grip` is suggested:  ``` pip install grip grip .,grip,SOFTWARE
To install Tensorflow GPU version 1.10 run this command from `pip`: ``` pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl ```  Otherwise you can install the CPU version:  ``` pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp27-none-linux_x86_64.whl ```  Additional packages can also be installed with `pip`:  ``` pip install opencv-python==3.4.3 pip install scikit-image==0.14 ```  All the procedures reported below will work only if the current terminal directory is the code folder.,Tensorflow GPU version 1.10,SOFTWARE
To install Tensorflow GPU version 1.10 run this command from `pip`: ``` pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl ```  Otherwise you can install the CPU version:  ``` pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp27-none-linux_x86_64.whl ```  Additional packages can also be installed with `pip`:  ``` pip install opencv-python==3.4.3 pip install scikit-image==0.14 ```  All the procedures reported below will work only if the current terminal directory is the code folder.,pip,SOFTWARE
To install Tensorflow GPU version 1.10 run this command from `pip`: ``` pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl ```  Otherwise you can install the CPU version:  ``` pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp27-none-linux_x86_64.whl ```  Additional packages can also be installed with `pip`:  ``` pip install opencv-python==3.4.3 pip install scikit-image==0.14 ```  All the procedures reported below will work only if the current terminal directory is the code folder.,pip,SOFTWARE
To install Tensorflow GPU version 1.10 run this command from `pip`: ``` pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl ```  Otherwise you can install the CPU version:  ``` pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp27-none-linux_x86_64.whl ```  Additional packages can also be installed with `pip`:  ``` pip install opencv-python==3.4.3 pip install scikit-image==0.14 ```  All the procedures reported below will work only if the current terminal directory is the code folder.,tensorflow_gpu-1.10.0,SOFTWARE
To install Tensorflow GPU version 1.10 run this command from `pip`: ``` pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl ```  Otherwise you can install the CPU version:  ``` pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp27-none-linux_x86_64.whl ```  Additional packages can also be installed with `pip`:  ``` pip install opencv-python==3.4.3 pip install scikit-image==0.14 ```  All the procedures reported below will work only if the current terminal directory is the code folder.,pip,SOFTWARE
To install Tensorflow GPU version 1.10 run this command from `pip`: ``` pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl ```  Otherwise you can install the CPU version:  ``` pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp27-none-linux_x86_64.whl ```  Additional packages can also be installed with `pip`:  ``` pip install opencv-python==3.4.3 pip install scikit-image==0.14 ```  All the procedures reported below will work only if the current terminal directory is the code folder.,tensorflow-1.10.0,SOFTWARE
To install Tensorflow GPU version 1.10 run this command from `pip`: ``` pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl ```  Otherwise you can install the CPU version:  ``` pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp27-none-linux_x86_64.whl ```  Additional packages can also be installed with `pip`:  ``` pip install opencv-python==3.4.3 pip install scikit-image==0.14 ```  All the procedures reported below will work only if the current terminal directory is the code folder.,linux_x86_64,SOFTWARE
To install Tensorflow GPU version 1.10 run this command from `pip`: ``` pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl ```  Otherwise you can install the CPU version:  ``` pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp27-none-linux_x86_64.whl ```  Additional packages can also be installed with `pip`:  ``` pip install opencv-python==3.4.3 pip install scikit-image==0.14 ```  All the procedures reported below will work only if the current terminal directory is the code folder.,pip,SOFTWARE
To install Tensorflow GPU version 1.10 run this command from `pip`: ``` pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl ```  Otherwise you can install the CPU version:  ``` pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp27-none-linux_x86_64.whl ```  Additional packages can also be installed with `pip`:  ``` pip install opencv-python==3.4.3 pip install scikit-image==0.14 ```  All the procedures reported below will work only if the current terminal directory is the code folder.,pip,SOFTWARE
To install Tensorflow GPU version 1.10 run this command from `pip`: ``` pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl ```  Otherwise you can install the CPU version:  ``` pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp27-none-linux_x86_64.whl ```  Additional packages can also be installed with `pip`:  ``` pip install opencv-python==3.4.3 pip install scikit-image==0.14 ```  All the procedures reported below will work only if the current terminal directory is the code folder.,opencv-python==3.4.3,SOFTWARE
To install Tensorflow GPU version 1.10 run this command from `pip`: ``` pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl ```  Otherwise you can install the CPU version:  ``` pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp27-none-linux_x86_64.whl ```  Additional packages can also be installed with `pip`:  ``` pip install opencv-python==3.4.3 pip install scikit-image==0.14 ```  All the procedures reported below will work only if the current terminal directory is the code folder.,pip,SOFTWARE
To install Tensorflow GPU version 1.10 run this command from `pip`: ``` pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.10.0-cp27-none-linux_x86_64.whl ```  Otherwise you can install the CPU version:  ``` pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.10.0-cp27-none-linux_x86_64.whl ```  Additional packages can also be installed with `pip`:  ``` pip install opencv-python==3.4.3 pip install scikit-image==0.14 ```  All the procedures reported below will work only if the current terminal directory is the code folder.,scikit-image==0.14,SOFTWARE
"Ablation study -------------- To reproduce the four ablation conditions simply set the parameters `lambda_e` and `lambda_i` in the command line:  **Condition lambda_e=0.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=0.0 ```  **Condition lambda_e=1.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **Condition lambda_e=0.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=1.0 ```  **Condition lambda_e=1.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  Comparison against baselines ------------------------------ The following commands can be used to train the four models:  **Y-AE**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  **Y-AE + ablation**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **cAE**  ``` python train.py --arch=""cae"" ```  **cAE + regularization**  ``` python train.py --arch=""cae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **adversarial-AE + regularization**  ``` python train.py --arch=""aae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **VAE**  ``` python train.py --arch=""vae"" ```  **beta-VAE**  ``` python train.py --arch=""vae"" --beta=2.0 ```  Note that the folders in `.",python,SOFTWARE
"Ablation study -------------- To reproduce the four ablation conditions simply set the parameters `lambda_e` and `lambda_i` in the command line:  **Condition lambda_e=0.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=0.0 ```  **Condition lambda_e=1.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **Condition lambda_e=0.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=1.0 ```  **Condition lambda_e=1.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  Comparison against baselines ------------------------------ The following commands can be used to train the four models:  **Y-AE**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  **Y-AE + ablation**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **cAE**  ``` python train.py --arch=""cae"" ```  **cAE + regularization**  ``` python train.py --arch=""cae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **adversarial-AE + regularization**  ``` python train.py --arch=""aae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **VAE**  ``` python train.py --arch=""vae"" ```  **beta-VAE**  ``` python train.py --arch=""vae"" --beta=2.0 ```  Note that the folders in `.",python,SOFTWARE
"Ablation study -------------- To reproduce the four ablation conditions simply set the parameters `lambda_e` and `lambda_i` in the command line:  **Condition lambda_e=0.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=0.0 ```  **Condition lambda_e=1.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **Condition lambda_e=0.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=1.0 ```  **Condition lambda_e=1.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  Comparison against baselines ------------------------------ The following commands can be used to train the four models:  **Y-AE**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  **Y-AE + ablation**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **cAE**  ``` python train.py --arch=""cae"" ```  **cAE + regularization**  ``` python train.py --arch=""cae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **adversarial-AE + regularization**  ``` python train.py --arch=""aae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **VAE**  ``` python train.py --arch=""vae"" ```  **beta-VAE**  ``` python train.py --arch=""vae"" --beta=2.0 ```  Note that the folders in `.",python,SOFTWARE
"Ablation study -------------- To reproduce the four ablation conditions simply set the parameters `lambda_e` and `lambda_i` in the command line:  **Condition lambda_e=0.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=0.0 ```  **Condition lambda_e=1.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **Condition lambda_e=0.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=1.0 ```  **Condition lambda_e=1.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  Comparison against baselines ------------------------------ The following commands can be used to train the four models:  **Y-AE**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  **Y-AE + ablation**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **cAE**  ``` python train.py --arch=""cae"" ```  **cAE + regularization**  ``` python train.py --arch=""cae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **adversarial-AE + regularization**  ``` python train.py --arch=""aae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **VAE**  ``` python train.py --arch=""vae"" ```  **beta-VAE**  ``` python train.py --arch=""vae"" --beta=2.0 ```  Note that the folders in `.",python,SOFTWARE
"Ablation study -------------- To reproduce the four ablation conditions simply set the parameters `lambda_e` and `lambda_i` in the command line:  **Condition lambda_e=0.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=0.0 ```  **Condition lambda_e=1.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **Condition lambda_e=0.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=1.0 ```  **Condition lambda_e=1.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  Comparison against baselines ------------------------------ The following commands can be used to train the four models:  **Y-AE**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  **Y-AE + ablation**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **cAE**  ``` python train.py --arch=""cae"" ```  **cAE + regularization**  ``` python train.py --arch=""cae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **adversarial-AE + regularization**  ``` python train.py --arch=""aae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **VAE**  ``` python train.py --arch=""vae"" ```  **beta-VAE**  ``` python train.py --arch=""vae"" --beta=2.0 ```  Note that the folders in `.",python,SOFTWARE
"Ablation study -------------- To reproduce the four ablation conditions simply set the parameters `lambda_e` and `lambda_i` in the command line:  **Condition lambda_e=0.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=0.0 ```  **Condition lambda_e=1.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **Condition lambda_e=0.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=1.0 ```  **Condition lambda_e=1.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  Comparison against baselines ------------------------------ The following commands can be used to train the four models:  **Y-AE**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  **Y-AE + ablation**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **cAE**  ``` python train.py --arch=""cae"" ```  **cAE + regularization**  ``` python train.py --arch=""cae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **adversarial-AE + regularization**  ``` python train.py --arch=""aae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **VAE**  ``` python train.py --arch=""vae"" ```  **beta-VAE**  ``` python train.py --arch=""vae"" --beta=2.0 ```  Note that the folders in `.",python,SOFTWARE
"Ablation study -------------- To reproduce the four ablation conditions simply set the parameters `lambda_e` and `lambda_i` in the command line:  **Condition lambda_e=0.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=0.0 ```  **Condition lambda_e=1.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **Condition lambda_e=0.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=1.0 ```  **Condition lambda_e=1.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  Comparison against baselines ------------------------------ The following commands can be used to train the four models:  **Y-AE**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  **Y-AE + ablation**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **cAE**  ``` python train.py --arch=""cae"" ```  **cAE + regularization**  ``` python train.py --arch=""cae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **adversarial-AE + regularization**  ``` python train.py --arch=""aae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **VAE**  ``` python train.py --arch=""vae"" ```  **beta-VAE**  ``` python train.py --arch=""vae"" --beta=2.0 ```  Note that the folders in `.",python,SOFTWARE
"Ablation study -------------- To reproduce the four ablation conditions simply set the parameters `lambda_e` and `lambda_i` in the command line:  **Condition lambda_e=0.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=0.0 ```  **Condition lambda_e=1.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **Condition lambda_e=0.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=1.0 ```  **Condition lambda_e=1.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  Comparison against baselines ------------------------------ The following commands can be used to train the four models:  **Y-AE**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  **Y-AE + ablation**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **cAE**  ``` python train.py --arch=""cae"" ```  **cAE + regularization**  ``` python train.py --arch=""cae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **adversarial-AE + regularization**  ``` python train.py --arch=""aae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **VAE**  ``` python train.py --arch=""vae"" ```  **beta-VAE**  ``` python train.py --arch=""vae"" --beta=2.0 ```  Note that the folders in `.",python,SOFTWARE
"Ablation study -------------- To reproduce the four ablation conditions simply set the parameters `lambda_e` and `lambda_i` in the command line:  **Condition lambda_e=0.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=0.0 ```  **Condition lambda_e=1.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **Condition lambda_e=0.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=1.0 ```  **Condition lambda_e=1.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  Comparison against baselines ------------------------------ The following commands can be used to train the four models:  **Y-AE**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  **Y-AE + ablation**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **cAE**  ``` python train.py --arch=""cae"" ```  **cAE + regularization**  ``` python train.py --arch=""cae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **adversarial-AE + regularization**  ``` python train.py --arch=""aae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **VAE**  ``` python train.py --arch=""vae"" ```  **beta-VAE**  ``` python train.py --arch=""vae"" --beta=2.0 ```  Note that the folders in `.",python,SOFTWARE
"Ablation study -------------- To reproduce the four ablation conditions simply set the parameters `lambda_e` and `lambda_i` in the command line:  **Condition lambda_e=0.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=0.0 ```  **Condition lambda_e=1.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **Condition lambda_e=0.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=1.0 ```  **Condition lambda_e=1.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  Comparison against baselines ------------------------------ The following commands can be used to train the four models:  **Y-AE**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  **Y-AE + ablation**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **cAE**  ``` python train.py --arch=""cae"" ```  **cAE + regularization**  ``` python train.py --arch=""cae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **adversarial-AE + regularization**  ``` python train.py --arch=""aae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **VAE**  ``` python train.py --arch=""vae"" ```  **beta-VAE**  ``` python train.py --arch=""vae"" --beta=2.0 ```  Note that the folders in `.",python,SOFTWARE
"Ablation study -------------- To reproduce the four ablation conditions simply set the parameters `lambda_e` and `lambda_i` in the command line:  **Condition lambda_e=0.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=0.0 ```  **Condition lambda_e=1.0 and lambda_i=0.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **Condition lambda_e=0.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=0.0 --lambda_i=1.0 ```  **Condition lambda_e=1.0 and lambda_i=1.0**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  Comparison against baselines ------------------------------ The following commands can be used to train the four models:  **Y-AE**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=1.0 ```  **Y-AE + ablation**  ``` python train.py --arch=""yae"" --lambda_e=1.0 --lambda_i=0.0 ```  **cAE**  ``` python train.py --arch=""cae"" ```  **cAE + regularization**  ``` python train.py --arch=""cae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **adversarial-AE + regularization**  ``` python train.py --arch=""aae"" --epochs=20 --implicit_units=16 --wdecay=0.0001 ```  **VAE**  ``` python train.py --arch=""vae"" ```  **beta-VAE**  ``` python train.py --arch=""vae"" --beta=2.0 ```  Note that the folders in `.",python,SOFTWARE
"The logs are saved in the `log` subdir and they can be visualized through tensorboard:  ``` tensorboard --logdir="".",tensorboard,SOFTWARE
"The logs are saved in the `log` subdir and they can be visualized through tensorboard:  ``` tensorboard --logdir="".",tensorboard,SOFTWARE
"LeNet classifiers ------------------  It is possible to train a LeNet classifier through the following command:  ``` python train.py --arch=""lenet"" --epochs=30 --lrate=0.0001 --wdecay=0.0001 ```  To increase the variance of the ensemble each network should be trained with different hyperparameters values.",LeNet,SOFTWARE
"LeNet classifiers ------------------  It is possible to train a LeNet classifier through the following command:  ``` python train.py --arch=""lenet"" --epochs=30 --lrate=0.0001 --wdecay=0.0001 ```  To increase the variance of the ensemble each network should be trained with different hyperparameters values.",LeNet,SOFTWARE
"LeNet classifiers ------------------  It is possible to train a LeNet classifier through the following command:  ``` python train.py --arch=""lenet"" --epochs=30 --lrate=0.0001 --wdecay=0.0001 ```  To increase the variance of the ensemble each network should be trained with different hyperparameters values.",python,SOFTWARE
"LeNet classifiers ------------------  It is possible to train a LeNet classifier through the following command:  ``` python train.py --arch=""lenet"" --epochs=30 --lrate=0.0001 --wdecay=0.0001 ```  To increase the variance of the ensemble each network should be trained with different hyperparameters values.",lenet,SOFTWARE
"Test =====  Losses on the test set -----------------------  It is possible to load a pretrained model in order to measure the losses on the test set.  ``` python test.py --arch=""cae"" --type=""loss"" --resume="".",python,SOFTWARE
"For instance to generate the data for a regularized cAE you may have to run something like this:  ``` python test.py --arch=""cae"" --implicit_units=16 --type=""gendata"" --resume="".",python,SOFTWARE
"/results/cae_ep10_wdecay0.0001_units16"" ```  This will create a new folder named `gendata` with numpy features and labels of the artificial dataset.",numpy,SOFTWARE
Those can be visualized with matplotlib.,matplotlib,SOFTWARE
"The command to run may look like this:  ``` python test.py --arch=""cae"" --implicit_units=16 --type=""metrics"" --tot_samples=20 --resume=""/home/mpatacchiola/Y-autoencoder-paper/code/results/cae_ep10_wdecay0.0001_units16/model/131315_27032019_4679/model.ckpt"" --path="".",python,SOFTWARE
"Ensemble accuracy ------------------  It is possible to use an external classifier, like a LeNet, to verify if the digit produced in the `gendata` step are recognized.",LeNet,SOFTWARE
Once the LeNet has been trained (see `Training` section) it is possible to load the model and get the accuracy metric.,LeNet,SOFTWARE
"If you use this code, please cite the paper using the bibtex reference below. ``` @inproceedings{tanl,     title={Structured Prediction as Translation between Augmented Natural Languages},     author={Giovanni Paolini and Ben Athiwaratkun and Jason Krone and Jie Ma and Alessandro Achille and Rishita Anubhai and Cicero Nogueira dos Santos and Bing Xiang and Stefano Soatto},     booktitle={9th International Conference on Learning Representations, {ICLR} 2021},     year={2021}, } ```   ## Requirements  - Python 3.6+ - PyTorch (tested with version 1.7.1) - Transformers (tested with version 4.0.0) - NetworkX (tested with version 2.5, only used in coreference resolution)  You can install all required Python packages with `pip install -r requirements.txt`   ## Datasets  By default, datasets are expected to be in `data/DATASET_NAME`.",Python 3.6+,SOFTWARE
"If you use this code, please cite the paper using the bibtex reference below. ``` @inproceedings{tanl,     title={Structured Prediction as Translation between Augmented Natural Languages},     author={Giovanni Paolini and Ben Athiwaratkun and Jason Krone and Jie Ma and Alessandro Achille and Rishita Anubhai and Cicero Nogueira dos Santos and Bing Xiang and Stefano Soatto},     booktitle={9th International Conference on Learning Representations, {ICLR} 2021},     year={2021}, } ```   ## Requirements  - Python 3.6+ - PyTorch (tested with version 1.7.1) - Transformers (tested with version 4.0.0) - NetworkX (tested with version 2.5, only used in coreference resolution)  You can install all required Python packages with `pip install -r requirements.txt`   ## Datasets  By default, datasets are expected to be in `data/DATASET_NAME`.",PyTorch,SOFTWARE
"If you use this code, please cite the paper using the bibtex reference below. ``` @inproceedings{tanl,     title={Structured Prediction as Translation between Augmented Natural Languages},     author={Giovanni Paolini and Ben Athiwaratkun and Jason Krone and Jie Ma and Alessandro Achille and Rishita Anubhai and Cicero Nogueira dos Santos and Bing Xiang and Stefano Soatto},     booktitle={9th International Conference on Learning Representations, {ICLR} 2021},     year={2021}, } ```   ## Requirements  - Python 3.6+ - PyTorch (tested with version 1.7.1) - Transformers (tested with version 4.0.0) - NetworkX (tested with version 2.5, only used in coreference resolution)  You can install all required Python packages with `pip install -r requirements.txt`   ## Datasets  By default, datasets are expected to be in `data/DATASET_NAME`.",Transformers,SOFTWARE
"If you use this code, please cite the paper using the bibtex reference below. ``` @inproceedings{tanl,     title={Structured Prediction as Translation between Augmented Natural Languages},     author={Giovanni Paolini and Ben Athiwaratkun and Jason Krone and Jie Ma and Alessandro Achille and Rishita Anubhai and Cicero Nogueira dos Santos and Bing Xiang and Stefano Soatto},     booktitle={9th International Conference on Learning Representations, {ICLR} 2021},     year={2021}, } ```   ## Requirements  - Python 3.6+ - PyTorch (tested with version 1.7.1) - Transformers (tested with version 4.0.0) - NetworkX (tested with version 2.5, only used in coreference resolution)  You can install all required Python packages with `pip install -r requirements.txt`   ## Datasets  By default, datasets are expected to be in `data/DATASET_NAME`.",NetworkX,SOFTWARE
"If you use this code, please cite the paper using the bibtex reference below. ``` @inproceedings{tanl,     title={Structured Prediction as Translation between Augmented Natural Languages},     author={Giovanni Paolini and Ben Athiwaratkun and Jason Krone and Jie Ma and Alessandro Achille and Rishita Anubhai and Cicero Nogueira dos Santos and Bing Xiang and Stefano Soatto},     booktitle={9th International Conference on Learning Representations, {ICLR} 2021},     year={2021}, } ```   ## Requirements  - Python 3.6+ - PyTorch (tested with version 1.7.1) - Transformers (tested with version 4.0.0) - NetworkX (tested with version 2.5, only used in coreference resolution)  You can install all required Python packages with `pip install -r requirements.txt`   ## Datasets  By default, datasets are expected to be in `data/DATASET_NAME`.",pip,SOFTWARE
"The final weights and intermediate checkpoints are written in a directory such as `experiments/conll04_final-t5-base-ep200-len256-b8-train`, with one subdirectory per episode.",t5-base,SOFTWARE
"`t5-base`) - `do_train` (bool): whether to run training (default is False) - `do_eval` (bool): whether to run evaluation on the `dev` set (default is False) - `do_predict` (bool): whether to run evaluation on the `test` set (default is False) - `train_split` (str): comma-separated list of data splits for training (default is `train`) - `num_train_epochs` (int): number of train epochs - `learning_rate` (float): initial learning rate (default is 5e-4) - `train_subset` (float > 0 and <=1): portion of training data to effectively use during training (default is 1, i.e., use all training data) - `per_device_train_batch_size` (int): batch size per GPU during training (default is 8) - `per_device_eval_batch_size` (int): batch size during evaluation (default is 8; only one GPU is used for evaluation) - `max_seq_length` (int): maximum input sequence length after tokenization; longer sequences are truncated - `max_output_seq_length` (int): maximum output sequence length (default is `max_seq_length`) - `max_seq_length_eval` (int): maximum input sequence length for evaluation (default is `max_seq_length`) - `max_output_seq_length_eval` (int): maximum output sequence length for evaluation (default is `max_output_seq_length` or `max_seq_length_eval` or `max_seq_length`) - `episodes` (str): episodes to run (default is `0`; an interval can be specified, such as `1-4`; the episode number is used as the random seed) - `num_beams` (int): number of beams for beam search during generation (default is 1) - `multitask` (bool): if True, the name of the dataset is prepended to each input sentence (default is False)  See [arguments.py](arguments.py) and [transformers.TrainingArguments](https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py) for additional config arguments.   ## Fine-tuned multi-task model  The weights of our multi-task model (released under the [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/)) can be downloaded here: https://tanl.s3.amazonaws.com/tanl-multitask.zip  Extract the zip file in the `experiments/` directory.",t5-base,SOFTWARE
"`t5-base`) - `do_train` (bool): whether to run training (default is False) - `do_eval` (bool): whether to run evaluation on the `dev` set (default is False) - `do_predict` (bool): whether to run evaluation on the `test` set (default is False) - `train_split` (str): comma-separated list of data splits for training (default is `train`) - `num_train_epochs` (int): number of train epochs - `learning_rate` (float): initial learning rate (default is 5e-4) - `train_subset` (float > 0 and <=1): portion of training data to effectively use during training (default is 1, i.e., use all training data) - `per_device_train_batch_size` (int): batch size per GPU during training (default is 8) - `per_device_eval_batch_size` (int): batch size during evaluation (default is 8; only one GPU is used for evaluation) - `max_seq_length` (int): maximum input sequence length after tokenization; longer sequences are truncated - `max_output_seq_length` (int): maximum output sequence length (default is `max_seq_length`) - `max_seq_length_eval` (int): maximum input sequence length for evaluation (default is `max_seq_length`) - `max_output_seq_length_eval` (int): maximum output sequence length for evaluation (default is `max_output_seq_length` or `max_seq_length_eval` or `max_seq_length`) - `episodes` (str): episodes to run (default is `0`; an interval can be specified, such as `1-4`; the episode number is used as the random seed) - `num_beams` (int): number of beams for beam search during generation (default is 1) - `multitask` (bool): if True, the name of the dataset is prepended to each input sentence (default is False)  See [arguments.py](arguments.py) and [transformers.TrainingArguments](https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py) for additional config arguments.   ## Fine-tuned multi-task model  The weights of our multi-task model (released under the [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/)) can be downloaded here: https://tanl.s3.amazonaws.com/tanl-multitask.zip  Extract the zip file in the `experiments/` directory.",transformers,SOFTWARE
"`t5-base`) - `do_train` (bool): whether to run training (default is False) - `do_eval` (bool): whether to run evaluation on the `dev` set (default is False) - `do_predict` (bool): whether to run evaluation on the `test` set (default is False) - `train_split` (str): comma-separated list of data splits for training (default is `train`) - `num_train_epochs` (int): number of train epochs - `learning_rate` (float): initial learning rate (default is 5e-4) - `train_subset` (float > 0 and <=1): portion of training data to effectively use during training (default is 1, i.e., use all training data) - `per_device_train_batch_size` (int): batch size per GPU during training (default is 8) - `per_device_eval_batch_size` (int): batch size during evaluation (default is 8; only one GPU is used for evaluation) - `max_seq_length` (int): maximum input sequence length after tokenization; longer sequences are truncated - `max_output_seq_length` (int): maximum output sequence length (default is `max_seq_length`) - `max_seq_length_eval` (int): maximum input sequence length for evaluation (default is `max_seq_length`) - `max_output_seq_length_eval` (int): maximum output sequence length for evaluation (default is `max_output_seq_length` or `max_seq_length_eval` or `max_seq_length`) - `episodes` (str): episodes to run (default is `0`; an interval can be specified, such as `1-4`; the episode number is used as the random seed) - `num_beams` (int): number of beams for beam search during generation (default is 1) - `multitask` (bool): if True, the name of the dataset is prepended to each input sentence (default is False)  See [arguments.py](arguments.py) and [transformers.TrainingArguments](https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py) for additional config arguments.   ## Fine-tuned multi-task model  The weights of our multi-task model (released under the [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/)) can be downloaded here: https://tanl.s3.amazonaws.com/tanl-multitask.zip  Extract the zip file in the `experiments/` directory.",transformers,SOFTWARE
"`t5-base`) - `do_train` (bool): whether to run training (default is False) - `do_eval` (bool): whether to run evaluation on the `dev` set (default is False) - `do_predict` (bool): whether to run evaluation on the `test` set (default is False) - `train_split` (str): comma-separated list of data splits for training (default is `train`) - `num_train_epochs` (int): number of train epochs - `learning_rate` (float): initial learning rate (default is 5e-4) - `train_subset` (float > 0 and <=1): portion of training data to effectively use during training (default is 1, i.e., use all training data) - `per_device_train_batch_size` (int): batch size per GPU during training (default is 8) - `per_device_eval_batch_size` (int): batch size during evaluation (default is 8; only one GPU is used for evaluation) - `max_seq_length` (int): maximum input sequence length after tokenization; longer sequences are truncated - `max_output_seq_length` (int): maximum output sequence length (default is `max_seq_length`) - `max_seq_length_eval` (int): maximum input sequence length for evaluation (default is `max_seq_length`) - `max_output_seq_length_eval` (int): maximum output sequence length for evaluation (default is `max_output_seq_length` or `max_seq_length_eval` or `max_seq_length`) - `episodes` (str): episodes to run (default is `0`; an interval can be specified, such as `1-4`; the episode number is used as the random seed) - `num_beams` (int): number of beams for beam search during generation (default is 1) - `multitask` (bool): if True, the name of the dataset is prepended to each input sentence (default is False)  See [arguments.py](arguments.py) and [transformers.TrainingArguments](https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py) for additional config arguments.   ## Fine-tuned multi-task model  The weights of our multi-task model (released under the [CC BY 4.0 license](https://creativecommons.org/licenses/by/4.0/)) can be downloaded here: https://tanl.s3.amazonaws.com/tanl-multitask.zip  Extract the zip file in the `experiments/` directory.",transformers,SOFTWARE
"This will create a subdirectory called `multitask-t5-base-ep50-len512-b8-train,dev-overlap96`.",t5-base,SOFTWARE
"Code was written to collect data across multiple protocol versions (e.g. both Version 1 and Version 2 of Uniswap), but only data from the most recent protocol was used. - **One-off data collection (NodeJS)** - These scripts were used to collect more targeted information",NodeJS,SOFTWARE
**utils**  This folder contains several files used by NodeJS scripts: - *abi.js*: ABIs for relevant contracts - *addr.js*: relevant addresses - *eventLib.js*: helper functions,NodeJS,SOFTWARE
# üå≥  GRANDE: Gradient-Based Decision Tree Ensembles üå≥   [!,GRANDE,SOFTWARE
[PyPI version](https://img.shields.io/pypi/v/GRANDE)](https://pypi.org/project/GRANDE/) [!,PyPI,SOFTWARE
"<div align=""center"">  <img src=""figures/grande.jpg"" alt=""GRANDE Overview"" width=""50%""/>  <p><strong>Figure 1: Overview GRANDE.",GRANDE,SOFTWARE
"</strong> GRANDE is a gradient-based decision tree ensemble that utilizes dynamic, instance-wise leaf weights.",GRANDE,SOFTWARE
"<div align=""center"">  <img src=""figures/results_hpo.jpg"" alt=""GRANDE Results"" width=""70%""/>  <p><strong>Figure 2: Performance Comparison.",GRANDE,SOFTWARE
"id=XEFWBxi075  ## Installation To download the latest official release of the package, use the pip command below: ```bash pip install GRANDE ``` More details can be found under: https://pypi.org/project/GRANDE/  ## Cite us  ``` @inproceedings{ marton2024grande, title={{GRANDE}: Gradient-Based Decision Tree Ensembles}, author={Sascha Marton and Stefan L{\""u}dtke and Christian Bartelt and Heiner Stuckenschmidt}, booktitle={The Twelfth International Conference on Learning Representations}, year={2024}, url={https://openreview.net/forum?",pip,SOFTWARE
"id=XEFWBxi075  ## Installation To download the latest official release of the package, use the pip command below: ```bash pip install GRANDE ``` More details can be found under: https://pypi.org/project/GRANDE/  ## Cite us  ``` @inproceedings{ marton2024grande, title={{GRANDE}: Gradient-Based Decision Tree Ensembles}, author={Sascha Marton and Stefan L{\""u}dtke and Christian Bartelt and Heiner Stuckenschmidt}, booktitle={The Twelfth International Conference on Learning Representations}, year={2024}, url={https://openreview.net/forum?",pip,SOFTWARE
"id=XEFWBxi075  ## Installation To download the latest official release of the package, use the pip command below: ```bash pip install GRANDE ``` More details can be found under: https://pypi.org/project/GRANDE/  ## Cite us  ``` @inproceedings{ marton2024grande, title={{GRANDE}: Gradient-Based Decision Tree Ensembles}, author={Sascha Marton and Stefan L{\""u}dtke and Christian Bartelt and Heiner Stuckenschmidt}, booktitle={The Twelfth International Conference on Learning Representations}, year={2024}, url={https://openreview.net/forum?",GRANDE,SOFTWARE
"id=XEFWBxi075  ## Installation To download the latest official release of the package, use the pip command below: ```bash pip install GRANDE ``` More details can be found under: https://pypi.org/project/GRANDE/  ## Cite us  ``` @inproceedings{ marton2024grande, title={{GRANDE}: Gradient-Based Decision Tree Ensembles}, author={Sascha Marton and Stefan L{\""u}dtke and Christian Bartelt and Heiner Stuckenschmidt}, booktitle={The Twelfth International Conference on Learning Representations}, year={2024}, url={https://openreview.net/forum?",GRANDE,SOFTWARE
"Also, please set 'objective' to 'binary', 'classification' or 'regression' based on your task.  ### Enable and specify GPU ```python import os os.environ['CUDA_VISIBLE_DEVICES'] = '0' os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true' ```  ### Load Data ```python from sklearn.model_selection import train_test_split import openml  dataset = openml.datasets.get_dataset(40536) X, y, categorical_indicator, attribute_names = dataset.get_data(target=dataset.default_target_attribute) categorical_feature_indices = [idx for idx, idx_bool in enumerate(categorical_indicator) if idx_bool]  X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42) ```  ### Preprocessing, Hyperparameters and Training  GRANDE requires categorical features to be encoded appropriately.",GRANDE,SOFTWARE
"GRANDE already archives great results with its default parameters, but a HPO can increase the performance even further.",GRANDE,SOFTWARE
"```python from GRANDE import GRANDE  params = {         'depth': 5, # tree depth         'n_estimators': 2048, # number of estimators / trees          'learning_rate_weights': 0.005, # learning rate for leaf weights         'learning_rate_index': 0.01, # learning rate for split indices         'learning_rate_values': 0.01, # learning rate for split values         'learning_rate_leaf': 0.01, # learning rate for leafs (logits)          'optimizer': 'adam', # optimizer         'cosine_decay_steps': 0, # decay steps for lr schedule (CosineDecayRestarts)          'loss': 'crossentropy', # loss function (default 'crossentropy' for binary & multi-class classification and 'mse' for regression)         'focal_loss': False, # use focal loss {True, False}         'temperature': 0.0, # temperature for stochastic re-weighted GD (0.0, 1.0)          'from_logits': True, # use logits for weighting {True, False}         'use_class_weights': True, # use class weights for training {True, False}          'dropout': 0.0, # dropout rate (here, dropout randomly disables individual estimators of the ensemble during training)          'selected_variables': 0.8, # feature subset percentage (0.0, 1.0)         'data_subset_fraction': 1.0, # data subset percentage (0.0, 1.0) }  args = {     'epochs': 1_000, # number of epochs for training     'early_stopping_epochs': 25, # patience for early stopping (best weights are restored)     'batch_size': 64,  # batch size for training      'cat_idx': categorical_feature_indices, # put list of categorical indices     'objective': 'binary', # objective / task {'binary', 'classification', 'regression'}          'random_seed': 42,     'verbose': 1,        }  model_grande = GRANDE(params=params, args=args)  model_grande.fit(X_train=X_train,           y_train=y_train,           X_val=X_valid,           y_val=y_valid)  preds_grande = model_grande.predict(X_test)  ```  ### Evaluate Model  ```python preds = model_grande.predict(X_test)  if args['objective'] == 'binary':     accuracy = sklearn.metrics.accuracy_score(y_test, np.round(preds_grande[:,1]))     f1_score = sklearn.metrics.f1_score(y_test, np.round(preds_grande[:,1]), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande[:,1], average='macro')          print('Accuracy:', accuracy)     print('F1 Score:', f1_score)     print('ROC AUC:', roc_auc) elif args['objective'] == 'classification':     accuracy = sklearn.metrics.accuracy_score(y_test, np.argmax(preds_grande, axis=1))     f1_score = sklearn.metrics.f1_score(y_test, np.argmax(preds_grande, axis=1), average='macro')     roc_auc = sklearn.metrics.roc_auc_score(y_test, preds_grande, average='macro', multi_class='ovo', labels=[i for i in range(preds_grande.shape[1])])      print('Accuracy GRANDE:', accuracy)     print('F1 Score GRANDE:', f1_score)     print('ROC AUC GRANDE:', roc_auc) else:     mean_absolute_error = sklearn.metrics.mean_absolute_error(y_test, np.round(preds_grande))     r2_score = sklearn.metrics.r2_score(y_test, np.round(preds_grande))      print('MAE GRANDE:', mean_absolute_error)     print('R2 Score GRANDE:', r2_score) ```  ## More  Please note that this is an experimental implementation which is not fully tested yet.",GRANDE,SOFTWARE
"It is built on top of ü§ó  HuggingFace [Transformers](https://github.com/huggingface/transformers) library, so you are free to choose among hundreds of models.",Transformers,SOFTWARE
"It is built on top of ü§ó  HuggingFace [Transformers](https://github.com/huggingface/transformers) library, so you are free to choose among hundreds of models.",transformers,SOFTWARE
# Demo üïπÔ∏è   We have realeased a demo on Zero-Shot Information Extraction using Textual Entailment ([ZS4IE: A toolkit for Zero-Shot Information Extraction with simple Verbalizations](https://arxiv.org/abs/2203.13602)) accepted in the [Demo Track of NAACL 2022]().,ZS4IE,SOFTWARE
The code is publicly availabe on its own GitHub repository: [ZS4IE](https://github.com/bbn-e/zs4ie),ZS4IE,SOFTWARE
# Installation  By using Pip (check the last release)  ```shell script pip install a2t ```  By clonning the repository  ```shell script git clone https://github.com/osainz59/Ask2Transformers.git cd Ask2Transformers pip install . ```  Or directly by ```shell script pip install git+https://github.com/osainz59/Ask2Transformers ```  <!,Pip,SOFTWARE
# Installation  By using Pip (check the last release)  ```shell script pip install a2t ```  By clonning the repository  ```shell script git clone https://github.com/osainz59/Ask2Transformers.git cd Ask2Transformers pip install . ```  Or directly by ```shell script pip install git+https://github.com/osainz59/Ask2Transformers ```  <!,pip,SOFTWARE
# Installation  By using Pip (check the last release)  ```shell script pip install a2t ```  By clonning the repository  ```shell script git clone https://github.com/osainz59/Ask2Transformers.git cd Ask2Transformers pip install . ```  Or directly by ```shell script pip install git+https://github.com/osainz59/Ask2Transformers ```  <!,a2t,SOFTWARE
# Installation  By using Pip (check the last release)  ```shell script pip install a2t ```  By clonning the repository  ```shell script git clone https://github.com/osainz59/Ask2Transformers.git cd Ask2Transformers pip install . ```  Or directly by ```shell script pip install git+https://github.com/osainz59/Ask2Transformers ```  <!,git,SOFTWARE
# Installation  By using Pip (check the last release)  ```shell script pip install a2t ```  By clonning the repository  ```shell script git clone https://github.com/osainz59/Ask2Transformers.git cd Ask2Transformers pip install . ```  Or directly by ```shell script pip install git+https://github.com/osainz59/Ask2Transformers ```  <!,pip,SOFTWARE
# Installation  By using Pip (check the last release)  ```shell script pip install a2t ```  By clonning the repository  ```shell script git clone https://github.com/osainz59/Ask2Transformers.git cd Ask2Transformers pip install . ```  Or directly by ```shell script pip install git+https://github.com/osainz59/Ask2Transformers ```  <!,pip,SOFTWARE
"You can try different models to perform the zero-shot classification, but they need to be finetuned on a NLI task and be compatible with the `AutoModelForSequenceClassification` class from Transformers.",Transformers,SOFTWARE
For example:  * `roberta-large-mnli` * `joeddav/xlm-roberta-large-xnli` * `facebook/bart-large-mnli` * `microsoft/deberta-v2-xlarge-mnli`   **Coming soon:** `t5-large` like generative models support.  ## Pre-trained models üÜï   We now provide (task specific) pre-trained entailment models to: (1) **reproduce** the results of the papers and (2) **reuse** them for new schemas of the same tasks.,roberta-large-mnli,SOFTWARE
For example:  * `roberta-large-mnli` * `joeddav/xlm-roberta-large-xnli` * `facebook/bart-large-mnli` * `microsoft/deberta-v2-xlarge-mnli`   **Coming soon:** `t5-large` like generative models support.  ## Pre-trained models üÜï   We now provide (task specific) pre-trained entailment models to: (1) **reproduce** the results of the papers and (2) **reuse** them for new schemas of the same tasks.,joeddav/xlm-roberta-large-xnli,SOFTWARE
For example:  * `roberta-large-mnli` * `joeddav/xlm-roberta-large-xnli` * `facebook/bart-large-mnli` * `microsoft/deberta-v2-xlarge-mnli`   **Coming soon:** `t5-large` like generative models support.  ## Pre-trained models üÜï   We now provide (task specific) pre-trained entailment models to: (1) **reproduce** the results of the papers and (2) **reuse** them for new schemas of the same tasks.,facebook/bart-large-mnli,SOFTWARE
For example:  * `roberta-large-mnli` * `joeddav/xlm-roberta-large-xnli` * `facebook/bart-large-mnli` * `microsoft/deberta-v2-xlarge-mnli`   **Coming soon:** `t5-large` like generative models support.  ## Pre-trained models üÜï   We now provide (task specific) pre-trained entailment models to: (1) **reproduce** the results of the papers and (2) **reuse** them for new schemas of the same tasks.,microsoft/deberta-v2-xlarge-mnli,SOFTWARE
For example:  * `roberta-large-mnli` * `joeddav/xlm-roberta-large-xnli` * `facebook/bart-large-mnli` * `microsoft/deberta-v2-xlarge-mnli`   **Coming soon:** `t5-large` like generative models support.  ## Pre-trained models üÜï   We now provide (task specific) pre-trained entailment models to: (1) **reproduce** the results of the papers and (2) **reuse** them for new schemas of the same tasks.,t5-large,SOFTWARE
"-- $$\text{HiTZ/A2T\_[pretrained\_model]\_[NLI\_datasets]\_[finetune\_datasets]}$$ -->  <h3 align=""center"">HiTZ/A2T_[pretrained_model]_[NLI_datasets]_[finetune_datasets]</h3>   - `pretrained_model`: The checkpoint used for initialization.",A2T,SOFTWARE
"In our experiments, we have used the publicly available [run_glue.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py) python script (from HuggingFace Transformers).",transformers,SOFTWARE
"In our experiments, we have used the publicly available [run_glue.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py) python script (from HuggingFace Transformers).",python,SOFTWARE
"In our experiments, we have used the publicly available [run_glue.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py) python script (from HuggingFace Transformers).",Transformers,SOFTWARE
"A configuration file containing the task and evaluation information should look like this:  ```json {     ""name"": ""BabelDomains"",     ""task_name"": ""topic-classification"",     ""features_class"": ""a2t.tasks.text_classification.TopicClassificationFeatures"",     ""hypothesis_template"": ""The domain of the sentence is about {label}."",     ""nli_models"": [         ""roberta-large-mnli""     ],     ""labels"": [         ""Animals"",         ""Art, architecture, and archaeology"",         ""Biology"",         ""Business, economics, and finance"",         ""Chemistry and mineralogy"",         ""Computing"",         ""Culture and society"",         ...",roberta-large-mnli,SOFTWARE
You can find the bibtex files in their corresponding [aclanthology](https://aclanthology.org/) page,bibtex,SOFTWARE
"-- ```bibtex @inproceedings{sainz-etal-2022-textual,   doi = {10.48550/ARXIV.2205.01376},   url = {https://arxiv.org/abs/2205.01376},   author = {Sainz, Oscar and Gonzalez-Dios, Itziar and de Lacalle, Oier Lopez and Min, Bonan and Agirre, Eneko},   keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},    title = {Textual Entailment for Event Argument Extraction: Zero- and Few-Shot with Multi-Source Learning},   publisher = {arXiv},   year = {2022},   copyright = {Creative Commons Attribution Share Alike 4.0 International} }  ```  Cite this paper if you want to cite stuff related to Relation Extraction, etc.",bibtex,SOFTWARE
[Python 3.8](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/) [!,Python 3.8,SOFTWARE
[Python 3.8](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/) [!,python-3.6,SOFTWARE
[Blueprint of FinGPT](https://arxiv.org/abs/2306.06031)  <https://huggingface.co/FinGPT>  [!,FinGPT,SOFTWARE
"user=AI4Finance-Foundation&repo=FinGPT&countColor=%23B17A)   ## What's New:  - [Model Release] Nov, 2023: We release [FinGPT-Forecaster](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster)!",FinGPT-Forecaster,SOFTWARE
"user=AI4Finance-Foundation&repo=FinGPT&countColor=%23B17A)   ## What's New:  - [Model Release] Nov, 2023: We release [FinGPT-Forecaster](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster)!",fingpt,SOFTWARE
"user=AI4Finance-Foundation&repo=FinGPT&countColor=%23B17A)   ## What's New:  - [Model Release] Nov, 2023: We release [FinGPT-Forecaster](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster)!",FinGPT_Forecaster,SOFTWARE
"üî• [Demo](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster), [Medium Blog](https://medium.datadriveninvestor.com/introducing-fingpt-forecaster-the-future-of-robo-advisory-services-50add34e3d3c) & [Model](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) are available on Huggingfaceü§ó !",FinGPT/FinGPT-Forecaster,SOFTWARE
"üî• [Demo](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster), [Medium Blog](https://medium.datadriveninvestor.com/introducing-fingpt-forecaster-the-future-of-robo-advisory-services-50add34e3d3c) & [Model](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) are available on Huggingfaceü§ó !",FinGPT/fingpt-forecaster_dow30_llama2-7b_lora,SOFTWARE
"[BloombergGPT](https://arxiv.org/abs/2303.17564) trained an LLM using a mixture of finance data and general-purpose data, which took about 53 days, at a cost of around **$3M**).",BloombergGPT,SOFTWARE
"It is costly to retrain an LLM model like BloombergGPT every month or every week, thus lightweight adaptation is highly favorable.",BloombergGPT,SOFTWARE
"The key technology is ""RLHF (Reinforcement learning from human feedback)"", which is missing in BloombergGPT.",BloombergGPT,SOFTWARE
"RLHF enables an LLM model to learn individual preferences (risk-aversion level, investing habits, personalized robo-advisor, etc.), which is the ""secret"" ingredient of ChatGPT and GPT4.   ### Milestone of AI Robo-Advisor: FinGPT-Forecaster  Try the latest released FinGPT-Forecaster demo at our [HuggingFace Space](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster)  The dataset for FinGPT-Forecaster: https://huggingface.co/datasets/FinGPT/fingpt-forecaster-dow30-202305-202405  !",ChatGPT,SOFTWARE
"RLHF enables an LLM model to learn individual preferences (risk-aversion level, investing habits, personalized robo-advisor, etc.), which is the ""secret"" ingredient of ChatGPT and GPT4.   ### Milestone of AI Robo-Advisor: FinGPT-Forecaster  Try the latest released FinGPT-Forecaster demo at our [HuggingFace Space](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster)  The dataset for FinGPT-Forecaster: https://huggingface.co/datasets/FinGPT/fingpt-forecaster-dow30-202305-202405  !",GPT4,SOFTWARE
"RLHF enables an LLM model to learn individual preferences (risk-aversion level, investing habits, personalized robo-advisor, etc.), which is the ""secret"" ingredient of ChatGPT and GPT4.   ### Milestone of AI Robo-Advisor: FinGPT-Forecaster  Try the latest released FinGPT-Forecaster demo at our [HuggingFace Space](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster)  The dataset for FinGPT-Forecaster: https://huggingface.co/datasets/FinGPT/fingpt-forecaster-dow30-202305-202405  !",FinGPT-Forecaster,SOFTWARE
"RLHF enables an LLM model to learn individual preferences (risk-aversion level, investing habits, personalized robo-advisor, etc.), which is the ""secret"" ingredient of ChatGPT and GPT4.   ### Milestone of AI Robo-Advisor: FinGPT-Forecaster  Try the latest released FinGPT-Forecaster demo at our [HuggingFace Space](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster)  The dataset for FinGPT-Forecaster: https://huggingface.co/datasets/FinGPT/fingpt-forecaster-dow30-202305-202405  !",FinGPT-Forecaster,SOFTWARE
"RLHF enables an LLM model to learn individual preferences (risk-aversion level, investing habits, personalized robo-advisor, etc.), which is the ""secret"" ingredient of ChatGPT and GPT4.   ### Milestone of AI Robo-Advisor: FinGPT-Forecaster  Try the latest released FinGPT-Forecaster demo at our [HuggingFace Space](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster)  The dataset for FinGPT-Forecaster: https://huggingface.co/datasets/FinGPT/fingpt-forecaster-dow30-202305-202405  !",FinGPT/FinGPT-Forecaster,SOFTWARE
"RLHF enables an LLM model to learn individual preferences (risk-aversion level, investing habits, personalized robo-advisor, etc.), which is the ""secret"" ingredient of ChatGPT and GPT4.   ### Milestone of AI Robo-Advisor: FinGPT-Forecaster  Try the latest released FinGPT-Forecaster demo at our [HuggingFace Space](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster)  The dataset for FinGPT-Forecaster: https://huggingface.co/datasets/FinGPT/fingpt-forecaster-dow30-202305-202405  !",FinGPT-Forecaster,SOFTWARE
"/fingpt)      * What's new: **Best trainable and inferable FinGPT for sentiment analysis on a single RTX 3090, which is even better than GPT-4 and ChatGPT Finetuning.**      * [FinGPT v3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) series are LLMs finetuned with the LoRA method on the News and Tweets sentiment analysis dataset which achieve the best scores on most of the financial sentiment analysis datasets with low cost",FinGPT,SOFTWARE
"/fingpt)      * What's new: **Best trainable and inferable FinGPT for sentiment analysis on a single RTX 3090, which is even better than GPT-4 and ChatGPT Finetuning.**      * [FinGPT v3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) series are LLMs finetuned with the LoRA method on the News and Tweets sentiment analysis dataset which achieve the best scores on most of the financial sentiment analysis datasets with low cost",GPT-4,SOFTWARE
"/fingpt)      * What's new: **Best trainable and inferable FinGPT for sentiment analysis on a single RTX 3090, which is even better than GPT-4 and ChatGPT Finetuning.**      * [FinGPT v3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) series are LLMs finetuned with the LoRA method on the News and Tweets sentiment analysis dataset which achieve the best scores on most of the financial sentiment analysis datasets with low cost",ChatGPT,SOFTWARE
"/fingpt)      * What's new: **Best trainable and inferable FinGPT for sentiment analysis on a single RTX 3090, which is even better than GPT-4 and ChatGPT Finetuning.**      * [FinGPT v3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) series are LLMs finetuned with the LoRA method on the News and Tweets sentiment analysis dataset which achieve the best scores on most of the financial sentiment analysis datasets with low cost",FinGPT v3,SOFTWARE
"/fingpt)      * What's new: **Best trainable and inferable FinGPT for sentiment analysis on a single RTX 3090, which is even better than GPT-4 and ChatGPT Finetuning.**      * [FinGPT v3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) series are LLMs finetuned with the LoRA method on the News and Tweets sentiment analysis dataset which achieve the best scores on most of the financial sentiment analysis datasets with low cost",FinGPT/fingpt-sentiment_llama2-13b_lora,SOFTWARE
* FinGPT v3.3 use llama2-13b as base model; FinGPT v3.2 uses llama2-7b as base model; FinGPT v3.1 uses chatglm2-6B as base model,FinGPT v3.3,SOFTWARE
* FinGPT v3.3 use llama2-13b as base model; FinGPT v3.2 uses llama2-7b as base model; FinGPT v3.1 uses chatglm2-6B as base model,llama2-13b,SOFTWARE
* FinGPT v3.3 use llama2-13b as base model; FinGPT v3.2 uses llama2-7b as base model; FinGPT v3.1 uses chatglm2-6B as base model,FinGPT v3.2,SOFTWARE
* FinGPT v3.3 use llama2-13b as base model; FinGPT v3.2 uses llama2-7b as base model; FinGPT v3.1 uses chatglm2-6B as base model,llama2-7b,SOFTWARE
* FinGPT v3.3 use llama2-13b as base model; FinGPT v3.2 uses llama2-7b as base model; FinGPT v3.1 uses chatglm2-6B as base model,FinGPT v3.1,SOFTWARE
* FinGPT v3.3 use llama2-13b as base model; FinGPT v3.2 uses llama2-7b as base model; FinGPT v3.1 uses chatglm2-6B as base model,chatglm2-6B,SOFTWARE
"* Benchmark Results:      * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |     | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |     | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 √ó RTX 3090    | 17.25 hours |     $17.25     |     | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 √ó RTX 3090    | 6.47 hours  |     $ 6.47     |     | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 √ó RTX 3090    | 4.15 hours  |     $ 4.15     |     | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |     | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |     | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 √ó NVIDIA K80 GPU |      -      |       -        |     | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 √ó A100     |   21 days   | $ 4.23 million |     | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 √ó A100     |   53 days   | $ 2.67 million |        **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs.",FinGPT v3.3,SOFTWARE
"* Benchmark Results:      * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |     | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |     | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 √ó RTX 3090    | 17.25 hours |     $17.25     |     | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 √ó RTX 3090    | 6.47 hours  |     $ 6.47     |     | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 √ó RTX 3090    | 4.15 hours  |     $ 4.15     |     | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |     | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |     | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 √ó NVIDIA K80 GPU |      -      |       -        |     | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 √ó A100     |   21 days   | $ 4.23 million |     | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 √ó A100     |   53 days   | $ 2.67 million |        **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs.",FinGPT/fingpt-sentiment_llama2-13b_lora,SOFTWARE
"* Benchmark Results:      * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |     | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |     | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 √ó RTX 3090    | 17.25 hours |     $17.25     |     | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 √ó RTX 3090    | 6.47 hours  |     $ 6.47     |     | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 √ó RTX 3090    | 4.15 hours  |     $ 4.15     |     | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |     | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |     | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 √ó NVIDIA K80 GPU |      -      |       -        |     | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 √ó A100     |   21 days   | $ 4.23 million |     | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 √ó A100     |   53 days   | $ 2.67 million |        **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs.",FinGPT v3.2,SOFTWARE
"* Benchmark Results:      * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |     | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |     | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 √ó RTX 3090    | 17.25 hours |     $17.25     |     | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 √ó RTX 3090    | 6.47 hours  |     $ 6.47     |     | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 √ó RTX 3090    | 4.15 hours  |     $ 4.15     |     | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |     | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |     | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 √ó NVIDIA K80 GPU |      -      |       -        |     | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 √ó A100     |   21 days   | $ 4.23 million |     | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 √ó A100     |   53 days   | $ 2.67 million |        **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs.",FinGPT v3.1,SOFTWARE
"* Benchmark Results:      * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |     | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |     | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 √ó RTX 3090    | 17.25 hours |     $17.25     |     | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 √ó RTX 3090    | 6.47 hours  |     $ 6.47     |     | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 √ó RTX 3090    | 4.15 hours  |     $ 4.15     |     | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |     | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |     | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 √ó NVIDIA K80 GPU |      -      |       -        |     | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 √ó A100     |   21 days   | $ 4.23 million |     | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 √ó A100     |   53 days   | $ 2.67 million |        **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs.",FinGPT (8bit),SOFTWARE
"* Benchmark Results:      * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |     | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |     | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 √ó RTX 3090    | 17.25 hours |     $17.25     |     | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 √ó RTX 3090    | 6.47 hours  |     $ 6.47     |     | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 √ó RTX 3090    | 4.15 hours  |     $ 4.15     |     | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |     | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |     | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 √ó NVIDIA K80 GPU |      -      |       -        |     | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 √ó A100     |   21 days   | $ 4.23 million |     | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 √ó A100     |   53 days   | $ 2.67 million |        **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs.",FinGPT (QLoRA),SOFTWARE
"* Benchmark Results:      * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |     | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |     | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 √ó RTX 3090    | 17.25 hours |     $17.25     |     | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 √ó RTX 3090    | 6.47 hours  |     $ 6.47     |     | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 √ó RTX 3090    | 4.15 hours  |     $ 4.15     |     | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |     | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |     | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 √ó NVIDIA K80 GPU |      -      |       -        |     | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 √ó A100     |   21 days   | $ 4.23 million |     | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 √ó A100     |   53 days   | $ 2.67 million |        **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs.",GPT-4,SOFTWARE
"* Benchmark Results:      * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |     | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |     | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 √ó RTX 3090    | 17.25 hours |     $17.25     |     | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 √ó A100      |  5.5 hours  |    $ 22.55     |     | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 √ó RTX 3090    | 6.47 hours  |     $ 6.47     |     | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 √ó RTX 3090    | 4.15 hours  |     $ 4.15     |     | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |     | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |     | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 √ó NVIDIA K80 GPU |      -      |       -        |     | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 √ó A100     |   21 days   | $ 4.23 million |     | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 √ó A100     |   53 days   | $ 2.67 million |        **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs.",BloombergGPT,SOFTWARE
"Note that BloombergGPT also used p4d.24xlarge As of July 11, 2023, the hourly rate for this instance stands at $32.773.",BloombergGPT,SOFTWARE
"**BloombergGPT estimated cost= 512 x 53 x 24 = 651,264 GPU hours x $4.10 = $2,670,182.40**.",BloombergGPT,SOFTWARE
* Finetune your own FinGPT v3 model with the LoRA method on only an RTX 3090 with this [notebook](.,FinGPT v3,SOFTWARE
/fingpt/FinGPT_Sentiment_Analysis_v3/training_int4/train.ipynb) in int4 (QLoRA)    * [FinGPT V1](.,FinGPT V1,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",fingpt-mt_llama2-7b_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT/fingpt-mt_llama2-7b_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",Llama2-7b,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",fingpt-mt_falcon-7b_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT/fingpt-mt_falcon-7b_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",falcon-7b,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",fingpt-mt_bloom-7b1_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT/fingpt-mt_bloom-7b1_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",bloom-7b1,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",fingpt-mt_mpt-7b_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT/fingpt-mt_mpt-7b_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",mpt-7b,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",fingpt-mt_chatglm2-6b_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT/fingpt-mt_chatglm2-6b_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",chatglm-6b,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT/fingpt-mt_qwen-7b_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",qwen-7b,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",fingpt-sentiment_llama2-13b_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT/fingpt-sentiment_llama2-13b_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",llama2-13b,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",fingpt-forecaster_dow30_llama2-7b_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",FinGPT/fingpt-forecaster_dow30_llama2-7b_lora,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",llama2-7b,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",ChatGLM2‚Äì6B,SOFTWARE
"Please choose an answer from {Yes/No}.',       'Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.',] ```    | Models | Description  | Function |   | --------- | --------------------- |---------------- |   | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |   | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |   | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |   | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |   | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |   | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |   | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |   | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |     ## Tutorials [[Training] Beginner‚Äôs Guide to FinGPT: Training with LoRA and ChatGLM2‚Äì6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)  ## Understanding FinGPT: An Educational Blog Series + [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications ](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8) + [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance ](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca) + [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models ](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)   ## FinGPT Ecosystem ### FinGPT embraces a full-stack framework for FinLLMs with five layers: 1.",chatglm2-6b,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",Llama-2,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",llama,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",Llama-2,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",llama-2-7b,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",meta-llama/Llama-2-7b-hf,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",Llama-2-13b,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",meta-llama/Llama-2-13b-hf,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",llama-2,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",Falcon,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",falcon,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",falcon-7b,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",tiiuae/falcon-7b,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",MPT,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",MPT,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",mpt-7b,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",mosaicml/mpt-7b,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",Bloom,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",bloom-7b1,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",bloom-7b1,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",ChatGLM2,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",ChatGLM2-6B,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",chatglm2-6b,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",chatglm2-6b,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",qwen-7b,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",tangger/Qwen-7B-Chat,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",InternLM,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",InternLM,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",internlm-7b,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",internlm/internlm-7b,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",Llama2,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",Falcon,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",MPT,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",Bloom,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",ChatGLM2,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",Qwen,SOFTWARE
"| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications | |  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  | | [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor | | [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis | | [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis | | [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| World‚Äôs largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis | | [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary | | [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis| | [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |  * Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):   | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |   | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |   | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |   | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |   | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|   | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|  ### All Thanks To Our Contributors : <a href=""https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors"">   <img src=""https://contrib.rocks/image?",InternLM,SOFTWARE
"repo=AI4Finance-Foundation/FinGPT"" /> </a>  ## News  + [Columbia Perspectives on ChatGPT](https://datascience.columbia.edu/news/2023/columbia-perspectives-on-chatgpt/?",ChatGPT,SOFTWARE
"repo=AI4Finance-Foundation/FinGPT"" /> </a>  ## News  + [Columbia Perspectives on ChatGPT](https://datascience.columbia.edu/news/2023/columbia-perspectives-on-chatgpt/?",chatgpt,SOFTWARE
utm_source=sendinblue&utm_campaign=DSI%20Newsletter%20April%202023&utm_medium=email) + [MIT Technology Review] [ChatGPT is about to revolutionize the economy.,ChatGPT,SOFTWARE
We need to decide what that looks like](https://www.technologyreview.com/2023/03/25/1070275/chatgpt-revolutionize-economy-decide-what-looks-like/) + [BloombergGPT] [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564) + [Finextra] [ChatGPT and Bing AI to sit as panellists at fintech conference](https://www.finextra.com/newsarticle/41973/chatgpt-and-bing-ai-to-sit-as-panellists-at-fintech-conference)  ## ChatGPT at AI4Finance  + [YouTube video] [I Built a Trading Bot with ChatGPT](https://www.youtube.com/watch?,chatgpt,SOFTWARE
We need to decide what that looks like](https://www.technologyreview.com/2023/03/25/1070275/chatgpt-revolutionize-economy-decide-what-looks-like/) + [BloombergGPT] [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564) + [Finextra] [ChatGPT and Bing AI to sit as panellists at fintech conference](https://www.finextra.com/newsarticle/41973/chatgpt-and-bing-ai-to-sit-as-panellists-at-fintech-conference)  ## ChatGPT at AI4Finance  + [YouTube video] [I Built a Trading Bot with ChatGPT](https://www.youtube.com/watch?,BloombergGPT,SOFTWARE
We need to decide what that looks like](https://www.technologyreview.com/2023/03/25/1070275/chatgpt-revolutionize-economy-decide-what-looks-like/) + [BloombergGPT] [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564) + [Finextra] [ChatGPT and Bing AI to sit as panellists at fintech conference](https://www.finextra.com/newsarticle/41973/chatgpt-and-bing-ai-to-sit-as-panellists-at-fintech-conference)  ## ChatGPT at AI4Finance  + [YouTube video] [I Built a Trading Bot with ChatGPT](https://www.youtube.com/watch?,ChatGPT,SOFTWARE
We need to decide what that looks like](https://www.technologyreview.com/2023/03/25/1070275/chatgpt-revolutionize-economy-decide-what-looks-like/) + [BloombergGPT] [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564) + [Finextra] [ChatGPT and Bing AI to sit as panellists at fintech conference](https://www.finextra.com/newsarticle/41973/chatgpt-and-bing-ai-to-sit-as-panellists-at-fintech-conference)  ## ChatGPT at AI4Finance  + [YouTube video] [I Built a Trading Bot with ChatGPT](https://www.youtube.com/watch?,chatgpt,SOFTWARE
We need to decide what that looks like](https://www.technologyreview.com/2023/03/25/1070275/chatgpt-revolutionize-economy-decide-what-looks-like/) + [BloombergGPT] [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564) + [Finextra] [ChatGPT and Bing AI to sit as panellists at fintech conference](https://www.finextra.com/newsarticle/41973/chatgpt-and-bing-ai-to-sit-as-panellists-at-fintech-conference)  ## ChatGPT at AI4Finance  + [YouTube video] [I Built a Trading Bot with ChatGPT](https://www.youtube.com/watch?,ChatGPT,SOFTWARE
We need to decide what that looks like](https://www.technologyreview.com/2023/03/25/1070275/chatgpt-revolutionize-economy-decide-what-looks-like/) + [BloombergGPT] [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564) + [Finextra] [ChatGPT and Bing AI to sit as panellists at fintech conference](https://www.finextra.com/newsarticle/41973/chatgpt-and-bing-ai-to-sit-as-panellists-at-fintech-conference)  ## ChatGPT at AI4Finance  + [YouTube video] [I Built a Trading Bot with ChatGPT](https://www.youtube.com/watch?,ChatGPT,SOFTWARE
"v=fhBw3j_O9LE), combining ChatGPT and FinRL. + [Hey, ChatGPT!",ChatGPT,SOFTWARE
"v=fhBw3j_O9LE), combining ChatGPT and FinRL. + [Hey, ChatGPT!",ChatGPT,SOFTWARE
(https://medium.com/@ai4finance/hey-chatgpt-explain-finrl-code-to-me-6a91d612296f)  ## Introductory  + [Sparks of artificial general intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712) + [GPT-4] [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) + [InstructGPT] [Training language models to follow instructions with human feedback](https://openreview.net/forum?,GPT-4,SOFTWARE
(https://medium.com/@ai4finance/hey-chatgpt-explain-finrl-code-to-me-6a91d612296f)  ## Introductory  + [Sparks of artificial general intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712) + [GPT-4] [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) + [InstructGPT] [Training language models to follow instructions with human feedback](https://openreview.net/forum?,GPT-4,SOFTWARE
(https://medium.com/@ai4finance/hey-chatgpt-explain-finrl-code-to-me-6a91d612296f)  ## Introductory  + [Sparks of artificial general intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712) + [GPT-4] [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) + [InstructGPT] [Training language models to follow instructions with human feedback](https://openreview.net/forum?,GPT-4,SOFTWARE
(https://medium.com/@ai4finance/hey-chatgpt-explain-finrl-code-to-me-6a91d612296f)  ## Introductory  + [Sparks of artificial general intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712) + [GPT-4] [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) + [InstructGPT] [Training language models to follow instructions with human feedback](https://openreview.net/forum?,InstructGPT,SOFTWARE
"Open AI's GPT-1, GPT-2, GPT-3",GPT-1,SOFTWARE
"Open AI's GPT-1, GPT-2, GPT-3",GPT-2,SOFTWARE
"Open AI's GPT-1, GPT-2, GPT-3",GPT-3,SOFTWARE
"(https://lifearchitect.ai/whats-in-my-ai/) A Comprehensive Analysis of Datasets Used to Train GPT-1, GPT-2, GPT-3, GPT-NeoX-20B, Megatron-11B, MT-NLG, and Gopher  + [FinRL-Meta Repo](https://github.com/AI4Finance-Foundation/FinRL-Meta) and paper [FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0bf54b80686d2c4dc0808c2e98d430f7-Abstract-Datasets_and_Benchmarks.html).",GPT-1,SOFTWARE
"(https://lifearchitect.ai/whats-in-my-ai/) A Comprehensive Analysis of Datasets Used to Train GPT-1, GPT-2, GPT-3, GPT-NeoX-20B, Megatron-11B, MT-NLG, and Gopher  + [FinRL-Meta Repo](https://github.com/AI4Finance-Foundation/FinRL-Meta) and paper [FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0bf54b80686d2c4dc0808c2e98d430f7-Abstract-Datasets_and_Benchmarks.html).",GPT-2,SOFTWARE
"(https://lifearchitect.ai/whats-in-my-ai/) A Comprehensive Analysis of Datasets Used to Train GPT-1, GPT-2, GPT-3, GPT-NeoX-20B, Megatron-11B, MT-NLG, and Gopher  + [FinRL-Meta Repo](https://github.com/AI4Finance-Foundation/FinRL-Meta) and paper [FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0bf54b80686d2c4dc0808c2e98d430f7-Abstract-Datasets_and_Benchmarks.html).",GPT-3,SOFTWARE
"(https://lifearchitect.ai/whats-in-my-ai/) A Comprehensive Analysis of Datasets Used to Train GPT-1, GPT-2, GPT-3, GPT-NeoX-20B, Megatron-11B, MT-NLG, and Gopher  + [FinRL-Meta Repo](https://github.com/AI4Finance-Foundation/FinRL-Meta) and paper [FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0bf54b80686d2c4dc0808c2e98d430f7-Abstract-Datasets_and_Benchmarks.html).",GPT-NeoX-20B,SOFTWARE
"(https://lifearchitect.ai/whats-in-my-ai/) A Comprehensive Analysis of Datasets Used to Train GPT-1, GPT-2, GPT-3, GPT-NeoX-20B, Megatron-11B, MT-NLG, and Gopher  + [FinRL-Meta Repo](https://github.com/AI4Finance-Foundation/FinRL-Meta) and paper [FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0bf54b80686d2c4dc0808c2e98d430f7-Abstract-Datasets_and_Benchmarks.html).",Megatron-11B,SOFTWARE
"(https://lifearchitect.ai/whats-in-my-ai/) A Comprehensive Analysis of Datasets Used to Train GPT-1, GPT-2, GPT-3, GPT-NeoX-20B, Megatron-11B, MT-NLG, and Gopher  + [FinRL-Meta Repo](https://github.com/AI4Finance-Foundation/FinRL-Meta) and paper [FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0bf54b80686d2c4dc0808c2e98d430f7-Abstract-Datasets_and_Benchmarks.html).",MT-NLG,SOFTWARE
"(https://lifearchitect.ai/whats-in-my-ai/) A Comprehensive Analysis of Datasets Used to Train GPT-1, GPT-2, GPT-3, GPT-NeoX-20B, Megatron-11B, MT-NLG, and Gopher  + [FinRL-Meta Repo](https://github.com/AI4Finance-Foundation/FinRL-Meta) and paper [FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0bf54b80686d2c4dc0808c2e98d430f7-Abstract-Datasets_and_Benchmarks.html).",Gopher,SOFTWARE
Plus advice on effective GPT-3 prompt programming & avoiding common errors.  ## ChatGPT for FinTech  **ChatGPT Trading Bot** + [YouTube video] [ChatGPT Trading strategy 20097% returns](https://www.youtube.com/watch?,GPT-3,SOFTWARE
Plus advice on effective GPT-3 prompt programming & avoiding common errors.  ## ChatGPT for FinTech  **ChatGPT Trading Bot** + [YouTube video] [ChatGPT Trading strategy 20097% returns](https://www.youtube.com/watch?,ChatGPT,SOFTWARE
Plus advice on effective GPT-3 prompt programming & avoiding common errors.  ## ChatGPT for FinTech  **ChatGPT Trading Bot** + [YouTube video] [ChatGPT Trading strategy 20097% returns](https://www.youtube.com/watch?,ChatGPT,SOFTWARE
Plus advice on effective GPT-3 prompt programming & avoiding common errors.  ## ChatGPT for FinTech  **ChatGPT Trading Bot** + [YouTube video] [ChatGPT Trading strategy 20097% returns](https://www.youtube.com/watch?,ChatGPT,SOFTWARE
v=unsa_gXPAJ4) + [YouTube video] [ChatGPT Coding - Make A Profitable Trading Strategy In Five Minutes!],ChatGPT,SOFTWARE
v=4SG2884RcDY) + [YouTube video] [Easy Automated Live Trading using ChatGPT (+9660.3% hands free)](https://www.youtube.com/watch?,ChatGPT,SOFTWARE
v=dIEZVPVOZPQ) + [YouTube video] [ChatGPT Trading Strategy 893% Returns](https://www.youtube.com/watch?,ChatGPT,SOFTWARE
v=YxjvjK5AD2M) + [YouTube video] [ChatGPT 10 Million Trading Strategy](https://www.youtube.com/watch?,ChatGPT,SOFTWARE
v=9VPfd08uU4Q) + [YouTube video] [ChatGPT: Your Crypto Assistant](https://www.youtube.com/watch?,ChatGPT,SOFTWARE
v=LpzeshX6s2w) + [YouTube video] [Generate Insane Trading Returns with ChatGPT and TradingView](https://www.youtube.com/watch?,ChatGPT,SOFTWARE
v=LpzeshX6s2w) + [YouTube video] [Generate Insane Trading Returns with ChatGPT and TradingView](https://www.youtube.com/watch?,TradingView,SOFTWARE
"---  **(Fast and accurate) Sentiment Analysis**     GPT-3 can help study customer surveys, social media tweets from customers/users.",GPT-3,SOFTWARE
"model=text-davinci-003)    Financial News + [Algorithmic Trading using Sentiment Analysis on News Articles](https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704) + [Accessing Historical Financial News Headlines with Python](https://python.plainenglish.io/access-historical-financial-news-headlines-with-python-be1b8faaea9f)  **PromptNet** Analogy to ImageNet and WordNet, it is critical to build a PromptNet",text-davinci-003,SOFTWARE
"+ [Awesome_Prompting_Papers_in_Computer_Vision](https://github.com/ttengwang/Awesome_Prompting_Papers_in_Computer_Vision) + [OpenPrompt](https://github.com/thunlp/OpenPrompt) + [promptsource](https://github.com/bigscience-workshop/promptsource)  **Robo-advisor**  **Coding-tutor**  + [Hey, ChatGPT!",ChatGPT,SOFTWARE
(https://medium.com/@ai4finance/hey-chatgpt-explain-finrl-code-to-me-6a91d612296f)  **Blogs about ChatGPT for FinTech**  ## ChatGPT APIs  Prompting as a new programming paradigm!,chatgpt,SOFTWARE
(https://medium.com/@ai4finance/hey-chatgpt-explain-finrl-code-to-me-6a91d612296f)  **Blogs about ChatGPT for FinTech**  ## ChatGPT APIs  Prompting as a new programming paradigm!,ChatGPT,SOFTWARE
(https://medium.com/@ai4finance/hey-chatgpt-explain-finrl-code-to-me-6a91d612296f)  **Blogs about ChatGPT for FinTech**  ## ChatGPT APIs  Prompting as a new programming paradigm!,ChatGPT,SOFTWARE
+ [Towards Data Science] [GPT-3: Creative Potential of NLP](https://towardsdatascience.com/gpt-3-creative-potential-of-nlp-d5ccae16c1ab) + [YouTube video] [OpenAI GPT-3 - Prompt Engineering For Financial NLP](https://www.youtube.com/watch?,GPT-3,SOFTWARE
+ [Towards Data Science] [GPT-3: Creative Potential of NLP](https://towardsdatascience.com/gpt-3-creative-potential-of-nlp-d5ccae16c1ab) + [YouTube video] [OpenAI GPT-3 - Prompt Engineering For Financial NLP](https://www.youtube.com/watch?,GPT-3,SOFTWARE
v=Nl2Cdbao5Ws)  + [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3) + [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper) + [OpenAI Examples Library](https://platform.openai.com/examples) + [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API. + [Exploring the Capabilities of the ChatGPT API: A Beginner‚Äôs Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f) + [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)  **Prompting programming**  ## ChatGPT relatives:   [A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.,GPT-3,SOFTWARE
v=Nl2Cdbao5Ws)  + [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3) + [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper) + [OpenAI Examples Library](https://platform.openai.com/examples) + [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API. + [Exploring the Capabilities of the ChatGPT API: A Beginner‚Äôs Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f) + [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)  **Prompting programming**  ## ChatGPT relatives:   [A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.,gpt-3,SOFTWARE
v=Nl2Cdbao5Ws)  + [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3) + [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper) + [OpenAI Examples Library](https://platform.openai.com/examples) + [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API. + [Exploring the Capabilities of the ChatGPT API: A Beginner‚Äôs Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f) + [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)  **Prompting programming**  ## ChatGPT relatives:   [A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.,ChatGPT,SOFTWARE
v=Nl2Cdbao5Ws)  + [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3) + [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper) + [OpenAI Examples Library](https://platform.openai.com/examples) + [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API. + [Exploring the Capabilities of the ChatGPT API: A Beginner‚Äôs Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f) + [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)  **Prompting programming**  ## ChatGPT relatives:   [A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.,chatgpt,SOFTWARE
v=Nl2Cdbao5Ws)  + [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3) + [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper) + [OpenAI Examples Library](https://platform.openai.com/examples) + [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API. + [Exploring the Capabilities of the ChatGPT API: A Beginner‚Äôs Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f) + [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)  **Prompting programming**  ## ChatGPT relatives:   [A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.,GPT-3,SOFTWARE
v=Nl2Cdbao5Ws)  + [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3) + [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper) + [OpenAI Examples Library](https://platform.openai.com/examples) + [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API. + [Exploring the Capabilities of the ChatGPT API: A Beginner‚Äôs Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f) + [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)  **Prompting programming**  ## ChatGPT relatives:   [A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.,GPT-3,SOFTWARE
v=Nl2Cdbao5Ws)  + [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3) + [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper) + [OpenAI Examples Library](https://platform.openai.com/examples) + [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API. + [Exploring the Capabilities of the ChatGPT API: A Beginner‚Äôs Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f) + [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)  **Prompting programming**  ## ChatGPT relatives:   [A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.,ChatGPT,SOFTWARE
v=Nl2Cdbao5Ws)  + [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3) + [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper) + [OpenAI Examples Library](https://platform.openai.com/examples) + [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API. + [Exploring the Capabilities of the ChatGPT API: A Beginner‚Äôs Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f) + [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)  **Prompting programming**  ## ChatGPT relatives:   [A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.,git,SOFTWARE
v=Nl2Cdbao5Ws)  + [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3) + [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper) + [OpenAI Examples Library](https://platform.openai.com/examples) + [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API. + [Exploring the Capabilities of the ChatGPT API: A Beginner‚Äôs Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f) + [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)  **Prompting programming**  ## ChatGPT relatives:   [A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.,ChatGPT,SOFTWARE
v=Nl2Cdbao5Ws)  + [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3) + [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper) + [OpenAI Examples Library](https://platform.openai.com/examples) + [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API. + [Exploring the Capabilities of the ChatGPT API: A Beginner‚Äôs Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f) + [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)  **Prompting programming**  ## ChatGPT relatives:   [A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.,ChatGPT,SOFTWARE
v=Nl2Cdbao5Ws)  + [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3) + [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper) + [OpenAI Examples Library](https://platform.openai.com/examples) + [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API. + [Exploring the Capabilities of the ChatGPT API: A Beginner‚Äôs Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f) + [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)  **Prompting programming**  ## ChatGPT relatives:   [A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.,ChatGPT,SOFTWARE
[PaLM](https://arxiv.org/abs/2204.02311)  [Chincella](https://arxiv.org/abs/2203.15556)  Interesting evaluations: + [RLHF for pretraining](https://arxiv.org/abs/2302.08582)  + [Compare ChatGPT with GPT3.5](https://arxiv.org/pdf/2302.06476.pdf)  + [Is ChatGPT A Good Translator?,PaLM,SOFTWARE
[PaLM](https://arxiv.org/abs/2204.02311)  [Chincella](https://arxiv.org/abs/2203.15556)  Interesting evaluations: + [RLHF for pretraining](https://arxiv.org/abs/2302.08582)  + [Compare ChatGPT with GPT3.5](https://arxiv.org/pdf/2302.06476.pdf)  + [Is ChatGPT A Good Translator?,ChatGPT,SOFTWARE
[PaLM](https://arxiv.org/abs/2204.02311)  [Chincella](https://arxiv.org/abs/2203.15556)  Interesting evaluations: + [RLHF for pretraining](https://arxiv.org/abs/2302.08582)  + [Compare ChatGPT with GPT3.5](https://arxiv.org/pdf/2302.06476.pdf)  + [Is ChatGPT A Good Translator?,GPT3.5,SOFTWARE
[PaLM](https://arxiv.org/abs/2204.02311)  [Chincella](https://arxiv.org/abs/2203.15556)  Interesting evaluations: + [RLHF for pretraining](https://arxiv.org/abs/2302.08582)  + [Compare ChatGPT with GPT3.5](https://arxiv.org/pdf/2302.06476.pdf)  + [Is ChatGPT A Good Translator?,ChatGPT,SOFTWARE
"A Preliminary Study](https://arxiv.org/pdf/2301.08745.pdf)  + [A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity](https://arxiv.org/pdf/2302.04023.pdf)  [YouTube video] [Physics Solution: ChatGPT vs.",ChatGPT,SOFTWARE
"A Preliminary Study](https://arxiv.org/pdf/2301.08745.pdf)  + [A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity](https://arxiv.org/pdf/2302.04023.pdf)  [YouTube video] [Physics Solution: ChatGPT vs.",ChatGPT,SOFTWARE
"If you decide to use this dataset for your own work, please consider citing the following works:  *Stochastic reconstruction of an oolitic limestone by generative adversarial networks*[[ArXiv](https://arxiv.org/abs/1712.02854)]   *Dynamic reservoir-condition microtomography of reactive transport in complex carbonates*[[Article](https://www.sciencedirect.com/science/article/pii/S0016703717300789)]  Due to their size we provide the necessary files via a [Google Drive](https://drive.google.com/open?",Google Drive,SOFTWARE
The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed.    ## Requirements  The conditioning tool is based on the following libraries: - [Python 2.7](https://anaconda.org/) - [Pytorch 0.3](www.pytorch.org) - [Scikit-Learn](www.scikit-learn.org) - [tqdm](https://github.com/noamraph/tqdm) - [numpy](www.numpy.org) - [matplotlib](www.matplotlib.org)  We recommend using the [anaconda](https://anaconda.org/) distribution to install the required dependencies.  ## Development  Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks.,Python 2.7,SOFTWARE
The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed.    ## Requirements  The conditioning tool is based on the following libraries: - [Python 2.7](https://anaconda.org/) - [Pytorch 0.3](www.pytorch.org) - [Scikit-Learn](www.scikit-learn.org) - [tqdm](https://github.com/noamraph/tqdm) - [numpy](www.numpy.org) - [matplotlib](www.matplotlib.org)  We recommend using the [anaconda](https://anaconda.org/) distribution to install the required dependencies.  ## Development  Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks.,anaconda,SOFTWARE
The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed.    ## Requirements  The conditioning tool is based on the following libraries: - [Python 2.7](https://anaconda.org/) - [Pytorch 0.3](www.pytorch.org) - [Scikit-Learn](www.scikit-learn.org) - [tqdm](https://github.com/noamraph/tqdm) - [numpy](www.numpy.org) - [matplotlib](www.matplotlib.org)  We recommend using the [anaconda](https://anaconda.org/) distribution to install the required dependencies.  ## Development  Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks.,Pytorch 0.3,SOFTWARE
The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed.    ## Requirements  The conditioning tool is based on the following libraries: - [Python 2.7](https://anaconda.org/) - [Pytorch 0.3](www.pytorch.org) - [Scikit-Learn](www.scikit-learn.org) - [tqdm](https://github.com/noamraph/tqdm) - [numpy](www.numpy.org) - [matplotlib](www.matplotlib.org)  We recommend using the [anaconda](https://anaconda.org/) distribution to install the required dependencies.  ## Development  Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks.,Scikit-Learn,SOFTWARE
The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed.    ## Requirements  The conditioning tool is based on the following libraries: - [Python 2.7](https://anaconda.org/) - [Pytorch 0.3](www.pytorch.org) - [Scikit-Learn](www.scikit-learn.org) - [tqdm](https://github.com/noamraph/tqdm) - [numpy](www.numpy.org) - [matplotlib](www.matplotlib.org)  We recommend using the [anaconda](https://anaconda.org/) distribution to install the required dependencies.  ## Development  Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks.,scikit-learn,SOFTWARE
The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed.    ## Requirements  The conditioning tool is based on the following libraries: - [Python 2.7](https://anaconda.org/) - [Pytorch 0.3](www.pytorch.org) - [Scikit-Learn](www.scikit-learn.org) - [tqdm](https://github.com/noamraph/tqdm) - [numpy](www.numpy.org) - [matplotlib](www.matplotlib.org)  We recommend using the [anaconda](https://anaconda.org/) distribution to install the required dependencies.  ## Development  Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks.,tqdm,SOFTWARE
The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed.    ## Requirements  The conditioning tool is based on the following libraries: - [Python 2.7](https://anaconda.org/) - [Pytorch 0.3](www.pytorch.org) - [Scikit-Learn](www.scikit-learn.org) - [tqdm](https://github.com/noamraph/tqdm) - [numpy](www.numpy.org) - [matplotlib](www.matplotlib.org)  We recommend using the [anaconda](https://anaconda.org/) distribution to install the required dependencies.  ## Development  Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks.,tqdm,SOFTWARE
The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed.    ## Requirements  The conditioning tool is based on the following libraries: - [Python 2.7](https://anaconda.org/) - [Pytorch 0.3](www.pytorch.org) - [Scikit-Learn](www.scikit-learn.org) - [tqdm](https://github.com/noamraph/tqdm) - [numpy](www.numpy.org) - [matplotlib](www.matplotlib.org)  We recommend using the [anaconda](https://anaconda.org/) distribution to install the required dependencies.  ## Development  Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks.,numpy,SOFTWARE
The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed.    ## Requirements  The conditioning tool is based on the following libraries: - [Python 2.7](https://anaconda.org/) - [Pytorch 0.3](www.pytorch.org) - [Scikit-Learn](www.scikit-learn.org) - [tqdm](https://github.com/noamraph/tqdm) - [numpy](www.numpy.org) - [matplotlib](www.matplotlib.org)  We recommend using the [anaconda](https://anaconda.org/) distribution to install the required dependencies.  ## Development  Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks.,numpy,SOFTWARE
The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed.    ## Requirements  The conditioning tool is based on the following libraries: - [Python 2.7](https://anaconda.org/) - [Pytorch 0.3](www.pytorch.org) - [Scikit-Learn](www.scikit-learn.org) - [tqdm](https://github.com/noamraph/tqdm) - [numpy](www.numpy.org) - [matplotlib](www.matplotlib.org)  We recommend using the [anaconda](https://anaconda.org/) distribution to install the required dependencies.  ## Development  Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks.,matplotlib,SOFTWARE
The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed.    ## Requirements  The conditioning tool is based on the following libraries: - [Python 2.7](https://anaconda.org/) - [Pytorch 0.3](www.pytorch.org) - [Scikit-Learn](www.scikit-learn.org) - [tqdm](https://github.com/noamraph/tqdm) - [numpy](www.numpy.org) - [matplotlib](www.matplotlib.org)  We recommend using the [anaconda](https://anaconda.org/) distribution to install the required dependencies.  ## Development  Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks.,matplotlib,SOFTWARE
The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed.    ## Requirements  The conditioning tool is based on the following libraries: - [Python 2.7](https://anaconda.org/) - [Pytorch 0.3](www.pytorch.org) - [Scikit-Learn](www.scikit-learn.org) - [tqdm](https://github.com/noamraph/tqdm) - [numpy](www.numpy.org) - [matplotlib](www.matplotlib.org)  We recommend using the [anaconda](https://anaconda.org/) distribution to install the required dependencies.  ## Development  Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks.,anaconda,SOFTWARE
The resulting images show that each realization honors the data at the well exactly and an ellipsoidal region of influence can be observed.    ## Requirements  The conditioning tool is based on the following libraries: - [Python 2.7](https://anaconda.org/) - [Pytorch 0.3](www.pytorch.org) - [Scikit-Learn](www.scikit-learn.org) - [tqdm](https://github.com/noamraph/tqdm) - [numpy](www.numpy.org) - [matplotlib](www.matplotlib.org)  We recommend using the [anaconda](https://anaconda.org/) distribution to install the required dependencies.  ## Development  Currently the code has limited object oriented design and is usable as demonstrated by the accompanying jupyter notebooks.,anaconda,SOFTWARE
"# Fine-Grained Representation Learning and Recognition by Exploiting Hierarchical Semantic Embedding  # Introduction This repository contains the pytorch codes, trained models, and datasets described in the paper ""[Fine-Grained Representation Learning and Recognition by Exploiting Hierarchical Semantic Embedding](https://arxiv.org/abs/1808.04505)"".",pytorch,SOFTWARE
"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1.",pytorch,SOFTWARE
"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1.",torch-0.4.0,SOFTWARE
"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1.",CUDA,SOFTWARE
"**Results**  - Accuracy on Caltech UCSD Birds  |        | order | family | genus | class | | :----: | :---: | :----: | :---: | :---: | |baseline| 98.8  |  95.0  |  91.5 |  85.2 | |HSE(ours)| 98.8 |  95.7  |  92.7 |  88.1 |    - Accuracy on Butterfly200  |        | family | subfamily | genus | species | | :----: | :----: | :-------: | :---: | :-----: | |baseline|  98.9  |   97.6    |  94.8 |  85.1   | |HSE(ours)| 98.9  |   97.7    |  95.4 |  86.1   |  - Accuracy on Vegfru  |           |  sup  |  sub  | | :-------: | :---: | :---: | |  baseline | 90.0  |  87.1 | | HSE(ours) | 90.0  |  89.4 |   # Installation  **Requirement**  - pytorch, tested on [v0.4.0](http://download.pytorch.org/whl/cu80/torch-0.4.0-cp27-cp27mu-linux_x86_64.whl) - CUDA, tested on v8.0 - Language: Python 2.7   ## 1.",Python 2.7,SOFTWARE
Clone the repository Clone the Hierarchical Semantic Embedding project by: ``` git clone https://github.com/HCPLab-SYSU/HSE.git ``` and we denote the folder `hse-mm2018` as `$HSE_ROOT`.,git,SOFTWARE
"[Vegfru](https://github.com/ustc-vim/vegfru) is proposed by [Hou et al., ICCV2017](http://home.ustc.edu.cn/~saihui/project/vegfru/iccv17_vegfru.pdf), and it covers two-level categories.  ## 3.",Vegfru,SOFTWARE
"[Vegfru](https://github.com/ustc-vim/vegfru) is proposed by [Hou et al., ICCV2017](http://home.ustc.edu.cn/~saihui/project/vegfru/iccv17_vegfru.pdf), and it covers two-level categories.  ## 3.",vegfru,SOFTWARE
"Download trained models The trained models of our HSE framework and the baseline methods on the extended Caltech UCSD birds, Butterfly-200, and VegFru datasets can be downloaded from [OneDrive](https://1drv.ms/f/s!",HSE,SOFTWARE
"Download trained models The trained models of our HSE framework and the baseline methods on the extended Caltech UCSD birds, Butterfly-200, and VegFru datasets can be downloaded from [OneDrive](https://1drv.ms/f/s!",OneDrive,SOFTWARE
ArFSFaZzVErwgQgrVfHGmHiaMzOc) or [Baidu Cloud](https://pan.baidu.com/s/1WWalFQFiNCCrWr30pvEA6A).  ## 4.,Baidu Cloud,SOFTWARE
"We show furthermore that our method also applies to 3d light microscopy data of drosophila neurons, which exhibit extreme cases of complex shape clusters.  ## Installation  This package requires Python 3 and PyTorch.",Python 3,SOFTWARE
"We show furthermore that our method also applies to 3d light microscopy data of drosophila neurons, which exhibit extreme cases of complex shape clusters.  ## Installation  This package requires Python 3 and PyTorch.",PyTorch,SOFTWARE
"**Note** Previous versions (e.g., for the experiments published in our ECCV 2020 paper) require TensorFlow 1.x.",TensorFlow 1.x,SOFTWARE
"If you want to run older experiments please checkout the respective tag: [eccv2020](https://github.com/Kainmueller-Lab/PatchPerPix/tree/ea4e2d4) If you have any questions, please open an issue (and mention that you're running the older code)  The recommended way is to install the package into your conda/python virtual environment.",conda,SOFTWARE
"If you want to run older experiments please checkout the respective tag: [eccv2020](https://github.com/Kainmueller-Lab/PatchPerPix/tree/ea4e2d4) If you have any questions, please open an issue (and mention that you're running the older code)  The recommended way is to install the package into your conda/python virtual environment.",python,SOFTWARE
"We recommend to use conda to install torch (tested with torch 1.13, but newer versions should work, too).",torch,SOFTWARE
"We recommend to use conda to install torch (tested with torch 1.13, but newer versions should work, too).",torch 1.13,SOFTWARE
"It expects all used arrays (e.g., raw image data and labels) to be placed in a single zarr file (organized into a hierarchy via groups, see [zarr documentation](https://zarr.readthedocs.io/en/stable/tutorial.html#groups)).",zarr,SOFTWARE
"It expects all used arrays (e.g., raw image data and labels) to be placed in a single zarr file (organized into a hierarchy via groups, see [zarr documentation](https://zarr.readthedocs.io/en/stable/tutorial.html#groups)).",zarr,SOFTWARE
"It expects all used arrays (e.g., raw image data and labels) to be placed in a single zarr file (organized into a hierarchy via groups, see [zarr documentation](https://zarr.readthedocs.io/en/stable/tutorial.html#groups)).",zarr,SOFTWARE
"The tasks specified after `--do` depend on what you want to do: ``` python run_ppp.py --setup setup01 --config ppp_experiments/flylight_setup01_230614__123456/config.toml --do  validate_checkpoints predict decode label evaluate --app wormbodies -id experiments/flylight_setup01_230614__123456 ```  ### Available Sub-Tasks  | Task                   | Short Description                                                                                      | |------------------------|--------------------------------------------------------------------------------------------------------| | `all`                  | equal to `mknet train validate_checkpoints predict decode label postprocess evaluate`                  | | `infer`                | equal to `predict decode label evaluate`                                                               | | `mknet`                | creates a graph of the network (only for tensorflow 1)                                                 | | `train`                | executes the training of the network                                                                   | | `validate_checkpoints` | performs validation (over stored model checkpoints and a set of hyperparameters)                       | | `validate`             | performs validation (for a specific model checkpoint and over a set of hyperparameters)                | | `predict`              | executes the trained network in inference mode and computes predictions                                | | `decode`               | decodes predicted patch encodings to full patches (only if model was trained to output encodings)      | | `label`                | computes final instances based on predicted patches                                                    | | `postprocess`          | post-processes predictions and predicted instances (optional, mostly for manual inspection of results) | | `evaluate`             | compares predicted instances to ground truth instances and computes quantitative evaluation            |   ## Results  (for more details on the results see [PatchPerPix for Instance Segmentation](https://arxiv.org/abs/2001.07626))  ### BBBC010  ([BBBC010: C. elegans live/dead assay](https://bbbc.broadinstitute.org/BBBC010))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method                   | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |--------------------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Inst.Seg via Layering[1] | 0.754                       | 0.936           | 0.919           | 0.865           | 0.761           | 0.290           | | PatchPerPix (ppp+dec)    | **0.816**                   | **0.960**       | **0.955**       | **0.931**       | **0.805**       | **0.428**       |  [1] results from: [Instance Segmentation of Dense and Overlapping Objects via Layering](https://arxiv.org/abs/2210.03551)   ### ISBI2012  (server with leaderboard is down, but data is still available: [ISBI 2012 Segmentation Challenge](https://imagej.net/events/isbi-2012-segmentation-challenge)  | Method      | rRAND        | rINF         | |-------------|--------------|--------------| | PatchPerPix | **0.988290** | 0.991544     | | MWS[2]      | 0.987922     | **0.991833** | | MWS-Dense   | 0.979112     | 0.989625     |  [2] results from leaderboard (offline, see also [The Mutex Watershed: Efficient, Parameter-Free Image Partitioning](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Steffen_Wolf_The_Mutex_Watershed_ECCV_2018_paper.pdf))   ### dsb2018  ([Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/), train/val/test split defined by [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method        | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |---------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | Mask R-CNN[3] | 0.594                       | -               | -               | -               | -               | 0.832           | 0.773           | 0.684           | 0.489           | 0.189           | | StarDist[3]   | 0.584                       | -               | -               | -               | -               | 0.864           | 0.804           | 0.685           | 0.450           | 0.119           | | PatchPerPix   | **0.693**                   | **0.919**       | **0.919**       | **0.915**       | **0.898**       | **0.868**       | **0.827**       | **0.755**       | **0.635**       | **0.379**       |  [3] results from [Cell Detection with Star-convex Polygons](https://arxiv.org/abs/1806.03535)   ### nuclei3d  ([https://doi.org/10.5281/zenodo.5942574](https://doi.org/10.5281/zenodo.5942574), train/val/test split defined by [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636))<br> ($S = \frac{TP}{TP+FP+FN}$; TP, FP, FN computed per image; averaged across images; localized using IoU)   | Method         | avS<sub>[0.5:0.9:0.1]</sub> | S<sub>0.1</sub> | S<sub>0.2</sub> | S<sub>0.3</sub> | S<sub>0.4</sub> | S<sub>0.5</sub> | S<sub>0.6</sub> | S<sub>0.7</sub> | S<sub>0.8</sub> | S<sub>0.9</sub> | |----------------|-----------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------| | MALA[4]        | 0.381                       | 0.895           | 0.887           | 0.859           | 0.803           | 0.699           | 0.605           | 0.424           | 0.166           | 0.012           | | StarDist3d[5]  | 0.406                       | 0.936           | 0.926           | 0.905           | **0.855**       | 0.765           | 0.647           | 0.460           | 0.154           | 0.004           | | 3-label+cpv[6] | 0.425                       | **0.937**       | **0.930**       | **0.907**       | 0.848           | 0.750           | 0.641           | 0.473           | 0.224           | **0.035**       | | PatchPerPix    | **0.436**                   | 0.926           | 0.918           | 0.900           | 0.853           | **0.766**       | **0.668**       | **0.493**       | **0.228**       | 0.027           |  [4] [Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction](https://arxiv.org/pdf/1709.02974.pdf), we computed the results<br> [5] results from [Star-convex Polyhedra for 3D Object Detection and Segmentation in Microscopy](https://arxiv.org/abs/1908.03636)<br> [6] results from [An Auxiliary Task for Learning Nuclei Segmentation in 3D Microscopy Images](https://arxiv.org/abs/2002.02857)   ### FlyLight  ([The FlyLight Instance Segmentation Datset](https://kainmueller-lab.github.io/flylight_inst_seg_dataset/), train/val/test split defined by tba)  | Metrik         | short description              | |----------------|--------------------------------| | S              | average of avF1 and C          | | avF1           | Multi-Threshold F1 Score       | | C              | Average ground Truth coverage  | | C<sub>TP</sub> | Average true positive coverage | | FS             | Number of false splits         | | FM             | Number of false merges         |  (for a precise definition see tba)  <br>Trained on *completely* labeled data, evaluated on *completely* labeled data and *partly* labeled data combined: | Method                                                                                                | S | avF1 | C | C<sub>TP</TP> | FS | FM | |-------------------------------------------------------------------------------------------------------|---|------|---|---------------|----|----| | PatchPerPix&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |   |      |   |               |    |    | |                                                                                                       |   |      |   |               |    |    |  Trained on *completely* labeled and *partly* labeled data combined, evaluated on *completely* labeled data and *partly* labeled data combined: | Method               | S | avF1 | C | C<sub>TP</TP> | FS | FM | |----------------------|---|------|---|---------------|----|----| | PatchPerPix(+partly) |   |      |   |               |    |    | |                      |   |      |   |               |    |    |   ## Contributing  If you would like to contribute, have encountered any issues or have any suggestions, please open an issue on this GitHub repository.",GitHub,SOFTWARE
"First download the sample dataset you want to run the reasoners on, then run the jar file and provide the location of the dataset as first argument like this:  ```java -jar semrec-caligraph-elk-hermit.jar <PATH-TO-DATASET-FILE> &> log_elk-hermit.txt```  The result is a realization of the input dataset through the selected reasoners.",java,SOFTWARE
"# Science-Driven Optimization of the LSST Observing Strategy  Welcome to the online community thinking about LSST survey strategy (""cadence""), with quantifications via the Metric Analysis Framework.",Metric Analysis Framework,SOFTWARE
MAF metric calculations are then being designed and implemented: these form the quantitative backbone of the document.,MAF,SOFTWARE
"branch=master)](https://travis-ci.org/LSSTScienceCollaborations/ObservingStrategy)  * **[Download v1.0 of the white paper](https://github.com/LSSTScienceCollaborations/ObservingStrategy/raw/master/whitepaper/releases/LSST_Observing_Strategy_White_Paper_v1.0.pdf)** This is the initial arxiv version, visible at https://arxiv.org/abs/1708.04058  * **[Join the conversation about this project at its issues list](https://github.com/LSSTScienceCollaborations/ObservingStrategy/issues)**  * **[Gauge the project's activity level](https://github.com/LSSTScienceCollaborations/ObservingStrategy/pulse/halfweekly)**  * **[Suggest a new `OpSim` experiment](https://github.com/LSSTScienceCollaborations/ObservingStrategy/blob/master/opsim/README.md)**  * **[Suggest some interesting commissioning observations](https://github.com/LSSTScienceCollaborations/ObservingStrategy/blob/master/commissioning/README.md)**   #### Shortcuts  * Guidelines for how to get involved are [in the introduction of the white paper itself, in Section 1.4](https://github.com/LSSTScienceCollaborations/ObservingStrategy/blob/master/whitepaper/preface.tex) * For help getting started with git and GitHub, see this [handy guide](https://github.com/drphilmarshall/GettingStarted#top). * Shortened URL for this repository (for passing around): [http://ls.st/o5k](http://ls.st/o5k) * Community-contributed MAF metric code at the [`sims_maf_contrib` repository](https://github.com/LSST-nonproject/sims_maf_contrib/wiki)  <!",git,SOFTWARE
"branch=master)](https://travis-ci.org/LSSTScienceCollaborations/ObservingStrategy)  * **[Download v1.0 of the white paper](https://github.com/LSSTScienceCollaborations/ObservingStrategy/raw/master/whitepaper/releases/LSST_Observing_Strategy_White_Paper_v1.0.pdf)** This is the initial arxiv version, visible at https://arxiv.org/abs/1708.04058  * **[Join the conversation about this project at its issues list](https://github.com/LSSTScienceCollaborations/ObservingStrategy/issues)**  * **[Gauge the project's activity level](https://github.com/LSSTScienceCollaborations/ObservingStrategy/pulse/halfweekly)**  * **[Suggest a new `OpSim` experiment](https://github.com/LSSTScienceCollaborations/ObservingStrategy/blob/master/opsim/README.md)**  * **[Suggest some interesting commissioning observations](https://github.com/LSSTScienceCollaborations/ObservingStrategy/blob/master/commissioning/README.md)**   #### Shortcuts  * Guidelines for how to get involved are [in the introduction of the white paper itself, in Section 1.4](https://github.com/LSSTScienceCollaborations/ObservingStrategy/blob/master/whitepaper/preface.tex) * For help getting started with git and GitHub, see this [handy guide](https://github.com/drphilmarshall/GettingStarted#top). * Shortened URL for this repository (for passing around): [http://ls.st/o5k](http://ls.st/o5k) * Community-contributed MAF metric code at the [`sims_maf_contrib` repository](https://github.com/LSST-nonproject/sims_maf_contrib/wiki)  <!",GitHub,SOFTWARE
"branch=master)](https://travis-ci.org/LSSTScienceCollaborations/ObservingStrategy)  * **[Download v1.0 of the white paper](https://github.com/LSSTScienceCollaborations/ObservingStrategy/raw/master/whitepaper/releases/LSST_Observing_Strategy_White_Paper_v1.0.pdf)** This is the initial arxiv version, visible at https://arxiv.org/abs/1708.04058  * **[Join the conversation about this project at its issues list](https://github.com/LSSTScienceCollaborations/ObservingStrategy/issues)**  * **[Gauge the project's activity level](https://github.com/LSSTScienceCollaborations/ObservingStrategy/pulse/halfweekly)**  * **[Suggest a new `OpSim` experiment](https://github.com/LSSTScienceCollaborations/ObservingStrategy/blob/master/opsim/README.md)**  * **[Suggest some interesting commissioning observations](https://github.com/LSSTScienceCollaborations/ObservingStrategy/blob/master/commissioning/README.md)**   #### Shortcuts  * Guidelines for how to get involved are [in the introduction of the white paper itself, in Section 1.4](https://github.com/LSSTScienceCollaborations/ObservingStrategy/blob/master/whitepaper/preface.tex) * For help getting started with git and GitHub, see this [handy guide](https://github.com/drphilmarshall/GettingStarted#top). * Shortened URL for this repository (for passing around): [http://ls.st/o5k](http://ls.st/o5k) * Community-contributed MAF metric code at the [`sims_maf_contrib` repository](https://github.com/LSST-nonproject/sims_maf_contrib/wiki)  <!",MAF,SOFTWARE
"# DiHT ""Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training"" [[Paper](https://arxiv.org/abs/2301.02280)]  ## Installation  ```bash git clone https://github.com/facebookresearch/diht cd diht pip install -r requirements.txt pip install -e",DiHT,SOFTWARE
"# DiHT ""Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training"" [[Paper](https://arxiv.org/abs/2301.02280)]  ## Installation  ```bash git clone https://github.com/facebookresearch/diht cd diht pip install -r requirements.txt pip install -e",git,SOFTWARE
"# DiHT ""Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training"" [[Paper](https://arxiv.org/abs/2301.02280)]  ## Installation  ```bash git clone https://github.com/facebookresearch/diht cd diht pip install -r requirements.txt pip install -e",diht,SOFTWARE
"# DiHT ""Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training"" [[Paper](https://arxiv.org/abs/2301.02280)]  ## Installation  ```bash git clone https://github.com/facebookresearch/diht cd diht pip install -r requirements.txt pip install -e",pip,SOFTWARE
"# DiHT ""Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training"" [[Paper](https://arxiv.org/abs/2301.02280)]  ## Installation  ```bash git clone https://github.com/facebookresearch/diht cd diht pip install -r requirements.txt pip install -e",pip,SOFTWARE
"# To install as an editable package ```  You can use `pip install .` to install the codebase as a non-editable package.  ## Example  ```python import torch  from diht import model_zoo from PIL import Image   text_tokenizer, image_transform, model = model_zoo.load_model(     ""diht_vitl14_336px"", is_train=False )  image = Image.open(""infer_image.png"").convert(""RGB"") image = image_transform(image).unsqueeze(0) text_captions = [""a mountain"", ""a beach"", ""a desert""] text = text_tokenizer(text_captions)  with torch.no_grad():     image_features, text_features, logit_scale = model(image, text)     logits_per_image = logit_scale * image_features @ text_features.T     probs = logits_per_image.softmax(dim=-1).numpy()  print(f""text captions: {text_captions}"") print(f""text caption probs: {probs}"") ```  The above code snippet should output ```bash text captions: ['a mountain', 'a beach', 'a desert'] text caption probs: [[0.99370664 0.00514017 0.00115326]] ```  ## Running on GPU/CPU  By default the model runs on CPU, to run on GPU you can do `model = model.to(torch.device(""cuda""))`.",pip,SOFTWARE
"# To install as an editable package ```  You can use `pip install .` to install the codebase as a non-editable package.  ## Example  ```python import torch  from diht import model_zoo from PIL import Image   text_tokenizer, image_transform, model = model_zoo.load_model(     ""diht_vitl14_336px"", is_train=False )  image = Image.open(""infer_image.png"").convert(""RGB"") image = image_transform(image).unsqueeze(0) text_captions = [""a mountain"", ""a beach"", ""a desert""] text = text_tokenizer(text_captions)  with torch.no_grad():     image_features, text_features, logit_scale = model(image, text)     logits_per_image = logit_scale * image_features @ text_features.T     probs = logits_per_image.softmax(dim=-1).numpy()  print(f""text captions: {text_captions}"") print(f""text caption probs: {probs}"") ```  The above code snippet should output ```bash text captions: ['a mountain', 'a beach', 'a desert'] text caption probs: [[0.99370664 0.00514017 0.00115326]] ```  ## Running on GPU/CPU  By default the model runs on CPU, to run on GPU you can do `model = model.to(torch.device(""cuda""))`.",torch,SOFTWARE
"# To install as an editable package ```  You can use `pip install .` to install the codebase as a non-editable package.  ## Example  ```python import torch  from diht import model_zoo from PIL import Image   text_tokenizer, image_transform, model = model_zoo.load_model(     ""diht_vitl14_336px"", is_train=False )  image = Image.open(""infer_image.png"").convert(""RGB"") image = image_transform(image).unsqueeze(0) text_captions = [""a mountain"", ""a beach"", ""a desert""] text = text_tokenizer(text_captions)  with torch.no_grad():     image_features, text_features, logit_scale = model(image, text)     logits_per_image = logit_scale * image_features @ text_features.T     probs = logits_per_image.softmax(dim=-1).numpy()  print(f""text captions: {text_captions}"") print(f""text caption probs: {probs}"") ```  The above code snippet should output ```bash text captions: ['a mountain', 'a beach', 'a desert'] text caption probs: [[0.99370664 0.00514017 0.00115326]] ```  ## Running on GPU/CPU  By default the model runs on CPU, to run on GPU you can do `model = model.to(torch.device(""cuda""))`.",diht,SOFTWARE
"# To install as an editable package ```  You can use `pip install .` to install the codebase as a non-editable package.  ## Example  ```python import torch  from diht import model_zoo from PIL import Image   text_tokenizer, image_transform, model = model_zoo.load_model(     ""diht_vitl14_336px"", is_train=False )  image = Image.open(""infer_image.png"").convert(""RGB"") image = image_transform(image).unsqueeze(0) text_captions = [""a mountain"", ""a beach"", ""a desert""] text = text_tokenizer(text_captions)  with torch.no_grad():     image_features, text_features, logit_scale = model(image, text)     logits_per_image = logit_scale * image_features @ text_features.T     probs = logits_per_image.softmax(dim=-1).numpy()  print(f""text captions: {text_captions}"") print(f""text caption probs: {probs}"") ```  The above code snippet should output ```bash text captions: ['a mountain', 'a beach', 'a desert'] text caption probs: [[0.99370664 0.00514017 0.00115326]] ```  ## Running on GPU/CPU  By default the model runs on CPU, to run on GPU you can do `model = model.to(torch.device(""cuda""))`.",PIL,SOFTWARE
"# To install as an editable package ```  You can use `pip install .` to install the codebase as a non-editable package.  ## Example  ```python import torch  from diht import model_zoo from PIL import Image   text_tokenizer, image_transform, model = model_zoo.load_model(     ""diht_vitl14_336px"", is_train=False )  image = Image.open(""infer_image.png"").convert(""RGB"") image = image_transform(image).unsqueeze(0) text_captions = [""a mountain"", ""a beach"", ""a desert""] text = text_tokenizer(text_captions)  with torch.no_grad():     image_features, text_features, logit_scale = model(image, text)     logits_per_image = logit_scale * image_features @ text_features.T     probs = logits_per_image.softmax(dim=-1).numpy()  print(f""text captions: {text_captions}"") print(f""text caption probs: {probs}"") ```  The above code snippet should output ```bash text captions: ['a mountain', 'a beach', 'a desert'] text caption probs: [[0.99370664 0.00514017 0.00115326]] ```  ## Running on GPU/CPU  By default the model runs on CPU, to run on GPU you can do `model = model.to(torch.device(""cuda""))`.",torch,SOFTWARE
"# To install as an editable package ```  You can use `pip install .` to install the codebase as a non-editable package.  ## Example  ```python import torch  from diht import model_zoo from PIL import Image   text_tokenizer, image_transform, model = model_zoo.load_model(     ""diht_vitl14_336px"", is_train=False )  image = Image.open(""infer_image.png"").convert(""RGB"") image = image_transform(image).unsqueeze(0) text_captions = [""a mountain"", ""a beach"", ""a desert""] text = text_tokenizer(text_captions)  with torch.no_grad():     image_features, text_features, logit_scale = model(image, text)     logits_per_image = logit_scale * image_features @ text_features.T     probs = logits_per_image.softmax(dim=-1).numpy()  print(f""text captions: {text_captions}"") print(f""text caption probs: {probs}"") ```  The above code snippet should output ```bash text captions: ['a mountain', 'a beach', 'a desert'] text caption probs: [[0.99370664 0.00514017 0.00115326]] ```  ## Running on GPU/CPU  By default the model runs on CPU, to run on GPU you can do `model = model.to(torch.device(""cuda""))`.",numpy,SOFTWARE
"# To install as an editable package ```  You can use `pip install .` to install the codebase as a non-editable package.  ## Example  ```python import torch  from diht import model_zoo from PIL import Image   text_tokenizer, image_transform, model = model_zoo.load_model(     ""diht_vitl14_336px"", is_train=False )  image = Image.open(""infer_image.png"").convert(""RGB"") image = image_transform(image).unsqueeze(0) text_captions = [""a mountain"", ""a beach"", ""a desert""] text = text_tokenizer(text_captions)  with torch.no_grad():     image_features, text_features, logit_scale = model(image, text)     logits_per_image = logit_scale * image_features @ text_features.T     probs = logits_per_image.softmax(dim=-1).numpy()  print(f""text captions: {text_captions}"") print(f""text caption probs: {probs}"") ```  The above code snippet should output ```bash text captions: ['a mountain', 'a beach', 'a desert'] text caption probs: [[0.99370664 0.00514017 0.00115326]] ```  ## Running on GPU/CPU  By default the model runs on CPU, to run on GPU you can do `model = model.to(torch.device(""cuda""))`.",torch,SOFTWARE
"# To install as an editable package ```  You can use `pip install .` to install the codebase as a non-editable package.  ## Example  ```python import torch  from diht import model_zoo from PIL import Image   text_tokenizer, image_transform, model = model_zoo.load_model(     ""diht_vitl14_336px"", is_train=False )  image = Image.open(""infer_image.png"").convert(""RGB"") image = image_transform(image).unsqueeze(0) text_captions = [""a mountain"", ""a beach"", ""a desert""] text = text_tokenizer(text_captions)  with torch.no_grad():     image_features, text_features, logit_scale = model(image, text)     logits_per_image = logit_scale * image_features @ text_features.T     probs = logits_per_image.softmax(dim=-1).numpy()  print(f""text captions: {text_captions}"") print(f""text caption probs: {probs}"") ```  The above code snippet should output ```bash text captions: ['a mountain', 'a beach', 'a desert'] text caption probs: [[0.99370664 0.00514017 0.00115326]] ```  ## Running on GPU/CPU  By default the model runs on CPU, to run on GPU you can do `model = model.to(torch.device(""cuda""))`.",cuda,SOFTWARE
The image and text tensors will also have to be transferred accordingly.  ## Available Models  ```python import diht   print(diht.available_models()) ```  ## Example ImageNet-1K zero-shot evaluation  A simple image classification zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download ImageNet-1K dataset from the original website.,diht,SOFTWARE
The image and text tensors will also have to be transferred accordingly.  ## Available Models  ```python import diht   print(diht.available_models()) ```  ## Example ImageNet-1K zero-shot evaluation  A simple image classification zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download ImageNet-1K dataset from the original website.,diht,SOFTWARE
Edit `IMAGENET_ROOT` in `example_imagenet_eval.py` to match the location on your machine. ``` python example_imagenet_eval.py ``` For DiHT-L/14@336 the output should look like: ``` ImageNet1K acc@1 for diht_vitl14_336px: 77.9 ```  ## Example retrieval zero-shot evaluation  A simple retrieval zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download COCO and Flickr30K datasets from the original websites.,DiHT,SOFTWARE
Edit `IMAGENET_ROOT` in `example_imagenet_eval.py` to match the location on your machine. ``` python example_imagenet_eval.py ``` For DiHT-L/14@336 the output should look like: ``` ImageNet1K acc@1 for diht_vitl14_336px: 77.9 ```  ## Example retrieval zero-shot evaluation  A simple retrieval zero-shot evaluation using a single GPU can be performed by running: > **Note**: Download COCO and Flickr30K datasets from the original websites.,diht,SOFTWARE
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",DiHT,SOFTWARE
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",diht,SOFTWARE
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",diht,SOFTWARE
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",diht,SOFTWARE
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",diht,SOFTWARE
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",diht,SOFTWARE
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",diht,SOFTWARE
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",diht,SOFTWARE
"Edit `COCO_ROOT` and `FLICKR30K_ROOT` in `example_retrieval_eval.py` to match the locations on your machine. ``` python example_retrieval_eval.py ``` For DiHT-L/14@336 the output should look like: ``` COCO T2I r@1 for diht_vitl14_336px: 49.3 COCO I2T r@1 for diht_vitl14_336px: 65.3  Flickr30K T2I r@1 for diht_vitl14_336px: 78.2 Flickr30K I2T r@1 for diht_vitl14_336px: 91.1 ```  ## Zero-shot model performance  | Model | ImageNet-1K | COCO T2I | COCO I2T | Flickr30K T2I | Flickr30K I2T | | :---  |    :----:   |  :----:  |  :----:  |     :----:    |     :----:    | |       |  Accuracy@1 | Recall@1 | Recall@1 |    Recall@1   |    Recall@1   | | diht_vitb32_224px      | 68.0 | 40.6 | 59.3 | 68.6 | 84.4 | | diht_vitb16_224px      | 72.2 | 43.3 | 60.3 | 72.9 | 89.8 | | diht_vitl14_224px      | 77.0 | 48.0 | 65.1 | 76.7 | 92.0 | | diht_vitl14_336px      | 77.9 | 49.3 | 65.3 | 78.2 | 91.1 |   ## Citation If you find this model useful, please consider citing our preprint using the citation below. ``` @article{rdk+23,   title = {Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training},   author = {Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv},   journal = {arXiv:2301.02280},   year = {2023} } ```  ## License ``` Copyright (c) Meta Platforms, Inc. and affiliates.",diht,SOFTWARE
For Debian / Ubuntu / similar:      sudo apt install python3 python3-pip python3-virtualenv poppler-utils  Download the code and prepare the python environment:      git clone https://github.com/mattbierbaum/arxiv-public-datasets     cd arxiv-public-datasets      virtualenv venv     . venv/bin/activate      pip3 install -e .,Debian,SOFTWARE
For Debian / Ubuntu / similar:      sudo apt install python3 python3-pip python3-virtualenv poppler-utils  Download the code and prepare the python environment:      git clone https://github.com/mattbierbaum/arxiv-public-datasets     cd arxiv-public-datasets      virtualenv venv     . venv/bin/activate      pip3 install -e .,python3,SOFTWARE
For Debian / Ubuntu / similar:      sudo apt install python3 python3-pip python3-virtualenv poppler-utils  Download the code and prepare the python environment:      git clone https://github.com/mattbierbaum/arxiv-public-datasets     cd arxiv-public-datasets      virtualenv venv     . venv/bin/activate      pip3 install -e .,pip,SOFTWARE
For Debian / Ubuntu / similar:      sudo apt install python3 python3-pip python3-virtualenv poppler-utils  Download the code and prepare the python environment:      git clone https://github.com/mattbierbaum/arxiv-public-datasets     cd arxiv-public-datasets      virtualenv venv     . venv/bin/activate      pip3 install -e .,git,SOFTWARE
For Debian / Ubuntu / similar:      sudo apt install python3 python3-pip python3-virtualenv poppler-utils  Download the code and prepare the python environment:      git clone https://github.com/mattbierbaum/arxiv-public-datasets     cd arxiv-public-datasets      virtualenv venv     . venv/bin/activate      pip3 install -e .,virtualenv,SOFTWARE
For Debian / Ubuntu / similar:      sudo apt install python3 python3-pip python3-virtualenv poppler-utils  Download the code and prepare the python environment:      git clone https://github.com/mattbierbaum/arxiv-public-datasets     cd arxiv-public-datasets      virtualenv venv     . venv/bin/activate      pip3 install -e .,pip3,SOFTWARE
"This directory needs to have adequate space to hold ~ 1TB of pdfs and ~ 70GB of text if you so choose to retrieve them:      cp config.json.example config.json     [set ARXIV_DATA in config.json to your own directory]  The scripts in `bin` will then create any of the three subdirectories:      $ARXIV_DATA/tarpdfs   # raw pdf files from Amazon AWS bucket     $ARXIV_DATA/fulltext  # .txt from raw .pdf     $ARXIV_DATA/output    # co-citation network, parsed author strings, etc  ## Article metadata  **Run OAI metadata harvester**      python bin/metadata.py [OPTIONAL filepath.json.gz]  This will download the entire ArXiv metadata set, saving it as a series of gzip-compressed JSON entries.",python,SOFTWARE
"A resumption token is saved, so the process can be restarted by running again.  ## PDFs (AWS download only)  **Prepare credentials**  In addition to the setup above, you need to prepare your AWS credentials for use with boto3, the Python AWS library.",AWS,SOFTWARE
"A resumption token is saved, so the process can be restarted by running again.  ## PDFs (AWS download only)  **Prepare credentials**  In addition to the setup above, you need to prepare your AWS credentials for use with boto3, the Python AWS library.",AWS,SOFTWARE
"A resumption token is saved, so the process can be restarted by running again.  ## PDFs (AWS download only)  **Prepare credentials**  In addition to the setup above, you need to prepare your AWS credentials for use with boto3, the Python AWS library.",boto3,SOFTWARE
"A resumption token is saved, so the process can be restarted by running again.  ## PDFs (AWS download only)  **Prepare credentials**  In addition to the setup above, you need to prepare your AWS credentials for use with boto3, the Python AWS library.",Python,SOFTWARE
"A resumption token is saved, so the process can be restarted by running again.  ## PDFs (AWS download only)  **Prepare credentials**  In addition to the setup above, you need to prepare your AWS credentials for use with boto3, the Python AWS library.",AWS,SOFTWARE
"A long explanation is available [here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html) while the quick method is to:      apt install awscli     aws configure   **Bulk download of ArXiv PDFs**  This download costs about 100 USD (and is 1.1TB) at the time of writing, as the [ArXiv bulk download](https://arxiv.org/help/bulk_data) only allows requester-pays AWS S3 downloads.",boto3,SOFTWARE
"A long explanation is available [here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html) while the quick method is to:      apt install awscli     aws configure   **Bulk download of ArXiv PDFs**  This download costs about 100 USD (and is 1.1TB) at the time of writing, as the [ArXiv bulk download](https://arxiv.org/help/bulk_data) only allows requester-pays AWS S3 downloads.",awscli,SOFTWARE
"A long explanation is available [here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html) while the quick method is to:      apt install awscli     aws configure   **Bulk download of ArXiv PDFs**  This download costs about 100 USD (and is 1.1TB) at the time of writing, as the [ArXiv bulk download](https://arxiv.org/help/bulk_data) only allows requester-pays AWS S3 downloads.",aws,SOFTWARE
"A long explanation is available [here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html) while the quick method is to:      apt install awscli     aws configure   **Bulk download of ArXiv PDFs**  This download costs about 100 USD (and is 1.1TB) at the time of writing, as the [ArXiv bulk download](https://arxiv.org/help/bulk_data) only allows requester-pays AWS S3 downloads.",Bulk,SOFTWARE
"A long explanation is available [here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html) while the quick method is to:      apt install awscli     aws configure   **Bulk download of ArXiv PDFs**  This download costs about 100 USD (and is 1.1TB) at the time of writing, as the [ArXiv bulk download](https://arxiv.org/help/bulk_data) only allows requester-pays AWS S3 downloads.",bulk,SOFTWARE
"A long explanation is available [here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html) while the quick method is to:      apt install awscli     aws configure   **Bulk download of ArXiv PDFs**  This download costs about 100 USD (and is 1.1TB) at the time of writing, as the [ArXiv bulk download](https://arxiv.org/help/bulk_data) only allows requester-pays AWS S3 downloads.",AWS,SOFTWARE
"Ensure that you have at least 1TB of free space in the directory specified in `config.json`:      python bin/pdfdownload.py [OPTIONAL manifest_file.json.gz]  ## Plain text The tool works for both AWS download (`.tar` files) and Kaggle download (plain PDFs) by adding `--PLAIN_PDFS` optional argument. ``` python bin/fulltext.py -N [OPTIONAL number_of_processes, default cpu_count] --PLAIN_PDFS [OPTIONAL, use if plain PDFs are dowloaded] ``` **Bulk PDF conversion for AWS download**  To use our tool for text conversion of all the PDFs from the ArXiv bulk download described above, execute the following.",python,SOFTWARE
"Ensure that you have at least 1TB of free space in the directory specified in `config.json`:      python bin/pdfdownload.py [OPTIONAL manifest_file.json.gz]  ## Plain text The tool works for both AWS download (`.tar` files) and Kaggle download (plain PDFs) by adding `--PLAIN_PDFS` optional argument. ``` python bin/fulltext.py -N [OPTIONAL number_of_processes, default cpu_count] --PLAIN_PDFS [OPTIONAL, use if plain PDFs are dowloaded] ``` **Bulk PDF conversion for AWS download**  To use our tool for text conversion of all the PDFs from the ArXiv bulk download described above, execute the following.",AWS,SOFTWARE
"Ensure that you have at least 1TB of free space in the directory specified in `config.json`:      python bin/pdfdownload.py [OPTIONAL manifest_file.json.gz]  ## Plain text The tool works for both AWS download (`.tar` files) and Kaggle download (plain PDFs) by adding `--PLAIN_PDFS` optional argument. ``` python bin/fulltext.py -N [OPTIONAL number_of_processes, default cpu_count] --PLAIN_PDFS [OPTIONAL, use if plain PDFs are dowloaded] ``` **Bulk PDF conversion for AWS download**  To use our tool for text conversion of all the PDFs from the ArXiv bulk download described above, execute the following.",python,SOFTWARE
"Ensure that you have at least 1TB of free space in the directory specified in `config.json`:      python bin/pdfdownload.py [OPTIONAL manifest_file.json.gz]  ## Plain text The tool works for both AWS download (`.tar` files) and Kaggle download (plain PDFs) by adding `--PLAIN_PDFS` optional argument. ``` python bin/fulltext.py -N [OPTIONAL number_of_processes, default cpu_count] --PLAIN_PDFS [OPTIONAL, use if plain PDFs are dowloaded] ``` **Bulk PDF conversion for AWS download**  To use our tool for text conversion of all the PDFs from the ArXiv bulk download described above, execute the following.",AWS,SOFTWARE
"python bin/fulltext.py -N [OPTIONAL number_of_processes, default cpu_count]   At the time of writing, converting 1.39 million articles requires over 400 core-hours using two Intel Xeon E5-2600 CPUs.",python,SOFTWARE
"**Bulk PDF conversion for plain PDFs, e.g. downdloaded from Kaggle (Google Cloud)**  Download from AWS are `.tar` files.",Google Cloud,SOFTWARE
"**Bulk PDF conversion for plain PDFs, e.g. downdloaded from Kaggle (Google Cloud)**  Download from AWS are `.tar` files.",AWS,SOFTWARE
"If plains PDFs are downloaded, e.g. from Kaggle, use the optional argument `--PLAIN_PDFS`: ``` python bin/fulltext.py -N [OPTIONAL number_of_processes, default cpu_count] --PLAIN_PDFS ```  ## Cocitation network  To generate the cocitation network, you first must have the full text.",python,SOFTWARE
"Then, with the directories still set up, run:      python bin/cocitations.py [OPTIONAL number_of_processes, default cpu_count]  The cocitation network will by default be saved in `$ARXIV_DATA/output/internal-citations.json.gz`.  ## Author string split  The OAI metadata from the ArXiv features author strings as submitted by article authors.",python,SOFTWARE
"To generate and save these author splittings, run:      python bin/authorsplit.py [OPTIONAL number_of_processes, default cpu_count]  The split author strings will by default be saved in `$ARXIV_DATA/output/authors-parsed.json.gz`.  ## Dataset Metadata The following table is necessary for this dataset to be indexed by search engines such as <a href=""https://g.co/datasetsearch"">Google Dataset Search</a>.",python,SOFTWARE
This code uses [python3.x](https://www.python.org/downloads/) (version 3.6 and higher) and requires the [Gurobi](https://www.gurobi.com/) solver.,Gurobi,SOFTWARE
This code uses [python3.x](https://www.python.org/downloads/) (version 3.6 and higher) and requires the [Gurobi](https://www.gurobi.com/) solver.,gurobi,SOFTWARE
"Required python3.x packages are outlined in `requirements.txt`.  ***  ***  ## Summary of Repository - `OBCT.py` contains the formulations of each model for solving using Gurobi9.x - `TREE.py` creates the necessary tree information including path, child, and other information - `SPEED_UP.py` contains the code for the callbacks used in user fractional separation procedures - `UTILS.py` contains the code for viewing model decision variable results and generating the .csv results files among other utility functions - `model_runs.py` contains the code necessary to create, solve, and report results in a `.csv` file of each instance called by the user - `results_files/` folder stores the generated `.csv` files with model metrics - `figures/` folder stores the generated `.png` files for experimental results - `log_files/` folder stores model `.lp` files and Gurobi `.txt` log files - `Datasets/` folder contains the datasets used for generating experimental results   - Note: `Datasets/` should also be used as the folder where user dataset files are stored  *** ***  ## Running Code  - Ensure the latest versions of the packages in `requirements.txt` are installed - For an instance of `OBCT` run with one of the functions in `model_runs.py` we use following arguments (not all functional call parameters apply to all functions)     - d : `str list`, name(s) of dataset file(s)     - h : `int list`, maximum depth(s) of trained tree(s)     - t : `float`, gurobi model time limit in s     - m : `str list`, list of model(s) to use     - r : `int list`, list of random seed(s) to use       - `rand_seed = [k,...,k]`  for repeat use of randome state `k`     - p : `str`, objective priority parameter used in bi-objective modeling     - e : `str list`, model extra(s), if applicable     - f : `str`, results output file `.csv`     - l : `boolean`, log console to `.txt` file and write model to `.lp` file, both saved to the `\log_files` folder for each model called by user Note: - We assume the target column is labeled `'target'`.",python3.x,SOFTWARE
"Required python3.x packages are outlined in `requirements.txt`.  ***  ***  ## Summary of Repository - `OBCT.py` contains the formulations of each model for solving using Gurobi9.x - `TREE.py` creates the necessary tree information including path, child, and other information - `SPEED_UP.py` contains the code for the callbacks used in user fractional separation procedures - `UTILS.py` contains the code for viewing model decision variable results and generating the .csv results files among other utility functions - `model_runs.py` contains the code necessary to create, solve, and report results in a `.csv` file of each instance called by the user - `results_files/` folder stores the generated `.csv` files with model metrics - `figures/` folder stores the generated `.png` files for experimental results - `log_files/` folder stores model `.lp` files and Gurobi `.txt` log files - `Datasets/` folder contains the datasets used for generating experimental results   - Note: `Datasets/` should also be used as the folder where user dataset files are stored  *** ***  ## Running Code  - Ensure the latest versions of the packages in `requirements.txt` are installed - For an instance of `OBCT` run with one of the functions in `model_runs.py` we use following arguments (not all functional call parameters apply to all functions)     - d : `str list`, name(s) of dataset file(s)     - h : `int list`, maximum depth(s) of trained tree(s)     - t : `float`, gurobi model time limit in s     - m : `str list`, list of model(s) to use     - r : `int list`, list of random seed(s) to use       - `rand_seed = [k,...,k]`  for repeat use of randome state `k`     - p : `str`, objective priority parameter used in bi-objective modeling     - e : `str list`, model extra(s), if applicable     - f : `str`, results output file `.csv`     - l : `boolean`, log console to `.txt` file and write model to `.lp` file, both saved to the `\log_files` folder for each model called by user Note: - We assume the target column is labeled `'target'`.",Gurobi9.x,SOFTWARE
"Required python3.x packages are outlined in `requirements.txt`.  ***  ***  ## Summary of Repository - `OBCT.py` contains the formulations of each model for solving using Gurobi9.x - `TREE.py` creates the necessary tree information including path, child, and other information - `SPEED_UP.py` contains the code for the callbacks used in user fractional separation procedures - `UTILS.py` contains the code for viewing model decision variable results and generating the .csv results files among other utility functions - `model_runs.py` contains the code necessary to create, solve, and report results in a `.csv` file of each instance called by the user - `results_files/` folder stores the generated `.csv` files with model metrics - `figures/` folder stores the generated `.png` files for experimental results - `log_files/` folder stores model `.lp` files and Gurobi `.txt` log files - `Datasets/` folder contains the datasets used for generating experimental results   - Note: `Datasets/` should also be used as the folder where user dataset files are stored  *** ***  ## Running Code  - Ensure the latest versions of the packages in `requirements.txt` are installed - For an instance of `OBCT` run with one of the functions in `model_runs.py` we use following arguments (not all functional call parameters apply to all functions)     - d : `str list`, name(s) of dataset file(s)     - h : `int list`, maximum depth(s) of trained tree(s)     - t : `float`, gurobi model time limit in s     - m : `str list`, list of model(s) to use     - r : `int list`, list of random seed(s) to use       - `rand_seed = [k,...,k]`  for repeat use of randome state `k`     - p : `str`, objective priority parameter used in bi-objective modeling     - e : `str list`, model extra(s), if applicable     - f : `str`, results output file `.csv`     - l : `boolean`, log console to `.txt` file and write model to `.lp` file, both saved to the `\log_files` folder for each model called by user Note: - We assume the target column is labeled `'target'`.",gurobi,SOFTWARE
"Change the hard code in `model_runs.py` to change the according target column - If results output file `-f file` is `None` the `models_run.py` calls automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder - `-e model_extras`, `-p priorities`, and `-f file` may be `None` input arguments, all others must hold a valid value  *** Call the `model_runs.py` `main` function within a python file as follows to generate a model ignorning our second objective,  ```python import model_runs  data_names = ['soybean_small', 'monk3', 'car', 'iris', 'climate'] heights = [3, 4, 5] models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 extras = ['max_features-25'] rand_seed = [13, 58, 94, None] tuning = None file = 'example_code_output.csv' consol_log = False model_runs.main(   [""-d"", data_names, ""-h"", heights, ""-m"", models, ""-t"", time_limit,    ""-e"", extras, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.main -d ['soybean-small','monk3','car','iris','climate'] -h [3,4,5] -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -e ['max_features-25'] -r [13, 58, 94, None] -c None -f 'test_results.csv' -l False ```  *** ## Bi-objective Modeling Call the `biobjective` function within a python file as follows to generate a model using the heirarchical modeling capabilities of Gurobi  ```python import model_runs  data_names = ['ionosphere', 'monk2', 'breat_cancer', 'climate'] height = 5 models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 rand_seed = [13, 58, 94, None] priorities = ['data','equal'] file = 'biobj_example.csv' consol_log = False model_runs.biobjective(   [""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", time_limit,    ""-p"", priorities, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.biobjective -d ['ionosphere', 'monk2', 'breat_cancer', 'climate'] -h 5 -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -p ['data','equal'] -r [13, 58, 94, None] -f 'biobj_example.csv' -l False ```  *** ## Pareto Frontier To generate the Pareto frontier call the `main` function in `pareto_runs.py` with the below parameters:   - d : str list, names of dataset files   - h : int, maximum depth of trained trees   - t : float, gurobi model time limit in s   - m : str list, models to use   - r : str list, random state(s) to use   - f : str, results output file .csv  A `.png` file for each dataset called by the user is generated and stored in `\results_figures\` folder  We assume `-f file` is located in the `\results_files` folder - If results output file `-f file` is `None` the `pareto` function automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder  You can generate pareto frontiers from within a python file as follows,  ```python import model_runs  height = 4 models = ['FlowOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] rand_states = [15, 78, 0] data_names = ['hayes_roth', 'house_votes_84'] file = 'pareto_example.csv' model_runs.pareto([""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", 3600, ""-r"", rand_states, ""-f"", file]) ```  To run from terminal do the following  ```bash python3 import model_runs; model_runs.pareto -d ['hayes_roth', 'house_votes_84'] -h 4 -m ['FOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] -t 3600 -r [15, 78, 0] -f 'pareto_example.csv' ``` - Note: `FlowOCT` must be the model name to generate the pareto frontier of FlowOCT ***  ## Models Functionality For understanding model functionality associated with integer and fractional separation procedures in **CUT1** and **CUT2** models, `-e model_extras` and `-c tuning` functionality please refer to the `USAGE.md` file.",python3,SOFTWARE
"Change the hard code in `model_runs.py` to change the according target column - If results output file `-f file` is `None` the `models_run.py` calls automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder - `-e model_extras`, `-p priorities`, and `-f file` may be `None` input arguments, all others must hold a valid value  *** Call the `model_runs.py` `main` function within a python file as follows to generate a model ignorning our second objective,  ```python import model_runs  data_names = ['soybean_small', 'monk3', 'car', 'iris', 'climate'] heights = [3, 4, 5] models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 extras = ['max_features-25'] rand_seed = [13, 58, 94, None] tuning = None file = 'example_code_output.csv' consol_log = False model_runs.main(   [""-d"", data_names, ""-h"", heights, ""-m"", models, ""-t"", time_limit,    ""-e"", extras, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.main -d ['soybean-small','monk3','car','iris','climate'] -h [3,4,5] -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -e ['max_features-25'] -r [13, 58, 94, None] -c None -f 'test_results.csv' -l False ```  *** ## Bi-objective Modeling Call the `biobjective` function within a python file as follows to generate a model using the heirarchical modeling capabilities of Gurobi  ```python import model_runs  data_names = ['ionosphere', 'monk2', 'breat_cancer', 'climate'] height = 5 models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 rand_seed = [13, 58, 94, None] priorities = ['data','equal'] file = 'biobj_example.csv' consol_log = False model_runs.biobjective(   [""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", time_limit,    ""-p"", priorities, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.biobjective -d ['ionosphere', 'monk2', 'breat_cancer', 'climate'] -h 5 -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -p ['data','equal'] -r [13, 58, 94, None] -f 'biobj_example.csv' -l False ```  *** ## Pareto Frontier To generate the Pareto frontier call the `main` function in `pareto_runs.py` with the below parameters:   - d : str list, names of dataset files   - h : int, maximum depth of trained trees   - t : float, gurobi model time limit in s   - m : str list, models to use   - r : str list, random state(s) to use   - f : str, results output file .csv  A `.png` file for each dataset called by the user is generated and stored in `\results_figures\` folder  We assume `-f file` is located in the `\results_files` folder - If results output file `-f file` is `None` the `pareto` function automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder  You can generate pareto frontiers from within a python file as follows,  ```python import model_runs  height = 4 models = ['FlowOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] rand_states = [15, 78, 0] data_names = ['hayes_roth', 'house_votes_84'] file = 'pareto_example.csv' model_runs.pareto([""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", 3600, ""-r"", rand_states, ""-f"", file]) ```  To run from terminal do the following  ```bash python3 import model_runs; model_runs.pareto -d ['hayes_roth', 'house_votes_84'] -h 4 -m ['FOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] -t 3600 -r [15, 78, 0] -f 'pareto_example.csv' ``` - Note: `FlowOCT` must be the model name to generate the pareto frontier of FlowOCT ***  ## Models Functionality For understanding model functionality associated with integer and fractional separation procedures in **CUT1** and **CUT2** models, `-e model_extras` and `-c tuning` functionality please refer to the `USAGE.md` file.",python,SOFTWARE
"Change the hard code in `model_runs.py` to change the according target column - If results output file `-f file` is `None` the `models_run.py` calls automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder - `-e model_extras`, `-p priorities`, and `-f file` may be `None` input arguments, all others must hold a valid value  *** Call the `model_runs.py` `main` function within a python file as follows to generate a model ignorning our second objective,  ```python import model_runs  data_names = ['soybean_small', 'monk3', 'car', 'iris', 'climate'] heights = [3, 4, 5] models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 extras = ['max_features-25'] rand_seed = [13, 58, 94, None] tuning = None file = 'example_code_output.csv' consol_log = False model_runs.main(   [""-d"", data_names, ""-h"", heights, ""-m"", models, ""-t"", time_limit,    ""-e"", extras, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.main -d ['soybean-small','monk3','car','iris','climate'] -h [3,4,5] -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -e ['max_features-25'] -r [13, 58, 94, None] -c None -f 'test_results.csv' -l False ```  *** ## Bi-objective Modeling Call the `biobjective` function within a python file as follows to generate a model using the heirarchical modeling capabilities of Gurobi  ```python import model_runs  data_names = ['ionosphere', 'monk2', 'breat_cancer', 'climate'] height = 5 models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 rand_seed = [13, 58, 94, None] priorities = ['data','equal'] file = 'biobj_example.csv' consol_log = False model_runs.biobjective(   [""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", time_limit,    ""-p"", priorities, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.biobjective -d ['ionosphere', 'monk2', 'breat_cancer', 'climate'] -h 5 -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -p ['data','equal'] -r [13, 58, 94, None] -f 'biobj_example.csv' -l False ```  *** ## Pareto Frontier To generate the Pareto frontier call the `main` function in `pareto_runs.py` with the below parameters:   - d : str list, names of dataset files   - h : int, maximum depth of trained trees   - t : float, gurobi model time limit in s   - m : str list, models to use   - r : str list, random state(s) to use   - f : str, results output file .csv  A `.png` file for each dataset called by the user is generated and stored in `\results_figures\` folder  We assume `-f file` is located in the `\results_files` folder - If results output file `-f file` is `None` the `pareto` function automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder  You can generate pareto frontiers from within a python file as follows,  ```python import model_runs  height = 4 models = ['FlowOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] rand_states = [15, 78, 0] data_names = ['hayes_roth', 'house_votes_84'] file = 'pareto_example.csv' model_runs.pareto([""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", 3600, ""-r"", rand_states, ""-f"", file]) ```  To run from terminal do the following  ```bash python3 import model_runs; model_runs.pareto -d ['hayes_roth', 'house_votes_84'] -h 4 -m ['FOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] -t 3600 -r [15, 78, 0] -f 'pareto_example.csv' ``` - Note: `FlowOCT` must be the model name to generate the pareto frontier of FlowOCT ***  ## Models Functionality For understanding model functionality associated with integer and fractional separation procedures in **CUT1** and **CUT2** models, `-e model_extras` and `-c tuning` functionality please refer to the `USAGE.md` file.",python3,SOFTWARE
"Change the hard code in `model_runs.py` to change the according target column - If results output file `-f file` is `None` the `models_run.py` calls automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder - `-e model_extras`, `-p priorities`, and `-f file` may be `None` input arguments, all others must hold a valid value  *** Call the `model_runs.py` `main` function within a python file as follows to generate a model ignorning our second objective,  ```python import model_runs  data_names = ['soybean_small', 'monk3', 'car', 'iris', 'climate'] heights = [3, 4, 5] models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 extras = ['max_features-25'] rand_seed = [13, 58, 94, None] tuning = None file = 'example_code_output.csv' consol_log = False model_runs.main(   [""-d"", data_names, ""-h"", heights, ""-m"", models, ""-t"", time_limit,    ""-e"", extras, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.main -d ['soybean-small','monk3','car','iris','climate'] -h [3,4,5] -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -e ['max_features-25'] -r [13, 58, 94, None] -c None -f 'test_results.csv' -l False ```  *** ## Bi-objective Modeling Call the `biobjective` function within a python file as follows to generate a model using the heirarchical modeling capabilities of Gurobi  ```python import model_runs  data_names = ['ionosphere', 'monk2', 'breat_cancer', 'climate'] height = 5 models = ['MCF1', 'MCF2', 'CUT1-ALL', 'CUT2-FRAC-3'] time_limit = 3600 rand_seed = [13, 58, 94, None] priorities = ['data','equal'] file = 'biobj_example.csv' consol_log = False model_runs.biobjective(   [""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", time_limit,    ""-p"", priorities, ""-r"", rand_seed, ""-f"", file, ""-l"", consol_log]) ``` To run from terminal do the following, ```bash python3 import model_runs; model_runs.biobjective -d ['ionosphere', 'monk2', 'breat_cancer', 'climate'] -h 5 -m ['MCF1','MCF2','CUT1-ALL','CUT2-FRAC-3'] -t 3600 -p ['data','equal'] -r [13, 58, 94, None] -f 'biobj_example.csv' -l False ```  *** ## Pareto Frontier To generate the Pareto frontier call the `main` function in `pareto_runs.py` with the below parameters:   - d : str list, names of dataset files   - h : int, maximum depth of trained trees   - t : float, gurobi model time limit in s   - m : str list, models to use   - r : str list, random state(s) to use   - f : str, results output file .csv  A `.png` file for each dataset called by the user is generated and stored in `\results_figures\` folder  We assume `-f file` is located in the `\results_files` folder - If results output file `-f file` is `None` the `pareto` function automatically generates a `.csv` results file with the parameters of the function call as the file name saved to the `\results_files` folder  You can generate pareto frontiers from within a python file as follows,  ```python import model_runs  height = 4 models = ['FlowOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] rand_states = [15, 78, 0] data_names = ['hayes_roth', 'house_votes_84'] file = 'pareto_example.csv' model_runs.pareto([""-d"", data_names, ""-h"", height, ""-m"", models, ""-t"", 3600, ""-r"", rand_states, ""-f"", file]) ```  To run from terminal do the following  ```bash python3 import model_runs; model_runs.pareto -d ['hayes_roth', 'house_votes_84'] -h 4 -m ['FOCT', 'MCF1', 'MCF2', 'CUT1', 'CUT2'] -t 3600 -r [15, 78, 0] -f 'pareto_example.csv' ``` - Note: `FlowOCT` must be the model name to generate the pareto frontier of FlowOCT ***  ## Models Functionality For understanding model functionality associated with integer and fractional separation procedures in **CUT1** and **CUT2** models, `-e model_extras` and `-c tuning` functionality please refer to the `USAGE.md` file.",python3,SOFTWARE
```sh python3.10 -m venv .venv . .venv/bin/activate pip install -r requirements.txt ```  We also provide a `Dockerfile` and a `DockerfileGPU` to build a Docker image with all the dependencies.,python3.10,SOFTWARE
```sh python3.10 -m venv .venv . .venv/bin/activate pip install -r requirements.txt ```  We also provide a `Dockerfile` and a `DockerfileGPU` to build a Docker image with all the dependencies.,pip,SOFTWARE
```sh python3.10 -m venv .venv . .venv/bin/activate pip install -r requirements.txt ```  We also provide a `Dockerfile` and a `DockerfileGPU` to build a Docker image with all the dependencies.,Docker,SOFTWARE
```sh # Build the Docker image docker build -t borealtc-gpu -f DockerfileGPU,Docker,SOFTWARE
```sh # Build the Docker image docker build -t borealtc-gpu -f DockerfileGPU,docker,SOFTWARE
"# Run the Docker image docker run --gpus all -e CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES --rm --ipc host \    --mount type=bind,source=.,target=/code/ \    --mount type=bind,source=/dev/shm,target=/dev/shm \    borealtc-gpu python3 main.py ```  ## Dataset  The `data` directory contains two different datasets:  * the `BorealTC` dataset, our publicly available dataset * the `Vulpi` dataset, from the 2021 [paper](https://doi.org/10.1016/j.jterra.2020.12.002) of Vulpi et al.",Docker,SOFTWARE
"# Run the Docker image docker run --gpus all -e CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES --rm --ipc host \    --mount type=bind,source=.,target=/code/ \    --mount type=bind,source=/dev/shm,target=/dev/shm \    borealtc-gpu python3 main.py ```  ## Dataset  The `data` directory contains two different datasets:  * the `BorealTC` dataset, our publicly available dataset * the `Vulpi` dataset, from the 2021 [paper](https://doi.org/10.1016/j.jterra.2020.12.002) of Vulpi et al.",docker,SOFTWARE
"# Run the Docker image docker run --gpus all -e CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES --rm --ipc host \    --mount type=bind,source=.,target=/code/ \    --mount type=bind,source=/dev/shm,target=/dev/shm \    borealtc-gpu python3 main.py ```  ## Dataset  The `data` directory contains two different datasets:  * the `BorealTC` dataset, our publicly available dataset * the `Vulpi` dataset, from the 2021 [paper](https://doi.org/10.1016/j.jterra.2020.12.002) of Vulpi et al.",python3,SOFTWARE
# EORB-SLAM: An Event-based ORB-SLAM  This project is a feature-based odometry/SLAM algorithm that is used to estimate the 6DoF pose of a robot and reconstruct the 3D point cloud of the scene using a variety of sensors.,ORB-SLAM,SOFTWARE
"It extends ORB-SLAM, which allows simultaneous localization and mapping using a monocular [DAVIS event camera](https://en.wikipedia.org/wiki/Event_camera).",ORB-SLAM,SOFTWARE
"**Contributions**:  - Support for DAVIS event cameras - Added these modes to the original algorithm: Event-only, Event-Image (monocular), Event-Image-Inertial (monocular), Event-Inertial (monocular) - Dataset loader can automatically load different datasets and configuration parameters - Introducing sensor configuration object - Mixed key point tracking (in addition to ORB features, the proposed algorithm can use both the AKAZE and ORB features for image-based SLAM)  ## Dependencies  Since this project is an extension of the ORB-SLAM algorithm, it inherits all its dependencies in addition to new ones.",ORB-SLAM,SOFTWARE
These instructions are for Ubuntu 20.04 LTS.,Ubuntu 20.04 LTS,SOFTWARE
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,g++-7,SOFTWARE
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,gcc-7,SOFTWARE
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,Ubuntu 20.04,SOFTWARE
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,cmake,SOFTWARE
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,Pangolin,SOFTWARE
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,Eigen3,SOFTWARE
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,libeigen3-dev,SOFTWARE
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,Ceres-solver 1.14,SOFTWARE
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,ceres-solver,SOFTWARE
and a powerful machine with about 16 GB RAM.  ### C++11 or C++0x Compiler  [Install g++-7 and gcc-7 along with version 9 on Ubuntu 20.04](https://vegastack.com/tutorials/how-to-install-gcc-compiler-on-ubuntu-20-04/)  ### Install the necessary tools  ```bash sudo apt install -y make cmake pkg-config unzip yasm git gfortran nano wget curl ```  ### Pangolin  [Pangolin project page](https://github.com/stevenlovegrove/Pangolin)  ### Eigen3  `sudo apt install libeigen3-dev`  ### Ceres-solver 1.14  Follow [these steps](http://ceres-solver.org/installation.html) to install Ceres-solver 1.14.,Ceres-solver 1.14,SOFTWARE
(Also see [this post in Medium](https://yunusmuhammad007.medium.com/jetson-tk1-install-ceres-solver-2-2-68787e237649))  ### Boost  `sudo apt-get install -y libboost-all-dev`  ### The OpenCV hell!,ceres-solver,SOFTWARE
(Also see [this post in Medium](https://yunusmuhammad007.medium.com/jetson-tk1-install-ceres-solver-2-2-68787e237649))  ### Boost  `sudo apt-get install -y libboost-all-dev`  ### The OpenCV hell!,Boost,SOFTWARE
(Also see [this post in Medium](https://yunusmuhammad007.medium.com/jetson-tk1-install-ceres-solver-2-2-68787e237649))  ### Boost  `sudo apt-get install -y libboost-all-dev`  ### The OpenCV hell!,libboost-all-dev,SOFTWARE
(Also see [this post in Medium](https://yunusmuhammad007.medium.com/jetson-tk1-install-ceres-solver-2-2-68787e237649))  ### Boost  `sudo apt-get install -y libboost-all-dev`  ### The OpenCV hell!,OpenCV,SOFTWARE
Follow the steps in [Install OpenCV-3.4.1 with CUDA](https://gist.github.com/raulqf/a3caa97db3f8760af33266a1475d0e5e) and [Install OpenCV-3.4.1 on Ubuntu 17.10](https://gist.github.com/okanon/c09669f3ff3351c864742bc2754b01ea) to install `OpenCV 3.4.1` on your machine.  ### ROS (optional)  The original code uses ROS `melodic` on Ubuntu 18.04.,OpenCV-3.4.1,SOFTWARE
Follow the steps in [Install OpenCV-3.4.1 with CUDA](https://gist.github.com/raulqf/a3caa97db3f8760af33266a1475d0e5e) and [Install OpenCV-3.4.1 on Ubuntu 17.10](https://gist.github.com/okanon/c09669f3ff3351c864742bc2754b01ea) to install `OpenCV 3.4.1` on your machine.  ### ROS (optional)  The original code uses ROS `melodic` on Ubuntu 18.04.,CUDA,SOFTWARE
Follow the steps in [Install OpenCV-3.4.1 with CUDA](https://gist.github.com/raulqf/a3caa97db3f8760af33266a1475d0e5e) and [Install OpenCV-3.4.1 on Ubuntu 17.10](https://gist.github.com/okanon/c09669f3ff3351c864742bc2754b01ea) to install `OpenCV 3.4.1` on your machine.  ### ROS (optional)  The original code uses ROS `melodic` on Ubuntu 18.04.,OpenCV-3.4.1,SOFTWARE
Follow the steps in [Install OpenCV-3.4.1 with CUDA](https://gist.github.com/raulqf/a3caa97db3f8760af33266a1475d0e5e) and [Install OpenCV-3.4.1 on Ubuntu 17.10](https://gist.github.com/okanon/c09669f3ff3351c864742bc2754b01ea) to install `OpenCV 3.4.1` on your machine.  ### ROS (optional)  The original code uses ROS `melodic` on Ubuntu 18.04.,Ubuntu 17.10,SOFTWARE
Follow the steps in [Install OpenCV-3.4.1 with CUDA](https://gist.github.com/raulqf/a3caa97db3f8760af33266a1475d0e5e) and [Install OpenCV-3.4.1 on Ubuntu 17.10](https://gist.github.com/okanon/c09669f3ff3351c864742bc2754b01ea) to install `OpenCV 3.4.1` on your machine.  ### ROS (optional)  The original code uses ROS `melodic` on Ubuntu 18.04.,OpenCV 3.4.1,SOFTWARE
Follow the steps in [Install OpenCV-3.4.1 with CUDA](https://gist.github.com/raulqf/a3caa97db3f8760af33266a1475d0e5e) and [Install OpenCV-3.4.1 on Ubuntu 17.10](https://gist.github.com/okanon/c09669f3ff3351c864742bc2754b01ea) to install `OpenCV 3.4.1` on your machine.  ### ROS (optional)  The original code uses ROS `melodic` on Ubuntu 18.04.,ROS,SOFTWARE
Follow the steps in [Install OpenCV-3.4.1 with CUDA](https://gist.github.com/raulqf/a3caa97db3f8760af33266a1475d0e5e) and [Install OpenCV-3.4.1 on Ubuntu 17.10](https://gist.github.com/okanon/c09669f3ff3351c864742bc2754b01ea) to install `OpenCV 3.4.1` on your machine.  ### ROS (optional)  The original code uses ROS `melodic` on Ubuntu 18.04.,ROS,SOFTWARE
Follow the steps in [Install OpenCV-3.4.1 with CUDA](https://gist.github.com/raulqf/a3caa97db3f8760af33266a1475d0e5e) and [Install OpenCV-3.4.1 on Ubuntu 17.10](https://gist.github.com/okanon/c09669f3ff3351c864742bc2754b01ea) to install `OpenCV 3.4.1` on your machine.  ### ROS (optional)  The original code uses ROS `melodic` on Ubuntu 18.04.,Ubuntu 18.04,SOFTWARE
"However, ROS `noetic` is supported on Ubuntu 20.04.",ROS,SOFTWARE
"However, ROS `noetic` is supported on Ubuntu 20.04.",Ubuntu 20.04,SOFTWARE
"Integration of this project with ROS is possible, but it requires compiling and installing the core project first and then linking it to your ROS logic.  ## Installation  Once you installed all the dependencies, you can download and install the project:  ```bash git clone https://github.com/m-dayani/EORB_SLAM.git cd EORB_SLAM  mkdir build cd build cmake .. make -j4 ```  To make life easier for you, I included a bash script (`build_eorb_slam.sh`) and a Docker file that contains all the necessary commands to download and install the required packages.  ## Usage  1.",ROS,SOFTWARE
"Integration of this project with ROS is possible, but it requires compiling and installing the core project first and then linking it to your ROS logic.  ## Installation  Once you installed all the dependencies, you can download and install the project:  ```bash git clone https://github.com/m-dayani/EORB_SLAM.git cd EORB_SLAM  mkdir build cd build cmake .. make -j4 ```  To make life easier for you, I included a bash script (`build_eorb_slam.sh`) and a Docker file that contains all the necessary commands to download and install the required packages.  ## Usage  1.",ROS,SOFTWARE
"Integration of this project with ROS is possible, but it requires compiling and installing the core project first and then linking it to your ROS logic.  ## Installation  Once you installed all the dependencies, you can download and install the project:  ```bash git clone https://github.com/m-dayani/EORB_SLAM.git cd EORB_SLAM  mkdir build cd build cmake .. make -j4 ```  To make life easier for you, I included a bash script (`build_eorb_slam.sh`) and a Docker file that contains all the necessary commands to download and install the required packages.  ## Usage  1.",git,SOFTWARE
"Integration of this project with ROS is possible, but it requires compiling and installing the core project first and then linking it to your ROS logic.  ## Installation  Once you installed all the dependencies, you can download and install the project:  ```bash git clone https://github.com/m-dayani/EORB_SLAM.git cd EORB_SLAM  mkdir build cd build cmake .. make -j4 ```  To make life easier for you, I included a bash script (`build_eorb_slam.sh`) and a Docker file that contains all the necessary commands to download and install the required packages.  ## Usage  1.",Docker,SOFTWARE
Disable the visualization mode in the configuration file if running a Docker container. 6.,Docker,SOFTWARE
Can you install the [ORB-SLAM3](https://github.com/UZ-SLAMLab/ORB_SLAM3)?,ORB-SLAM3,SOFTWARE
Can you install the [ORB-SLAM3](https://github.com/UZ-SLAMLab/ORB_SLAM3)?,ORB_SLAM3,SOFTWARE
Do you use `g++ 7.5` and `gcc 7.5` to compile packages?,g++ 7.5,SOFTWARE
Do you use `g++ 7.5` and `gcc 7.5` to compile packages?,gcc 7.5,SOFTWARE
Is the Ceres version `1.14`?,Ceres,SOFTWARE
Did you build and install the OpenCV last so that it compiles the library with the necessary packages (`sfm::reconstruct`)?,OpenCV,SOFTWARE
There are some issues in the ORB-SLAM3 algorithm itself.,ORB-SLAM3,SOFTWARE
"Such optimization exceptions might randomly happen for the EORB-SLAM algorithm. - For performance, most main components of this algorithm are executed in parallel threads.",EORB-SLAM,SOFTWARE
"<div align=""center"">  # DeepViewAgg  [!",DeepViewAgg,SOFTWARE
[pytorch](https://img.shields.io/badge/PyTorch_1.7.1-ee4c2c?,pytorch,SOFTWARE
[pytorch](https://img.shields.io/badge/PyTorch_1.7.1-ee4c2c?,PyTorch_1.7.1,SOFTWARE
logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/) [!,pytorch,SOFTWARE
logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/) [!,pytorch,SOFTWARE
labelColor=gray)](https://github.com/drprojects/DeepViewAgg/blob/release/LICENSE.md)  Official implementation for <br> <br> [_Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation_](https://arxiv.org/abs/2204.07548) <br> ([CVPR'22 Best Paper Finalist üéâ ](https://twitter.com/CVPR/status/1539772091112857600)) <br> [!,DeepViewAgg,SOFTWARE
[Project page](https://img.shields.io/badge/Project_page-8A2BE2)](https://drprojects.github.io/deepviewagg) [!,deepviewagg,SOFTWARE
"<div align=""center"">  |                               ‚ú® DeepViewAgg in short ‚ú®                               | |:------------------------------------------------------------------------------------:| |                       ü§ñ  Learns **2D+3D features** end-to-end                        | |              üëÄ  **Attentive multi-view aggregation** from **viewing conditions**     | | üö´  No need for 3D colorization, meshing, depth sensor, synthetic views, or 2D labels | |                 ‚úÖ Only needs **raw point clouds, images, and poses**                 |  [!",DeepViewAgg,SOFTWARE
"p=learning-multi-view-aggregation-in-the-wild)  </div>  <br>  ## üì∞  Change log - 2023-01-11 Fixed some bug when using intermediate fusion - 2022-04-20 Added notebooks and scripts to get started with DeepViewAgg - 2022-04-27 Added pretrained weights and features to help reproduce our results  <br>  ## üìù  Requirements The following must be installed before installing this project. - Anaconda3 - cuda >= 10.1 - gcc >= 7  All remaining dependencies (PyTorch, PyTorch Geometric, etc.) should be installed using the provided [installation script](install.sh).",DeepViewAgg,SOFTWARE
"p=learning-multi-view-aggregation-in-the-wild)  </div>  <br>  ## üì∞  Change log - 2023-01-11 Fixed some bug when using intermediate fusion - 2022-04-20 Added notebooks and scripts to get started with DeepViewAgg - 2022-04-27 Added pretrained weights and features to help reproduce our results  <br>  ## üìù  Requirements The following must be installed before installing this project. - Anaconda3 - cuda >= 10.1 - gcc >= 7  All remaining dependencies (PyTorch, PyTorch Geometric, etc.) should be installed using the provided [installation script](install.sh).",Anaconda3,SOFTWARE
"p=learning-multi-view-aggregation-in-the-wild)  </div>  <br>  ## üì∞  Change log - 2023-01-11 Fixed some bug when using intermediate fusion - 2022-04-20 Added notebooks and scripts to get started with DeepViewAgg - 2022-04-27 Added pretrained weights and features to help reproduce our results  <br>  ## üìù  Requirements The following must be installed before installing this project. - Anaconda3 - cuda >= 10.1 - gcc >= 7  All remaining dependencies (PyTorch, PyTorch Geometric, etc.) should be installed using the provided [installation script](install.sh).",cuda >= 10.1,SOFTWARE
"p=learning-multi-view-aggregation-in-the-wild)  </div>  <br>  ## üì∞  Change log - 2023-01-11 Fixed some bug when using intermediate fusion - 2022-04-20 Added notebooks and scripts to get started with DeepViewAgg - 2022-04-27 Added pretrained weights and features to help reproduce our results  <br>  ## üìù  Requirements The following must be installed before installing this project. - Anaconda3 - cuda >= 10.1 - gcc >= 7  All remaining dependencies (PyTorch, PyTorch Geometric, etc.) should be installed using the provided [installation script](install.sh).",gcc >= 7,SOFTWARE
"p=learning-multi-view-aggregation-in-the-wild)  </div>  <br>  ## üì∞  Change log - 2023-01-11 Fixed some bug when using intermediate fusion - 2022-04-20 Added notebooks and scripts to get started with DeepViewAgg - 2022-04-27 Added pretrained weights and features to help reproduce our results  <br>  ## üìù  Requirements The following must be installed before installing this project. - Anaconda3 - cuda >= 10.1 - gcc >= 7  All remaining dependencies (PyTorch, PyTorch Geometric, etc.) should be installed using the provided [installation script](install.sh).",PyTorch,SOFTWARE
"p=learning-multi-view-aggregation-in-the-wild)  </div>  <br>  ## üì∞  Change log - 2023-01-11 Fixed some bug when using intermediate fusion - 2022-04-20 Added notebooks and scripts to get started with DeepViewAgg - 2022-04-27 Added pretrained weights and features to help reproduce our results  <br>  ## üìù  Requirements The following must be installed before installing this project. - Anaconda3 - cuda >= 10.1 - gcc >= 7  All remaining dependencies (PyTorch, PyTorch Geometric, etc.) should be installed using the provided [installation script](install.sh).",PyTorch Geometric,SOFTWARE
"The code has been tested in the following environment: - Ubuntu 18.04.6 LTS - Python 3.8.5 - PyTorch 1.7.1 - CUDA 10.2, 11.2 and 11.4 - NVIDIA V100 32G - 64G RAM  <br>  ## üèóÔ∏è  Installation To install DeepViewAgg, simply run `.",Ubuntu 18.04.6 LTS,SOFTWARE
"The code has been tested in the following environment: - Ubuntu 18.04.6 LTS - Python 3.8.5 - PyTorch 1.7.1 - CUDA 10.2, 11.2 and 11.4 - NVIDIA V100 32G - 64G RAM  <br>  ## üèóÔ∏è  Installation To install DeepViewAgg, simply run `.",PyTorch 1.7.1,SOFTWARE
"The code has been tested in the following environment: - Ubuntu 18.04.6 LTS - Python 3.8.5 - PyTorch 1.7.1 - CUDA 10.2, 11.2 and 11.4 - NVIDIA V100 32G - 64G RAM  <br>  ## üèóÔ∏è  Installation To install DeepViewAgg, simply run `.","CUDA 10.2, 11.2 and 11.4",SOFTWARE
"The code has been tested in the following environment: - Ubuntu 18.04.6 LTS - Python 3.8.5 - PyTorch 1.7.1 - CUDA 10.2, 11.2 and 11.4 - NVIDIA V100 32G - 64G RAM  <br>  ## üèóÔ∏è  Installation To install DeepViewAgg, simply run `.",DeepViewAgg,SOFTWARE
"- You will need to have **sudo rights** to install [MinkowskiEngine](https://github.com/NVIDIA/MinkowskiEngine) and [TorchSparse](https://github.com/mit-han-lab/torchsparse) dependencies. - ‚ö†Ô∏è **Do not** install Torch-Points3D from the official repository, or with `pip`.",MinkowskiEngine,SOFTWARE
"- You will need to have **sudo rights** to install [MinkowskiEngine](https://github.com/NVIDIA/MinkowskiEngine) and [TorchSparse](https://github.com/mit-han-lab/torchsparse) dependencies. - ‚ö†Ô∏è **Do not** install Torch-Points3D from the official repository, or with `pip`.",MinkowskiEngine,SOFTWARE
"- You will need to have **sudo rights** to install [MinkowskiEngine](https://github.com/NVIDIA/MinkowskiEngine) and [TorchSparse](https://github.com/mit-han-lab/torchsparse) dependencies. - ‚ö†Ô∏è **Do not** install Torch-Points3D from the official repository, or with `pip`.",TorchSparse,SOFTWARE
"- You will need to have **sudo rights** to install [MinkowskiEngine](https://github.com/NVIDIA/MinkowskiEngine) and [TorchSparse](https://github.com/mit-han-lab/torchsparse) dependencies. - ‚ö†Ô∏è **Do not** install Torch-Points3D from the official repository, or with `pip`.",torchsparse,SOFTWARE
"- You will need to have **sudo rights** to install [MinkowskiEngine](https://github.com/NVIDIA/MinkowskiEngine) and [TorchSparse](https://github.com/mit-han-lab/torchsparse) dependencies. - ‚ö†Ô∏è **Do not** install Torch-Points3D from the official repository, or with `pip`.",Torch-Points3D,SOFTWARE
"- You will need to have **sudo rights** to install [MinkowskiEngine](https://github.com/NVIDIA/MinkowskiEngine) and [TorchSparse](https://github.com/mit-han-lab/torchsparse) dependencies. - ‚ö†Ô∏è **Do not** install Torch-Points3D from the official repository, or with `pip`.",pip,SOFTWARE
<br>  ## Disclaimer This is **not the official [Torch-Points3D](https://github.com/nicolas-chaulet/torch-points3d) framework**.,Torch-Points3D,SOFTWARE
<br>  ## Disclaimer This is **not the official [Torch-Points3D](https://github.com/nicolas-chaulet/torch-points3d) framework**.,torch-points3d,SOFTWARE
"In this repository, some TP3D-specific files were removed for simplicity.",TP3D,SOFTWARE
<br>  ## üî©  Project structure The project follows the original [Torch-Points3D framework](https://github.com/nicolas-chaulet/torch-points3d) structure.,Torch-Points3D,SOFTWARE
<br>  ## üî©  Project structure The project follows the original [Torch-Points3D framework](https://github.com/nicolas-chaulet/torch-points3d) structure.,torch-points3d,SOFTWARE
```bash ‚îú‚îÄ conf                    # All configurations live there ‚îú‚îÄ notebooks               # Notebooks to get started with multimodal datasets and models ‚îú‚îÄ eval.py                 # Eval script ‚îú‚îÄ insall.sh               # Installation script for DeepViewAgg ‚îú‚îÄ scripts                 # Some scripts to help manage the project ‚îú‚îÄ torch_points3d     ‚îú‚îÄ core                # Core components     ‚îú‚îÄ datasets            # All code related to datasets     ‚îú‚îÄ metrics             # All metrics and trackers     ‚îú‚îÄ models              # All models     ‚îú‚îÄ modules             # Basic modules that can be used in a modular way     ‚îú‚îÄ utils               # Various utils     ‚îî‚îÄ visualization       # Visualization ‚îî‚îÄ train.py                # Main script to launch a training ```  Several changes were made to extend the original project to multimodal learning on point clouds with images.,torch_points3d,SOFTWARE
"The most important ones can be found in the following: - `conf/data/segmentation/multimodal`: configs for the 3D+2D datasets. - `conf/models/segmentation/multimodal`: configs for the 3D+2D models. - `torch_points3d/core/data_transform/multimodal`: transforms for 3D+2D data. - `torch_points3d/core/multimodal`: multimodal data and mapping objects. - `torch_points3d/datasets/segmentation/multimodal`: 3D+2D datasets (eg S3DIS, ScanNet, KITTI360). - `torch_points3d/models/segmentation/multimodal`: 3D+2D architectures. - `torch_points3d/modules/multimodal`: 3D+2D modules.",torch_points3d,SOFTWARE
"The most important ones can be found in the following: - `conf/data/segmentation/multimodal`: configs for the 3D+2D datasets. - `conf/models/segmentation/multimodal`: configs for the 3D+2D models. - `torch_points3d/core/data_transform/multimodal`: transforms for 3D+2D data. - `torch_points3d/core/multimodal`: multimodal data and mapping objects. - `torch_points3d/datasets/segmentation/multimodal`: 3D+2D datasets (eg S3DIS, ScanNet, KITTI360). - `torch_points3d/models/segmentation/multimodal`: 3D+2D architectures. - `torch_points3d/modules/multimodal`: 3D+2D modules.",torch_points3d,SOFTWARE
"The most important ones can be found in the following: - `conf/data/segmentation/multimodal`: configs for the 3D+2D datasets. - `conf/models/segmentation/multimodal`: configs for the 3D+2D models. - `torch_points3d/core/data_transform/multimodal`: transforms for 3D+2D data. - `torch_points3d/core/multimodal`: multimodal data and mapping objects. - `torch_points3d/datasets/segmentation/multimodal`: 3D+2D datasets (eg S3DIS, ScanNet, KITTI360). - `torch_points3d/models/segmentation/multimodal`: 3D+2D architectures. - `torch_points3d/modules/multimodal`: 3D+2D modules.",torch_points3d,SOFTWARE
"The most important ones can be found in the following: - `conf/data/segmentation/multimodal`: configs for the 3D+2D datasets. - `conf/models/segmentation/multimodal`: configs for the 3D+2D models. - `torch_points3d/core/data_transform/multimodal`: transforms for 3D+2D data. - `torch_points3d/core/multimodal`: multimodal data and mapping objects. - `torch_points3d/datasets/segmentation/multimodal`: 3D+2D datasets (eg S3DIS, ScanNet, KITTI360). - `torch_points3d/models/segmentation/multimodal`: 3D+2D architectures. - `torch_points3d/modules/multimodal`: 3D+2D modules.",torch_points3d,SOFTWARE
"The most important ones can be found in the following: - `conf/data/segmentation/multimodal`: configs for the 3D+2D datasets. - `conf/models/segmentation/multimodal`: configs for the 3D+2D models. - `torch_points3d/core/data_transform/multimodal`: transforms for 3D+2D data. - `torch_points3d/core/multimodal`: multimodal data and mapping objects. - `torch_points3d/datasets/segmentation/multimodal`: 3D+2D datasets (eg S3DIS, ScanNet, KITTI360). - `torch_points3d/models/segmentation/multimodal`: 3D+2D architectures. - `torch_points3d/modules/multimodal`: 3D+2D modules.",torch_points3d,SOFTWARE
This is where the DeepViewAgg module can be found. - `torch_points3d/visualization/multimodal_data.py`: tools for interactive visualization of multimodal data.,DeepViewAgg,SOFTWARE
This is where the DeepViewAgg module can be found. - `torch_points3d/visualization/multimodal_data.py`: tools for interactive visualization of multimodal data.,torch_points3d,SOFTWARE
"usp=sharing) |  <br>  ## üìö  Documentation The official documentation of [Pytorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/index.html) and [Torch-Points3D](https://torch-points3d.readthedocs.io/en/latest/index.html#) are good starting points, since this project largely builds on top of these  frameworks.",Pytorch Geometric,SOFTWARE
"usp=sharing) |  <br>  ## üìö  Documentation The official documentation of [Pytorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/index.html) and [Torch-Points3D](https://torch-points3d.readthedocs.io/en/latest/index.html#) are good starting points, since this project largely builds on top of these  frameworks.",pytorch-geometric,SOFTWARE
"usp=sharing) |  <br>  ## üìö  Documentation The official documentation of [Pytorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/index.html) and [Torch-Points3D](https://torch-points3d.readthedocs.io/en/latest/index.html#) are good starting points, since this project largely builds on top of these  frameworks.",Torch-Points3D,SOFTWARE
"usp=sharing) |  <br>  ## üìö  Documentation The official documentation of [Pytorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/index.html) and [Torch-Points3D](https://torch-points3d.readthedocs.io/en/latest/index.html#) are good starting points, since this project largely builds on top of these  frameworks.",torch-points3d,SOFTWARE
"For DeepViewAgg-specific features (*i.e.* all that concerns  multimodal learning), the provided code is commented as much as possible, but  hit me up üí¨  if some parts need clarification.",DeepViewAgg,SOFTWARE
"<br>  ## üë©  üîß  Troubleshooting & known issues - Setting `use_faiss=True` or `use_cuda=True` to accelerate  `PCAComputePointwise`, `MapImages` or `NeighborhoodBasedMappingFeatures`.",faiss,SOFTWARE
"<br>  ## üë©  üîß  Troubleshooting & known issues - Setting `use_faiss=True` or `use_cuda=True` to accelerate  `PCAComputePointwise`, `MapImages` or `NeighborhoodBasedMappingFeatures`.",cuda,SOFTWARE
"<br>  ## üí≥  Credits - This implementation of DeepViewAgg largely relies on the  [Torch-Points3D framework](https://github.com/nicolas-chaulet/torch-points3d), although not merged with the official project  at this point",DeepViewAgg,SOFTWARE
"<br>  ## üí≥  Credits - This implementation of DeepViewAgg largely relies on the  [Torch-Points3D framework](https://github.com/nicolas-chaulet/torch-points3d), although not merged with the official project  at this point",Torch-Points3D,SOFTWARE
"<br>  ## Citing our work In case you use all or part of the present code, please include the following citation:  ``` @article{robert2022dva,   title={Learning Multi-View Aggregation In the Wild for Large-Scale 3D Semantic Segmentation},   author={Robert, Damien and Vallet, Bruno and Landrieu, Loic},   journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},   year={2022} } ```  You can find our [DeepViewAgg paper üìÑ ](https://arxiv.org/abs/2204.07548) on arxiv.",DeepViewAgg,SOFTWARE
Visualize example images with annotations     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/visualizations/visualize_data.py     ``` 4.,conda,SOFTWARE
Visualize example images with annotations     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/visualizations/visualize_data.py     ``` 4.,python,SOFTWARE
Run example inference     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/inference.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth -img data/processed/RumexLeaves/iNaturalist/4150.jpg     ``` 6.,conda,SOFTWARE
Run example inference     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/inference.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth -img data/processed/RumexLeaves/iNaturalist/4150.jpg     ``` 6.,python,SOFTWARE
Run validation of final model on iNaturalist data     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/evaluate.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth     ``` 7.,conda,SOFTWARE
Run validation of final model on iNaturalist data     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/evaluate.py --exp_file exp_files/eval_inat.py -ckpt models/final_model.pth     ``` 7.,python,SOFTWARE
Training final model from scratch     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/train.py --exp_file exp_files/train_final_model.py     ```  ## Getting started with Docker 1.,conda,SOFTWARE
Training final model from scratch     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/train.py --exp_file exp_files/train_final_model.py     ```  ## Getting started with Docker 1.,python,SOFTWARE
Training final model from scratch     ```     conda run -n rumexleaves_centernet python rumexleaves_centernet/tools/train.py --exp_file exp_files/train_final_model.py     ```  ## Getting started with Docker 1.,Docker,SOFTWARE
Get submodules     ```     git submodule update --init --recursive     ``` 3.,git,SOFTWARE
"Run training in docker container     ```     docker run --gpus all -v ""$(pwd)/data/processed:/data/processed"" -v ""$(pwd)/log:/log"" -v ""$(pwd)/exp_files/train_final_model.py:/exp_file.py"" -e WANDB_API_KEY=<your-api-key> --shm-size=500m train_model     ```   ## Project structure  The directory structure of the project looks like this:  ```txt  ‚îú‚îÄ‚îÄ Makefile             <- Makefile with convenience commands like `make data` ‚îú‚îÄ‚îÄ README.md            <- The top-level README for developers using this project. ‚îú‚îÄ‚îÄ data ‚îÇ   ‚îú‚îÄ‚îÄ processed        <- The final, canonical data sets for modeling. ‚îÇ   ‚îî‚îÄ‚îÄ raw              <- The original, immutable data dump. ‚îÇ ‚îú‚îÄ‚îÄ exp_files            <- files to define the experiment configuration | ‚îú‚îÄ‚îÄ models               <- checkpoint models ‚îÇ ‚îú‚îÄ‚îÄ pyproject.toml       <- Project configuration file ‚îÇ ‚îú‚îÄ‚îÄ requirements.txt     <- The requirements file for reproducing the analysis environment | ‚îú‚îÄ‚îÄ requirements_dev.txt <- The requirements file for reproducing the analysis environment ‚îÇ ‚îú‚îÄ‚îÄ tests                <- Test files ‚îÇ ‚îú‚îÄ‚îÄ rumexleaves_centernet  <- Source code for use in this project. ‚îÇ ‚îú‚îÄ‚îÄ submodules          <- relevant submodules are stored here ‚îÇ ‚îî‚îÄ‚îÄ LICENSE              <- MIT License ``` Created using [mlops_template](https://github.com/SkafteNicki/mlops_template), a [cookiecutter template](https://github.com/cookiecutter/cookiecutter) for getting started with Machine Learning Operations (MLOps).  ## Citation  If you find this work useful in your research, please cite: ``` @article{RumexLeaves-CenterNet, author = {G√ºldenring, Ronja and Andersen, Rasmus Eckholdt and Nalpantidis, Lazaros}, title = {Zoom in on the Plant: Fine-grained Analysis of Leaf, Stem and Vein Instances}, journal = {IEEE Robotics and Automation Letters (RA-L)}, year = {2024} } ```  ## References Our code is partially based on the following code bases. * [CenterNet](https://github.com/xingyizhou/CenterNet) * [YOLOX](https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX) * [Deformabel Convolutions v2 (pytorch)](https://github.com/developer0hye/PyTorch-Deformable-Convolution-v2)",docker,SOFTWARE
"Run training in docker container     ```     docker run --gpus all -v ""$(pwd)/data/processed:/data/processed"" -v ""$(pwd)/log:/log"" -v ""$(pwd)/exp_files/train_final_model.py:/exp_file.py"" -e WANDB_API_KEY=<your-api-key> --shm-size=500m train_model     ```   ## Project structure  The directory structure of the project looks like this:  ```txt  ‚îú‚îÄ‚îÄ Makefile             <- Makefile with convenience commands like `make data` ‚îú‚îÄ‚îÄ README.md            <- The top-level README for developers using this project. ‚îú‚îÄ‚îÄ data ‚îÇ   ‚îú‚îÄ‚îÄ processed        <- The final, canonical data sets for modeling. ‚îÇ   ‚îî‚îÄ‚îÄ raw              <- The original, immutable data dump. ‚îÇ ‚îú‚îÄ‚îÄ exp_files            <- files to define the experiment configuration | ‚îú‚îÄ‚îÄ models               <- checkpoint models ‚îÇ ‚îú‚îÄ‚îÄ pyproject.toml       <- Project configuration file ‚îÇ ‚îú‚îÄ‚îÄ requirements.txt     <- The requirements file for reproducing the analysis environment | ‚îú‚îÄ‚îÄ requirements_dev.txt <- The requirements file for reproducing the analysis environment ‚îÇ ‚îú‚îÄ‚îÄ tests                <- Test files ‚îÇ ‚îú‚îÄ‚îÄ rumexleaves_centernet  <- Source code for use in this project. ‚îÇ ‚îú‚îÄ‚îÄ submodules          <- relevant submodules are stored here ‚îÇ ‚îî‚îÄ‚îÄ LICENSE              <- MIT License ``` Created using [mlops_template](https://github.com/SkafteNicki/mlops_template), a [cookiecutter template](https://github.com/cookiecutter/cookiecutter) for getting started with Machine Learning Operations (MLOps).  ## Citation  If you find this work useful in your research, please cite: ``` @article{RumexLeaves-CenterNet, author = {G√ºldenring, Ronja and Andersen, Rasmus Eckholdt and Nalpantidis, Lazaros}, title = {Zoom in on the Plant: Fine-grained Analysis of Leaf, Stem and Vein Instances}, journal = {IEEE Robotics and Automation Letters (RA-L)}, year = {2024} } ```  ## References Our code is partially based on the following code bases. * [CenterNet](https://github.com/xingyizhou/CenterNet) * [YOLOX](https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX) * [Deformabel Convolutions v2 (pytorch)](https://github.com/developer0hye/PyTorch-Deformable-Convolution-v2)",docker,SOFTWARE
"Run training in docker container     ```     docker run --gpus all -v ""$(pwd)/data/processed:/data/processed"" -v ""$(pwd)/log:/log"" -v ""$(pwd)/exp_files/train_final_model.py:/exp_file.py"" -e WANDB_API_KEY=<your-api-key> --shm-size=500m train_model     ```   ## Project structure  The directory structure of the project looks like this:  ```txt  ‚îú‚îÄ‚îÄ Makefile             <- Makefile with convenience commands like `make data` ‚îú‚îÄ‚îÄ README.md            <- The top-level README for developers using this project. ‚îú‚îÄ‚îÄ data ‚îÇ   ‚îú‚îÄ‚îÄ processed        <- The final, canonical data sets for modeling. ‚îÇ   ‚îî‚îÄ‚îÄ raw              <- The original, immutable data dump. ‚îÇ ‚îú‚îÄ‚îÄ exp_files            <- files to define the experiment configuration | ‚îú‚îÄ‚îÄ models               <- checkpoint models ‚îÇ ‚îú‚îÄ‚îÄ pyproject.toml       <- Project configuration file ‚îÇ ‚îú‚îÄ‚îÄ requirements.txt     <- The requirements file for reproducing the analysis environment | ‚îú‚îÄ‚îÄ requirements_dev.txt <- The requirements file for reproducing the analysis environment ‚îÇ ‚îú‚îÄ‚îÄ tests                <- Test files ‚îÇ ‚îú‚îÄ‚îÄ rumexleaves_centernet  <- Source code for use in this project. ‚îÇ ‚îú‚îÄ‚îÄ submodules          <- relevant submodules are stored here ‚îÇ ‚îî‚îÄ‚îÄ LICENSE              <- MIT License ``` Created using [mlops_template](https://github.com/SkafteNicki/mlops_template), a [cookiecutter template](https://github.com/cookiecutter/cookiecutter) for getting started with Machine Learning Operations (MLOps).  ## Citation  If you find this work useful in your research, please cite: ``` @article{RumexLeaves-CenterNet, author = {G√ºldenring, Ronja and Andersen, Rasmus Eckholdt and Nalpantidis, Lazaros}, title = {Zoom in on the Plant: Fine-grained Analysis of Leaf, Stem and Vein Instances}, journal = {IEEE Robotics and Automation Letters (RA-L)}, year = {2024} } ```  ## References Our code is partially based on the following code bases. * [CenterNet](https://github.com/xingyizhou/CenterNet) * [YOLOX](https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX) * [Deformabel Convolutions v2 (pytorch)](https://github.com/developer0hye/PyTorch-Deformable-Convolution-v2)",CenterNet,SOFTWARE
"Run training in docker container     ```     docker run --gpus all -v ""$(pwd)/data/processed:/data/processed"" -v ""$(pwd)/log:/log"" -v ""$(pwd)/exp_files/train_final_model.py:/exp_file.py"" -e WANDB_API_KEY=<your-api-key> --shm-size=500m train_model     ```   ## Project structure  The directory structure of the project looks like this:  ```txt  ‚îú‚îÄ‚îÄ Makefile             <- Makefile with convenience commands like `make data` ‚îú‚îÄ‚îÄ README.md            <- The top-level README for developers using this project. ‚îú‚îÄ‚îÄ data ‚îÇ   ‚îú‚îÄ‚îÄ processed        <- The final, canonical data sets for modeling. ‚îÇ   ‚îî‚îÄ‚îÄ raw              <- The original, immutable data dump. ‚îÇ ‚îú‚îÄ‚îÄ exp_files            <- files to define the experiment configuration | ‚îú‚îÄ‚îÄ models               <- checkpoint models ‚îÇ ‚îú‚îÄ‚îÄ pyproject.toml       <- Project configuration file ‚îÇ ‚îú‚îÄ‚îÄ requirements.txt     <- The requirements file for reproducing the analysis environment | ‚îú‚îÄ‚îÄ requirements_dev.txt <- The requirements file for reproducing the analysis environment ‚îÇ ‚îú‚îÄ‚îÄ tests                <- Test files ‚îÇ ‚îú‚îÄ‚îÄ rumexleaves_centernet  <- Source code for use in this project. ‚îÇ ‚îú‚îÄ‚îÄ submodules          <- relevant submodules are stored here ‚îÇ ‚îî‚îÄ‚îÄ LICENSE              <- MIT License ``` Created using [mlops_template](https://github.com/SkafteNicki/mlops_template), a [cookiecutter template](https://github.com/cookiecutter/cookiecutter) for getting started with Machine Learning Operations (MLOps).  ## Citation  If you find this work useful in your research, please cite: ``` @article{RumexLeaves-CenterNet, author = {G√ºldenring, Ronja and Andersen, Rasmus Eckholdt and Nalpantidis, Lazaros}, title = {Zoom in on the Plant: Fine-grained Analysis of Leaf, Stem and Vein Instances}, journal = {IEEE Robotics and Automation Letters (RA-L)}, year = {2024} } ```  ## References Our code is partially based on the following code bases. * [CenterNet](https://github.com/xingyizhou/CenterNet) * [YOLOX](https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX) * [Deformabel Convolutions v2 (pytorch)](https://github.com/developer0hye/PyTorch-Deformable-Convolution-v2)",CenterNet,SOFTWARE
"Run training in docker container     ```     docker run --gpus all -v ""$(pwd)/data/processed:/data/processed"" -v ""$(pwd)/log:/log"" -v ""$(pwd)/exp_files/train_final_model.py:/exp_file.py"" -e WANDB_API_KEY=<your-api-key> --shm-size=500m train_model     ```   ## Project structure  The directory structure of the project looks like this:  ```txt  ‚îú‚îÄ‚îÄ Makefile             <- Makefile with convenience commands like `make data` ‚îú‚îÄ‚îÄ README.md            <- The top-level README for developers using this project. ‚îú‚îÄ‚îÄ data ‚îÇ   ‚îú‚îÄ‚îÄ processed        <- The final, canonical data sets for modeling. ‚îÇ   ‚îî‚îÄ‚îÄ raw              <- The original, immutable data dump. ‚îÇ ‚îú‚îÄ‚îÄ exp_files            <- files to define the experiment configuration | ‚îú‚îÄ‚îÄ models               <- checkpoint models ‚îÇ ‚îú‚îÄ‚îÄ pyproject.toml       <- Project configuration file ‚îÇ ‚îú‚îÄ‚îÄ requirements.txt     <- The requirements file for reproducing the analysis environment | ‚îú‚îÄ‚îÄ requirements_dev.txt <- The requirements file for reproducing the analysis environment ‚îÇ ‚îú‚îÄ‚îÄ tests                <- Test files ‚îÇ ‚îú‚îÄ‚îÄ rumexleaves_centernet  <- Source code for use in this project. ‚îÇ ‚îú‚îÄ‚îÄ submodules          <- relevant submodules are stored here ‚îÇ ‚îî‚îÄ‚îÄ LICENSE              <- MIT License ``` Created using [mlops_template](https://github.com/SkafteNicki/mlops_template), a [cookiecutter template](https://github.com/cookiecutter/cookiecutter) for getting started with Machine Learning Operations (MLOps).  ## Citation  If you find this work useful in your research, please cite: ``` @article{RumexLeaves-CenterNet, author = {G√ºldenring, Ronja and Andersen, Rasmus Eckholdt and Nalpantidis, Lazaros}, title = {Zoom in on the Plant: Fine-grained Analysis of Leaf, Stem and Vein Instances}, journal = {IEEE Robotics and Automation Letters (RA-L)}, year = {2024} } ```  ## References Our code is partially based on the following code bases. * [CenterNet](https://github.com/xingyizhou/CenterNet) * [YOLOX](https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX) * [Deformabel Convolutions v2 (pytorch)](https://github.com/developer0hye/PyTorch-Deformable-Convolution-v2)",YOLOX,SOFTWARE
"Run training in docker container     ```     docker run --gpus all -v ""$(pwd)/data/processed:/data/processed"" -v ""$(pwd)/log:/log"" -v ""$(pwd)/exp_files/train_final_model.py:/exp_file.py"" -e WANDB_API_KEY=<your-api-key> --shm-size=500m train_model     ```   ## Project structure  The directory structure of the project looks like this:  ```txt  ‚îú‚îÄ‚îÄ Makefile             <- Makefile with convenience commands like `make data` ‚îú‚îÄ‚îÄ README.md            <- The top-level README for developers using this project. ‚îú‚îÄ‚îÄ data ‚îÇ   ‚îú‚îÄ‚îÄ processed        <- The final, canonical data sets for modeling. ‚îÇ   ‚îî‚îÄ‚îÄ raw              <- The original, immutable data dump. ‚îÇ ‚îú‚îÄ‚îÄ exp_files            <- files to define the experiment configuration | ‚îú‚îÄ‚îÄ models               <- checkpoint models ‚îÇ ‚îú‚îÄ‚îÄ pyproject.toml       <- Project configuration file ‚îÇ ‚îú‚îÄ‚îÄ requirements.txt     <- The requirements file for reproducing the analysis environment | ‚îú‚îÄ‚îÄ requirements_dev.txt <- The requirements file for reproducing the analysis environment ‚îÇ ‚îú‚îÄ‚îÄ tests                <- Test files ‚îÇ ‚îú‚îÄ‚îÄ rumexleaves_centernet  <- Source code for use in this project. ‚îÇ ‚îú‚îÄ‚îÄ submodules          <- relevant submodules are stored here ‚îÇ ‚îî‚îÄ‚îÄ LICENSE              <- MIT License ``` Created using [mlops_template](https://github.com/SkafteNicki/mlops_template), a [cookiecutter template](https://github.com/cookiecutter/cookiecutter) for getting started with Machine Learning Operations (MLOps).  ## Citation  If you find this work useful in your research, please cite: ``` @article{RumexLeaves-CenterNet, author = {G√ºldenring, Ronja and Andersen, Rasmus Eckholdt and Nalpantidis, Lazaros}, title = {Zoom in on the Plant: Fine-grained Analysis of Leaf, Stem and Vein Instances}, journal = {IEEE Robotics and Automation Letters (RA-L)}, year = {2024} } ```  ## References Our code is partially based on the following code bases. * [CenterNet](https://github.com/xingyizhou/CenterNet) * [YOLOX](https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX) * [Deformabel Convolutions v2 (pytorch)](https://github.com/developer0hye/PyTorch-Deformable-Convolution-v2)",YOLOX,SOFTWARE
"Run training in docker container     ```     docker run --gpus all -v ""$(pwd)/data/processed:/data/processed"" -v ""$(pwd)/log:/log"" -v ""$(pwd)/exp_files/train_final_model.py:/exp_file.py"" -e WANDB_API_KEY=<your-api-key> --shm-size=500m train_model     ```   ## Project structure  The directory structure of the project looks like this:  ```txt  ‚îú‚îÄ‚îÄ Makefile             <- Makefile with convenience commands like `make data` ‚îú‚îÄ‚îÄ README.md            <- The top-level README for developers using this project. ‚îú‚îÄ‚îÄ data ‚îÇ   ‚îú‚îÄ‚îÄ processed        <- The final, canonical data sets for modeling. ‚îÇ   ‚îî‚îÄ‚îÄ raw              <- The original, immutable data dump. ‚îÇ ‚îú‚îÄ‚îÄ exp_files            <- files to define the experiment configuration | ‚îú‚îÄ‚îÄ models               <- checkpoint models ‚îÇ ‚îú‚îÄ‚îÄ pyproject.toml       <- Project configuration file ‚îÇ ‚îú‚îÄ‚îÄ requirements.txt     <- The requirements file for reproducing the analysis environment | ‚îú‚îÄ‚îÄ requirements_dev.txt <- The requirements file for reproducing the analysis environment ‚îÇ ‚îú‚îÄ‚îÄ tests                <- Test files ‚îÇ ‚îú‚îÄ‚îÄ rumexleaves_centernet  <- Source code for use in this project. ‚îÇ ‚îú‚îÄ‚îÄ submodules          <- relevant submodules are stored here ‚îÇ ‚îî‚îÄ‚îÄ LICENSE              <- MIT License ``` Created using [mlops_template](https://github.com/SkafteNicki/mlops_template), a [cookiecutter template](https://github.com/cookiecutter/cookiecutter) for getting started with Machine Learning Operations (MLOps).  ## Citation  If you find this work useful in your research, please cite: ``` @article{RumexLeaves-CenterNet, author = {G√ºldenring, Ronja and Andersen, Rasmus Eckholdt and Nalpantidis, Lazaros}, title = {Zoom in on the Plant: Fine-grained Analysis of Leaf, Stem and Vein Instances}, journal = {IEEE Robotics and Automation Letters (RA-L)}, year = {2024} } ```  ## References Our code is partially based on the following code bases. * [CenterNet](https://github.com/xingyizhou/CenterNet) * [YOLOX](https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX) * [Deformabel Convolutions v2 (pytorch)](https://github.com/developer0hye/PyTorch-Deformable-Convolution-v2)",Deformabel Convolutions v2,SOFTWARE
"Run training in docker container     ```     docker run --gpus all -v ""$(pwd)/data/processed:/data/processed"" -v ""$(pwd)/log:/log"" -v ""$(pwd)/exp_files/train_final_model.py:/exp_file.py"" -e WANDB_API_KEY=<your-api-key> --shm-size=500m train_model     ```   ## Project structure  The directory structure of the project looks like this:  ```txt  ‚îú‚îÄ‚îÄ Makefile             <- Makefile with convenience commands like `make data` ‚îú‚îÄ‚îÄ README.md            <- The top-level README for developers using this project. ‚îú‚îÄ‚îÄ data ‚îÇ   ‚îú‚îÄ‚îÄ processed        <- The final, canonical data sets for modeling. ‚îÇ   ‚îî‚îÄ‚îÄ raw              <- The original, immutable data dump. ‚îÇ ‚îú‚îÄ‚îÄ exp_files            <- files to define the experiment configuration | ‚îú‚îÄ‚îÄ models               <- checkpoint models ‚îÇ ‚îú‚îÄ‚îÄ pyproject.toml       <- Project configuration file ‚îÇ ‚îú‚îÄ‚îÄ requirements.txt     <- The requirements file for reproducing the analysis environment | ‚îú‚îÄ‚îÄ requirements_dev.txt <- The requirements file for reproducing the analysis environment ‚îÇ ‚îú‚îÄ‚îÄ tests                <- Test files ‚îÇ ‚îú‚îÄ‚îÄ rumexleaves_centernet  <- Source code for use in this project. ‚îÇ ‚îú‚îÄ‚îÄ submodules          <- relevant submodules are stored here ‚îÇ ‚îî‚îÄ‚îÄ LICENSE              <- MIT License ``` Created using [mlops_template](https://github.com/SkafteNicki/mlops_template), a [cookiecutter template](https://github.com/cookiecutter/cookiecutter) for getting started with Machine Learning Operations (MLOps).  ## Citation  If you find this work useful in your research, please cite: ``` @article{RumexLeaves-CenterNet, author = {G√ºldenring, Ronja and Andersen, Rasmus Eckholdt and Nalpantidis, Lazaros}, title = {Zoom in on the Plant: Fine-grained Analysis of Leaf, Stem and Vein Instances}, journal = {IEEE Robotics and Automation Letters (RA-L)}, year = {2024} } ```  ## References Our code is partially based on the following code bases. * [CenterNet](https://github.com/xingyizhou/CenterNet) * [YOLOX](https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX) * [Deformabel Convolutions v2 (pytorch)](https://github.com/developer0hye/PyTorch-Deformable-Convolution-v2)",pytorch,SOFTWARE
"Run training in docker container     ```     docker run --gpus all -v ""$(pwd)/data/processed:/data/processed"" -v ""$(pwd)/log:/log"" -v ""$(pwd)/exp_files/train_final_model.py:/exp_file.py"" -e WANDB_API_KEY=<your-api-key> --shm-size=500m train_model     ```   ## Project structure  The directory structure of the project looks like this:  ```txt  ‚îú‚îÄ‚îÄ Makefile             <- Makefile with convenience commands like `make data` ‚îú‚îÄ‚îÄ README.md            <- The top-level README for developers using this project. ‚îú‚îÄ‚îÄ data ‚îÇ   ‚îú‚îÄ‚îÄ processed        <- The final, canonical data sets for modeling. ‚îÇ   ‚îî‚îÄ‚îÄ raw              <- The original, immutable data dump. ‚îÇ ‚îú‚îÄ‚îÄ exp_files            <- files to define the experiment configuration | ‚îú‚îÄ‚îÄ models               <- checkpoint models ‚îÇ ‚îú‚îÄ‚îÄ pyproject.toml       <- Project configuration file ‚îÇ ‚îú‚îÄ‚îÄ requirements.txt     <- The requirements file for reproducing the analysis environment | ‚îú‚îÄ‚îÄ requirements_dev.txt <- The requirements file for reproducing the analysis environment ‚îÇ ‚îú‚îÄ‚îÄ tests                <- Test files ‚îÇ ‚îú‚îÄ‚îÄ rumexleaves_centernet  <- Source code for use in this project. ‚îÇ ‚îú‚îÄ‚îÄ submodules          <- relevant submodules are stored here ‚îÇ ‚îî‚îÄ‚îÄ LICENSE              <- MIT License ``` Created using [mlops_template](https://github.com/SkafteNicki/mlops_template), a [cookiecutter template](https://github.com/cookiecutter/cookiecutter) for getting started with Machine Learning Operations (MLOps).  ## Citation  If you find this work useful in your research, please cite: ``` @article{RumexLeaves-CenterNet, author = {G√ºldenring, Ronja and Andersen, Rasmus Eckholdt and Nalpantidis, Lazaros}, title = {Zoom in on the Plant: Fine-grained Analysis of Leaf, Stem and Vein Instances}, journal = {IEEE Robotics and Automation Letters (RA-L)}, year = {2024} } ```  ## References Our code is partially based on the following code bases. * [CenterNet](https://github.com/xingyizhou/CenterNet) * [YOLOX](https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX) * [Deformabel Convolutions v2 (pytorch)](https://github.com/developer0hye/PyTorch-Deformable-Convolution-v2)",PyTorch,SOFTWARE
"Run training in docker container     ```     docker run --gpus all -v ""$(pwd)/data/processed:/data/processed"" -v ""$(pwd)/log:/log"" -v ""$(pwd)/exp_files/train_final_model.py:/exp_file.py"" -e WANDB_API_KEY=<your-api-key> --shm-size=500m train_model     ```   ## Project structure  The directory structure of the project looks like this:  ```txt  ‚îú‚îÄ‚îÄ Makefile             <- Makefile with convenience commands like `make data` ‚îú‚îÄ‚îÄ README.md            <- The top-level README for developers using this project. ‚îú‚îÄ‚îÄ data ‚îÇ   ‚îú‚îÄ‚îÄ processed        <- The final, canonical data sets for modeling. ‚îÇ   ‚îî‚îÄ‚îÄ raw              <- The original, immutable data dump. ‚îÇ ‚îú‚îÄ‚îÄ exp_files            <- files to define the experiment configuration | ‚îú‚îÄ‚îÄ models               <- checkpoint models ‚îÇ ‚îú‚îÄ‚îÄ pyproject.toml       <- Project configuration file ‚îÇ ‚îú‚îÄ‚îÄ requirements.txt     <- The requirements file for reproducing the analysis environment | ‚îú‚îÄ‚îÄ requirements_dev.txt <- The requirements file for reproducing the analysis environment ‚îÇ ‚îú‚îÄ‚îÄ tests                <- Test files ‚îÇ ‚îú‚îÄ‚îÄ rumexleaves_centernet  <- Source code for use in this project. ‚îÇ ‚îú‚îÄ‚îÄ submodules          <- relevant submodules are stored here ‚îÇ ‚îî‚îÄ‚îÄ LICENSE              <- MIT License ``` Created using [mlops_template](https://github.com/SkafteNicki/mlops_template), a [cookiecutter template](https://github.com/cookiecutter/cookiecutter) for getting started with Machine Learning Operations (MLOps).  ## Citation  If you find this work useful in your research, please cite: ``` @article{RumexLeaves-CenterNet, author = {G√ºldenring, Ronja and Andersen, Rasmus Eckholdt and Nalpantidis, Lazaros}, title = {Zoom in on the Plant: Fine-grained Analysis of Leaf, Stem and Vein Instances}, journal = {IEEE Robotics and Automation Letters (RA-L)}, year = {2024} } ```  ## References Our code is partially based on the following code bases. * [CenterNet](https://github.com/xingyizhou/CenterNet) * [YOLOX](https://raw.githubusercontent.com/Megvii-BaseDetection/YOLOX) * [Deformabel Convolutions v2 (pytorch)](https://github.com/developer0hye/PyTorch-Deformable-Convolution-v2)",Deformable-Convolution-v2,SOFTWARE
Experiments on real-world temporal graphs with ground-truth communities validate the superior quality of the solutions obtained and the efficiency of our model in both temporal and interactive static settings.            ### Dependecies & Hardware ---------------- To run our codes you need to have these packages installed:  ``` torch==1.13.0 torch-geometric==2.2.0 torch-scatter==2.1.0 torch-sparse==0.6.15 torch-spline-conv==1.2.1 torchmetrics==0.11.0 ```  A single GPU should be enough to train models.,torch==1.13.0,SOFTWARE
Experiments on real-world temporal graphs with ground-truth communities validate the superior quality of the solutions obtained and the efficiency of our model in both temporal and interactive static settings.            ### Dependecies & Hardware ---------------- To run our codes you need to have these packages installed:  ``` torch==1.13.0 torch-geometric==2.2.0 torch-scatter==2.1.0 torch-sparse==0.6.15 torch-spline-conv==1.2.1 torchmetrics==0.11.0 ```  A single GPU should be enough to train models.,torch-scatter==2.1.0,SOFTWARE
Experiments on real-world temporal graphs with ground-truth communities validate the superior quality of the solutions obtained and the efficiency of our model in both temporal and interactive static settings.            ### Dependecies & Hardware ---------------- To run our codes you need to have these packages installed:  ``` torch==1.13.0 torch-geometric==2.2.0 torch-scatter==2.1.0 torch-sparse==0.6.15 torch-spline-conv==1.2.1 torchmetrics==0.11.0 ```  A single GPU should be enough to train models.,torch-sparse==0.6.15,SOFTWARE
Experiments on real-world temporal graphs with ground-truth communities validate the superior quality of the solutions obtained and the efficiency of our model in both temporal and interactive static settings.            ### Dependecies & Hardware ---------------- To run our codes you need to have these packages installed:  ``` torch==1.13.0 torch-geometric==2.2.0 torch-scatter==2.1.0 torch-sparse==0.6.15 torch-spline-conv==1.2.1 torchmetrics==0.11.0 ```  A single GPU should be enough to train models.,torch-spline-conv==1.2.1,SOFTWARE
Experiments on real-world temporal graphs with ground-truth communities validate the superior quality of the solutions obtained and the efficiency of our model in both temporal and interactive static settings.            ### Dependecies & Hardware ---------------- To run our codes you need to have these packages installed:  ``` torch==1.13.0 torch-geometric==2.2.0 torch-scatter==2.1.0 torch-sparse==0.6.15 torch-spline-conv==1.2.1 torchmetrics==0.11.0 ```  A single GPU should be enough to train models.,torchmetrics==0.11.0,SOFTWARE
"The `util/` directory contains some codes to parse specific datasets and generate queries.        ### Dataset Structure ---------------- The graphs and query/answer sets can be in any format but in order to use our train script and dataset utilities these should be in following format.        ### File Structure ---------------- Our training script assume that the data stored in the following format: ``` data/     dataset_name/         graph_{snapshot_id}.txt         graph_{snapshot_id}.txt         queries_{snapshot_id}.txt         queries_{snapshot_id}.txt ```  So for example for a a dataset named `foo` with 3 snapshots it would be like:  ``` data/     foo/         graph_1.txt         graph_2.txt         graph_3.txt         queries_1.txt         queries_2.txt         queries_3.txt         test_queries.txt         valid_queries.txt ```        ### Graph file and QueryFiles ---------------- The graph files must have the following format.  ``` node_id1 node_id2 .. .. ```  The query files must have the following format.   ``` q1_node1,q1_node2,... answer1_node1,answer1_node2,... q2_node1,q2_node2,... answer2_node1,answer2_node2 ```         ### Training the Models & Execution ---------------- Finally the models could be trained and evaluated with the following command.   ``` python3 train.py <DATASET_NAME> <MAX_VERTICES> <START_SNAPSHOT_ID> <END_SNAPSHOT_ID> <THRESHOLD> <HIDDEN_DIM_SIZE> <EPOCHS> <LEARNING_RATE> <REGULARIZATION> ```  For example: ``` python3 train.py football 200 1 4 0.4 64 100 0.001 0.00001 ```   ### Reference ----------------   ``` @inproceedings{CS-TGN, author = {Hashemi, Farnoosh and Behrouz, Ali and Rezaei Hajidehi, Milad}, title = {CS-TGN: Community Search via Temporal Graph Neural Networks}, year = {2023}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3543873.3587654}, doi = {10.1145/3543873.3587654}, booktitle = {Companion Proceedings of the Web Conference 2023}, numpages = {8}, location = {AUSTIN, TEXAS, USA}, series = {WWW '23} } ```",python3,SOFTWARE
[Python](https://img.shields.io/badge/python-3.10%5E-blue)](https://www.python.org/)  # StarCraft II Datasets  Library can be used to interface with datasets that were pre-processed with our pipeline as described in: - [SC2DatasetPreparator](https://github.com/Kaszanas/SC2DatasetPreparator)  Currently we have exposed PyTorch and PyTorch Lightning API.,PyTorch,SOFTWARE
[Python](https://img.shields.io/badge/python-3.10%5E-blue)](https://www.python.org/)  # StarCraft II Datasets  Library can be used to interface with datasets that were pre-processed with our pipeline as described in: - [SC2DatasetPreparator](https://github.com/Kaszanas/SC2DatasetPreparator)  Currently we have exposed PyTorch and PyTorch Lightning API.,PyTorch,SOFTWARE
"Please refer to the [**official documentation**](https://sc2-datasets.readthedocs.io/), or contact contributors directly for all of the details.  ## Supported Datasets  ### SC2EGSet: StarCraft II Esport Game State Dataset  This project contains official API implementation for the [SC2EGSet: StarCraft II Esport Game State Dataset](https://doi.org/10.5281/zenodo.5503997), which is built based on [SC2ReSet: StarCraft II Esport Replaypack Set](https://doi.org/10.5281/zenodo.5575796).",zenodo,SOFTWARE
Contents of this library provide PyTorch and PyTorch Lightning API for pre-processed StarCraft II dataset.  ## Installation  1.,PyTorch,SOFTWARE
Contents of this library provide PyTorch and PyTorch Lightning API for pre-processed StarCraft II dataset.  ## Installation  1.,PyTorch,SOFTWARE
Manually install PyTorch with minimal version of ```^1.11.0+cu116```. 2.,PyTorch,SOFTWARE
Perform the following command:  ```bash $ pip install sc2_datasets ```  ## Usage  Basic example usage can be seen below.,pip,SOFTWARE
"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir="".",zenodo,SOFTWARE
"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir="".",PyTorch,SOFTWARE
"Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch: ```python from sc2_datasets.torch.sc2_egset_dataset import SC2EGSetDataset from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     # Initialize the dataset:     sc2_egset_dataset = SC2EGSetDataset(         unpack_dir="".",torch,SOFTWARE
"download=True,         names_urls=EXAMPLE_SYNTHETIC_REPLAYPACKS, # Use a synthetic replaypack containing 1 replay.     )      # Iterate over instances:     for i in range(len(sc2_egset_dataset)):         sc2_egset_dataset[i] ```  Use [SC2EGSet](https://doi.org/10.5281/zenodo.5503997) with PyTorch Lightning: ```python from sc2_datasets.lightning.sc2_egset_datamodule import SC2EGSetDataModule from sc2_datasets.available_replaypacks import EXAMPLE_SYNTHETIC_REPLAYPACKS  if __name__ == ""__main__"":     sc2_egset_datamodule = SC2EGSetDataModule(                 unpack_dir="".",PyTorch,SOFTWARE
Installation We tested DSM int two different system configurations: **Ubuntu 18.04** and **Windows 10** (VS15 and VS17).,Ubuntu 18.04,SOFTWARE
Installation We tested DSM int two different system configurations: **Ubuntu 18.04** and **Windows 10** (VS15 and VS17).,Windows 10,SOFTWARE
Clone the repository:  ```sh git clone https://github.com/jzubizarreta/dsm.git ```  ### 2.1 Required Dependencies  #### Eigen3  We use [Eigen3](http://eigen.tuxfamily.org) for almost any mathematical operation.,git,SOFTWARE
Clone the repository:  ```sh git clone https://github.com/jzubizarreta/dsm.git ```  ### 2.1 Required Dependencies  #### Eigen3  We use [Eigen3](http://eigen.tuxfamily.org) for almost any mathematical operation.,Eigen3,SOFTWARE
Install with   ```sh sudo apt-get install libeigen3-dev ```  #### OpenCV  We use [OpenCV](https://opencv.org/) to manipulate images (read/write/display) and to bootstrap the monocular system.,libeigen3-dev,SOFTWARE
Install with   ```sh sudo apt-get install libeigen3-dev ```  #### OpenCV  We use [OpenCV](https://opencv.org/) to manipulate images (read/write/display) and to bootstrap the monocular system.,OpenCV,SOFTWARE
Install with   ```sh sudo apt-get install libeigen3-dev ```  #### OpenCV  We use [OpenCV](https://opencv.org/) to manipulate images (read/write/display) and to bootstrap the monocular system.,OpenCV,SOFTWARE
"Feel free to implement those functionalities with your prefered library, if you want to get rid off Opencv.",Opencv,SOFTWARE
Install with   ```sh sudo apt-get install libopencv-dev ```  #### Ceres Solver  We use [Ceres Solver](http://ceres-solver.org) to perform the photometric bundle adjustment.,libopencv-dev,SOFTWARE
Install with   ```sh sudo apt-get install libopencv-dev ```  #### Ceres Solver  We use [Ceres Solver](http://ceres-solver.org) to perform the photometric bundle adjustment.,Ceres Solver,SOFTWARE
Install with   ```sh sudo apt-get install libopencv-dev ```  #### Ceres Solver  We use [Ceres Solver](http://ceres-solver.org) to perform the photometric bundle adjustment.,Ceres Solver,SOFTWARE
Install with   ```sh sudo apt-get install libopencv-dev ```  #### Ceres Solver  We use [Ceres Solver](http://ceres-solver.org) to perform the photometric bundle adjustment.,ceres-solver,SOFTWARE
```sh git clone https://ceres-solver.googlesource.com/ceres-solver ```   Ceres dependencies:   ```sh # glog & gflags sudo apt-get install libgoogle-glog-dev  # BLAS & LAPACK sudo apt-get install libatlas-base-dev  # SuiteSparse sudo apt-get install libsuitesparse-dev ```    Generate the custom template specialization (optional):  ```sh cp .,git,SOFTWARE
```sh git clone https://ceres-solver.googlesource.com/ceres-solver ```   Ceres dependencies:   ```sh # glog & gflags sudo apt-get install libgoogle-glog-dev  # BLAS & LAPACK sudo apt-get install libatlas-base-dev  # SuiteSparse sudo apt-get install libsuitesparse-dev ```    Generate the custom template specialization (optional):  ```sh cp .,libgoogle-glog-dev,SOFTWARE
/ceres-solver/internal/ceres/ python2 ceres-solver/internal/ceres/generate_template_specializations.py ```  Install with   ```sh cd ceres-solver mkdir build cd build cmake .. make -j4 sudo make install  ```  ### 2.2 Optional Dependencies  #### Qt  We use [Qt](https://www.qt.io/) for GUI and visualization.,ceres-solver,SOFTWARE
/ceres-solver/internal/ceres/ python2 ceres-solver/internal/ceres/generate_template_specializations.py ```  Install with   ```sh cd ceres-solver mkdir build cd build cmake .. make -j4 sudo make install  ```  ### 2.2 Optional Dependencies  #### Qt  We use [Qt](https://www.qt.io/) for GUI and visualization.,ceres-solver,SOFTWARE
/ceres-solver/internal/ceres/ python2 ceres-solver/internal/ceres/generate_template_specializations.py ```  Install with   ```sh cd ceres-solver mkdir build cd build cmake .. make -j4 sudo make install  ```  ### 2.2 Optional Dependencies  #### Qt  We use [Qt](https://www.qt.io/) for GUI and visualization.,Qt,SOFTWARE
/ceres-solver/internal/ceres/ python2 ceres-solver/internal/ceres/generate_template_specializations.py ```  Install with   ```sh cd ceres-solver mkdir build cd build cmake .. make -j4 sudo make install  ```  ### 2.2 Optional Dependencies  #### Qt  We use [Qt](https://www.qt.io/) for GUI and visualization.,Qt,SOFTWARE
"Although Qt is required to compile the whole project in the current version, it is easy to remove it.",Qt,SOFTWARE
DSM does not depend on Qt.,Qt,SOFTWARE
All the code related with Qt is in the `QtVisualizer` folder.,Qt,SOFTWARE
Feel free to implement your own version of `IVisualizer` and replace the current visualizer `QtVisualizer`.,IVisualizer,SOFTWARE
Feel free to implement your own version of `IVisualizer` and replace the current visualizer `QtVisualizer`.,QtVisualizer,SOFTWARE
Install with  ```sh sudo apt-get install qt5-default ```  ### 2.3 DSM build  Now we are ready to build DSM.,qt5,SOFTWARE
"External dependencies on macOS can be installed with [Homebrew](https://brew.sh/)  ```sh brew install cmake eigen opencv ceres-solver qt ```  From this point, you can follow the instructions above for *2.3 DSM build*.",Homebrew,SOFTWARE
"External dependencies on macOS can be installed with [Homebrew](https://brew.sh/)  ```sh brew install cmake eigen opencv ceres-solver qt ```  From this point, you can follow the instructions above for *2.3 DSM build*.",opencv,SOFTWARE
"External dependencies on macOS can be installed with [Homebrew](https://brew.sh/)  ```sh brew install cmake eigen opencv ceres-solver qt ```  From this point, you can follow the instructions above for *2.3 DSM build*.",ceres-solver,SOFTWARE
"External dependencies on macOS can be installed with [Homebrew](https://brew.sh/)  ```sh brew install cmake eigen opencv ceres-solver qt ```  From this point, you can follow the instructions above for *2.3 DSM build*.",qt,SOFTWARE
"**Note**: In some cases, the Qt5 installation location is not automatically found and you need to specify the modules manually to cmake with  ```sh cmake -DQt5Core_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5Core"" -DQt5Widgets_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5Widgets"" -DQt5OpenGL_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5OpenGL"" .. ```  ## 3.",Qt5,SOFTWARE
"**Note**: In some cases, the Qt5 installation location is not automatically found and you need to specify the modules manually to cmake with  ```sh cmake -DQt5Core_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5Core"" -DQt5Widgets_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5Widgets"" -DQt5OpenGL_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5OpenGL"" .. ```  ## 3.",qt5,SOFTWARE
"**Note**: In some cases, the Qt5 installation location is not automatically found and you need to specify the modules manually to cmake with  ```sh cmake -DQt5Core_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5Core"" -DQt5Widgets_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5Widgets"" -DQt5OpenGL_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5OpenGL"" .. ```  ## 3.",Qt5,SOFTWARE
"**Note**: In some cases, the Qt5 installation location is not automatically found and you need to specify the modules manually to cmake with  ```sh cmake -DQt5Core_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5Core"" -DQt5Widgets_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5Widgets"" -DQt5OpenGL_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5OpenGL"" .. ```  ## 3.",qt5,SOFTWARE
"**Note**: In some cases, the Qt5 installation location is not automatically found and you need to specify the modules manually to cmake with  ```sh cmake -DQt5Core_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5Core"" -DQt5Widgets_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5Widgets"" -DQt5OpenGL_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5OpenGL"" .. ```  ## 3.",Qt5,SOFTWARE
"**Note**: In some cases, the Qt5 installation location is not automatically found and you need to specify the modules manually to cmake with  ```sh cmake -DQt5Core_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5Core"" -DQt5Widgets_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5Widgets"" -DQt5OpenGL_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5OpenGL"" .. ```  ## 3.",qt5,SOFTWARE
"**Note**: In some cases, the Qt5 installation location is not automatically found and you need to specify the modules manually to cmake with  ```sh cmake -DQt5Core_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5Core"" -DQt5Widgets_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5Widgets"" -DQt5OpenGL_DIR=""$(brew --prefix qt5)/lib/cmake/Qt5OpenGL"" .. ```  ## 3.",Qt5,SOFTWARE
Currently it uses the radial-tangential model of OpenCV.,OpenCV,SOFTWARE
You have to add the additional distortion coefficients using the OpenCV order.,OpenCV,SOFTWARE
/VideoExample <VIDEO_FILE> <CALIB_FILE> <SETTINGS_FILE> ```  where     * `<VIDEO_FILE>` Custom video file with a supported format by OpenCV.  ## 4.,OpenCV,SOFTWARE
"It accompanies the paper ""Iterative Graph Reasoning"" and contains:  - 5 distinct rule-based scenarios - 200 training queries per scenario (low-data learning scenario) - 100 test queries per scenario - Weak annotations provided for each query  ## :new: Updates - [08/2024] [Arxiv paper](https://arxiv.org/abs/2408.16667) released. - [08/2024] RuleAlign dataset announced.  ## :gear: Evaluation  To run the evaluation on the RuleAlign dataset, use the following command:  ```shell python -m script.eval --model_name xxx ```  Replace `xxx` with the name of the model you want to evaluate.  ## :hugs: Citation If you find this dataset useful for your research, please kindly cite our paper:  ``` @misc{yu2024iterativegraphreasoning,       title={Iterative Graph Reasoning},        author={Fangyuan Yu},       year={2024},       eprint={2408.03615},       archivePrefix={arXiv},       primaryClass={cs.AI} } ```  ## :mailbox: Contact  For any questions or issues regarding the RuleEval dataset, please contact the corresponding author.",python,SOFTWARE
It is based on [mmsegmentaion](https://github.com/open-mmlab/mmsegmentation/tree/v0.11.0).,mmsegmentaion,SOFTWARE
It is based on [mmsegmentaion](https://github.com/open-mmlab/mmsegmentation/tree/v0.11.0).,mmsegmentation,SOFTWARE
"Fortunately, we can ignore them during the training phase, but we also lose some information, which is a problem: we thus removed the tiles that had more than 50% of unidentified pixels to try to improve the training.  ## Usage  ### Installation  Please refer to [get_started.md](https://github.com/open-mmlab/mmsegmentation/blob/master/docs/get_started.md#installation) for installation and dataset preparation.",mmsegmentation,SOFTWARE
"**Notes:**  During the installation, it is important to:    * Install MMSegmentation in dev mode: ``` git clone https://github.com/open-mmlab/mmsegmentation.git cd mmsegmentation pip install -e . ``` * Copy the *mmcv_custom* and *mmseg* folders into the *mmsegmentation* folder  ### Inference The pre-trained model (i.e. checkpoint file) for satellite image segmentation is available for download [here](https://drive.google.com/file/d/1EarMOBHx6meawa6izNXJUfXRCTzhKT2M/view?",MMSegmentation,SOFTWARE
"**Notes:**  During the installation, it is important to:    * Install MMSegmentation in dev mode: ``` git clone https://github.com/open-mmlab/mmsegmentation.git cd mmsegmentation pip install -e . ``` * Copy the *mmcv_custom* and *mmseg* folders into the *mmsegmentation* folder  ### Inference The pre-trained model (i.e. checkpoint file) for satellite image segmentation is available for download [here](https://drive.google.com/file/d/1EarMOBHx6meawa6izNXJUfXRCTzhKT2M/view?",git,SOFTWARE
"**Notes:**  During the installation, it is important to:    * Install MMSegmentation in dev mode: ``` git clone https://github.com/open-mmlab/mmsegmentation.git cd mmsegmentation pip install -e . ``` * Copy the *mmcv_custom* and *mmseg* folders into the *mmsegmentation* folder  ### Inference The pre-trained model (i.e. checkpoint file) for satellite image segmentation is available for download [here](https://drive.google.com/file/d/1EarMOBHx6meawa6izNXJUfXRCTzhKT2M/view?",mmsegmentation,SOFTWARE
"**Notes:**  During the installation, it is important to:    * Install MMSegmentation in dev mode: ``` git clone https://github.com/open-mmlab/mmsegmentation.git cd mmsegmentation pip install -e . ``` * Copy the *mmcv_custom* and *mmseg* folders into the *mmsegmentation* folder  ### Inference The pre-trained model (i.e. checkpoint file) for satellite image segmentation is available for download [here](https://drive.google.com/file/d/1EarMOBHx6meawa6izNXJUfXRCTzhKT2M/view?",git,SOFTWARE
"**Notes:**  During the installation, it is important to:    * Install MMSegmentation in dev mode: ``` git clone https://github.com/open-mmlab/mmsegmentation.git cd mmsegmentation pip install -e . ``` * Copy the *mmcv_custom* and *mmseg* folders into the *mmsegmentation* folder  ### Inference The pre-trained model (i.e. checkpoint file) for satellite image segmentation is available for download [here](https://drive.google.com/file/d/1EarMOBHx6meawa6izNXJUfXRCTzhKT2M/view?",pip,SOFTWARE
"usp=sharing).  ``` # single-gpu testing python tools/test.py <CONFIG_FILE> <SEG_CHECKPOINT_FILE> --eval mIoU  # multi-gpu testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --eval mIoU  # multi-gpu, multi-scale testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --aug-test --eval mIoU ```  Example on the Ampli ANR project:   ``` # Evaluate checkpoint on a single GPU python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --eval mIoU  # Display segmentation results python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --show ```  ### Training  To train with pre-trained models, run: ``` # single-gpu training python tools/train.py <CONFIG_FILE> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  # multi-gpu training tools/dist_train.sh <CONFIG_FILE> <GPU_NUM> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  ```  Example on the Ampli ANR project with the ImageNet-22K pretrained model (available [here](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth)) :   ``` python tools/train.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py --options model.pretrained="".",python,SOFTWARE
"usp=sharing).  ``` # single-gpu testing python tools/test.py <CONFIG_FILE> <SEG_CHECKPOINT_FILE> --eval mIoU  # multi-gpu testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --eval mIoU  # multi-gpu, multi-scale testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --aug-test --eval mIoU ```  Example on the Ampli ANR project:   ``` # Evaluate checkpoint on a single GPU python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --eval mIoU  # Display segmentation results python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --show ```  ### Training  To train with pre-trained models, run: ``` # single-gpu training python tools/train.py <CONFIG_FILE> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  # multi-gpu training tools/dist_train.sh <CONFIG_FILE> <GPU_NUM> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  ```  Example on the Ampli ANR project with the ImageNet-22K pretrained model (available [here](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth)) :   ``` python tools/train.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py --options model.pretrained="".",python,SOFTWARE
"usp=sharing).  ``` # single-gpu testing python tools/test.py <CONFIG_FILE> <SEG_CHECKPOINT_FILE> --eval mIoU  # multi-gpu testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --eval mIoU  # multi-gpu, multi-scale testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --aug-test --eval mIoU ```  Example on the Ampli ANR project:   ``` # Evaluate checkpoint on a single GPU python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --eval mIoU  # Display segmentation results python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --show ```  ### Training  To train with pre-trained models, run: ``` # single-gpu training python tools/train.py <CONFIG_FILE> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  # multi-gpu training tools/dist_train.sh <CONFIG_FILE> <GPU_NUM> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  ```  Example on the Ampli ANR project with the ImageNet-22K pretrained model (available [here](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth)) :   ``` python tools/train.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py --options model.pretrained="".",python,SOFTWARE
"usp=sharing).  ``` # single-gpu testing python tools/test.py <CONFIG_FILE> <SEG_CHECKPOINT_FILE> --eval mIoU  # multi-gpu testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --eval mIoU  # multi-gpu, multi-scale testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --aug-test --eval mIoU ```  Example on the Ampli ANR project:   ``` # Evaluate checkpoint on a single GPU python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --eval mIoU  # Display segmentation results python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --show ```  ### Training  To train with pre-trained models, run: ``` # single-gpu training python tools/train.py <CONFIG_FILE> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  # multi-gpu training tools/dist_train.sh <CONFIG_FILE> <GPU_NUM> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  ```  Example on the Ampli ANR project with the ImageNet-22K pretrained model (available [here](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth)) :   ``` python tools/train.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py --options model.pretrained="".",python,SOFTWARE
"usp=sharing).  ``` # single-gpu testing python tools/test.py <CONFIG_FILE> <SEG_CHECKPOINT_FILE> --eval mIoU  # multi-gpu testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --eval mIoU  # multi-gpu, multi-scale testing tools/dist_test.sh <CONFIG_FILE> <SEG_CHECKPOINT_FILE> <GPU_NUM> --aug-test --eval mIoU ```  Example on the Ampli ANR project:   ``` # Evaluate checkpoint on a single GPU python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --eval mIoU  # Display segmentation results python tools/test.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py checkpoints/ign_60k_swin_large_patch4_window12_384.pth --show ```  ### Training  To train with pre-trained models, run: ``` # single-gpu training python tools/train.py <CONFIG_FILE> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  # multi-gpu training tools/dist_train.sh <CONFIG_FILE> <GPU_NUM> --options model.pretrained=<PRETRAIN_MODEL> [model.backbone.use_checkpoint=True] [other optional arguments]  ```  Example on the Ampli ANR project with the ImageNet-22K pretrained model (available [here](https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth)) :   ``` python tools/train.py configs/swin/config_upernet_swin_large_patch4_window12_384x384_60k_ign.py --options model.pretrained="".",python,SOFTWARE
"Please refer to [this page](https://pytorch.org/docs/stable/checkpoint.html) for more details. - The default learning rate and training schedule is for 8 GPUs and 2 imgs/gpu.   ## Citing Swin Transformer ``` @article{liu2021Swin,   title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},   author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},   journal={arXiv preprint arXiv:2103.14030},   year={2021} } ```  ## Citing this work  See the complete description of this work in the [dedicated arXiv paper](https://arxiv.org/abs/2110.05812).",pytorch,SOFTWARE
"If you use this work, please cite it: ``` @misc{guerin2021satellite,       title={Satellite Image Semantic Segmentation},        author={Eric Gu√©rin and Killian Oechslin and Christian Wolf and Beno√Æt Martinez},       year={2021},       eprint={2110.05812},       archivePrefix={arXiv},       primaryClass={cs.CV} } ```  ## Other Links  > **Image Classification**: See [Swin Transformer for Image Classification](https://github.com/microsoft/Swin-Transformer)",Transformer,SOFTWARE
> **Self-Supervised Learning**: See [MoBY with Swin Transformer](https://github.com/SwinTransformer/Transformer-SSL),Transformer,SOFTWARE
Install relevant dependencies with `pip install -r requirements.txt`.  1.,pip,SOFTWARE
"This script produces the final architecture, using FastText encoding",FastText,SOFTWARE
"- **End-to-end one-step person search** with Transformers, which does not requre NMS post-processing. - **Pre-trained models** with ResNet50, ResNet50-DCN, and PVTv2b2. - **Curves of different methods** on CUHK under different gallery sizes ([plot_cuhk.py](plot_cuhk.py)).",ResNet50,SOFTWARE
"- **End-to-end one-step person search** with Transformers, which does not requre NMS post-processing. - **Pre-trained models** with ResNet50, ResNet50-DCN, and PVTv2b2. - **Curves of different methods** on CUHK under different gallery sizes ([plot_cuhk.py](plot_cuhk.py)).",ResNet50-DCN,SOFTWARE
"- **End-to-end one-step person search** with Transformers, which does not requre NMS post-processing. - **Pre-trained models** with ResNet50, ResNet50-DCN, and PVTv2b2. - **Curves of different methods** on CUHK under different gallery sizes ([plot_cuhk.py](plot_cuhk.py)).",PVTv2b2,SOFTWARE
"<tr> <div align=""center"">   <img src=""demo/fig-cuhk.jpg"" width=""700px"" /> </div> </tr>  ## Installation - We install this project using cuda11.1 and PyTorch1.8.0 (or PyTorch1.9.0) as follows.",cuda11.1,SOFTWARE
"<tr> <div align=""center"">   <img src=""demo/fig-cuhk.jpg"" width=""700px"" /> </div> </tr>  ## Installation - We install this project using cuda11.1 and PyTorch1.8.0 (or PyTorch1.9.0) as follows.",PyTorch1.8.0,SOFTWARE
"<tr> <div align=""center"">   <img src=""demo/fig-cuhk.jpg"" width=""700px"" /> </div> </tr>  ## Installation - We install this project using cuda11.1 and PyTorch1.8.0 (or PyTorch1.9.0) as follows.",PyTorch1.9.0,SOFTWARE
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",conda,SOFTWARE
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",conda,SOFTWARE
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",pip,SOFTWARE
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",torch==1.8.1,SOFTWARE
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",torchvision==0.9.1,SOFTWARE
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",torchaudio==0.8.1,SOFTWARE
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",pytorch,SOFTWARE
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",pytorch==1.8.0,SOFTWARE
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",torchvision==0.9.0,SOFTWARE
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",torchaudio==0.8.0,SOFTWARE
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",udatoolkit=11.1,SOFTWARE
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",pytorch,SOFTWARE
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",conda,SOFTWARE
"```bash # Download this project git clone https://github.com/JialeCao001/PSTR.git  # Create a new conda enviroment for PSTR conda create -n pstr python=3.7 -y conda activate pstr pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html #conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge  # Comiple mmcv, which has been included in this project cd PSTR/mmcv MMCV_WITH_OPS=1 pip install -e",pip,SOFTWARE
# Comiple this project  cd PSTR pip install -r requirements/build.txt pip install -v -e,pip,SOFTWARE
# Comiple this project  cd PSTR pip install -r requirements/build.txt pip install -v -e,pip,SOFTWARE
"# or ""python setup.py develop"" pip install sklearn ```  - If you have the problem ```local variable 'beta1' referenced before assignment``` with PyTorch1.8, add one table space in L110 of [```optim/adamw.py```](https://github.com/pytorch/pytorch/issues/55740)  ## Train and Inference   #####  Datasets and Annotations  - Download [PRW](https://github.com/liangzheng06/PRW-baseline) and [CUHK-SYSU](https://github.com/ShuangLI59/person_search) datasets. - Download the [json annotations](https://drive.google.com/file/d/1J2YAU7n954TiSwqopJCWdK25IaF6Mb9_/view?",sklearn,SOFTWARE
"# or ""python setup.py develop"" pip install sklearn ```  - If you have the problem ```local variable 'beta1' referenced before assignment``` with PyTorch1.8, add one table space in L110 of [```optim/adamw.py```](https://github.com/pytorch/pytorch/issues/55740)  ## Train and Inference   #####  Datasets and Annotations  - Download [PRW](https://github.com/liangzheng06/PRW-baseline) and [CUHK-SYSU](https://github.com/ShuangLI59/person_search) datasets. - Download the [json annotations](https://drive.google.com/file/d/1J2YAU7n954TiSwqopJCWdK25IaF6Mb9_/view?",PyTorch1.8,SOFTWARE
"# or ""python setup.py develop"" pip install sklearn ```  - If you have the problem ```local variable 'beta1' referenced before assignment``` with PyTorch1.8, add one table space in L110 of [```optim/adamw.py```](https://github.com/pytorch/pytorch/issues/55740)  ## Train and Inference   #####  Datasets and Annotations  - Download [PRW](https://github.com/liangzheng06/PRW-baseline) and [CUHK-SYSU](https://github.com/ShuangLI59/person_search) datasets. - Download the [json annotations](https://drive.google.com/file/d/1J2YAU7n954TiSwqopJCWdK25IaF6Mb9_/view?",pytorch,SOFTWARE
"# or ""python setup.py develop"" pip install sklearn ```  - If you have the problem ```local variable 'beta1' referenced before assignment``` with PyTorch1.8, add one table space in L110 of [```optim/adamw.py```](https://github.com/pytorch/pytorch/issues/55740)  ## Train and Inference   #####  Datasets and Annotations  - Download [PRW](https://github.com/liangzheng06/PRW-baseline) and [CUHK-SYSU](https://github.com/ShuangLI59/person_search) datasets. - Download the [json annotations](https://drive.google.com/file/d/1J2YAU7n954TiSwqopJCWdK25IaF6Mb9_/view?",pytorch,SOFTWARE
|    name  | dataset  | backbone |  mAP  | top-1 |  mAP+ | top-1+  | download| | :-------------: | :-----: | :-----: | :-------------------: | :-----: | :-----: | :------: | :-----------------: | |     PSTR | PRW    | PVTv2-B2  |   57.46  |   90.57   |58.07   |    92.03     |          [model](https://drive.google.com/file/d/1hrmyvS9f8fzflpoIlEhWQ-XDyNp_qCGq/view?,PSTR,SOFTWARE
|    name  | dataset  | backbone |  mAP  | top-1 |  mAP+ | top-1+  | download| | :-------------: | :-----: | :-----: | :-------------------: | :-----: | :-----: | :------: | :-----------------: | |     PSTR | PRW    | PVTv2-B2  |   57.46  |   90.57   |58.07   |    92.03     |          [model](https://drive.google.com/file/d/1hrmyvS9f8fzflpoIlEhWQ-XDyNp_qCGq/view?,PVTv2-B2,SOFTWARE
usp=sharing)         | |     PSTR |  PRW   | ResNet50  |   50.03   | 88.04   | 50.64   |    89.94   |        [model](https://drive.google.com/file/d/12j71smXyc3QAyCvIPRlQCbyhSXZENBIX/view?,PSTR,SOFTWARE
usp=sharing)         | |     PSTR |  PRW   | ResNet50  |   50.03   | 88.04   | 50.64   |    89.94   |        [model](https://drive.google.com/file/d/12j71smXyc3QAyCvIPRlQCbyhSXZENBIX/view?,ResNet50,SOFTWARE
usp=sharing)         | |     PSTR |  PRW   | ResNet50-DCN  |   51.09   | 88.33   | 51.62   |    90.13   |        [model](https://drive.google.com/file/d/111f_efZOYMFkz9i76TgqcO7a88npoJV5/view?,PSTR,SOFTWARE
usp=sharing)         | |     PSTR |  PRW   | ResNet50-DCN  |   51.09   | 88.33   | 51.62   |    90.13   |        [model](https://drive.google.com/file/d/111f_efZOYMFkz9i76TgqcO7a88npoJV5/view?,ResNet50-DCN,SOFTWARE
usp=sharing)         | |     PSTR | CUHK-SYSU     | PVTv2-B2    |   95.31  |   96.28   |95.78   |    96.83      |       [model](https://drive.google.com/file/d/1vrQdZTVgJ2D6ty_XJAYmsJgziW9TZHHW/view?,PSTR,SOFTWARE
usp=sharing)         | |     PSTR | CUHK-SYSU     | PVTv2-B2    |   95.31  |   96.28   |95.78   |    96.83      |       [model](https://drive.google.com/file/d/1vrQdZTVgJ2D6ty_XJAYmsJgziW9TZHHW/view?,PVTv2-B2,SOFTWARE
usp=sharing)         | |     PSTR | CUHK-SYSU    | ResNet50|   93.55   | 94.93   | 94.16   | 95.48   |          [model](https://drive.google.com/file/d/1U4r_WaTfODmuhslL_15u5bXdFwLLBC5m/view?,PSTR,SOFTWARE
usp=sharing)         | |     PSTR | CUHK-SYSU    | ResNet50|   93.55   | 94.93   | 94.16   | 95.48   |          [model](https://drive.google.com/file/d/1U4r_WaTfODmuhslL_15u5bXdFwLLBC5m/view?,ResNet50,SOFTWARE
usp=sharing)         | |     PSTR | CUHK-SYSU    | ResNet50-DCN|   94.22   | 95.28   | 94.90   | 95.97   |          [model](https://drive.google.com/file/d/1cCbpAGrldxQaRrF7FCZXqx4VaNP-C278/view?,PSTR,SOFTWARE
usp=sharing)         | |     PSTR | CUHK-SYSU    | ResNet50-DCN|   94.22   | 95.28   | 94.90   | 95.97   |          [model](https://drive.google.com/file/d/1cCbpAGrldxQaRrF7FCZXqx4VaNP-C278/view?,ResNet50-DCN,SOFTWARE
All the software requirements are already pre-installed in the Docker image below.,Docker,SOFTWARE
"Note that `DGL 0.8` is not released yet when I did this work, so I installed `DGL 0.8` manually from the source code.",DGL 0.8,SOFTWARE
"Note that `DGL 0.8` is not released yet when I did this work, so I installed `DGL 0.8` manually from the source code.",DGL 0.8,SOFTWARE
PyTorch version should be equal to or greater than 1.11.0.,PyTorch,SOFTWARE
```bash    docker pull lizytalk/dejavu    ``` 2.,docker,SOFTWARE
```bash    docker pull lizytalk/dejavu    ``` 2.,lizytalk/dejavu,SOFTWARE
Pull the code from GitHub    ```bash    git pull https://github.com/NetManAIOps/DejaVu.git DejaVu    ``` 3.,git,SOFTWARE
"I use the command `realpath` in the example commands below, which is not bundled in macOS and Windows.",realpath,SOFTWARE
"On macOS, you can install it by `brew install coreutils`. 5.",brew,SOFTWARE
"On macOS, you can install it by `brew install coreutils`. 5.",coreutils,SOFTWARE
Start a Docker container with our image and enter its shell    ```bash    docker run -it --rm -v $(realpath DejaVu):/workspace lizytalk/dejavu bash    ``` 6.,Docker,SOFTWARE
Start a Docker container with our image and enter its shell    ```bash    docker run -it --rm -v $(realpath DejaVu):/workspace lizytalk/dejavu bash    ``` 6.,docker,SOFTWARE
Start a Docker container with our image and enter its shell    ```bash    docker run -it --rm -v $(realpath DejaVu):/workspace lizytalk/dejavu bash    ``` 6.,lizytalk/dejavu,SOFTWARE
Start a Docker container with our image and enter its shell    ```bash    docker run -it --rm -v $(realpath DejaVu):/workspace lizytalk/dejavu bash    ``` 6.,bash,SOFTWARE
Run `direnv allow` in the shell of the Docker container to set the environment variables. 7.,direnv allow,SOFTWARE
Run `direnv allow` in the shell of the Docker container to set the environment variables. 7.,Docker,SOFTWARE
Note that the pickle files are not compatible in different Python and Pandas versions.,Python,SOFTWARE
Note that the pickle files are not compatible in different Python and Pandas versions.,Pandas,SOFTWARE
"# XTR: Rethinking the Role of Token Retrieval in Multi-Vector Retrieval  In this repository, we provide how you can run XTR (conteXtualized Token Retriever) for document retrieval.",XTR,SOFTWARE
"# XTR: Rethinking the Role of Token Retrieval in Multi-Vector Retrieval  In this repository, we provide how you can run XTR (conteXtualized Token Retriever) for document retrieval.",XTR,SOFTWARE
"# XTR: Rethinking the Role of Token Retrieval in Multi-Vector Retrieval  In this repository, we provide how you can run XTR (conteXtualized Token Retriever) for document retrieval.",conteXtualized Token Retriever,SOFTWARE
"Please refer to our NeurIPS 2023 paper ([Lee et al., 2023](https://arxiv.org/abs/2304.01982)) for technical details.  ## Usage  XTR is available through [Kaggle Models](https://www.kaggle.com/models/deepmind/xtr/).",XTR,SOFTWARE
"Please refer to our NeurIPS 2023 paper ([Lee et al., 2023](https://arxiv.org/abs/2304.01982)) for technical details.  ## Usage  XTR is available through [Kaggle Models](https://www.kaggle.com/models/deepmind/xtr/).",Kaggle Models,SOFTWARE
"Please refer to our NeurIPS 2023 paper ([Lee et al., 2023](https://arxiv.org/abs/2304.01982)) for technical details.  ## Usage  XTR is available through [Kaggle Models](https://www.kaggle.com/models/deepmind/xtr/).",kaggle,SOFTWARE
"Please refer to our NeurIPS 2023 paper ([Lee et al., 2023](https://arxiv.org/abs/2304.01982)) for technical details.  ## Usage  XTR is available through [Kaggle Models](https://www.kaggle.com/models/deepmind/xtr/).",deepmind/xtr,SOFTWARE
"For instance, you can load XTR checkpoints as follows:  ```python ## Model Usage import tensorflow_hub as hub import tensorflow as tf import tensorflow_text as text  # Registers the ops.",XTR,SOFTWARE
"For instance, you can load XTR checkpoints as follows:  ```python ## Model Usage import tensorflow_hub as hub import tensorflow as tf import tensorflow_text as text  # Registers the ops.",tensorflow,SOFTWARE
"For instance, you can load XTR checkpoints as follows:  ```python ## Model Usage import tensorflow_hub as hub import tensorflow as tf import tensorflow_text as text  # Registers the ops.",tensorflow,SOFTWARE
"For instance, you can load XTR checkpoints as follows:  ```python ## Model Usage import tensorflow_hub as hub import tensorflow as tf import tensorflow_text as text  # Registers the ops.",tensorflow,SOFTWARE
"hub_url = ""/kaggle/input/xtr/tensorflow2/base-en/2/"" # if using Kaggle Notebooks, otherwise: hub_url = ""https://www.kaggle.com/models/deepmind/xtr/frameworks/tensorFlow2/variations/base-en/versions/2"" encoder = hub.KerasLayer(hub_url, signature=""serving_default"", signature_outputs_as_dict=True)  # Sample texts to encode. sample_texts = tf.constant([""dog"", ""Puppies are nice."", ""I enjoy taking long walks along the beach with my dog.""]) sample_embeds = encoder(sample_texts)  # This returns token-level representations from XTR. encodings = sample_embeds[""encodings""].numpy() mask = sample_embeds[""mask""].numpy() print(f""encodings: {encodings.shape}, mask: {mask.shape}"") ```  [!",xtr,SOFTWARE
"hub_url = ""/kaggle/input/xtr/tensorflow2/base-en/2/"" # if using Kaggle Notebooks, otherwise: hub_url = ""https://www.kaggle.com/models/deepmind/xtr/frameworks/tensorFlow2/variations/base-en/versions/2"" encoder = hub.KerasLayer(hub_url, signature=""serving_default"", signature_outputs_as_dict=True)  # Sample texts to encode. sample_texts = tf.constant([""dog"", ""Puppies are nice."", ""I enjoy taking long walks along the beach with my dog.""]) sample_embeds = encoder(sample_texts)  # This returns token-level representations from XTR. encodings = sample_embeds[""encodings""].numpy() mask = sample_embeds[""mask""].numpy() print(f""encodings: {encodings.shape}, mask: {mask.shape}"") ```  [!",tensorflow2,SOFTWARE
"hub_url = ""/kaggle/input/xtr/tensorflow2/base-en/2/"" # if using Kaggle Notebooks, otherwise: hub_url = ""https://www.kaggle.com/models/deepmind/xtr/frameworks/tensorFlow2/variations/base-en/versions/2"" encoder = hub.KerasLayer(hub_url, signature=""serving_default"", signature_outputs_as_dict=True)  # Sample texts to encode. sample_texts = tf.constant([""dog"", ""Puppies are nice."", ""I enjoy taking long walks along the beach with my dog.""]) sample_embeds = encoder(sample_texts)  # This returns token-level representations from XTR. encodings = sample_embeds[""encodings""].numpy() mask = sample_embeds[""mask""].numpy() print(f""encodings: {encodings.shape}, mask: {mask.shape}"") ```  [!",Kaggle Notebooks,SOFTWARE
"hub_url = ""/kaggle/input/xtr/tensorflow2/base-en/2/"" # if using Kaggle Notebooks, otherwise: hub_url = ""https://www.kaggle.com/models/deepmind/xtr/frameworks/tensorFlow2/variations/base-en/versions/2"" encoder = hub.KerasLayer(hub_url, signature=""serving_default"", signature_outputs_as_dict=True)  # Sample texts to encode. sample_texts = tf.constant([""dog"", ""Puppies are nice."", ""I enjoy taking long walks along the beach with my dog.""]) sample_embeds = encoder(sample_texts)  # This returns token-level representations from XTR. encodings = sample_embeds[""encodings""].numpy() mask = sample_embeds[""mask""].numpy() print(f""encodings: {encodings.shape}, mask: {mask.shape}"") ```  [!",kaggle,SOFTWARE
"hub_url = ""/kaggle/input/xtr/tensorflow2/base-en/2/"" # if using Kaggle Notebooks, otherwise: hub_url = ""https://www.kaggle.com/models/deepmind/xtr/frameworks/tensorFlow2/variations/base-en/versions/2"" encoder = hub.KerasLayer(hub_url, signature=""serving_default"", signature_outputs_as_dict=True)  # Sample texts to encode. sample_texts = tf.constant([""dog"", ""Puppies are nice."", ""I enjoy taking long walks along the beach with my dog.""]) sample_embeds = encoder(sample_texts)  # This returns token-level representations from XTR. encodings = sample_embeds[""encodings""].numpy() mask = sample_embeds[""mask""].numpy() print(f""encodings: {encodings.shape}, mask: {mask.shape}"") ```  [!",xtr,SOFTWARE
"hub_url = ""/kaggle/input/xtr/tensorflow2/base-en/2/"" # if using Kaggle Notebooks, otherwise: hub_url = ""https://www.kaggle.com/models/deepmind/xtr/frameworks/tensorFlow2/variations/base-en/versions/2"" encoder = hub.KerasLayer(hub_url, signature=""serving_default"", signature_outputs_as_dict=True)  # Sample texts to encode. sample_texts = tf.constant([""dog"", ""Puppies are nice."", ""I enjoy taking long walks along the beach with my dog.""]) sample_embeds = encoder(sample_texts)  # This returns token-level representations from XTR. encodings = sample_embeds[""encodings""].numpy() mask = sample_embeds[""mask""].numpy() print(f""encodings: {encodings.shape}, mask: {mask.shape}"") ```  [!",tensorFlow2,SOFTWARE
"hub_url = ""/kaggle/input/xtr/tensorflow2/base-en/2/"" # if using Kaggle Notebooks, otherwise: hub_url = ""https://www.kaggle.com/models/deepmind/xtr/frameworks/tensorFlow2/variations/base-en/versions/2"" encoder = hub.KerasLayer(hub_url, signature=""serving_default"", signature_outputs_as_dict=True)  # Sample texts to encode. sample_texts = tf.constant([""dog"", ""Puppies are nice."", ""I enjoy taking long walks along the beach with my dog.""]) sample_embeds = encoder(sample_texts)  # This returns token-level representations from XTR. encodings = sample_embeds[""encodings""].numpy() mask = sample_embeds[""mask""].numpy() print(f""encodings: {encodings.shape}, mask: {mask.shape}"") ```  [!",Keras,SOFTWARE
"hub_url = ""/kaggle/input/xtr/tensorflow2/base-en/2/"" # if using Kaggle Notebooks, otherwise: hub_url = ""https://www.kaggle.com/models/deepmind/xtr/frameworks/tensorFlow2/variations/base-en/versions/2"" encoder = hub.KerasLayer(hub_url, signature=""serving_default"", signature_outputs_as_dict=True)  # Sample texts to encode. sample_texts = tf.constant([""dog"", ""Puppies are nice."", ""I enjoy taking long walks along the beach with my dog.""]) sample_embeds = encoder(sample_texts)  # This returns token-level representations from XTR. encodings = sample_embeds[""encodings""].numpy() mask = sample_embeds[""mask""].numpy() print(f""encodings: {encodings.shape}, mask: {mask.shape}"") ```  [!",XTR,SOFTWARE
"hub_url = ""/kaggle/input/xtr/tensorflow2/base-en/2/"" # if using Kaggle Notebooks, otherwise: hub_url = ""https://www.kaggle.com/models/deepmind/xtr/frameworks/tensorFlow2/variations/base-en/versions/2"" encoder = hub.KerasLayer(hub_url, signature=""serving_default"", signature_outputs_as_dict=True)  # Sample texts to encode. sample_texts = tf.constant([""dog"", ""Puppies are nice."", ""I enjoy taking long walks along the beach with my dog.""]) sample_embeds = encoder(sample_texts)  # This returns token-level representations from XTR. encodings = sample_embeds[""encodings""].numpy() mask = sample_embeds[""mask""].numpy() print(f""encodings: {encodings.shape}, mask: {mask.shape}"") ```  [!",numpy,SOFTWARE
"hub_url = ""/kaggle/input/xtr/tensorflow2/base-en/2/"" # if using Kaggle Notebooks, otherwise: hub_url = ""https://www.kaggle.com/models/deepmind/xtr/frameworks/tensorFlow2/variations/base-en/versions/2"" encoder = hub.KerasLayer(hub_url, signature=""serving_default"", signature_outputs_as_dict=True)  # Sample texts to encode. sample_texts = tf.constant([""dog"", ""Puppies are nice."", ""I enjoy taking long walks along the beach with my dog.""]) sample_embeds = encoder(sample_texts)  # This returns token-level representations from XTR. encodings = sample_embeds[""encodings""].numpy() mask = sample_embeds[""mask""].numpy() print(f""encodings: {encodings.shape}, mask: {mask.shape}"") ```  [!",numpy,SOFTWARE
"[Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/xtr/blob/main/xtr_evaluation_on_beir_miracl.ipynb)  Please check out our Notebook above, which contains the full inference for running document retrieval with XTR.",Colab,SOFTWARE
"[Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/xtr/blob/main/xtr_evaluation_on_beir_miracl.ipynb)  Please check out our Notebook above, which contains the full inference for running document retrieval with XTR.",colab,SOFTWARE
"[Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/xtr/blob/main/xtr_evaluation_on_beir_miracl.ipynb)  Please check out our Notebook above, which contains the full inference for running document retrieval with XTR.",colab,SOFTWARE
"[Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/xtr/blob/main/xtr_evaluation_on_beir_miracl.ipynb)  Please check out our Notebook above, which contains the full inference for running document retrieval with XTR.",colab,SOFTWARE
"[Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/xtr/blob/main/xtr_evaluation_on_beir_miracl.ipynb)  Please check out our Notebook above, which contains the full inference for running document retrieval with XTR.",xtr,SOFTWARE
"[Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/xtr/blob/main/xtr_evaluation_on_beir_miracl.ipynb)  Please check out our Notebook above, which contains the full inference for running document retrieval with XTR.",xtr,SOFTWARE
"[Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/xtr/blob/main/xtr_evaluation_on_beir_miracl.ipynb)  Please check out our Notebook above, which contains the full inference for running document retrieval with XTR.",XTR,SOFTWARE
"XTR is also available in [Huggingface](https://huggingface.co/google/xtr-base-en) thanks to [Mujeen Sung](https://github.com/mjeensung).  ## Citing this work  ```bibtex @article{lee2024rethinking,   title={Rethinking the role of token retrieval in multi-vector retrieval},   author={Lee, Jinhyuk and Dai, Zhuyun and Duddu, Sai Meher Karthik and Lei, Tao and Naim, Iftekhar and Chang, Ming-Wei and Zhao, Vincent},   journal={Advances in Neural Information Processing Systems},   volume={36},   year={2024} } ```  ## License and disclaimer  Copyright 2024 DeepMind Technologies Limited  All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license.",XTR,SOFTWARE
"XTR is also available in [Huggingface](https://huggingface.co/google/xtr-base-en) thanks to [Mujeen Sung](https://github.com/mjeensung).  ## Citing this work  ```bibtex @article{lee2024rethinking,   title={Rethinking the role of token retrieval in multi-vector retrieval},   author={Lee, Jinhyuk and Dai, Zhuyun and Duddu, Sai Meher Karthik and Lei, Tao and Naim, Iftekhar and Chang, Ming-Wei and Zhao, Vincent},   journal={Advances in Neural Information Processing Systems},   volume={36},   year={2024} } ```  ## License and disclaimer  Copyright 2024 DeepMind Technologies Limited  All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license.",Huggingface,SOFTWARE
"XTR is also available in [Huggingface](https://huggingface.co/google/xtr-base-en) thanks to [Mujeen Sung](https://github.com/mjeensung).  ## Citing this work  ```bibtex @article{lee2024rethinking,   title={Rethinking the role of token retrieval in multi-vector retrieval},   author={Lee, Jinhyuk and Dai, Zhuyun and Duddu, Sai Meher Karthik and Lei, Tao and Naim, Iftekhar and Chang, Ming-Wei and Zhao, Vincent},   journal={Advances in Neural Information Processing Systems},   volume={36},   year={2024} } ```  ## License and disclaimer  Copyright 2024 DeepMind Technologies Limited  All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license.",huggingface,SOFTWARE
"XTR is also available in [Huggingface](https://huggingface.co/google/xtr-base-en) thanks to [Mujeen Sung](https://github.com/mjeensung).  ## Citing this work  ```bibtex @article{lee2024rethinking,   title={Rethinking the role of token retrieval in multi-vector retrieval},   author={Lee, Jinhyuk and Dai, Zhuyun and Duddu, Sai Meher Karthik and Lei, Tao and Naim, Iftekhar and Chang, Ming-Wei and Zhao, Vincent},   journal={Advances in Neural Information Processing Systems},   volume={36},   year={2024} } ```  ## License and disclaimer  Copyright 2024 DeepMind Technologies Limited  All software is licensed under the Apache License, Version 2.0 (Apache 2.0); you may not use this file except in compliance with the Apache 2.0 license.",xtr,SOFTWARE
<br> Temperature Network Meets Large Foundation Models via DRO </h1>  The temperature parameter plays a profound role  during training and/or inference with large foundation models (LFMs) such as large language models (LLMs) and CLIP models.,large foundation models,SOFTWARE
<br> Temperature Network Meets Large Foundation Models via DRO </h1>  The temperature parameter plays a profound role  during training and/or inference with large foundation models (LFMs) such as large language models (LLMs) and CLIP models.,large language models,SOFTWARE
<br> Temperature Network Meets Large Foundation Models via DRO </h1>  The temperature parameter plays a profound role  during training and/or inference with large foundation models (LFMs) such as large language models (LLMs) and CLIP models.,CLIP,SOFTWARE
"Particularly, it adjusts the logits in the softmax function in LLMs, which is crucial for next token generation, and it scales the similarities in the contrastive loss for training CLIP models.",CLIP,SOFTWARE
"Our experiments on LLMs and CLIP models demonstrate that TempNet greatly improves the performance of existing solutions or models.  ### Table of Contents    - [Introduction](#introduction) - [Training](#training) - [Inference](#inference) - [Acknowledgment](#acknowledgment) - [Citation](#citation)  ## Introduction  ### Our Proposed Method  We introduce **a principled framework** for developing a small yet generalizable network for temperature prediction, TempNet, aimed at enhancing large foundation models (LFMs) such as large language models (LLMs) and CLIP models.",LLMs,SOFTWARE
"Our experiments on LLMs and CLIP models demonstrate that TempNet greatly improves the performance of existing solutions or models.  ### Table of Contents    - [Introduction](#introduction) - [Training](#training) - [Inference](#inference) - [Acknowledgment](#acknowledgment) - [Citation](#citation)  ## Introduction  ### Our Proposed Method  We introduce **a principled framework** for developing a small yet generalizable network for temperature prediction, TempNet, aimed at enhancing large foundation models (LFMs) such as large language models (LLMs) and CLIP models.",CLIP,SOFTWARE
"Our experiments on LLMs and CLIP models demonstrate that TempNet greatly improves the performance of existing solutions or models.  ### Table of Contents    - [Introduction](#introduction) - [Training](#training) - [Inference](#inference) - [Acknowledgment](#acknowledgment) - [Citation](#citation)  ## Introduction  ### Our Proposed Method  We introduce **a principled framework** for developing a small yet generalizable network for temperature prediction, TempNet, aimed at enhancing large foundation models (LFMs) such as large language models (LLMs) and CLIP models.",TempNet,SOFTWARE
"Our experiments on LLMs and CLIP models demonstrate that TempNet greatly improves the performance of existing solutions or models.  ### Table of Contents    - [Introduction](#introduction) - [Training](#training) - [Inference](#inference) - [Acknowledgment](#acknowledgment) - [Citation](#citation)  ## Introduction  ### Our Proposed Method  We introduce **a principled framework** for developing a small yet generalizable network for temperature prediction, TempNet, aimed at enhancing large foundation models (LFMs) such as large language models (LLMs) and CLIP models.",large foundation models,SOFTWARE
"Our experiments on LLMs and CLIP models demonstrate that TempNet greatly improves the performance of existing solutions or models.  ### Table of Contents    - [Introduction](#introduction) - [Training](#training) - [Inference](#inference) - [Acknowledgment](#acknowledgment) - [Citation](#citation)  ## Introduction  ### Our Proposed Method  We introduce **a principled framework** for developing a small yet generalizable network for temperature prediction, TempNet, aimed at enhancing large foundation models (LFMs) such as large language models (LLMs) and CLIP models.",large language models,SOFTWARE
"Our experiments on LLMs and CLIP models demonstrate that TempNet greatly improves the performance of existing solutions or models.  ### Table of Contents    - [Introduction](#introduction) - [Training](#training) - [Inference](#inference) - [Acknowledgment](#acknowledgment) - [Citation](#citation)  ## Introduction  ### Our Proposed Method  We introduce **a principled framework** for developing a small yet generalizable network for temperature prediction, TempNet, aimed at enhancing large foundation models (LFMs) such as large language models (LLMs) and CLIP models.",CLIP,SOFTWARE
The Temperature Network is a plug-and-play architecture that can be implemented atop LFMs.,LFMs,SOFTWARE
"Our solution is composed of a novel learning framework with robust losses underpinned by constrained distributionally robust optimization (DRO), and a properly designed TempNet with theoretical inspiration.",TempNet,SOFTWARE
TempNet can be trained together with a large foundation model from scratch or learned separately given a pretrained foundation model.,TempNet,SOFTWARE
"<div align=""center"" style=""display: flex; justify-content: center; align-items: center;"">   <img src=""images/tempnet_overall.jpg"" style=""width: 70%;""/> </div>  In the figure above, we present the framework of training LFMs with TempNet on the left and the structure of TempNet on the right.   ### Experimental Results  Results of training LLMs in various settings, including training from scratch, finetuning a pretrained LLM model, and learning TempNet only with a frozen LLM model.",TempNet,SOFTWARE
"<div align=""center"" style=""display: flex; justify-content: center; align-items: center;"">   <img src=""images/tempnet_overall.jpg"" style=""width: 70%;""/> </div>  In the figure above, we present the framework of training LFMs with TempNet on the left and the structure of TempNet on the right.   ### Experimental Results  Results of training LLMs in various settings, including training from scratch, finetuning a pretrained LLM model, and learning TempNet only with a frozen LLM model.",TempNet,SOFTWARE
"<div align=""center"" style=""display: flex; justify-content: center; align-items: center;"">   <img src=""images/tempnet_overall.jpg"" style=""width: 70%;""/> </div>  In the figure above, we present the framework of training LFMs with TempNet on the left and the structure of TempNet on the right.   ### Experimental Results  Results of training LLMs in various settings, including training from scratch, finetuning a pretrained LLM model, and learning TempNet only with a frozen LLM model.",TempNet,SOFTWARE
"<div align=""center"">   <img src=""images/exp3.jpg"" width=""80%""/> </div>  To test TempNet's performance in instruction-following tasks, we fix the LLaMA2 Chat models and trained TempNet, then test them on the [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/) benchmark.",TempNet's,SOFTWARE
"<div align=""center"">   <img src=""images/exp3.jpg"" width=""80%""/> </div>  To test TempNet's performance in instruction-following tasks, we fix the LLaMA2 Chat models and trained TempNet, then test them on the [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/) benchmark.",LLaMA2,SOFTWARE
"<div align=""center"">   <img src=""images/exp3.jpg"" width=""80%""/> </div>  To test TempNet's performance in instruction-following tasks, we fix the LLaMA2 Chat models and trained TempNet, then test them on the [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/) benchmark.",TempNet,SOFTWARE
"<div align=""center"">   <img src=""images/exp3.jpg"" width=""80%""/> </div>  To test TempNet's performance in instruction-following tasks, we fix the LLaMA2 Chat models and trained TempNet, then test them on the [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/) benchmark.",AlpacaEval,SOFTWARE
"<div align=""center"">   <img src=""images/exp3.jpg"" width=""80%""/> </div>  To test TempNet's performance in instruction-following tasks, we fix the LLaMA2 Chat models and trained TempNet, then test them on the [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/) benchmark.",alpaca_eval,SOFTWARE
"We present the results on three different model sizes in the table below, including the training times of TempNet on Nvidia A100-80G GPUs and their win rates on AlpacaEval data.",TempNet,SOFTWARE
The results demonstrate that our TempNet can converge quickly and achieve consistent improvements.,TempNet,SOFTWARE
"<div align=""center"">   <img src=""images/exp4.jpg"" width=""40%""/> </div>  Here, we reveal why TempNet enhances performance by comparing the performances of LLaMA2 7B Chat (with the default $\tau=0.7$) and LLaMA2 7B Chat + TempNet on the AlpacaEval dataset.",LLaMA2 7B Chat,SOFTWARE
"<div align=""center"">   <img src=""images/exp4.jpg"" width=""40%""/> </div>  Here, we reveal why TempNet enhances performance by comparing the performances of LLaMA2 7B Chat (with the default $\tau=0.7$) and LLaMA2 7B Chat + TempNet on the AlpacaEval dataset.",LLaMA2 7B Chat + TempNet,SOFTWARE
"We select a representative example for which the AlpacaEval annotator, GPT-4, deems the response from LLaMA2 + TempNet to be not only superior to that of LLaMA but also better than the baseline response generated by GPT-4.",AlpacaEval,SOFTWARE
"We select a representative example for which the AlpacaEval annotator, GPT-4, deems the response from LLaMA2 + TempNet to be not only superior to that of LLaMA but also better than the baseline response generated by GPT-4.",GPT-4,SOFTWARE
"We select a representative example for which the AlpacaEval annotator, GPT-4, deems the response from LLaMA2 + TempNet to be not only superior to that of LLaMA but also better than the baseline response generated by GPT-4.",LLaMA2 + TempNet,SOFTWARE
"We select a representative example for which the AlpacaEval annotator, GPT-4, deems the response from LLaMA2 + TempNet to be not only superior to that of LLaMA but also better than the baseline response generated by GPT-4.",LLaMA,SOFTWARE
"We select a representative example for which the AlpacaEval annotator, GPT-4, deems the response from LLaMA2 + TempNet to be not only superior to that of LLaMA but also better than the baseline response generated by GPT-4.",GPT-4,SOFTWARE
"When the temperature value is lower, it can be observed that the LLaMA2 7B Chat model's output is relatively fixed and lacks creativity.",LLaMA2 7B Chat,SOFTWARE
"With TempNet, in the process of generating names for this task, LLaMA2 7B Chat produces a higher averaged temperature value of 0.82, ultimately creating a novel name **Tunanadoes**.",TempNet,SOFTWARE
"With TempNet, in the process of generating names for this task, LLaMA2 7B Chat produces a higher averaged temperature value of 0.82, ultimately creating a novel name **Tunanadoes**.",LLaMA2 7B Chat,SOFTWARE
"<div align=""center"">   <img src=""images/pred_tau.jpg"" width=""100%""/> </div>  ### More Details For more details, please refer to our [paper](http://arxiv.org/abs/2404.04575)    ## Training  We conduct experiments across various tasks and models to validate the effectiveness of TempNet.",TempNet,SOFTWARE
"Given the different training frameworks required by each model, we distribute the training code for different models across four directories: `GPT_2`, `LLaMA-1`, `LLaMA-2`, and `Bimodal-CL`.  ## Inference  We upload the base models for LLaMA 2 Chat 7B, 13B, 70B, and their respective TempNets to [Hugging Face](https://huggingface.co/LLM-Opt).",GPT_2,SOFTWARE
"Given the different training frameworks required by each model, we distribute the training code for different models across four directories: `GPT_2`, `LLaMA-1`, `LLaMA-2`, and `Bimodal-CL`.  ## Inference  We upload the base models for LLaMA 2 Chat 7B, 13B, 70B, and their respective TempNets to [Hugging Face](https://huggingface.co/LLM-Opt).",LLaMA-1,SOFTWARE
"Given the different training frameworks required by each model, we distribute the training code for different models across four directories: `GPT_2`, `LLaMA-1`, `LLaMA-2`, and `Bimodal-CL`.  ## Inference  We upload the base models for LLaMA 2 Chat 7B, 13B, 70B, and their respective TempNets to [Hugging Face](https://huggingface.co/LLM-Opt).",LLaMA-2,SOFTWARE
"Given the different training frameworks required by each model, we distribute the training code for different models across four directories: `GPT_2`, `LLaMA-1`, `LLaMA-2`, and `Bimodal-CL`.  ## Inference  We upload the base models for LLaMA 2 Chat 7B, 13B, 70B, and their respective TempNets to [Hugging Face](https://huggingface.co/LLM-Opt).",Bimodal-CL,SOFTWARE
"Given the different training frameworks required by each model, we distribute the training code for different models across four directories: `GPT_2`, `LLaMA-1`, `LLaMA-2`, and `Bimodal-CL`.  ## Inference  We upload the base models for LLaMA 2 Chat 7B, 13B, 70B, and their respective TempNets to [Hugging Face](https://huggingface.co/LLM-Opt).",LLaMA 2 Chat 7B,SOFTWARE
"Given the different training frameworks required by each model, we distribute the training code for different models across four directories: `GPT_2`, `LLaMA-1`, `LLaMA-2`, and `Bimodal-CL`.  ## Inference  We upload the base models for LLaMA 2 Chat 7B, 13B, 70B, and their respective TempNets to [Hugging Face](https://huggingface.co/LLM-Opt).",TempNets,SOFTWARE
"The `tempnet.py` in the [repository](https://github.com/zhqiu/TempNet) contains the definition of the TempNet class and a class that inherits from Hugging Face's LLaMA, including TempNet.",tempnet.py,SOFTWARE
"The `tempnet.py` in the [repository](https://github.com/zhqiu/TempNet) contains the definition of the TempNet class and a class that inherits from Hugging Face's LLaMA, including TempNet.",TempNet,SOFTWARE
"The `tempnet.py` in the [repository](https://github.com/zhqiu/TempNet) contains the definition of the TempNet class and a class that inherits from Hugging Face's LLaMA, including TempNet.",LLaMA,SOFTWARE
"The `tempnet.py` in the [repository](https://github.com/zhqiu/TempNet) contains the definition of the TempNet class and a class that inherits from Hugging Face's LLaMA, including TempNet.",TempNet,SOFTWARE
People can download this file and use the following code to perform inference with LLaMA that incorporates TempNet.,LLaMA,SOFTWARE
People can download this file and use the following code to perform inference with LLaMA that incorporates TempNet.,TempNet,SOFTWARE
"```python import torch from tempnet import LLaMA_TempNet from transformers import AutoTokenizer, GenerationConfig  model_name = 'LLM-Opt/TempNet-LLaMA2-Chat-7B-v0.1'  tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=False) generation_config = GenerationConfig.from_pretrained(model_name) model = LLaMA_TempNet.from_pretrained(model_name, device_map=""auto"", torch_dtype=torch.float16)  inputs = 'How do you get water in the desert?'",torch,SOFTWARE
"```python import torch from tempnet import LLaMA_TempNet from transformers import AutoTokenizer, GenerationConfig  model_name = 'LLM-Opt/TempNet-LLaMA2-Chat-7B-v0.1'  tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=False) generation_config = GenerationConfig.from_pretrained(model_name) model = LLaMA_TempNet.from_pretrained(model_name, device_map=""auto"", torch_dtype=torch.float16)  inputs = 'How do you get water in the desert?'",tempnet,SOFTWARE
"```python import torch from tempnet import LLaMA_TempNet from transformers import AutoTokenizer, GenerationConfig  model_name = 'LLM-Opt/TempNet-LLaMA2-Chat-7B-v0.1'  tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=False) generation_config = GenerationConfig.from_pretrained(model_name) model = LLaMA_TempNet.from_pretrained(model_name, device_map=""auto"", torch_dtype=torch.float16)  inputs = 'How do you get water in the desert?'",transformers,SOFTWARE
"input_ids = tokenizer(inputs, return_tensors=""pt"").input_ids.cuda()  outputs = model.generate(input_ids, generation_config=generation_config) response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(inputs)-1:].strip() ```  ## Acknowledgment  This repository benefits from [ALBEF](https://github.com/salesforce/ALBEF), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), and [DeepSpeed](https://github.com/microsoft/DeepSpeed).",ALBEF,SOFTWARE
"input_ids = tokenizer(inputs, return_tensors=""pt"").input_ids.cuda()  outputs = model.generate(input_ids, generation_config=generation_config) response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(inputs)-1:].strip() ```  ## Acknowledgment  This repository benefits from [ALBEF](https://github.com/salesforce/ALBEF), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), and [DeepSpeed](https://github.com/microsoft/DeepSpeed).",ALBEF,SOFTWARE
"input_ids = tokenizer(inputs, return_tensors=""pt"").input_ids.cuda()  outputs = model.generate(input_ids, generation_config=generation_config) response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(inputs)-1:].strip() ```  ## Acknowledgment  This repository benefits from [ALBEF](https://github.com/salesforce/ALBEF), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), and [DeepSpeed](https://github.com/microsoft/DeepSpeed).",GPT-NeoX,SOFTWARE
"input_ids = tokenizer(inputs, return_tensors=""pt"").input_ids.cuda()  outputs = model.generate(input_ids, generation_config=generation_config) response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(inputs)-1:].strip() ```  ## Acknowledgment  This repository benefits from [ALBEF](https://github.com/salesforce/ALBEF), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), and [DeepSpeed](https://github.com/microsoft/DeepSpeed).",gpt-neox,SOFTWARE
"input_ids = tokenizer(inputs, return_tensors=""pt"").input_ids.cuda()  outputs = model.generate(input_ids, generation_config=generation_config) response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(inputs)-1:].strip() ```  ## Acknowledgment  This repository benefits from [ALBEF](https://github.com/salesforce/ALBEF), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), and [DeepSpeed](https://github.com/microsoft/DeepSpeed).",LLaMA,SOFTWARE
"input_ids = tokenizer(inputs, return_tensors=""pt"").input_ids.cuda()  outputs = model.generate(input_ids, generation_config=generation_config) response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(inputs)-1:].strip() ```  ## Acknowledgment  This repository benefits from [ALBEF](https://github.com/salesforce/ALBEF), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), and [DeepSpeed](https://github.com/microsoft/DeepSpeed).",Megatron-LM,SOFTWARE
"input_ids = tokenizer(inputs, return_tensors=""pt"").input_ids.cuda()  outputs = model.generate(input_ids, generation_config=generation_config) response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(inputs)-1:].strip() ```  ## Acknowledgment  This repository benefits from [ALBEF](https://github.com/salesforce/ALBEF), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), and [DeepSpeed](https://github.com/microsoft/DeepSpeed).",Megatron-LM,SOFTWARE
"input_ids = tokenizer(inputs, return_tensors=""pt"").input_ids.cuda()  outputs = model.generate(input_ids, generation_config=generation_config) response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(inputs)-1:].strip() ```  ## Acknowledgment  This repository benefits from [ALBEF](https://github.com/salesforce/ALBEF), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), and [DeepSpeed](https://github.com/microsoft/DeepSpeed).",lm-evaluation-harness,SOFTWARE
"input_ids = tokenizer(inputs, return_tensors=""pt"").input_ids.cuda()  outputs = model.generate(input_ids, generation_config=generation_config) response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(inputs)-1:].strip() ```  ## Acknowledgment  This repository benefits from [ALBEF](https://github.com/salesforce/ALBEF), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), and [DeepSpeed](https://github.com/microsoft/DeepSpeed).",lm-evaluation-harness,SOFTWARE
"input_ids = tokenizer(inputs, return_tensors=""pt"").input_ids.cuda()  outputs = model.generate(input_ids, generation_config=generation_config) response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(inputs)-1:].strip() ```  ## Acknowledgment  This repository benefits from [ALBEF](https://github.com/salesforce/ALBEF), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), and [DeepSpeed](https://github.com/microsoft/DeepSpeed).",Stanford Alpaca,SOFTWARE
"input_ids = tokenizer(inputs, return_tensors=""pt"").input_ids.cuda()  outputs = model.generate(input_ids, generation_config=generation_config) response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(inputs)-1:].strip() ```  ## Acknowledgment  This repository benefits from [ALBEF](https://github.com/salesforce/ALBEF), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), and [DeepSpeed](https://github.com/microsoft/DeepSpeed).",stanford_alpaca,SOFTWARE
"input_ids = tokenizer(inputs, return_tensors=""pt"").input_ids.cuda()  outputs = model.generate(input_ids, generation_config=generation_config) response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(inputs)-1:].strip() ```  ## Acknowledgment  This repository benefits from [ALBEF](https://github.com/salesforce/ALBEF), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), and [DeepSpeed](https://github.com/microsoft/DeepSpeed).",DeepSpeed,SOFTWARE
"input_ids = tokenizer(inputs, return_tensors=""pt"").input_ids.cuda()  outputs = model.generate(input_ids, generation_config=generation_config) response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(inputs)-1:].strip() ```  ## Acknowledgment  This repository benefits from [ALBEF](https://github.com/salesforce/ALBEF), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness), [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca), and [DeepSpeed](https://github.com/microsoft/DeepSpeed).",DeepSpeed,SOFTWARE
Please first install the following python packages. ``` torch==1.12.1 datasets evaluate accelerate tqdm ```  ## Implementation of DP fine-tuning  The transformers package in `dp_finetuning` enables DP training.,torch==1.12.1,SOFTWARE
Please first install the following python packages. ``` torch==1.12.1 datasets evaluate accelerate tqdm ```  ## Implementation of DP fine-tuning  The transformers package in `dp_finetuning` enables DP training.,datasets,SOFTWARE
Please first install the following python packages. ``` torch==1.12.1 datasets evaluate accelerate tqdm ```  ## Implementation of DP fine-tuning  The transformers package in `dp_finetuning` enables DP training.,evaluate,SOFTWARE
Please first install the following python packages. ``` torch==1.12.1 datasets evaluate accelerate tqdm ```  ## Implementation of DP fine-tuning  The transformers package in `dp_finetuning` enables DP training.,accelerate,SOFTWARE
Please first install the following python packages. ``` torch==1.12.1 datasets evaluate accelerate tqdm ```  ## Implementation of DP fine-tuning  The transformers package in `dp_finetuning` enables DP training.,tqdm,SOFTWARE
Please first install the following python packages. ``` torch==1.12.1 datasets evaluate accelerate tqdm ```  ## Implementation of DP fine-tuning  The transformers package in `dp_finetuning` enables DP training.,transformers,SOFTWARE
We register Pytorch backward hooks to linear layers to enable per-example gradient computation.,Pytorch,SOFTWARE
The implementation of hooks is in `src/transformers/models/grad_sample_utils.py`.,transformers,SOFTWARE
The hooks are attached to the model in `src/transformers/models/gpt2.py`  ## The first stage: selective pre-training.  1.,transformers,SOFTWARE
The resulting dataset is saved at `data/flatten_wiki_book_sentences.ds`.  ``` cd pretraining_data_selection python splitting_into_sentences.py ```  2.,python,SOFTWARE
The results are saved at `data/sst2/filter_train_nonewline.json` and `data/sst2/filter_val_nonewline.json`.  ``` python build_classifier_data_for_sst2.py ```  3.,python,SOFTWARE
"Please install the local transformers directory, in which we implement per-example gradient computation (`transformers/src/transformers/models/grad_sample_utils.py`), and clipping + noising (`line 709-744 in dp_finetuning/run_glue_no_trainer.py`).",transformers,SOFTWARE
"Please install the local transformers directory, in which we implement per-example gradient computation (`transformers/src/transformers/models/grad_sample_utils.py`), and clipping + noising (`line 709-744 in dp_finetuning/run_glue_no_trainer.py`).",transformers,SOFTWARE
"Please install the local transformers directory, in which we implement per-example gradient computation (`transformers/src/transformers/models/grad_sample_utils.py`), and clipping + noising (`line 709-744 in dp_finetuning/run_glue_no_trainer.py`).",transformers,SOFTWARE
"/dp_finetuning cd transformers pip install --editable . cd .. ```   ``` bash scripts/train_domain_classifier.sh 1.4 1 64 32 1e-3 ```  > The arguments are: noise_multiplier, clip, pergpu_bs, gradient accumulation steps, and learning rate.",transformers,SOFTWARE
"/dp_finetuning cd transformers pip install --editable . cd .. ```   ``` bash scripts/train_domain_classifier.sh 1.4 1 64 32 1e-3 ```  > The arguments are: noise_multiplier, clip, pergpu_bs, gradient accumulation steps, and learning rate.",pip,SOFTWARE
/pretraining_data_selection python sampling_with_logits.py --num_tokens 40 ```  > You can also select random sentences.  ``` python sampling_with_logits.py --num_tokens 40 --random ```  5.,python,SOFTWARE
/pretraining_data_selection python sampling_with_logits.py --num_tokens 40 ```  > You can also select random sentences.  ``` python sampling_with_logits.py --num_tokens 40 --random ```  5.,python,SOFTWARE
"Install standard transformers package, i.e., without pre-example gradients computation.",transformers,SOFTWARE
"/pretraining cd transformers pip install --editable . cd .. ```    ``` bash scripts/pretraining.sh pretraining_data_40m.ds tiny 3e-4 1000000 32 8 1 ```  > The arguments are: pre-training data path, model size (tiny=5M), lr, pre-training steps, per-gpu-bs, num_gpus, gradient accumulation",transformers,SOFTWARE
"/pretraining cd transformers pip install --editable . cd .. ```    ``` bash scripts/pretraining.sh pretraining_data_40m.ds tiny 3e-4 1000000 32 8 1 ```  > The arguments are: pre-training data path, model size (tiny=5M), lr, pre-training steps, per-gpu-bs, num_gpus, gradient accumulation",pip,SOFTWARE
Don't forget installing dp enabled transformers package.  ``` cd ..,transformers,SOFTWARE
"/dp_finetuning cd transformers pip install --editable . cd .. ```  > The argumentments are: pre-trained model path, noise_multiplier, clip, pergpu_bs, gradient accumulation, lr, epochs, seed.",transformers,SOFTWARE
"/dp_finetuning cd transformers pip install --editable . cd .. ```  > The argumentments are: pre-trained model path, noise_multiplier, clip, pergpu_bs, gradient accumulation, lr, epochs, seed.",pip,SOFTWARE
"To measure reasoning beyond final-answer accuracy, we develop **ReasonEval**, a suite comprising a new evaluation methodology with defined metrics for assessing mathematical reasoning quality and corresponding LLM-based evaluators for automated calculation.",ReasonEval,SOFTWARE
**ReasonEval** assesses the problem-solving process in a step-by-step format from the following perspectives: - **Validity**: The step contains no mistakes in calculation and logic. - **Redundancy**: The step lacks utility in solving the problem but is still valid.,ReasonEval,SOFTWARE
"/images/introduction.jpg"" alt=""error"" style=""width:95%;""> </p>  With ReasonEval, you can  - üìè  quantify the quality of reasoning steps free of human or close-source models",ReasonEval,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",bash,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",git,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",ReasonEval,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",bash,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",conda,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",python=3.10,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",conda,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",bash,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",pip,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",ReasonEval,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",ReasonEval-7B,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",GAIR/ReasonEval-7B,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",GAIR/ReasonEval-7B,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",Llama 2,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",llama,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",WizardMath-7B-V1.1,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",WizardLM/WizardMath-7B-V1.1,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",ReasonEval-34B,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",GAIR/ReasonEval-34B,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",GAIR/ReasonEval-34B,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",llemma_34b,SOFTWARE
"- üõ†Ô∏è  select high-quality training data for downstream tasks (e.g., fine-tuning).      ## Quick Start ### Setup  * Clone the repository ```bash git clone https://github.com/GAIR-NLP/ReasonEval cd ReasonEval ``` * Create a conda environment and activate the environment ```bash conda create -n ReasonEval python=3.10 conda activate ReasonEval ``` * Install the required libraries ```bash pip install -r requirements.txt ```     ### Model  ReasonEval is now available on huggingface-hub:  | Model Name | HF Checkpoint                                                | Size    | License | Fine-tuned from model |   | ---------- | ------------------------------------------------------------ | ------- | ------------------------------------------------------------ | --------------------------- | | ReasonEval-7B     | [ü§ó  GAIR/ReasonEval-7B](https://huggingface.co/GAIR/ReasonEval-7B) | **7B** | [Llama 2](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [WizardMath-7B-V1.1](https://huggingface.co/WizardLM/WizardMath-7B-V1.1)| | ReasonEval-34B    |[ü§ó  GAIR/ReasonEval-34B](https://huggingface.co/GAIR/ReasonEval-34B) | **34B** | [Apache License 2.0](https://www.apache.org/licenses/) | [llemma_34b](https://huggingface.co/EleutherAI/llemma_34b)|   ### Usage  Provide the question and the solution in a step-by-step format.",EleutherAI/llemma_34b,SOFTWARE
"/codes/examples.py --model_name_or_path GAIR/ReasonEval-7B # Specify the model name or path here --model_size 7B # Indicate the model size of ReasonEval (7B or 34B) ```  ## Meta Evaluation ### Datasets The datasets for meta-evaluations are composed of three parts:  - **Meta-Reasoning-MATH**: This dataset is constructed as follows:    - To collect the first type of errors affecting the correctness of steps, we recruit undergraduates who have a solid mathematical background to label solutions generated by Abel and WizardMath.",GAIR/ReasonEval-7B,SOFTWARE
"/codes/examples.py --model_name_or_path GAIR/ReasonEval-7B # Specify the model name or path here --model_size 7B # Indicate the model size of ReasonEval (7B or 34B) ```  ## Meta Evaluation ### Datasets The datasets for meta-evaluations are composed of three parts:  - **Meta-Reasoning-MATH**: This dataset is constructed as follows:    - To collect the first type of errors affecting the correctness of steps, we recruit undergraduates who have a solid mathematical background to label solutions generated by Abel and WizardMath.",ReasonEval,SOFTWARE
"/codes/examples.py --model_name_or_path GAIR/ReasonEval-7B # Specify the model name or path here --model_size 7B # Indicate the model size of ReasonEval (7B or 34B) ```  ## Meta Evaluation ### Datasets The datasets for meta-evaluations are composed of three parts:  - **Meta-Reasoning-MATH**: This dataset is constructed as follows:    - To collect the first type of errors affecting the correctness of steps, we recruit undergraduates who have a solid mathematical background to label solutions generated by Abel and WizardMath.",Abel,SOFTWARE
"/codes/examples.py --model_name_or_path GAIR/ReasonEval-7B # Specify the model name or path here --model_size 7B # Indicate the model size of ReasonEval (7B or 34B) ```  ## Meta Evaluation ### Datasets The datasets for meta-evaluations are composed of three parts:  - **Meta-Reasoning-MATH**: This dataset is constructed as follows:    - To collect the first type of errors affecting the correctness of steps, we recruit undergraduates who have a solid mathematical background to label solutions generated by Abel and WizardMath.",WizardMath,SOFTWARE
"""generator"": ""Abel13B-001"",        // The raw solution.",Abel13B-001,SOFTWARE
"""generator"": ""GPT-4 (PRM800K)"",        // The solution in a step-by-step format.",GPT-4,SOFTWARE
"/dataset/`  ### Code To reproduce our results in the meta-evaluations, run the following commands: ```bash python .",bash,SOFTWARE
"<img src='images/f1.png' width=""600px""/>  ## Visualization  Visualization examples of the fitted TnT graph structures:  <img src='images/f3.png' width=""1000px""/>  - (Optional) install sknetwork to enable visualization: ```bash pip install scikit-network ```   ## Results \# S indicates the number of split nodes, which is an estimate of model complexity",sknetwork,SOFTWARE
"<img src='images/f1.png' width=""600px""/>  ## Visualization  Visualization examples of the fitted TnT graph structures:  <img src='images/f3.png' width=""1000px""/>  - (Optional) install sknetwork to enable visualization: ```bash pip install scikit-network ```   ## Results \# S indicates the number of split nodes, which is an estimate of model complexity",pip,SOFTWARE
"<img src='images/f1.png' width=""600px""/>  ## Visualization  Visualization examples of the fitted TnT graph structures:  <img src='images/f3.png' width=""1000px""/>  - (Optional) install sknetwork to enable visualization: ```bash pip install scikit-network ```   ## Results \# S indicates the number of split nodes, which is an estimate of model complexity",scikit-network,SOFTWARE
"> <img src=""https://github.com/open-mmlab/mmdetection3d/blob/main/resources/mmdet3d-logo.png"" width=""30%""/><br> > MMDetection3D is an open-source object detection toolbox based on PyTorch, towards the next-generation platform for general 3D perception.",PyTorch,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",SemanticKITTI-API,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",semantic-kitti-api,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",nuScenes-devkit,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",nuscenes-devkit,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",SemanticPOSS-API,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",semantic-poss-api,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",Robo3D,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",Robo3D,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",PCSeg,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",PCSeg,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",SalsaNext,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",SalsaNext,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",IROS21-FIDNet-SemanticKITTI,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",CENet,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",CENet,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",RangeViT,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",rangevit,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",SphereFormer,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",SphereFormer,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",2DPASS,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",2DPASS,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",Cylinder3D,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",Cylinder3D,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",SPVNAS,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",spvnas,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",KPConv,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",KPConv-PyTorch,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",LaserMix,SOFTWARE
"We acknowledge the use of the following public resources during the course of this work: <sup>1</sup>[SemanticKITTI](http://www.semantic-kitti.org), <sup>2</sup>[SemanticKITTI-API](https://github.com/PRBonn/semantic-kitti-api), <sup>3</sup>[nuScenes](https://www.nuscenes.org/nuscenes), <sup>4</sup>[nuScenes-devkit](https://github.com/nutonomy/nuscenes-devkit), <sup>5</sup>[ScribbleKITTI](https://github.com/ouenal/scribblekitti), <sup>6</sup>[SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html), <sup>7</sup>[SemanticPOSS-API](https://github.com/Theia-4869/semantic-poss-api), <sup>8</sup>[Robo3D](https://github.com/ldkong1205/Robo3D), <sup>9</sup>[PCSeg](https://github.com/PJLab-ADG/PCSeg), <sup>10</sup>[SalsaNext](https://github.com/TiagoCortinhal/SalsaNext), <sup>11</sup>[FIDNet](https://github.com/placeforyiming/IROS21-FIDNet-SemanticKITTI), <sup>12</sup>[CENet](https://github.com/huixiancheng/CENet), <sup>13</sup>[RangeViT](https://github.com/valeoai/rangevit), <sup>14</sup>[SphereFormer](https://github.com/dvlab-research/SphereFormer), <sup>15</sup>[2DPASS](https://github.com/yanx27/2DPASS), <sup>16</sup>[Cylinder3D](https://github.com/xinge008/Cylinder3D), <sup>17</sup>[SPVNAS](https://github.com/mit-han-lab/spvnas), <sup>18</sup>[KPConv](https://github.com/HuguesTHOMAS/KPConv-PyTorch), and <sup>19</sup>[LaserMix](https://github.com/ldkong1205/LaserMix).",LaserMix,SOFTWARE
"# OntoProtein  This is the implement of the ICLR2022 paper ""[OntoProtein: Protein Pretraining With Ontology Embedding](https://arxiv.org/pdf/2201.11147.pdf)"".",OntoProtein,SOFTWARE
OntoProtein is an effective method that make use of structure in GO (Gene Ontology) into text-enhanced protein pre-training model. - ‚ùóNOTE: We provide a NLP for science paper-list at [https://github.com/zjunlp/NLP4Science_Papers](https://github.com/zjunlp/NLP4Science_Papers).,OntoProtein,SOFTWARE
"<div align=center><img src=""resources/img/model.png"" width=""80%"" height=""80%"" /></div>  ## Quick links  * [Overview](#overview) * [Requirements](#requirements)   * [Environment for pre-training data generation](#environment-for-pre-training-data-generation)   * [Environmen for OntoProtein pre-training](#environment-for-ontoprotein-pre-training)   * [Environment for protein-related tasks](#environment-for-protein-related-tasks) * [Data preparation](#data-preparation)   * [Pre-training data](#pre-training-data)   * [Downstream task data](#downstream-task-data) * [Protein pre-training model](#protein-pre-training-model) * [Usage for protein-related tasks](#usage-for-protein-related-tasks) * [Citation](#citation)  ## Overview <span id=""overview""></span>  In this work we present OntoProtein, a knowledge-enhanced protein language model that jointly optimize the KE and MLM objectives, which bring excellent improvements to a wide range of protein tasks.",OntoProtein,SOFTWARE
"<div align=center><img src=""resources/img/model.png"" width=""80%"" height=""80%"" /></div>  ## Quick links  * [Overview](#overview) * [Requirements](#requirements)   * [Environment for pre-training data generation](#environment-for-pre-training-data-generation)   * [Environmen for OntoProtein pre-training](#environment-for-ontoprotein-pre-training)   * [Environment for protein-related tasks](#environment-for-protein-related-tasks) * [Data preparation](#data-preparation)   * [Pre-training data](#pre-training-data)   * [Downstream task data](#downstream-task-data) * [Protein pre-training model](#protein-pre-training-model) * [Usage for protein-related tasks](#usage-for-protein-related-tasks) * [Citation](#citation)  ## Overview <span id=""overview""></span>  In this work we present OntoProtein, a knowledge-enhanced protein language model that jointly optimize the KE and MLM objectives, which bring excellent improvements to a wide range of protein tasks.",OntoProtein,SOFTWARE
"<div align=center><img src=""resources/img/main.jpg"" width=""60%"" height=""60%"" /></div>  ## Requirements <span id=""requirements""></span> To run our code, please install dependency packages for related steps.  ### Environment for pre-training data generation <span id=""environment-for-pre-training-data-generation""></span> python3.8 / biopython 1.37 / goatools  For extracting the definition of the GO term, we motified the code in `goatools` library.",python3.8,SOFTWARE
"<div align=center><img src=""resources/img/main.jpg"" width=""60%"" height=""60%"" /></div>  ## Requirements <span id=""requirements""></span> To run our code, please install dependency packages for related steps.  ### Environment for pre-training data generation <span id=""environment-for-pre-training-data-generation""></span> python3.8 / biopython 1.37 / goatools  For extracting the definition of the GO term, we motified the code in `goatools` library.",biopython 1.37,SOFTWARE
"<div align=center><img src=""resources/img/main.jpg"" width=""60%"" height=""60%"" /></div>  ## Requirements <span id=""requirements""></span> To run our code, please install dependency packages for related steps.  ### Environment for pre-training data generation <span id=""environment-for-pre-training-data-generation""></span> python3.8 / biopython 1.37 / goatools  For extracting the definition of the GO term, we motified the code in `goatools` library.",goatools,SOFTWARE
"<div align=center><img src=""resources/img/main.jpg"" width=""60%"" height=""60%"" /></div>  ## Requirements <span id=""requirements""></span> To run our code, please install dependency packages for related steps.  ### Environment for pre-training data generation <span id=""environment-for-pre-training-data-generation""></span> python3.8 / biopython 1.37 / goatools  For extracting the definition of the GO term, we motified the code in `goatools` library.",goatools,SOFTWARE
"The changes in `goatools.obo_parser` are as follows:  ```python # line 132 elif line[:5] == ""def: "":     rec_curr.definition = line[5:]  # line 169 self.definition = """" ```  ### Environment for OntoProtein pre-training <span id=""environment-for-ontoprotein-pre-training""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / deepspeed 0.5.1/ lmdb /   ### Environment for protein-related tasks <span id=""environment-for-protein-related-tasks""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / lmdb / tape_proteins  Specially, in library `tape_proteins`, it only implements the calculation of metric `P@L` for the contact prediction task.",python3.8,SOFTWARE
"The changes in `goatools.obo_parser` are as follows:  ```python # line 132 elif line[:5] == ""def: "":     rec_curr.definition = line[5:]  # line 169 self.definition = """" ```  ### Environment for OntoProtein pre-training <span id=""environment-for-ontoprotein-pre-training""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / deepspeed 0.5.1/ lmdb /   ### Environment for protein-related tasks <span id=""environment-for-protein-related-tasks""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / lmdb / tape_proteins  Specially, in library `tape_proteins`, it only implements the calculation of metric `P@L` for the contact prediction task.",pytorch 1.9,SOFTWARE
"The changes in `goatools.obo_parser` are as follows:  ```python # line 132 elif line[:5] == ""def: "":     rec_curr.definition = line[5:]  # line 169 self.definition = """" ```  ### Environment for OntoProtein pre-training <span id=""environment-for-ontoprotein-pre-training""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / deepspeed 0.5.1/ lmdb /   ### Environment for protein-related tasks <span id=""environment-for-protein-related-tasks""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / lmdb / tape_proteins  Specially, in library `tape_proteins`, it only implements the calculation of metric `P@L` for the contact prediction task.",transformer 4.5.1+,SOFTWARE
"The changes in `goatools.obo_parser` are as follows:  ```python # line 132 elif line[:5] == ""def: "":     rec_curr.definition = line[5:]  # line 169 self.definition = """" ```  ### Environment for OntoProtein pre-training <span id=""environment-for-ontoprotein-pre-training""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / deepspeed 0.5.1/ lmdb /   ### Environment for protein-related tasks <span id=""environment-for-protein-related-tasks""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / lmdb / tape_proteins  Specially, in library `tape_proteins`, it only implements the calculation of metric `P@L` for the contact prediction task.",deepspeed 0.5.1,SOFTWARE
"The changes in `goatools.obo_parser` are as follows:  ```python # line 132 elif line[:5] == ""def: "":     rec_curr.definition = line[5:]  # line 169 self.definition = """" ```  ### Environment for OntoProtein pre-training <span id=""environment-for-ontoprotein-pre-training""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / deepspeed 0.5.1/ lmdb /   ### Environment for protein-related tasks <span id=""environment-for-protein-related-tasks""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / lmdb / tape_proteins  Specially, in library `tape_proteins`, it only implements the calculation of metric `P@L` for the contact prediction task.",lmdb,SOFTWARE
"The changes in `goatools.obo_parser` are as follows:  ```python # line 132 elif line[:5] == ""def: "":     rec_curr.definition = line[5:]  # line 169 self.definition = """" ```  ### Environment for OntoProtein pre-training <span id=""environment-for-ontoprotein-pre-training""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / deepspeed 0.5.1/ lmdb /   ### Environment for protein-related tasks <span id=""environment-for-protein-related-tasks""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / lmdb / tape_proteins  Specially, in library `tape_proteins`, it only implements the calculation of metric `P@L` for the contact prediction task.",python3.8,SOFTWARE
"The changes in `goatools.obo_parser` are as follows:  ```python # line 132 elif line[:5] == ""def: "":     rec_curr.definition = line[5:]  # line 169 self.definition = """" ```  ### Environment for OntoProtein pre-training <span id=""environment-for-ontoprotein-pre-training""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / deepspeed 0.5.1/ lmdb /   ### Environment for protein-related tasks <span id=""environment-for-protein-related-tasks""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / lmdb / tape_proteins  Specially, in library `tape_proteins`, it only implements the calculation of metric `P@L` for the contact prediction task.",pytorch 1.9,SOFTWARE
"The changes in `goatools.obo_parser` are as follows:  ```python # line 132 elif line[:5] == ""def: "":     rec_curr.definition = line[5:]  # line 169 self.definition = """" ```  ### Environment for OntoProtein pre-training <span id=""environment-for-ontoprotein-pre-training""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / deepspeed 0.5.1/ lmdb /   ### Environment for protein-related tasks <span id=""environment-for-protein-related-tasks""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / lmdb / tape_proteins  Specially, in library `tape_proteins`, it only implements the calculation of metric `P@L` for the contact prediction task.",transformer 4.5.1+,SOFTWARE
"The changes in `goatools.obo_parser` are as follows:  ```python # line 132 elif line[:5] == ""def: "":     rec_curr.definition = line[5:]  # line 169 self.definition = """" ```  ### Environment for OntoProtein pre-training <span id=""environment-for-ontoprotein-pre-training""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / deepspeed 0.5.1/ lmdb /   ### Environment for protein-related tasks <span id=""environment-for-protein-related-tasks""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / lmdb / tape_proteins  Specially, in library `tape_proteins`, it only implements the calculation of metric `P@L` for the contact prediction task.",lmdb,SOFTWARE
"The changes in `goatools.obo_parser` are as follows:  ```python # line 132 elif line[:5] == ""def: "":     rec_curr.definition = line[5:]  # line 169 self.definition = """" ```  ### Environment for OntoProtein pre-training <span id=""environment-for-ontoprotein-pre-training""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / deepspeed 0.5.1/ lmdb /   ### Environment for protein-related tasks <span id=""environment-for-protein-related-tasks""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / lmdb / tape_proteins  Specially, in library `tape_proteins`, it only implements the calculation of metric `P@L` for the contact prediction task.",tape_proteins,SOFTWARE
"The changes in `goatools.obo_parser` are as follows:  ```python # line 132 elif line[:5] == ""def: "":     rec_curr.definition = line[5:]  # line 169 self.definition = """" ```  ### Environment for OntoProtein pre-training <span id=""environment-for-ontoprotein-pre-training""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / deepspeed 0.5.1/ lmdb /   ### Environment for protein-related tasks <span id=""environment-for-protein-related-tasks""></span> python3.8 / pytorch 1.9 / transformer 4.5.1+ / lmdb / tape_proteins  Specially, in library `tape_proteins`, it only implements the calculation of metric `P@L` for the contact prediction task.",tape_proteins,SOFTWARE
"Detailed changes could be seen in [[isssue #8]](https://github.com/zjunlp/OntoProtein/issues/8#issuecomment-1109975025)   **Note:** environments configurations of some baseline models or methods in our experiments, e.g.",OntoProtein,SOFTWARE
"BLAST, DeepGraphGO, we provide related links to configurate as follows:  [BLAST](https://www.ncbi.nlm.nih.gov/books/NBK569861/) / [Interproscan](https://github.com/ebi-pf-team/interproscan) / [DeepGraphGO](https://github.com/yourh/DeepGraphGO) / [GNN-PPI](https://github.com/lvguofeng/GNN_PPI)  ## Data preparation <span id=""data-preparation""></span> For pretraining OntoProtein, fine-tuning on protein-related tasks and inference, we provide acquirement approach of related data.  ### Pre-training data <span id=""pre-training-data""></span> To incorporate Gene Ontology knowledge into language models and train OntoProtein, we construct [ProteinKG25](https://zjunlp.github.io/project/ProteinKG25/), a large-scale KG dataset with aligned descriptions and protein sequences respectively to GO terms and protein entities.",OntoProtein,SOFTWARE
"BLAST, DeepGraphGO, we provide related links to configurate as follows:  [BLAST](https://www.ncbi.nlm.nih.gov/books/NBK569861/) / [Interproscan](https://github.com/ebi-pf-team/interproscan) / [DeepGraphGO](https://github.com/yourh/DeepGraphGO) / [GNN-PPI](https://github.com/lvguofeng/GNN_PPI)  ## Data preparation <span id=""data-preparation""></span> For pretraining OntoProtein, fine-tuning on protein-related tasks and inference, we provide acquirement approach of related data.  ### Pre-training data <span id=""pre-training-data""></span> To incorporate Gene Ontology knowledge into language models and train OntoProtein, we construct [ProteinKG25](https://zjunlp.github.io/project/ProteinKG25/), a large-scale KG dataset with aligned descriptions and protein sequences respectively to GO terms and protein entities.",OntoProtein,SOFTWARE
"[[link]](https://ftp.ebi.ac.uk/pub/databases/GO/goa/old/UNIPROT/)  When download these raw data, you can excute following script to generate pre-training data:  ```bash python tools/gen_onto_protein_data.py ```  ### Downstream task data <span id=""downstream-task-data""></span> Our experiments involved with several protein-related downstream tasks.",python,SOFTWARE
"usp=sharing)  ## Protein pre-training model <span id=""protein-pre-training-model""></span> You can pre-training your own OntoProtein based above pretraining dataset.",OntoProtein,SOFTWARE
"Before pretraining OntoProtein, you need to download two pretrained model, respectively [ProtBERT](https://huggingface.co/Rostlab/prot_bert) and [PubMedBERT](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext) and save them in `data/model_data/ProtBERT` and `data/model_data/PubMedBERT`.",OntoProtein,SOFTWARE
"Before pretraining OntoProtein, you need to download two pretrained model, respectively [ProtBERT](https://huggingface.co/Rostlab/prot_bert) and [PubMedBERT](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext) and save them in `data/model_data/ProtBERT` and `data/model_data/PubMedBERT`.",ProtBERT,SOFTWARE
"Before pretraining OntoProtein, you need to download two pretrained model, respectively [ProtBERT](https://huggingface.co/Rostlab/prot_bert) and [PubMedBERT](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext) and save them in `data/model_data/ProtBERT` and `data/model_data/PubMedBERT`.",prot_bert,SOFTWARE
"Before pretraining OntoProtein, you need to download two pretrained model, respectively [ProtBERT](https://huggingface.co/Rostlab/prot_bert) and [PubMedBERT](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext) and save them in `data/model_data/ProtBERT` and `data/model_data/PubMedBERT`.",PubMedBERT,SOFTWARE
"Before pretraining OntoProtein, you need to download two pretrained model, respectively [ProtBERT](https://huggingface.co/Rostlab/prot_bert) and [PubMedBERT](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext) and save them in `data/model_data/ProtBERT` and `data/model_data/PubMedBERT`.",PubMedBERT,SOFTWARE
"Before pretraining OntoProtein, you need to download two pretrained model, respectively [ProtBERT](https://huggingface.co/Rostlab/prot_bert) and [PubMedBERT](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext) and save them in `data/model_data/ProtBERT` and `data/model_data/PubMedBERT`.",ProtBERT,SOFTWARE
"Before pretraining OntoProtein, you need to download two pretrained model, respectively [ProtBERT](https://huggingface.co/Rostlab/prot_bert) and [PubMedBERT](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext) and save them in `data/model_data/ProtBERT` and `data/model_data/PubMedBERT`.",PubMedBERT,SOFTWARE
"[[Download model]](https://huggingface.co/zjunlp/OntoProtein).  ### Running examples  The shell files of training and evaluation for every task are provided in `script/` , and could directly run.",OntoProtein,SOFTWARE
"Also, you can utilize the running codes `run_downstream.py` , and write your shell files according to your need:  - `run_downstream.py`: support `{ss3, ss8, contact, remote_homology, fluorescence, stability}` tasks;  #### Training models  Running shell files: `bash script/run_{task}.sh`, and the contents of shell files are as follow:  ```shell bash run_main.sh \     --model model_data/ProtBertModel \     --output_file ss3-ProtBert \     --task_name ss3 \     --do_train True \     --epoch 5 \     --optimizer AdamW \     --per_device_batch_size 2 \     --gradient_accumulation_steps 8 \     --eval_step 100 \     --eval_batchsize 4 \     --warmup_ratio 0.08 \     --frozen_bert False ```  Arguments for the training and evalution script are as follows,  - `--task_name`: Specify which task to evaluate on, and now the script supports `{ss3, ss8, contact, remote_homology, fluorescence, stability}` tasks; - `--model`: The name or path of a protein pre-trained checkpoint. - `--output_file`: The path of the fine-tuned checkpoint saved. - `--do_train`: Specify if you want to finetune the pretrained model on downstream tasks. - `--epoch`: Epochs for training model. - `--optimizer`: The optimizer to use, e.g., `AdamW`. - `--per_device_batch_size`: Batch size per GPU. - `--gradient_accumulation_steps`: The number of gradient accumulation steps. - `--warmup_ratio`: Ratio of total training steps used for a linear warmup from 0 to `learning_rate`. - `--frozen_bert`: Specify if you want to frozen the encoder in the pretrained model.",bash,SOFTWARE
"Also, you can utilize the running codes `run_downstream.py` , and write your shell files according to your need:  - `run_downstream.py`: support `{ss3, ss8, contact, remote_homology, fluorescence, stability}` tasks;  #### Training models  Running shell files: `bash script/run_{task}.sh`, and the contents of shell files are as follow:  ```shell bash run_main.sh \     --model model_data/ProtBertModel \     --output_file ss3-ProtBert \     --task_name ss3 \     --do_train True \     --epoch 5 \     --optimizer AdamW \     --per_device_batch_size 2 \     --gradient_accumulation_steps 8 \     --eval_step 100 \     --eval_batchsize 4 \     --warmup_ratio 0.08 \     --frozen_bert False ```  Arguments for the training and evalution script are as follows,  - `--task_name`: Specify which task to evaluate on, and now the script supports `{ss3, ss8, contact, remote_homology, fluorescence, stability}` tasks; - `--model`: The name or path of a protein pre-trained checkpoint. - `--output_file`: The path of the fine-tuned checkpoint saved. - `--do_train`: Specify if you want to finetune the pretrained model on downstream tasks. - `--epoch`: Epochs for training model. - `--optimizer`: The optimizer to use, e.g., `AdamW`. - `--per_device_batch_size`: Batch size per GPU. - `--gradient_accumulation_steps`: The number of gradient accumulation steps. - `--warmup_ratio`: Ratio of total training steps used for a linear warmup from 0 to `learning_rate`. - `--frozen_bert`: Specify if you want to frozen the encoder in the pretrained model.",ProtBert,SOFTWARE
[Tulip Software](http://tulip.labri.fr/TulipDrupal/sites/default/files/logo_web.png)  # Graph Algorithms for [Tulip Software](https://github.com/Tulip-Dev/tulip)  > Tulip is an information visualization framework dedicated to the analysis and visualization of relational data.,Tulip,SOFTWARE
[Tulip Software](http://tulip.labri.fr/TulipDrupal/sites/default/files/logo_web.png)  # Graph Algorithms for [Tulip Software](https://github.com/Tulip-Dev/tulip)  > Tulip is an information visualization framework dedicated to the analysis and visualization of relational data.,tulip,SOFTWARE
[Tulip Software](http://tulip.labri.fr/TulipDrupal/sites/default/files/logo_web.png)  # Graph Algorithms for [Tulip Software](https://github.com/Tulip-Dev/tulip)  > Tulip is an information visualization framework dedicated to the analysis and visualization of relational data.,Tulip,SOFTWARE
[Tulip Software](http://tulip.labri.fr/TulipDrupal/sites/default/files/logo_web.png)  # Graph Algorithms for [Tulip Software](https://github.com/Tulip-Dev/tulip)  > Tulip is an information visualization framework dedicated to the analysis and visualization of relational data.,Tulip,SOFTWARE
[Tulip Software](http://tulip.labri.fr/TulipDrupal/sites/default/files/logo_web.png)  # Graph Algorithms for [Tulip Software](https://github.com/Tulip-Dev/tulip)  > Tulip is an information visualization framework dedicated to the analysis and visualization of relational data.,Tulip,SOFTWARE
[Tulip Software](http://tulip.labri.fr/TulipDrupal/sites/default/files/logo_web.png)  # Graph Algorithms for [Tulip Software](https://github.com/Tulip-Dev/tulip)  > Tulip is an information visualization framework dedicated to the analysis and visualization of relational data.,tulip,SOFTWARE
[Tulip Software](http://tulip.labri.fr/TulipDrupal/sites/default/files/logo_web.png)  # Graph Algorithms for [Tulip Software](https://github.com/Tulip-Dev/tulip)  > Tulip is an information visualization framework dedicated to the analysis and visualization of relational data.,Tulip,SOFTWARE
"Tulip aims to provide the developer with a complete library, supporting the design of interactive information visualization applications for relational data that can be tailored to the problems he or she is addressing.",Tulip,SOFTWARE
Find more info [here](https://github.com/Tulip-Dev/tulip).,Tulip,SOFTWARE
Find more info [here](https://github.com/Tulip-Dev/tulip).,tulip,SOFTWARE
This page contains Tulip plugins using [Tulip API](http://tulip.labri.fr/Documentation/current/doxygen/html/index.html) or [Tulip Python API](http://tulip.labri.fr/Documentation/current/tulip-python/html).,Tulip,SOFTWARE
This page contains Tulip plugins using [Tulip API](http://tulip.labri.fr/Documentation/current/doxygen/html/index.html) or [Tulip Python API](http://tulip.labri.fr/Documentation/current/tulip-python/html).,Tulip,SOFTWARE
This page contains Tulip plugins using [Tulip API](http://tulip.labri.fr/Documentation/current/doxygen/html/index.html) or [Tulip Python API](http://tulip.labri.fr/Documentation/current/tulip-python/html).,tulip,SOFTWARE
This page contains Tulip plugins using [Tulip API](http://tulip.labri.fr/Documentation/current/doxygen/html/index.html) or [Tulip Python API](http://tulip.labri.fr/Documentation/current/tulip-python/html).,Tulip,SOFTWARE
This page contains Tulip plugins using [Tulip API](http://tulip.labri.fr/Documentation/current/doxygen/html/index.html) or [Tulip Python API](http://tulip.labri.fr/Documentation/current/tulip-python/html).,tulip,SOFTWARE
This page contains Tulip plugins using [Tulip API](http://tulip.labri.fr/Documentation/current/doxygen/html/index.html) or [Tulip Python API](http://tulip.labri.fr/Documentation/current/tulip-python/html).,tulip,SOFTWARE
"They are implementation of well-known (or less-known) graph algorithms and network analysis methods.  ## [Transportation Networks Design](https://github.com/fqueyroi/tulip_plugins/tree/master/TransportationNetworks)  Directory ""TransportationNetworks"" contains two transportation network construction algorithms",tulip,SOFTWARE
Biological and Shortest-Path Routing Procedures for Transportation Network Design. arXiv preprint [arXiv:1803.03528](https://arxiv.org/abs/1803.03528).  ## Clustering  ### [Cliques](https://github.com/fqueyroi/tulip_plugins/tree/master/Cliques)  A plugin to enumerate all [maximal cliques](https://en.wikipedia.org/wiki/Clique_(graph_theory)) in the graph.,Cliques,SOFTWARE
Biological and Shortest-Path Routing Procedures for Transportation Network Design. arXiv preprint [arXiv:1803.03528](https://arxiv.org/abs/1803.03528).  ## Clustering  ### [Cliques](https://github.com/fqueyroi/tulip_plugins/tree/master/Cliques)  A plugin to enumerate all [maximal cliques](https://en.wikipedia.org/wiki/Clique_(graph_theory)) in the graph.,tulip,SOFTWARE
Biological and Shortest-Path Routing Procedures for Transportation Network Design. arXiv preprint [arXiv:1803.03528](https://arxiv.org/abs/1803.03528).  ## Clustering  ### [Cliques](https://github.com/fqueyroi/tulip_plugins/tree/master/Cliques)  A plugin to enumerate all [maximal cliques](https://en.wikipedia.org/wiki/Clique_(graph_theory)) in the graph.,Cliques,SOFTWARE
Biological and Shortest-Path Routing Procedures for Transportation Network Design. arXiv preprint [arXiv:1803.03528](https://arxiv.org/abs/1803.03528).  ## Clustering  ### [Cliques](https://github.com/fqueyroi/tulip_plugins/tree/master/Cliques)  A plugin to enumerate all [maximal cliques](https://en.wikipedia.org/wiki/Clique_(graph_theory)) in the graph.,cliques,SOFTWARE
Biological and Shortest-Path Routing Procedures for Transportation Network Design. arXiv preprint [arXiv:1803.03528](https://arxiv.org/abs/1803.03528).  ## Clustering  ### [Cliques](https://github.com/fqueyroi/tulip_plugins/tree/master/Cliques)  A plugin to enumerate all [maximal cliques](https://en.wikipedia.org/wiki/Clique_(graph_theory)) in the graph.,Clique,SOFTWARE
"For python version prior to 2.7, you need to install the package: `pip install ordereddict`  ### [Clique Percolation](https://github.com/fqueyroi/tulip_plugins/tree/master/Cliques)  An implementation of the [Clique Percolation Method](https://en.wikipedia.org/wiki/Clique_percolation_method) that finds an overlapping clustering of the graph.",pip,SOFTWARE
"For python version prior to 2.7, you need to install the package: `pip install ordereddict`  ### [Clique Percolation](https://github.com/fqueyroi/tulip_plugins/tree/master/Cliques)  An implementation of the [Clique Percolation Method](https://en.wikipedia.org/wiki/Clique_percolation_method) that finds an overlapping clustering of the graph.",Clique Percolation,SOFTWARE
"For python version prior to 2.7, you need to install the package: `pip install ordereddict`  ### [Clique Percolation](https://github.com/fqueyroi/tulip_plugins/tree/master/Cliques)  An implementation of the [Clique Percolation Method](https://en.wikipedia.org/wiki/Clique_percolation_method) that finds an overlapping clustering of the graph.",tulip,SOFTWARE
"For python version prior to 2.7, you need to install the package: `pip install ordereddict`  ### [Clique Percolation](https://github.com/fqueyroi/tulip_plugins/tree/master/Cliques)  An implementation of the [Clique Percolation Method](https://en.wikipedia.org/wiki/Clique_percolation_method) that finds an overlapping clustering of the graph.",Cliques,SOFTWARE
"We actually use the plugin `CliqueEnum` to find maximal cliques rather than enumerating all $k$-cliques.   ### [Label Propagation](https://github.com/fqueyroi/tulip_plugins/tree/master/LabelPropagation)  A simple, fast and efficient graph clustering algorithm.",CliqueEnum,SOFTWARE
"We actually use the plugin `CliqueEnum` to find maximal cliques rather than enumerating all $k$-cliques.   ### [Label Propagation](https://github.com/fqueyroi/tulip_plugins/tree/master/LabelPropagation)  A simple, fast and efficient graph clustering algorithm.",Label Propagation,SOFTWARE
"We actually use the plugin `CliqueEnum` to find maximal cliques rather than enumerating all $k$-cliques.   ### [Label Propagation](https://github.com/fqueyroi/tulip_plugins/tree/master/LabelPropagation)  A simple, fast and efficient graph clustering algorithm.",LabelPropagation,SOFTWARE
"Therefore, the Tulip Python plugin ""SecretSanta"" find a solution where the knowledge of a given assignment can not be used to infer another assignment.",Tulip,SOFTWARE
"To find label errors in your own token classification data, you should instead use [the implementation](https://docs.cleanlab.ai/stable/tutorials/token_classification.html) from the official [cleanlab library](https://github.com/cleanlab/cleanlab).   #### Install Cleanlab Package  ---  Install the Cleanlab version used for our experiments: `pip install .",cleanlab,SOFTWARE
"To find label errors in your own token classification data, you should instead use [the implementation](https://docs.cleanlab.ai/stable/tutorials/token_classification.html) from the official [cleanlab library](https://github.com/cleanlab/cleanlab).   #### Install Cleanlab Package  ---  Install the Cleanlab version used for our experiments: `pip install .",cleanlab,SOFTWARE
"To find label errors in your own token classification data, you should instead use [the implementation](https://docs.cleanlab.ai/stable/tutorials/token_classification.html) from the official [cleanlab library](https://github.com/cleanlab/cleanlab).   #### Install Cleanlab Package  ---  Install the Cleanlab version used for our experiments: `pip install .",cleanlab,SOFTWARE
"To find label errors in your own token classification data, you should instead use [the implementation](https://docs.cleanlab.ai/stable/tutorials/token_classification.html) from the official [cleanlab library](https://github.com/cleanlab/cleanlab).   #### Install Cleanlab Package  ---  Install the Cleanlab version used for our experiments: `pip install .",Cleanlab,SOFTWARE
"To find label errors in your own token classification data, you should instead use [the implementation](https://docs.cleanlab.ai/stable/tutorials/token_classification.html) from the official [cleanlab library](https://github.com/cleanlab/cleanlab).   #### Install Cleanlab Package  ---  Install the Cleanlab version used for our experiments: `pip install .",Cleanlab,SOFTWARE
/cleanlab`  #### Download Datasets  --- CoNLL-2003:  - Original paper: [Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition](https://arxiv.org/pdf/cs/0306050v1.pdf)  - Original dataset: [Papers with Code](https://paperswithcode.com/dataset/conll-2003),cleanlab,SOFTWARE
"Model zoo: Please refer to [MODEL_ZOO.md](assets/MODEL_ZOO.md) for more details.  ## Citing UNINEXT If you find UNINEXT useful in your research, please consider citing: ```bibtex @inproceedings{UNINEXT,   title={Universal Instance Perception as Object Discovery and Retrieval},   author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Yuan, Zehuan and Luo, Ping and Lu, Huchuan},   booktitle={CVPR},   year={2023} } ```  ## Acknowledgments - Thanks [Unicorn](https://github.com/MasterBin-IIAU/Unicorn) for providing experience of unifying four object tracking tasks (SOT, MOT, VOS, MOTS). - Thanks [VNext](https://github.com/wjf5203/VNext) for providing experience of Video Instance Segmentation (VIS). - Thanks [ReferFormer](https://github.com/wjn922/ReferFormer) for providing experience of REC, RES, and R-VOS. - Thanks [GLIP](https://github.com/microsoft/GLIP) for the idea of unifying object detection and phrase grounding. - Thanks [Detic](https://github.com/facebookresearch/Detic) for the implementation of multi-dataset training. - Thanks [detrex](https://github.com/IDEA-Research/detrex) for the implementation of denoising mechnism.",Unicorn,SOFTWARE
"Model zoo: Please refer to [MODEL_ZOO.md](assets/MODEL_ZOO.md) for more details.  ## Citing UNINEXT If you find UNINEXT useful in your research, please consider citing: ```bibtex @inproceedings{UNINEXT,   title={Universal Instance Perception as Object Discovery and Retrieval},   author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Yuan, Zehuan and Luo, Ping and Lu, Huchuan},   booktitle={CVPR},   year={2023} } ```  ## Acknowledgments - Thanks [Unicorn](https://github.com/MasterBin-IIAU/Unicorn) for providing experience of unifying four object tracking tasks (SOT, MOT, VOS, MOTS). - Thanks [VNext](https://github.com/wjf5203/VNext) for providing experience of Video Instance Segmentation (VIS). - Thanks [ReferFormer](https://github.com/wjn922/ReferFormer) for providing experience of REC, RES, and R-VOS. - Thanks [GLIP](https://github.com/microsoft/GLIP) for the idea of unifying object detection and phrase grounding. - Thanks [Detic](https://github.com/facebookresearch/Detic) for the implementation of multi-dataset training. - Thanks [detrex](https://github.com/IDEA-Research/detrex) for the implementation of denoising mechnism.",Unicorn,SOFTWARE
"Model zoo: Please refer to [MODEL_ZOO.md](assets/MODEL_ZOO.md) for more details.  ## Citing UNINEXT If you find UNINEXT useful in your research, please consider citing: ```bibtex @inproceedings{UNINEXT,   title={Universal Instance Perception as Object Discovery and Retrieval},   author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Yuan, Zehuan and Luo, Ping and Lu, Huchuan},   booktitle={CVPR},   year={2023} } ```  ## Acknowledgments - Thanks [Unicorn](https://github.com/MasterBin-IIAU/Unicorn) for providing experience of unifying four object tracking tasks (SOT, MOT, VOS, MOTS). - Thanks [VNext](https://github.com/wjf5203/VNext) for providing experience of Video Instance Segmentation (VIS). - Thanks [ReferFormer](https://github.com/wjn922/ReferFormer) for providing experience of REC, RES, and R-VOS. - Thanks [GLIP](https://github.com/microsoft/GLIP) for the idea of unifying object detection and phrase grounding. - Thanks [Detic](https://github.com/facebookresearch/Detic) for the implementation of multi-dataset training. - Thanks [detrex](https://github.com/IDEA-Research/detrex) for the implementation of denoising mechnism.",VNext,SOFTWARE
"Model zoo: Please refer to [MODEL_ZOO.md](assets/MODEL_ZOO.md) for more details.  ## Citing UNINEXT If you find UNINEXT useful in your research, please consider citing: ```bibtex @inproceedings{UNINEXT,   title={Universal Instance Perception as Object Discovery and Retrieval},   author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Yuan, Zehuan and Luo, Ping and Lu, Huchuan},   booktitle={CVPR},   year={2023} } ```  ## Acknowledgments - Thanks [Unicorn](https://github.com/MasterBin-IIAU/Unicorn) for providing experience of unifying four object tracking tasks (SOT, MOT, VOS, MOTS). - Thanks [VNext](https://github.com/wjf5203/VNext) for providing experience of Video Instance Segmentation (VIS). - Thanks [ReferFormer](https://github.com/wjn922/ReferFormer) for providing experience of REC, RES, and R-VOS. - Thanks [GLIP](https://github.com/microsoft/GLIP) for the idea of unifying object detection and phrase grounding. - Thanks [Detic](https://github.com/facebookresearch/Detic) for the implementation of multi-dataset training. - Thanks [detrex](https://github.com/IDEA-Research/detrex) for the implementation of denoising mechnism.",VNext,SOFTWARE
"Model zoo: Please refer to [MODEL_ZOO.md](assets/MODEL_ZOO.md) for more details.  ## Citing UNINEXT If you find UNINEXT useful in your research, please consider citing: ```bibtex @inproceedings{UNINEXT,   title={Universal Instance Perception as Object Discovery and Retrieval},   author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Yuan, Zehuan and Luo, Ping and Lu, Huchuan},   booktitle={CVPR},   year={2023} } ```  ## Acknowledgments - Thanks [Unicorn](https://github.com/MasterBin-IIAU/Unicorn) for providing experience of unifying four object tracking tasks (SOT, MOT, VOS, MOTS). - Thanks [VNext](https://github.com/wjf5203/VNext) for providing experience of Video Instance Segmentation (VIS). - Thanks [ReferFormer](https://github.com/wjn922/ReferFormer) for providing experience of REC, RES, and R-VOS. - Thanks [GLIP](https://github.com/microsoft/GLIP) for the idea of unifying object detection and phrase grounding. - Thanks [Detic](https://github.com/facebookresearch/Detic) for the implementation of multi-dataset training. - Thanks [detrex](https://github.com/IDEA-Research/detrex) for the implementation of denoising mechnism.",ReferFormer,SOFTWARE
"Model zoo: Please refer to [MODEL_ZOO.md](assets/MODEL_ZOO.md) for more details.  ## Citing UNINEXT If you find UNINEXT useful in your research, please consider citing: ```bibtex @inproceedings{UNINEXT,   title={Universal Instance Perception as Object Discovery and Retrieval},   author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Yuan, Zehuan and Luo, Ping and Lu, Huchuan},   booktitle={CVPR},   year={2023} } ```  ## Acknowledgments - Thanks [Unicorn](https://github.com/MasterBin-IIAU/Unicorn) for providing experience of unifying four object tracking tasks (SOT, MOT, VOS, MOTS). - Thanks [VNext](https://github.com/wjf5203/VNext) for providing experience of Video Instance Segmentation (VIS). - Thanks [ReferFormer](https://github.com/wjn922/ReferFormer) for providing experience of REC, RES, and R-VOS. - Thanks [GLIP](https://github.com/microsoft/GLIP) for the idea of unifying object detection and phrase grounding. - Thanks [Detic](https://github.com/facebookresearch/Detic) for the implementation of multi-dataset training. - Thanks [detrex](https://github.com/IDEA-Research/detrex) for the implementation of denoising mechnism.",ReferFormer,SOFTWARE
"Model zoo: Please refer to [MODEL_ZOO.md](assets/MODEL_ZOO.md) for more details.  ## Citing UNINEXT If you find UNINEXT useful in your research, please consider citing: ```bibtex @inproceedings{UNINEXT,   title={Universal Instance Perception as Object Discovery and Retrieval},   author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Yuan, Zehuan and Luo, Ping and Lu, Huchuan},   booktitle={CVPR},   year={2023} } ```  ## Acknowledgments - Thanks [Unicorn](https://github.com/MasterBin-IIAU/Unicorn) for providing experience of unifying four object tracking tasks (SOT, MOT, VOS, MOTS). - Thanks [VNext](https://github.com/wjf5203/VNext) for providing experience of Video Instance Segmentation (VIS). - Thanks [ReferFormer](https://github.com/wjn922/ReferFormer) for providing experience of REC, RES, and R-VOS. - Thanks [GLIP](https://github.com/microsoft/GLIP) for the idea of unifying object detection and phrase grounding. - Thanks [Detic](https://github.com/facebookresearch/Detic) for the implementation of multi-dataset training. - Thanks [detrex](https://github.com/IDEA-Research/detrex) for the implementation of denoising mechnism.",GLIP,SOFTWARE
"Model zoo: Please refer to [MODEL_ZOO.md](assets/MODEL_ZOO.md) for more details.  ## Citing UNINEXT If you find UNINEXT useful in your research, please consider citing: ```bibtex @inproceedings{UNINEXT,   title={Universal Instance Perception as Object Discovery and Retrieval},   author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Yuan, Zehuan and Luo, Ping and Lu, Huchuan},   booktitle={CVPR},   year={2023} } ```  ## Acknowledgments - Thanks [Unicorn](https://github.com/MasterBin-IIAU/Unicorn) for providing experience of unifying four object tracking tasks (SOT, MOT, VOS, MOTS). - Thanks [VNext](https://github.com/wjf5203/VNext) for providing experience of Video Instance Segmentation (VIS). - Thanks [ReferFormer](https://github.com/wjn922/ReferFormer) for providing experience of REC, RES, and R-VOS. - Thanks [GLIP](https://github.com/microsoft/GLIP) for the idea of unifying object detection and phrase grounding. - Thanks [Detic](https://github.com/facebookresearch/Detic) for the implementation of multi-dataset training. - Thanks [detrex](https://github.com/IDEA-Research/detrex) for the implementation of denoising mechnism.",GLIP,SOFTWARE
"Model zoo: Please refer to [MODEL_ZOO.md](assets/MODEL_ZOO.md) for more details.  ## Citing UNINEXT If you find UNINEXT useful in your research, please consider citing: ```bibtex @inproceedings{UNINEXT,   title={Universal Instance Perception as Object Discovery and Retrieval},   author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Yuan, Zehuan and Luo, Ping and Lu, Huchuan},   booktitle={CVPR},   year={2023} } ```  ## Acknowledgments - Thanks [Unicorn](https://github.com/MasterBin-IIAU/Unicorn) for providing experience of unifying four object tracking tasks (SOT, MOT, VOS, MOTS). - Thanks [VNext](https://github.com/wjf5203/VNext) for providing experience of Video Instance Segmentation (VIS). - Thanks [ReferFormer](https://github.com/wjn922/ReferFormer) for providing experience of REC, RES, and R-VOS. - Thanks [GLIP](https://github.com/microsoft/GLIP) for the idea of unifying object detection and phrase grounding. - Thanks [Detic](https://github.com/facebookresearch/Detic) for the implementation of multi-dataset training. - Thanks [detrex](https://github.com/IDEA-Research/detrex) for the implementation of denoising mechnism.",Detic,SOFTWARE
"Model zoo: Please refer to [MODEL_ZOO.md](assets/MODEL_ZOO.md) for more details.  ## Citing UNINEXT If you find UNINEXT useful in your research, please consider citing: ```bibtex @inproceedings{UNINEXT,   title={Universal Instance Perception as Object Discovery and Retrieval},   author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Yuan, Zehuan and Luo, Ping and Lu, Huchuan},   booktitle={CVPR},   year={2023} } ```  ## Acknowledgments - Thanks [Unicorn](https://github.com/MasterBin-IIAU/Unicorn) for providing experience of unifying four object tracking tasks (SOT, MOT, VOS, MOTS). - Thanks [VNext](https://github.com/wjf5203/VNext) for providing experience of Video Instance Segmentation (VIS). - Thanks [ReferFormer](https://github.com/wjn922/ReferFormer) for providing experience of REC, RES, and R-VOS. - Thanks [GLIP](https://github.com/microsoft/GLIP) for the idea of unifying object detection and phrase grounding. - Thanks [Detic](https://github.com/facebookresearch/Detic) for the implementation of multi-dataset training. - Thanks [detrex](https://github.com/IDEA-Research/detrex) for the implementation of denoising mechnism.",Detic,SOFTWARE
"Model zoo: Please refer to [MODEL_ZOO.md](assets/MODEL_ZOO.md) for more details.  ## Citing UNINEXT If you find UNINEXT useful in your research, please consider citing: ```bibtex @inproceedings{UNINEXT,   title={Universal Instance Perception as Object Discovery and Retrieval},   author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Yuan, Zehuan and Luo, Ping and Lu, Huchuan},   booktitle={CVPR},   year={2023} } ```  ## Acknowledgments - Thanks [Unicorn](https://github.com/MasterBin-IIAU/Unicorn) for providing experience of unifying four object tracking tasks (SOT, MOT, VOS, MOTS). - Thanks [VNext](https://github.com/wjf5203/VNext) for providing experience of Video Instance Segmentation (VIS). - Thanks [ReferFormer](https://github.com/wjn922/ReferFormer) for providing experience of REC, RES, and R-VOS. - Thanks [GLIP](https://github.com/microsoft/GLIP) for the idea of unifying object detection and phrase grounding. - Thanks [Detic](https://github.com/facebookresearch/Detic) for the implementation of multi-dataset training. - Thanks [detrex](https://github.com/IDEA-Research/detrex) for the implementation of denoising mechnism.",detrex,SOFTWARE
"Model zoo: Please refer to [MODEL_ZOO.md](assets/MODEL_ZOO.md) for more details.  ## Citing UNINEXT If you find UNINEXT useful in your research, please consider citing: ```bibtex @inproceedings{UNINEXT,   title={Universal Instance Perception as Object Discovery and Retrieval},   author={Yan, Bin and Jiang, Yi and Wu, Jiannan and Wang, Dong and Yuan, Zehuan and Luo, Ping and Lu, Huchuan},   booktitle={CVPR},   year={2023} } ```  ## Acknowledgments - Thanks [Unicorn](https://github.com/MasterBin-IIAU/Unicorn) for providing experience of unifying four object tracking tasks (SOT, MOT, VOS, MOTS). - Thanks [VNext](https://github.com/wjf5203/VNext) for providing experience of Video Instance Segmentation (VIS). - Thanks [ReferFormer](https://github.com/wjn922/ReferFormer) for providing experience of REC, RES, and R-VOS. - Thanks [GLIP](https://github.com/microsoft/GLIP) for the idea of unifying object detection and phrase grounding. - Thanks [Detic](https://github.com/facebookresearch/Detic) for the implementation of multi-dataset training. - Thanks [detrex](https://github.com/IDEA-Research/detrex) for the implementation of denoising mechnism.",detrex,SOFTWARE
"The code use to run the learning-based models are found in the following repository forks: - [FCGF](https://github.com/luxiya01/FCGF/tree/mbes_data) - [DGR](https://github.com/luxiya01/DeepGlobalRegistration/tree/mbes_dataset) - [Predator](https://github.com/luxiya01/OverlapPredator/tree/mbes_data) - [BathyNN](https://github.com/luxiya01/bathy_nn_learning/tree/mbes-data)  The dataset, pretrained models and evaluation results can be found [here](https://kth-my.sharepoint.com/:f:/g/personal/liling_ug_kth_se/EpXHLtknBFVIpvBIdMcNSOMBu8SPQIOt7tUNeUvQwB-O8g?",FCGF,SOFTWARE
"The code use to run the learning-based models are found in the following repository forks: - [FCGF](https://github.com/luxiya01/FCGF/tree/mbes_data) - [DGR](https://github.com/luxiya01/DeepGlobalRegistration/tree/mbes_dataset) - [Predator](https://github.com/luxiya01/OverlapPredator/tree/mbes_data) - [BathyNN](https://github.com/luxiya01/bathy_nn_learning/tree/mbes-data)  The dataset, pretrained models and evaluation results can be found [here](https://kth-my.sharepoint.com/:f:/g/personal/liling_ug_kth_se/EpXHLtknBFVIpvBIdMcNSOMBu8SPQIOt7tUNeUvQwB-O8g?",DGR,SOFTWARE
"The code use to run the learning-based models are found in the following repository forks: - [FCGF](https://github.com/luxiya01/FCGF/tree/mbes_data) - [DGR](https://github.com/luxiya01/DeepGlobalRegistration/tree/mbes_dataset) - [Predator](https://github.com/luxiya01/OverlapPredator/tree/mbes_data) - [BathyNN](https://github.com/luxiya01/bathy_nn_learning/tree/mbes-data)  The dataset, pretrained models and evaluation results can be found [here](https://kth-my.sharepoint.com/:f:/g/personal/liling_ug_kth_se/EpXHLtknBFVIpvBIdMcNSOMBu8SPQIOt7tUNeUvQwB-O8g?",Predator,SOFTWARE
"The code use to run the learning-based models are found in the following repository forks: - [FCGF](https://github.com/luxiya01/FCGF/tree/mbes_data) - [DGR](https://github.com/luxiya01/DeepGlobalRegistration/tree/mbes_dataset) - [Predator](https://github.com/luxiya01/OverlapPredator/tree/mbes_data) - [BathyNN](https://github.com/luxiya01/bathy_nn_learning/tree/mbes-data)  The dataset, pretrained models and evaluation results can be found [here](https://kth-my.sharepoint.com/:f:/g/personal/liling_ug_kth_se/EpXHLtknBFVIpvBIdMcNSOMBu8SPQIOt7tUNeUvQwB-O8g?",BathyNN,SOFTWARE
"If you want the exact data pairs and transforms as used in the paper, you can also extract these from the _npz_ files containing in each method's evaluation results.  ## Citation If you find this code useful for your work, please consider citing: ```bibtex @inproceedings{ling2024benchmarking,             title={Benchmarking Classical and Learning-Based Multibeam Point Cloud Registration},              author={Ling, Li and Zhang, Jun and Bore, Nils and Folkesson, John and W√•hlin, Anna},             booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},             year={2024},             organization={IEEE} } ```  ## Acknowledgements In this project, we use part of the official implementations of the following work: - [FCGF](https://github.com/chrischoy/FCGF) - [DGR](https://github.com/chrischoy/DeepGlobalRegistration) - [Predator](https://github.com/prs-eth/OverlapPredator) - [BathyNN](https://github.com/tjr16/bathy_nn_learning)  We thank the respective authors for open sourcing their work.",FCGF,SOFTWARE
"If you want the exact data pairs and transforms as used in the paper, you can also extract these from the _npz_ files containing in each method's evaluation results.  ## Citation If you find this code useful for your work, please consider citing: ```bibtex @inproceedings{ling2024benchmarking,             title={Benchmarking Classical and Learning-Based Multibeam Point Cloud Registration},              author={Ling, Li and Zhang, Jun and Bore, Nils and Folkesson, John and W√•hlin, Anna},             booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},             year={2024},             organization={IEEE} } ```  ## Acknowledgements In this project, we use part of the official implementations of the following work: - [FCGF](https://github.com/chrischoy/FCGF) - [DGR](https://github.com/chrischoy/DeepGlobalRegistration) - [Predator](https://github.com/prs-eth/OverlapPredator) - [BathyNN](https://github.com/tjr16/bathy_nn_learning)  We thank the respective authors for open sourcing their work.",DGR,SOFTWARE
"If you want the exact data pairs and transforms as used in the paper, you can also extract these from the _npz_ files containing in each method's evaluation results.  ## Citation If you find this code useful for your work, please consider citing: ```bibtex @inproceedings{ling2024benchmarking,             title={Benchmarking Classical and Learning-Based Multibeam Point Cloud Registration},              author={Ling, Li and Zhang, Jun and Bore, Nils and Folkesson, John and W√•hlin, Anna},             booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},             year={2024},             organization={IEEE} } ```  ## Acknowledgements In this project, we use part of the official implementations of the following work: - [FCGF](https://github.com/chrischoy/FCGF) - [DGR](https://github.com/chrischoy/DeepGlobalRegistration) - [Predator](https://github.com/prs-eth/OverlapPredator) - [BathyNN](https://github.com/tjr16/bathy_nn_learning)  We thank the respective authors for open sourcing their work.",Predator,SOFTWARE
"If you want the exact data pairs and transforms as used in the paper, you can also extract these from the _npz_ files containing in each method's evaluation results.  ## Citation If you find this code useful for your work, please consider citing: ```bibtex @inproceedings{ling2024benchmarking,             title={Benchmarking Classical and Learning-Based Multibeam Point Cloud Registration},              author={Ling, Li and Zhang, Jun and Bore, Nils and Folkesson, John and W√•hlin, Anna},             booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},             year={2024},             organization={IEEE} } ```  ## Acknowledgements In this project, we use part of the official implementations of the following work: - [FCGF](https://github.com/chrischoy/FCGF) - [DGR](https://github.com/chrischoy/DeepGlobalRegistration) - [Predator](https://github.com/prs-eth/OverlapPredator) - [BathyNN](https://github.com/tjr16/bathy_nn_learning)  We thank the respective authors for open sourcing their work.",BathyNN,SOFTWARE
"You can find an online version of our paper at arXiv: https://arxiv.org/abs/1904.10678  **If you use our method, please cite our paper.**  ---  ## Table of Contents  1. [ Dependencies, pre-requisites, and setting up the code ](#dependencies-pre-requisites-and-setting-up-the-code) 2. [ Reproduce the results of the paper ](#reproduce-the-results-of-the-paper) 3. [ Use the code with your own data ](#use-the-code-with-your-own-data) 4. [ Previous work ](#previous-work) 5. [ Acknowledgement ](#acknowledgement)  ---  ## Dependencies, pre-requisites, and setting up the code   In order to use our code, you have to firstly:   * use Python 3.x and install all the required packages listed at the [requirements file for PiP](https://github.com/dr-costas/undaw//blob/master/requirements.txt) or at the [requirements file for Anaconda](https://github.com/dr-costas/undaw//blob/master/conda_requirements.txt)  * download the data (the file ``AUDASC_features_labels.zip``) from  [!",PiP,SOFTWARE
"You can find an online version of our paper at arXiv: https://arxiv.org/abs/1904.10678  **If you use our method, please cite our paper.**  ---  ## Table of Contents  1. [ Dependencies, pre-requisites, and setting up the code ](#dependencies-pre-requisites-and-setting-up-the-code) 2. [ Reproduce the results of the paper ](#reproduce-the-results-of-the-paper) 3. [ Use the code with your own data ](#use-the-code-with-your-own-data) 4. [ Previous work ](#previous-work) 5. [ Acknowledgement ](#acknowledgement)  ---  ## Dependencies, pre-requisites, and setting up the code   In order to use our code, you have to firstly:   * use Python 3.x and install all the required packages listed at the [requirements file for PiP](https://github.com/dr-costas/undaw//blob/master/requirements.txt) or at the [requirements file for Anaconda](https://github.com/dr-costas/undaw//blob/master/conda_requirements.txt)  * download the data (the file ``AUDASC_features_labels.zip``) from  [!",Anaconda,SOFTWARE
"You can find an online version of our paper at arXiv: https://arxiv.org/abs/1904.10678  **If you use our method, please cite our paper.**  ---  ## Table of Contents  1. [ Dependencies, pre-requisites, and setting up the code ](#dependencies-pre-requisites-and-setting-up-the-code) 2. [ Reproduce the results of the paper ](#reproduce-the-results-of-the-paper) 3. [ Use the code with your own data ](#use-the-code-with-your-own-data) 4. [ Previous work ](#previous-work) 5. [ Acknowledgement ](#acknowledgement)  ---  ## Dependencies, pre-requisites, and setting up the code   In order to use our code, you have to firstly:   * use Python 3.x and install all the required packages listed at the [requirements file for PiP](https://github.com/dr-costas/undaw//blob/master/requirements.txt) or at the [requirements file for Anaconda](https://github.com/dr-costas/undaw//blob/master/conda_requirements.txt)  * download the data (the file ``AUDASC_features_labels.zip``) from  [!",conda,SOFTWARE
"For example, like:       python scripts/main.py --config-file new_settings_file  Notice that the file name is without extension, meaning that only YAML (i.e. `*.yaml` extension) files can be used.",python,SOFTWARE
"The paper is available on: - *Computer Vision Foundation (CVF)*: https://openaccess.thecvf.com/content/WACV2021/html/Muller-Budack_Ontology-Driven_Event_Type_Classification_in_Images_WACV_2021_paper.html - *arXiv*: https://arxiv.org/pdf/2011.04714.pdf  Further information can be found on the **EventKG** website: http://eventkg.l3s.uni-hannover.de/VisE   ## Content  - [Ontology-driven Event Type Classification in Images](#ontology-driven-event-type-classification-in-images)   - [Content](#content)   - [Setup](#setup)     - [Setup with Singularity (for Reproducibility)](#setup-with-singularity-for-reproducibility)     - [Setup with Virtual Environment](#setup-with-virtual-environment)     - [Setup with Docker](#setup-with-docker)   - [Download Ontology, Dataset and Models](#download-ontology-dataset-and-models)   - [Models](#models)   - [Inference](#inference)   - [Test](#test)   - [VisE-D: Visual Event Classification Dataset](#vise-d-visual-event-classification-dataset)   - [VisE-O: Visual Event Ontology](#vise-o-visual-event-ontology)   - [Benchmark Ontologies](#benchmark-ontologies)   - [Supplemental Material](#supplemental-material)   - [LICENSE](#license)   ## Setup  We provide three different ways to setup the project.",Docker,SOFTWARE
The results can be reproduced using the [setup with singularity](#setup-with-singularity-for-reproducibility).,singularity,SOFTWARE
The results can be reproduced using the [setup with singularity](#setup-with-singularity-for-reproducibility).,singularity,SOFTWARE
"The singularity image is built with an optimized pytorch implementation on arch linux, which we used for training and testing.",singularity,SOFTWARE
"The singularity image is built with an optimized pytorch implementation on arch linux, which we used for training and testing.",pytorch,SOFTWARE
"While the other two setups using a [virtual environment](#setup-with-virtual-environment) or [docker](#setup-with-docker) produce the same result on our testsets, they slightly differ from the results reported in the paper (deviation around 0.1%).   ### Setup with Singularity (for Reproducibility)  To install singularity please follow the instructions on: https://sylabs.io/guides/3.6/admin-guide/installation.html  Download our singularity image from: [link](https://tib.eu/cloud/s/fPMwLMWo3wCmMRy/download) (Filesize is 5 GB)  To run code using sinularity, please run:   ```shell script singularity exec \   -B </PATH/TO/REPOSITORY>:/src \   --nv </PATH/TO/SINGULARITY/IMAGE>.sif \   bash  cd /src ```   ### Setup with Virtual Environment  Please run the following command to setup the project in your (virtual) environment:  ```shell script pip install -r requirements.txt ```  **NOTE:** This setup produces slightly different results (deviation around 0.1%) while [testing](#test).",docker,SOFTWARE
"While the other two setups using a [virtual environment](#setup-with-virtual-environment) or [docker](#setup-with-docker) produce the same result on our testsets, they slightly differ from the results reported in the paper (deviation around 0.1%).   ### Setup with Singularity (for Reproducibility)  To install singularity please follow the instructions on: https://sylabs.io/guides/3.6/admin-guide/installation.html  Download our singularity image from: [link](https://tib.eu/cloud/s/fPMwLMWo3wCmMRy/download) (Filesize is 5 GB)  To run code using sinularity, please run:   ```shell script singularity exec \   -B </PATH/TO/REPOSITORY>:/src \   --nv </PATH/TO/SINGULARITY/IMAGE>.sif \   bash  cd /src ```   ### Setup with Virtual Environment  Please run the following command to setup the project in your (virtual) environment:  ```shell script pip install -r requirements.txt ```  **NOTE:** This setup produces slightly different results (deviation around 0.1%) while [testing](#test).",docker,SOFTWARE
"While the other two setups using a [virtual environment](#setup-with-virtual-environment) or [docker](#setup-with-docker) produce the same result on our testsets, they slightly differ from the results reported in the paper (deviation around 0.1%).   ### Setup with Singularity (for Reproducibility)  To install singularity please follow the instructions on: https://sylabs.io/guides/3.6/admin-guide/installation.html  Download our singularity image from: [link](https://tib.eu/cloud/s/fPMwLMWo3wCmMRy/download) (Filesize is 5 GB)  To run code using sinularity, please run:   ```shell script singularity exec \   -B </PATH/TO/REPOSITORY>:/src \   --nv </PATH/TO/SINGULARITY/IMAGE>.sif \   bash  cd /src ```   ### Setup with Virtual Environment  Please run the following command to setup the project in your (virtual) environment:  ```shell script pip install -r requirements.txt ```  **NOTE:** This setup produces slightly different results (deviation around 0.1%) while [testing](#test).",Singularity,SOFTWARE
"While the other two setups using a [virtual environment](#setup-with-virtual-environment) or [docker](#setup-with-docker) produce the same result on our testsets, they slightly differ from the results reported in the paper (deviation around 0.1%).   ### Setup with Singularity (for Reproducibility)  To install singularity please follow the instructions on: https://sylabs.io/guides/3.6/admin-guide/installation.html  Download our singularity image from: [link](https://tib.eu/cloud/s/fPMwLMWo3wCmMRy/download) (Filesize is 5 GB)  To run code using sinularity, please run:   ```shell script singularity exec \   -B </PATH/TO/REPOSITORY>:/src \   --nv </PATH/TO/SINGULARITY/IMAGE>.sif \   bash  cd /src ```   ### Setup with Virtual Environment  Please run the following command to setup the project in your (virtual) environment:  ```shell script pip install -r requirements.txt ```  **NOTE:** This setup produces slightly different results (deviation around 0.1%) while [testing](#test).",singularity,SOFTWARE
"While the other two setups using a [virtual environment](#setup-with-virtual-environment) or [docker](#setup-with-docker) produce the same result on our testsets, they slightly differ from the results reported in the paper (deviation around 0.1%).   ### Setup with Singularity (for Reproducibility)  To install singularity please follow the instructions on: https://sylabs.io/guides/3.6/admin-guide/installation.html  Download our singularity image from: [link](https://tib.eu/cloud/s/fPMwLMWo3wCmMRy/download) (Filesize is 5 GB)  To run code using sinularity, please run:   ```shell script singularity exec \   -B </PATH/TO/REPOSITORY>:/src \   --nv </PATH/TO/SINGULARITY/IMAGE>.sif \   bash  cd /src ```   ### Setup with Virtual Environment  Please run the following command to setup the project in your (virtual) environment:  ```shell script pip install -r requirements.txt ```  **NOTE:** This setup produces slightly different results (deviation around 0.1%) while [testing](#test).",singularity,SOFTWARE
"While the other two setups using a [virtual environment](#setup-with-virtual-environment) or [docker](#setup-with-docker) produce the same result on our testsets, they slightly differ from the results reported in the paper (deviation around 0.1%).   ### Setup with Singularity (for Reproducibility)  To install singularity please follow the instructions on: https://sylabs.io/guides/3.6/admin-guide/installation.html  Download our singularity image from: [link](https://tib.eu/cloud/s/fPMwLMWo3wCmMRy/download) (Filesize is 5 GB)  To run code using sinularity, please run:   ```shell script singularity exec \   -B </PATH/TO/REPOSITORY>:/src \   --nv </PATH/TO/SINGULARITY/IMAGE>.sif \   bash  cd /src ```   ### Setup with Virtual Environment  Please run the following command to setup the project in your (virtual) environment:  ```shell script pip install -r requirements.txt ```  **NOTE:** This setup produces slightly different results (deviation around 0.1%) while [testing](#test).",sinularity,SOFTWARE
"While the other two setups using a [virtual environment](#setup-with-virtual-environment) or [docker](#setup-with-docker) produce the same result on our testsets, they slightly differ from the results reported in the paper (deviation around 0.1%).   ### Setup with Singularity (for Reproducibility)  To install singularity please follow the instructions on: https://sylabs.io/guides/3.6/admin-guide/installation.html  Download our singularity image from: [link](https://tib.eu/cloud/s/fPMwLMWo3wCmMRy/download) (Filesize is 5 GB)  To run code using sinularity, please run:   ```shell script singularity exec \   -B </PATH/TO/REPOSITORY>:/src \   --nv </PATH/TO/SINGULARITY/IMAGE>.sif \   bash  cd /src ```   ### Setup with Virtual Environment  Please run the following command to setup the project in your (virtual) environment:  ```shell script pip install -r requirements.txt ```  **NOTE:** This setup produces slightly different results (deviation around 0.1%) while [testing](#test).",singularity,SOFTWARE
"While the other two setups using a [virtual environment](#setup-with-virtual-environment) or [docker](#setup-with-docker) produce the same result on our testsets, they slightly differ from the results reported in the paper (deviation around 0.1%).   ### Setup with Singularity (for Reproducibility)  To install singularity please follow the instructions on: https://sylabs.io/guides/3.6/admin-guide/installation.html  Download our singularity image from: [link](https://tib.eu/cloud/s/fPMwLMWo3wCmMRy/download) (Filesize is 5 GB)  To run code using sinularity, please run:   ```shell script singularity exec \   -B </PATH/TO/REPOSITORY>:/src \   --nv </PATH/TO/SINGULARITY/IMAGE>.sif \   bash  cd /src ```   ### Setup with Virtual Environment  Please run the following command to setup the project in your (virtual) environment:  ```shell script pip install -r requirements.txt ```  **NOTE:** This setup produces slightly different results (deviation around 0.1%) while [testing](#test).",pip,SOFTWARE
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ### Setup with Docker  We have provided a Docker container to execute our code.",singularity,SOFTWARE
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ### Setup with Docker  We have provided a Docker container to execute our code.",singularity,SOFTWARE
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ### Setup with Docker  We have provided a Docker container to execute our code.",pytorch,SOFTWARE
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ### Setup with Docker  We have provided a Docker container to execute our code.",Docker,SOFTWARE
"To fully reproduce our results we have provided a [singularity image](#setup-with-singularity-for-reproducibility), which is a copy of our training and testing environment and uses a highly optimized pytorch implementation.   ### Setup with Docker  We have provided a Docker container to execute our code.",Docker,SOFTWARE
You can build the container with:  ```shell script docker build <PATH/TO/REPOSITORY> -t <DOCKER_NAME> ```  To run the container please use:  ```shell script docker run \   --volume <PATH/TO/REPOSITORY>:/src \   --shm-size=256m \   -u $(id -u):$(id -g) \   -it <DOCKER_NAME> bash   cd /src ```  **NOTE:** This setup produces slightly different results (deviation around 0.1%) while [testing](#test).,docker,SOFTWARE
You can build the container with:  ```shell script docker build <PATH/TO/REPOSITORY> -t <DOCKER_NAME> ```  To run the container please use:  ```shell script docker run \   --volume <PATH/TO/REPOSITORY>:/src \   --shm-size=256m \   -u $(id -u):$(id -g) \   -it <DOCKER_NAME> bash   cd /src ```  **NOTE:** This setup produces slightly different results (deviation around 0.1%) while [testing](#test).,docker,SOFTWARE
"# Sparse Coding with Multi-Layer Decoders using Variance Regularization This is a PyTorch implementation for the setup described in  [Sparse Coding with Multi-Layer Decoders using Variance Regularization](https://arxiv.org/abs/2112.09214).   ### Requirements  - Python 3.7 - [PyTorch](https://pytorch.org/get-started/previous-versions/) 1.6.0 with torchvision 0.7.0 - Other dependencies: numpy, tensorboardX  ### Datasets  In our experiments, we use: - the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset.",PyTorch,SOFTWARE
"# Sparse Coding with Multi-Layer Decoders using Variance Regularization This is a PyTorch implementation for the setup described in  [Sparse Coding with Multi-Layer Decoders using Variance Regularization](https://arxiv.org/abs/2112.09214).   ### Requirements  - Python 3.7 - [PyTorch](https://pytorch.org/get-started/previous-versions/) 1.6.0 with torchvision 0.7.0 - Other dependencies: numpy, tensorboardX  ### Datasets  In our experiments, we use: - the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset.",PyTorch,SOFTWARE
"# Sparse Coding with Multi-Layer Decoders using Variance Regularization This is a PyTorch implementation for the setup described in  [Sparse Coding with Multi-Layer Decoders using Variance Regularization](https://arxiv.org/abs/2112.09214).   ### Requirements  - Python 3.7 - [PyTorch](https://pytorch.org/get-started/previous-versions/) 1.6.0 with torchvision 0.7.0 - Other dependencies: numpy, tensorboardX  ### Datasets  In our experiments, we use: - the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset.",pytorch,SOFTWARE
"# Sparse Coding with Multi-Layer Decoders using Variance Regularization This is a PyTorch implementation for the setup described in  [Sparse Coding with Multi-Layer Decoders using Variance Regularization](https://arxiv.org/abs/2112.09214).   ### Requirements  - Python 3.7 - [PyTorch](https://pytorch.org/get-started/previous-versions/) 1.6.0 with torchvision 0.7.0 - Other dependencies: numpy, tensorboardX  ### Datasets  In our experiments, we use: - the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset.",torchvision 0.7.0,SOFTWARE
"# Sparse Coding with Multi-Layer Decoders using Variance Regularization This is a PyTorch implementation for the setup described in  [Sparse Coding with Multi-Layer Decoders using Variance Regularization](https://arxiv.org/abs/2112.09214).   ### Requirements  - Python 3.7 - [PyTorch](https://pytorch.org/get-started/previous-versions/) 1.6.0 with torchvision 0.7.0 - Other dependencies: numpy, tensorboardX  ### Datasets  In our experiments, we use: - the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset.",numpy,SOFTWARE
"# Sparse Coding with Multi-Layer Decoders using Variance Regularization This is a PyTorch implementation for the setup described in  [Sparse Coding with Multi-Layer Decoders using Variance Regularization](https://arxiv.org/abs/2112.09214).   ### Requirements  - Python 3.7 - [PyTorch](https://pytorch.org/get-started/previous-versions/) 1.6.0 with torchvision 0.7.0 - Other dependencies: numpy, tensorboardX  ### Datasets  In our experiments, we use: - the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset.",tensorboardX,SOFTWARE
| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,SDL,SOFTWARE
| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,SDL-NL,SOFTWARE
| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,VDL,SOFTWARE
| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,VDL-NL,SOFTWARE
| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,SDL-NL,SOFTWARE
| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,VDL,SOFTWARE
| dataset          | model    | script | |------------------|----------|--------| | MNIST            | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL.sh)       | | MNIST            | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_SDL-NL.sh)       | | MNIST            | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL.sh)       | | MNIST            | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/MNIST_VDL-NL.sh)       | | ImageNet_patches | SDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL.sh)       | | ImageNet_patches | SDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_SDL-NL.sh)       | | ImageNet_patches | VDL      | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL.sh)       | | Imagenet_patches | VDL-NL   | [link](https://github.com/kevtimova/deep-sparse/blob/main/scripts/ImageNet_VDL-NL.sh)       |  ### Evaluation  We evaluate our pre-trained sparse autoencoders on the downstream tasks of denoising (for MNIST and our custom  ImageNet patches dataset) and classification in the low-data regime (for MNIST only).  #### Denoising  The denoising perfomance on the test set can be measured at the end of training by providing a list with levels of  random noise (measured by std of Gaussian noise; the noise is added to the input images) in the ```noise``` argument  in ```main.py```.,VDL-NL,SOFTWARE
"Based on [srsLTE][srslte] library, the software can be run on a plain x86 general-purpose PCs with any compatible SDR.    !",srsLTE,SOFTWARE
"Based on [srsLTE][srslte] library, the software can be run on a plain x86 general-purpose PCs with any compatible SDR.    !",srslte,SOFTWARE
[](https://img.shields.io/aur/maintainer/tudo-falcon)](https://aur.archlinux.org/packages/tudo-falcon)  ### Installation on Ubuntu  #### 1) Required Dependencies FALCON installation automatically downloads a patched version of srsLTE and c-mnalib as subproject during the build process.,falcon,SOFTWARE
[](https://img.shields.io/aur/maintainer/tudo-falcon)](https://aur.archlinux.org/packages/tudo-falcon)  ### Installation on Ubuntu  #### 1) Required Dependencies FALCON installation automatically downloads a patched version of srsLTE and c-mnalib as subproject during the build process.,falcon,SOFTWARE
[](https://img.shields.io/aur/maintainer/tudo-falcon)](https://aur.archlinux.org/packages/tudo-falcon)  ### Installation on Ubuntu  #### 1) Required Dependencies FALCON installation automatically downloads a patched version of srsLTE and c-mnalib as subproject during the build process.,FALCON,SOFTWARE
[](https://img.shields.io/aur/maintainer/tudo-falcon)](https://aur.archlinux.org/packages/tudo-falcon)  ### Installation on Ubuntu  #### 1) Required Dependencies FALCON installation automatically downloads a patched version of srsLTE and c-mnalib as subproject during the build process.,srsLTE,SOFTWARE
[](https://img.shields.io/aur/maintainer/tudo-falcon)](https://aur.archlinux.org/packages/tudo-falcon)  ### Installation on Ubuntu  #### 1) Required Dependencies FALCON installation automatically downloads a patched version of srsLTE and c-mnalib as subproject during the build process.,c-mnalib,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),FALCON,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),srsLTE,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),build-essential,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),git,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),cmake,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libfftw3-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libmbedtls-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libboost-program-options-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libconfig++-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libsctp-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),srsGUI,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libboost-system-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libboost-test-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libboost-thread-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libqwt-qt5-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),qtbase5-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),git,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),srsLTE,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),srsGUI,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),srsgui,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),cmake,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libuhd-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),uhd-host,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),LimeSDR,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),limesuite,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),limesuite-udev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),limesuite-images,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),soapysdr,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),soapysdr-module-lms7,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),FALCON,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libglib2.0-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libudev-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libcurl4-gnutls-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libboost-all-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),qtdeclarative5-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),libqt5charts5-dev,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),FALCON,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),git,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),falcon,SOFTWARE
Please install the following dependencies which are required by FALCON or its included components:  For srsLTE: ```sh sudo apt-get install build-essential git cmake libfftw3-dev libmbedtls-dev libboost-program-options-dev libconfig++-dev libsctp-dev ``` For srsGUI (only required to build the ported version of IMDEA OWL): ```sh sudo apt-get install libboost-system-dev libboost-test-dev libboost-thread-dev libqwt-qt5-dev qtbase5-dev git clone https://github.com/srsLTE/srsGUI.git cd srsgui mkdir build cd build cmake ../ make sudo make install ``` For USRP support: ```sh sudo add-apt-repository ppa:ettusresearch/uhd sudo apt-get update sudo apt-get install libuhd-dev uhd-host ```  For LimeSDR support: ```sh sudo add-apt-repository -y ppa:myriadrf/drivers sudo apt-get update sudo apt-get install limesuite limesuite-udev limesuite-images sudo apt-get install soapysdr soapysdr-module-lms7 ```  For FALCON: ```sh sudo apt-get install libglib2.0-dev libudev-dev libcurl4-gnutls-dev libboost-all-dev qtdeclarative5-dev libqt5charts5-dev ```  #### 2) FALCON: ```sh git clone https://github.com/falkenber9/falcon.git cd falcon mkdir build cd build cmake -DCMAKE_INSTALL_PREFIX=/usr ../ make  # Install (optional) sudo make install  # Uninstall (use carefully!),cmake,SOFTWARE
sudo xargs rm < install_manifest.txt ```  **Note:** FALCON requires a [patched version][srslte-falcon-patch] of srsLTE 18.09 that is automatically downloaded and included as subproject during the build process.,FALCON,SOFTWARE
sudo xargs rm < install_manifest.txt ```  **Note:** FALCON requires a [patched version][srslte-falcon-patch] of srsLTE 18.09 that is automatically downloaded and included as subproject during the build process.,srsLTE,SOFTWARE
"However if a diffferent version of srsLTE is already installed on the computer, the build system will link against that version.",srsLTE,SOFTWARE
"In case of conflicts, force the use of srsLTE as a subproject by adding ``-DFORCE_SUBPROJECT_SRSLTE=ON`` to the ``cmake`` options.",srsLTE,SOFTWARE
"In case of conflicts, force the use of srsLTE as a subproject by adding ``-DFORCE_SUBPROJECT_SRSLTE=ON`` to the ``cmake`` options.",SRSLTE,SOFTWARE
"In case of conflicts, force the use of srsLTE as a subproject by adding ``-DFORCE_SUBPROJECT_SRSLTE=ON`` to the ``cmake`` options.",cmake,SOFTWARE
"In this case, take care with ``make install``, since this may overwrite your existing version of srsLTE.",srsLTE,SOFTWARE
Example: ``` ... cmake -DFORCE_SUBPROJECT_SRSLTE=ON -DCMAKE_INSTALL_PREFIX=/usr ../ ... ```   ### Installation on Archlinux On Archlinux build and install the package ``tudo-falcon`` [!,SRSLTE,SOFTWARE
Example: ``` ... cmake -DFORCE_SUBPROJECT_SRSLTE=ON -DCMAKE_INSTALL_PREFIX=/usr ../ ... ```   ### Installation on Archlinux On Archlinux build and install the package ``tudo-falcon`` [!,Archlinux,SOFTWARE
Example: ``` ... cmake -DFORCE_SUBPROJECT_SRSLTE=ON -DCMAKE_INSTALL_PREFIX=/usr ../ ... ```   ### Installation on Archlinux On Archlinux build and install the package ``tudo-falcon`` [!,Archlinux,SOFTWARE
Example: ``` ... cmake -DFORCE_SUBPROJECT_SRSLTE=ON -DCMAKE_INSTALL_PREFIX=/usr ../ ... ```   ### Installation on Archlinux On Archlinux build and install the package ``tudo-falcon`` [!,tudo-falcon,SOFTWARE
"```sh # Install yay -Sy tudo-falcon  # Uninstall sudo pacman -Rs tudo-falcon ```  #### Installation without AUR Helper  Without an AUR Helper, the package(s) can be built in a local directory with ``makepkg`` and installed via ``pacman``: ```sh # Prerequisites pacman -S --needed base-devel  # Build directory mkdir falcon-packages cd falcon-packages  # Dependency c-mnalib git clone https://aur.archlinux.org/c-mnalib.git cd c-mnalib makepkg -si cd",yay,SOFTWARE
"```sh # Install yay -Sy tudo-falcon  # Uninstall sudo pacman -Rs tudo-falcon ```  #### Installation without AUR Helper  Without an AUR Helper, the package(s) can be built in a local directory with ``makepkg`` and installed via ``pacman``: ```sh # Prerequisites pacman -S --needed base-devel  # Build directory mkdir falcon-packages cd falcon-packages  # Dependency c-mnalib git clone https://aur.archlinux.org/c-mnalib.git cd c-mnalib makepkg -si cd",tudo-falcon,SOFTWARE
"```sh # Install yay -Sy tudo-falcon  # Uninstall sudo pacman -Rs tudo-falcon ```  #### Installation without AUR Helper  Without an AUR Helper, the package(s) can be built in a local directory with ``makepkg`` and installed via ``pacman``: ```sh # Prerequisites pacman -S --needed base-devel  # Build directory mkdir falcon-packages cd falcon-packages  # Dependency c-mnalib git clone https://aur.archlinux.org/c-mnalib.git cd c-mnalib makepkg -si cd",pacman,SOFTWARE
"```sh # Install yay -Sy tudo-falcon  # Uninstall sudo pacman -Rs tudo-falcon ```  #### Installation without AUR Helper  Without an AUR Helper, the package(s) can be built in a local directory with ``makepkg`` and installed via ``pacman``: ```sh # Prerequisites pacman -S --needed base-devel  # Build directory mkdir falcon-packages cd falcon-packages  # Dependency c-mnalib git clone https://aur.archlinux.org/c-mnalib.git cd c-mnalib makepkg -si cd",tudo-falcon,SOFTWARE
"```sh # Install yay -Sy tudo-falcon  # Uninstall sudo pacman -Rs tudo-falcon ```  #### Installation without AUR Helper  Without an AUR Helper, the package(s) can be built in a local directory with ``makepkg`` and installed via ``pacman``: ```sh # Prerequisites pacman -S --needed base-devel  # Build directory mkdir falcon-packages cd falcon-packages  # Dependency c-mnalib git clone https://aur.archlinux.org/c-mnalib.git cd c-mnalib makepkg -si cd",makepkg,SOFTWARE
"```sh # Install yay -Sy tudo-falcon  # Uninstall sudo pacman -Rs tudo-falcon ```  #### Installation without AUR Helper  Without an AUR Helper, the package(s) can be built in a local directory with ``makepkg`` and installed via ``pacman``: ```sh # Prerequisites pacman -S --needed base-devel  # Build directory mkdir falcon-packages cd falcon-packages  # Dependency c-mnalib git clone https://aur.archlinux.org/c-mnalib.git cd c-mnalib makepkg -si cd",pacman,SOFTWARE
"```sh # Install yay -Sy tudo-falcon  # Uninstall sudo pacman -Rs tudo-falcon ```  #### Installation without AUR Helper  Without an AUR Helper, the package(s) can be built in a local directory with ``makepkg`` and installed via ``pacman``: ```sh # Prerequisites pacman -S --needed base-devel  # Build directory mkdir falcon-packages cd falcon-packages  # Dependency c-mnalib git clone https://aur.archlinux.org/c-mnalib.git cd c-mnalib makepkg -si cd",falcon,SOFTWARE
"```sh # Install yay -Sy tudo-falcon  # Uninstall sudo pacman -Rs tudo-falcon ```  #### Installation without AUR Helper  Without an AUR Helper, the package(s) can be built in a local directory with ``makepkg`` and installed via ``pacman``: ```sh # Prerequisites pacman -S --needed base-devel  # Build directory mkdir falcon-packages cd falcon-packages  # Dependency c-mnalib git clone https://aur.archlinux.org/c-mnalib.git cd c-mnalib makepkg -si cd",c-mnalib,SOFTWARE
"```sh # Install yay -Sy tudo-falcon  # Uninstall sudo pacman -Rs tudo-falcon ```  #### Installation without AUR Helper  Without an AUR Helper, the package(s) can be built in a local directory with ``makepkg`` and installed via ``pacman``: ```sh # Prerequisites pacman -S --needed base-devel  # Build directory mkdir falcon-packages cd falcon-packages  # Dependency c-mnalib git clone https://aur.archlinux.org/c-mnalib.git cd c-mnalib makepkg -si cd",git,SOFTWARE
"```sh # Install yay -Sy tudo-falcon  # Uninstall sudo pacman -Rs tudo-falcon ```  #### Installation without AUR Helper  Without an AUR Helper, the package(s) can be built in a local directory with ``makepkg`` and installed via ``pacman``: ```sh # Prerequisites pacman -S --needed base-devel  # Build directory mkdir falcon-packages cd falcon-packages  # Dependency c-mnalib git clone https://aur.archlinux.org/c-mnalib.git cd c-mnalib makepkg -si cd",archlinux,SOFTWARE
"```sh # Install yay -Sy tudo-falcon  # Uninstall sudo pacman -Rs tudo-falcon ```  #### Installation without AUR Helper  Without an AUR Helper, the package(s) can be built in a local directory with ``makepkg`` and installed via ``pacman``: ```sh # Prerequisites pacman -S --needed base-devel  # Build directory mkdir falcon-packages cd falcon-packages  # Dependency c-mnalib git clone https://aur.archlinux.org/c-mnalib.git cd c-mnalib makepkg -si cd",c-mnalib,SOFTWARE
"```sh # Install yay -Sy tudo-falcon  # Uninstall sudo pacman -Rs tudo-falcon ```  #### Installation without AUR Helper  Without an AUR Helper, the package(s) can be built in a local directory with ``makepkg`` and installed via ``pacman``: ```sh # Prerequisites pacman -S --needed base-devel  # Build directory mkdir falcon-packages cd falcon-packages  # Dependency c-mnalib git clone https://aur.archlinux.org/c-mnalib.git cd c-mnalib makepkg -si cd",makepkg,SOFTWARE
# Dependency srsLTE (patched) git clone https://aur.archlinux.org/srslte-falcon-patch-git.git cd srslte-falcon-patch-git makepkg -si cd,srsLTE,SOFTWARE
# Dependency srsLTE (patched) git clone https://aur.archlinux.org/srslte-falcon-patch-git.git cd srslte-falcon-patch-git makepkg -si cd,git,SOFTWARE
# Dependency srsLTE (patched) git clone https://aur.archlinux.org/srslte-falcon-patch-git.git cd srslte-falcon-patch-git makepkg -si cd,archlinux,SOFTWARE
# Dependency srsLTE (patched) git clone https://aur.archlinux.org/srslte-falcon-patch-git.git cd srslte-falcon-patch-git makepkg -si cd,srslte,SOFTWARE
# Dependency srsLTE (patched) git clone https://aur.archlinux.org/srslte-falcon-patch-git.git cd srslte-falcon-patch-git makepkg -si cd,falcon,SOFTWARE
"# FALCON git clone https://aur.archlinux.org/tudo-falcon.git cd tudo-falcon makepkg -si ```  ## SDR Hardware FALCON has been tested with the following Software Defined Radios (SDRs):  * Ettus Research: USRP B210 * Ettus Research: USRP B205mini * Lime Microsystems: LimeSDR Mini  In addition, any SDR supported by [srsLTE library][srslte] should work as well.  ## Computer Hardware Requirements / Performance Real-time decoding of LTE signals requires a mature multicore CPU, especially when monitoring busy cells and large bandwidths (i.e. 15MHz and 20MHz cells).",git,SOFTWARE
"# FALCON git clone https://aur.archlinux.org/tudo-falcon.git cd tudo-falcon makepkg -si ```  ## SDR Hardware FALCON has been tested with the following Software Defined Radios (SDRs):  * Ettus Research: USRP B210 * Ettus Research: USRP B205mini * Lime Microsystems: LimeSDR Mini  In addition, any SDR supported by [srsLTE library][srslte] should work as well.  ## Computer Hardware Requirements / Performance Real-time decoding of LTE signals requires a mature multicore CPU, especially when monitoring busy cells and large bandwidths (i.e. 15MHz and 20MHz cells).",tudo-falcon,SOFTWARE
"# FALCON git clone https://aur.archlinux.org/tudo-falcon.git cd tudo-falcon makepkg -si ```  ## SDR Hardware FALCON has been tested with the following Software Defined Radios (SDRs):  * Ettus Research: USRP B210 * Ettus Research: USRP B205mini * Lime Microsystems: LimeSDR Mini  In addition, any SDR supported by [srsLTE library][srslte] should work as well.  ## Computer Hardware Requirements / Performance Real-time decoding of LTE signals requires a mature multicore CPU, especially when monitoring busy cells and large bandwidths (i.e. 15MHz and 20MHz cells).",FALCON,SOFTWARE
"The software collection comprises the following components:  * FalconGUI: A visualization for online/offline PDCCH decoding * FalconEye: A command-line version of the PDCCH decoder for automated/batch processing * FalconCaptureProbe: Signal recorder with optional network probing * FalconCaptureWarden: A command-line controller for synchronized recordings by multiple instances of FalconCaptureProbe * imdea_cc_decoder: Port of IMDEA OWL's PDCCH decoder * imdea_capture_sync: Port of IMDEA OWL's signal recorder  ### FALCON GUI To start the GUI version of FALCON's decoder, simply launch ``FalconGUI`` from a terminal or from your preferred window manager.",FalconGUI,SOFTWARE
"The software collection comprises the following components:  * FalconGUI: A visualization for online/offline PDCCH decoding * FalconEye: A command-line version of the PDCCH decoder for automated/batch processing * FalconCaptureProbe: Signal recorder with optional network probing * FalconCaptureWarden: A command-line controller for synchronized recordings by multiple instances of FalconCaptureProbe * imdea_cc_decoder: Port of IMDEA OWL's PDCCH decoder * imdea_capture_sync: Port of IMDEA OWL's signal recorder  ### FALCON GUI To start the GUI version of FALCON's decoder, simply launch ``FalconGUI`` from a terminal or from your preferred window manager.",FalconEye,SOFTWARE
"The software collection comprises the following components:  * FalconGUI: A visualization for online/offline PDCCH decoding * FalconEye: A command-line version of the PDCCH decoder for automated/batch processing * FalconCaptureProbe: Signal recorder with optional network probing * FalconCaptureWarden: A command-line controller for synchronized recordings by multiple instances of FalconCaptureProbe * imdea_cc_decoder: Port of IMDEA OWL's PDCCH decoder * imdea_capture_sync: Port of IMDEA OWL's signal recorder  ### FALCON GUI To start the GUI version of FALCON's decoder, simply launch ``FalconGUI`` from a terminal or from your preferred window manager.",FalconCaptureProbe,SOFTWARE
"The software collection comprises the following components:  * FalconGUI: A visualization for online/offline PDCCH decoding * FalconEye: A command-line version of the PDCCH decoder for automated/batch processing * FalconCaptureProbe: Signal recorder with optional network probing * FalconCaptureWarden: A command-line controller for synchronized recordings by multiple instances of FalconCaptureProbe * imdea_cc_decoder: Port of IMDEA OWL's PDCCH decoder * imdea_capture_sync: Port of IMDEA OWL's signal recorder  ### FALCON GUI To start the GUI version of FALCON's decoder, simply launch ``FalconGUI`` from a terminal or from your preferred window manager.",FalconCaptureWarden,SOFTWARE
"The software collection comprises the following components:  * FalconGUI: A visualization for online/offline PDCCH decoding * FalconEye: A command-line version of the PDCCH decoder for automated/batch processing * FalconCaptureProbe: Signal recorder with optional network probing * FalconCaptureWarden: A command-line controller for synchronized recordings by multiple instances of FalconCaptureProbe * imdea_cc_decoder: Port of IMDEA OWL's PDCCH decoder * imdea_capture_sync: Port of IMDEA OWL's signal recorder  ### FALCON GUI To start the GUI version of FALCON's decoder, simply launch ``FalconGUI`` from a terminal or from your preferred window manager.",imdea_cc_decoder,SOFTWARE
"The software collection comprises the following components:  * FalconGUI: A visualization for online/offline PDCCH decoding * FalconEye: A command-line version of the PDCCH decoder for automated/batch processing * FalconCaptureProbe: Signal recorder with optional network probing * FalconCaptureWarden: A command-line controller for synchronized recordings by multiple instances of FalconCaptureProbe * imdea_cc_decoder: Port of IMDEA OWL's PDCCH decoder * imdea_capture_sync: Port of IMDEA OWL's signal recorder  ### FALCON GUI To start the GUI version of FALCON's decoder, simply launch ``FalconGUI`` from a terminal or from your preferred window manager.",imdea_capture_sync,SOFTWARE
"The software collection comprises the following components:  * FalconGUI: A visualization for online/offline PDCCH decoding * FalconEye: A command-line version of the PDCCH decoder for automated/batch processing * FalconCaptureProbe: Signal recorder with optional network probing * FalconCaptureWarden: A command-line controller for synchronized recordings by multiple instances of FalconCaptureProbe * imdea_cc_decoder: Port of IMDEA OWL's PDCCH decoder * imdea_capture_sync: Port of IMDEA OWL's signal recorder  ### FALCON GUI To start the GUI version of FALCON's decoder, simply launch ``FalconGUI`` from a terminal or from your preferred window manager.",FALCON GUI,SOFTWARE
"The software collection comprises the following components:  * FalconGUI: A visualization for online/offline PDCCH decoding * FalconEye: A command-line version of the PDCCH decoder for automated/batch processing * FalconCaptureProbe: Signal recorder with optional network probing * FalconCaptureWarden: A command-line controller for synchronized recordings by multiple instances of FalconCaptureProbe * imdea_cc_decoder: Port of IMDEA OWL's PDCCH decoder * imdea_capture_sync: Port of IMDEA OWL's signal recorder  ### FALCON GUI To start the GUI version of FALCON's decoder, simply launch ``FalconGUI`` from a terminal or from your preferred window manager.",FALCON,SOFTWARE
"The software collection comprises the following components:  * FalconGUI: A visualization for online/offline PDCCH decoding * FalconEye: A command-line version of the PDCCH decoder for automated/batch processing * FalconCaptureProbe: Signal recorder with optional network probing * FalconCaptureWarden: A command-line controller for synchronized recordings by multiple instances of FalconCaptureProbe * imdea_cc_decoder: Port of IMDEA OWL's PDCCH decoder * imdea_capture_sync: Port of IMDEA OWL's signal recorder  ### FALCON GUI To start the GUI version of FALCON's decoder, simply launch ``FalconGUI`` from a terminal or from your preferred window manager.",FalconGUI,SOFTWARE
"Without installation, ``FalconGUI`` is located in ``<build-dir>/src/gui/FalconGUI``.",FalconGUI,SOFTWARE
"Without installation, ``FalconGUI`` is located in ``<build-dir>/src/gui/FalconGUI``.",FalconGUI,SOFTWARE
"[FALCON Screenshot](gfx/FalconGUI.png ""Falcon GUI Screenshot"")   ### FALCON Eye A command-line version of FALCON Decoder.",FALCON Eye,SOFTWARE
"For real-time monitoring of a cell, e.g. at 1829.4 MHz, run the following command: ```sh FalconEye -f 1829.4e6 -D /tmp/dci.csv ``` This will print an ASCII visualization of the discovered resource allocations to the terminal and a detailed log of all captured DCI into the trace file ``/tmp/dci.csv``.",FalconEye,SOFTWARE
The output of Falcon Eye for a 15MHz (75 PRB) cell should look as follows:  !,Falcon Eye,SOFTWARE
"For synchronized recordings by multiple (distributed) instances of ``FalconCaptureProbe``, the ``FalconCaptureWarden`` provides a text-based command prompt for synchronous remote control.",FalconCaptureProbe,SOFTWARE
"For synchronized recordings by multiple (distributed) instances of ``FalconCaptureProbe``, the ``FalconCaptureWarden`` provides a text-based command prompt for synchronous remote control.",FalconCaptureWarden,SOFTWARE
"Note: In order to reduce the IO-load of the capturing system, ``FalconCaptureProbe`` will store the captured samples in RAM and write them to file after the capturing has ended.",FalconCaptureProbe,SOFTWARE
"The capturing process stops if the allocated buffer size is exceeded.  #### Minimum example: capture raw data from a cell In order to capture raw data from an LTE cell and store it on the hard disk for later (offline) analysis, launch ``FalconCaptureProbe`` as follows:  ```sh FalconCaptureProbe -f <carrier_frequency_Hz> -n <nof_subframes> -o example ``` * carrier_frequency_Hz: Center frequency in Hz of the LTE cell to capture.",FalconCaptureProbe,SOFTWARE
"To counteract this, the *short-cut* detector can be deactivated (option ``-L`` in ``FalconEye``).",FalconEye,SOFTWARE
"By this, both applications benefit from future advancements of srsLTE library.  ### Validation of the port against the original version  Every port requires at least slight adaptations of the code, especially if the underlying libraries evolve.",srsLTE,SOFTWARE
"This required the following precautions:  - **Switch Viterbi decoder to 8 bit**: srsLTE uses a 16-bit Viterbi decoder if AVX2 is available, whereas the version underlying IMDEA OWL uses 8-bit Viterbi decoder.",srsLTE,SOFTWARE
"Therefore, ``#undef VITERBI_16`` in file ``dci.c`` of srsLTE library even if ``LV_HAVE_AVX2`` is defined to achieve the same behavior.",srsLTE,SOFTWARE
"-- Identifiers, in alphabetical order -->  [imdea-owl]: https://git.networks.imdea.org/nicola_bui/imdeaowl [examples]: https://github.com/falkenber9/falcon-examples [srslte]: https://github.com/srsLTE/srsLTE [srslte-falcon-patch]: https://github.com/falkenber9/srsLTE [video-presentation]: https://www.youtube.com/watch?",srsLTE,SOFTWARE
"-- Identifiers, in alphabetical order -->  [imdea-owl]: https://git.networks.imdea.org/nicola_bui/imdeaowl [examples]: https://github.com/falkenber9/falcon-examples [srslte]: https://github.com/srsLTE/srsLTE [srslte-falcon-patch]: https://github.com/falkenber9/srsLTE [video-presentation]: https://www.youtube.com/watch?",srsLTE,SOFTWARE
"-- Identifiers, in alphabetical order -->  [imdea-owl]: https://git.networks.imdea.org/nicola_bui/imdeaowl [examples]: https://github.com/falkenber9/falcon-examples [srslte]: https://github.com/srsLTE/srsLTE [srslte-falcon-patch]: https://github.com/falkenber9/srsLTE [video-presentation]: https://www.youtube.com/watch?",srsLTE,SOFTWARE
branch=master)](https://travis-ci.org/MarkusRabe/cadet)  # CADET  CADET is a solver for _quantified Boolean formulas_ with a forall-exists quantifier alternation (2QBF).,CADET,SOFTWARE
branch=master)](https://travis-ci.org/MarkusRabe/cadet)  # CADET  CADET is a solver for _quantified Boolean formulas_ with a forall-exists quantifier alternation (2QBF).,CADET,SOFTWARE
"As of 2018, CADET is one of the fastest and most reliable solvers for 2QBF formulas.",CADET,SOFTWARE
"It won second price in the 2QBF track of [QBFEval](http://www.qbflib.org/qbfeval17.php) and can also _prove_ its results little overhead, which is unique in the current landscape of QBF solvers.   ## Example  Here we discuss how to encode the formula ‚àÄ x1 x2:  ‚àÉ y:  y = (x1 & x2) and how to interpret CADET's results.",CADET,SOFTWARE
"Natively, CADET reads the [QDIMACS](http://www.qbflib.org/qdimacs.html) format (and will soon also support [QAIGER](https://github.com/ltentrup/QAIGER)).",CADET,SOFTWARE
"Natively, CADET reads the [QDIMACS](http://www.qbflib.org/qdimacs.html) format (and will soon also support [QAIGER](https://github.com/ltentrup/QAIGER)).",QDIMACS,SOFTWARE
"Natively, CADET reads the [QDIMACS](http://www.qbflib.org/qdimacs.html) format (and will soon also support [QAIGER](https://github.com/ltentrup/QAIGER)).",qdimacs,SOFTWARE
"Natively, CADET reads the [QDIMACS](http://www.qbflib.org/qdimacs.html) format (and will soon also support [QAIGER](https://github.com/ltentrup/QAIGER)).",QAIGER,SOFTWARE
"Natively, CADET reads the [QDIMACS](http://www.qbflib.org/qdimacs.html) format (and will soon also support [QAIGER](https://github.com/ltentrup/QAIGER)).",QAIGER,SOFTWARE
"The example formula looks as follows in QDIMACS:  ```qdimacs c This QDIMACS file encodes the formula      c forall x1, x2 exists y. y <-> x1 & x2. c x1 is represented by number 1 c x2 is represented by number 2 c y  is represented by number 3 p cnf 3 3                                    a 1 2 0                                      e 3 0                                        1 -3 0                                       2 -3 0                                       -1 -2 3 0                                    ```  Lines starting with `c` are comments.",QDIMACS,SOFTWARE
"The example formula looks as follows in QDIMACS:  ```qdimacs c This QDIMACS file encodes the formula      c forall x1, x2 exists y. y <-> x1 & x2. c x1 is represented by number 1 c x2 is represented by number 2 c y  is represented by number 3 p cnf 3 3                                    a 1 2 0                                      e 3 0                                        1 -3 0                                       2 -3 0                                       -1 -2 3 0                                    ```  Lines starting with `c` are comments.",qdimacs,SOFTWARE
"The example formula looks as follows in QDIMACS:  ```qdimacs c This QDIMACS file encodes the formula      c forall x1, x2 exists y. y <-> x1 & x2. c x1 is represented by number 1 c x2 is represented by number 2 c y  is represented by number 3 p cnf 3 3                                    a 1 2 0                                      e 3 0                                        1 -3 0                                       2 -3 0                                       -1 -2 3 0                                    ```  Lines starting with `c` are comments.",QDIMACS,SOFTWARE
CADET will solve this file easily:  ```bash $ .,CADET,SOFTWARE
"/cadet formula.qdimacs ```  Output: ``` Processing file ""formula.qdimacs"".",cadet,SOFTWARE
"/cadet formula.qdimacs ```  Output: ``` Processing file ""formula.qdimacs"".",qdimacs,SOFTWARE
"/cadet formula.qdimacs ```  Output: ``` Processing file ""formula.qdimacs"".",qdimacs,SOFTWARE
"CADET v2.3 SAT ```  This indicates that the formula is satsifiable (i.e. true, as we consider only closed formulas).",CADET v2.3,SOFTWARE
To prove formulas true CADET constructs a function assigning a value to y for every assignment to x1 and x2 (a _Skolem function_).,CADET,SOFTWARE
"For many applications, such as circuit repair, safety games, and [strategy extraction for LTL synthesis](https://www.react.uni-saarland.de/publications/FFRT17.html) we are interested in the function that CADET computed as it represents the solution of the encoded problem.",CADET,SOFTWARE
With the command line argument `-c <filename>` CADET outputs this function as an [AIGER](fmv.jku.at/aiger/) circuit:    ```bash $ .,CADET,SOFTWARE
With the command line argument `-c <filename>` CADET outputs this function as an [AIGER](fmv.jku.at/aiger/) circuit:    ```bash $ .,AIGER,SOFTWARE
With the command line argument `-c <filename>` CADET outputs this function as an [AIGER](fmv.jku.at/aiger/) circuit:    ```bash $ .,aiger,SOFTWARE
"/cadet -c result.aig formula.qdimacs ```  The result is written to the file `result.aig`, which is typically a bit bloated and it is intended to be minimized after generation.",cadet,SOFTWARE
"/cadet -c result.aig formula.qdimacs ```  The result is written to the file `result.aig`, which is typically a bit bloated and it is intended to be minimized after generation.",qdimacs,SOFTWARE
"For example, you can use the following command to minimize circuits with [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/):  ```bash $ abc -c ""read result.aig; dc2; write result.aig"" ```  After the minimization a circuit with a single AND-gate remains:  ```aiger aag 3 2 0 1 1 2 4 6 6 4 2 i0 1 i1 2 o0 3 ```  To view a human-readable version of the circuit as shown above you have to convert the AIGER binrary formag `.aig` to the AIGER ASCII format `.aag` using the tool `aigtoaig` available in the [AIGER toolset](http://fmv.jku.at/aiger/aiger-1.9.9.tar.gz).   ## Installing CADET   CADET can be built from source with both clang and gcc.",ABC,SOFTWARE
"For example, you can use the following command to minimize circuits with [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/):  ```bash $ abc -c ""read result.aig; dc2; write result.aig"" ```  After the minimization a circuit with a single AND-gate remains:  ```aiger aag 3 2 0 1 1 2 4 6 6 4 2 i0 1 i1 2 o0 3 ```  To view a human-readable version of the circuit as shown above you have to convert the AIGER binrary formag `.aig` to the AIGER ASCII format `.aag` using the tool `aigtoaig` available in the [AIGER toolset](http://fmv.jku.at/aiger/aiger-1.9.9.tar.gz).   ## Installing CADET   CADET can be built from source with both clang and gcc.",abc,SOFTWARE
"For example, you can use the following command to minimize circuits with [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/):  ```bash $ abc -c ""read result.aig; dc2; write result.aig"" ```  After the minimization a circuit with a single AND-gate remains:  ```aiger aag 3 2 0 1 1 2 4 6 6 4 2 i0 1 i1 2 o0 3 ```  To view a human-readable version of the circuit as shown above you have to convert the AIGER binrary formag `.aig` to the AIGER ASCII format `.aag` using the tool `aigtoaig` available in the [AIGER toolset](http://fmv.jku.at/aiger/aiger-1.9.9.tar.gz).   ## Installing CADET   CADET can be built from source with both clang and gcc.",aiger,SOFTWARE
"For example, you can use the following command to minimize circuits with [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/):  ```bash $ abc -c ""read result.aig; dc2; write result.aig"" ```  After the minimization a circuit with a single AND-gate remains:  ```aiger aag 3 2 0 1 1 2 4 6 6 4 2 i0 1 i1 2 o0 3 ```  To view a human-readable version of the circuit as shown above you have to convert the AIGER binrary formag `.aig` to the AIGER ASCII format `.aag` using the tool `aigtoaig` available in the [AIGER toolset](http://fmv.jku.at/aiger/aiger-1.9.9.tar.gz).   ## Installing CADET   CADET can be built from source with both clang and gcc.",AIGER,SOFTWARE
"For example, you can use the following command to minimize circuits with [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/):  ```bash $ abc -c ""read result.aig; dc2; write result.aig"" ```  After the minimization a circuit with a single AND-gate remains:  ```aiger aag 3 2 0 1 1 2 4 6 6 4 2 i0 1 i1 2 o0 3 ```  To view a human-readable version of the circuit as shown above you have to convert the AIGER binrary formag `.aig` to the AIGER ASCII format `.aag` using the tool `aigtoaig` available in the [AIGER toolset](http://fmv.jku.at/aiger/aiger-1.9.9.tar.gz).   ## Installing CADET   CADET can be built from source with both clang and gcc.",AIGER,SOFTWARE
"For example, you can use the following command to minimize circuits with [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/):  ```bash $ abc -c ""read result.aig; dc2; write result.aig"" ```  After the minimization a circuit with a single AND-gate remains:  ```aiger aag 3 2 0 1 1 2 4 6 6 4 2 i0 1 i1 2 o0 3 ```  To view a human-readable version of the circuit as shown above you have to convert the AIGER binrary formag `.aig` to the AIGER ASCII format `.aag` using the tool `aigtoaig` available in the [AIGER toolset](http://fmv.jku.at/aiger/aiger-1.9.9.tar.gz).   ## Installing CADET   CADET can be built from source with both clang and gcc.",aigtoaig,SOFTWARE
"For example, you can use the following command to minimize circuits with [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/):  ```bash $ abc -c ""read result.aig; dc2; write result.aig"" ```  After the minimization a circuit with a single AND-gate remains:  ```aiger aag 3 2 0 1 1 2 4 6 6 4 2 i0 1 i1 2 o0 3 ```  To view a human-readable version of the circuit as shown above you have to convert the AIGER binrary formag `.aig` to the AIGER ASCII format `.aag` using the tool `aigtoaig` available in the [AIGER toolset](http://fmv.jku.at/aiger/aiger-1.9.9.tar.gz).   ## Installing CADET   CADET can be built from source with both clang and gcc.",AIGER,SOFTWARE
"For example, you can use the following command to minimize circuits with [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/):  ```bash $ abc -c ""read result.aig; dc2; write result.aig"" ```  After the minimization a circuit with a single AND-gate remains:  ```aiger aag 3 2 0 1 1 2 4 6 6 4 2 i0 1 i1 2 o0 3 ```  To view a human-readable version of the circuit as shown above you have to convert the AIGER binrary formag `.aig` to the AIGER ASCII format `.aag` using the tool `aigtoaig` available in the [AIGER toolset](http://fmv.jku.at/aiger/aiger-1.9.9.tar.gz).   ## Installing CADET   CADET can be built from source with both clang and gcc.",aiger,SOFTWARE
"For example, you can use the following command to minimize circuits with [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/):  ```bash $ abc -c ""read result.aig; dc2; write result.aig"" ```  After the minimization a circuit with a single AND-gate remains:  ```aiger aag 3 2 0 1 1 2 4 6 6 4 2 i0 1 i1 2 o0 3 ```  To view a human-readable version of the circuit as shown above you have to convert the AIGER binrary formag `.aig` to the AIGER ASCII format `.aag` using the tool `aigtoaig` available in the [AIGER toolset](http://fmv.jku.at/aiger/aiger-1.9.9.tar.gz).   ## Installing CADET   CADET can be built from source with both clang and gcc.",aiger-1.9.9,SOFTWARE
"For example, you can use the following command to minimize circuits with [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/):  ```bash $ abc -c ""read result.aig; dc2; write result.aig"" ```  After the minimization a circuit with a single AND-gate remains:  ```aiger aag 3 2 0 1 1 2 4 6 6 4 2 i0 1 i1 2 o0 3 ```  To view a human-readable version of the circuit as shown above you have to convert the AIGER binrary formag `.aig` to the AIGER ASCII format `.aag` using the tool `aigtoaig` available in the [AIGER toolset](http://fmv.jku.at/aiger/aiger-1.9.9.tar.gz).   ## Installing CADET   CADET can be built from source with both clang and gcc.",CADET,SOFTWARE
"For example, you can use the following command to minimize circuits with [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/):  ```bash $ abc -c ""read result.aig; dc2; write result.aig"" ```  After the minimization a circuit with a single AND-gate remains:  ```aiger aag 3 2 0 1 1 2 4 6 6 4 2 i0 1 i1 2 o0 3 ```  To view a human-readable version of the circuit as shown above you have to convert the AIGER binrary formag `.aig` to the AIGER ASCII format `.aag` using the tool `aigtoaig` available in the [AIGER toolset](http://fmv.jku.at/aiger/aiger-1.9.9.tar.gz).   ## Installing CADET   CADET can be built from source with both clang and gcc.",CADET,SOFTWARE
You can find pre-built binaries of CADET for Linux and OSX.,CADET,SOFTWARE
The testing scripts require Python 3.,Python 3,SOFTWARE
"/configure && make ```  To make sure the solver works correctly execute the test suite:  ```bash $ make test ```  One of the test cases will timeout as part of the testsuite and a number of tests will return with the result UNKNOWN, which is intended.    ## Usage  The most common use case for the solver is to solve formulas specified as a [QDIMACS](http://www.qbflib.org/qdimacs.html) file.",make,SOFTWARE
"/configure && make ```  To make sure the solver works correctly execute the test suite:  ```bash $ make test ```  One of the test cases will timeout as part of the testsuite and a number of tests will return with the result UNKNOWN, which is intended.    ## Usage  The most common use case for the solver is to solve formulas specified as a [QDIMACS](http://www.qbflib.org/qdimacs.html) file.",make,SOFTWARE
"/configure && make ```  To make sure the solver works correctly execute the test suite:  ```bash $ make test ```  One of the test cases will timeout as part of the testsuite and a number of tests will return with the result UNKNOWN, which is intended.    ## Usage  The most common use case for the solver is to solve formulas specified as a [QDIMACS](http://www.qbflib.org/qdimacs.html) file.",QDIMACS,SOFTWARE
"/configure && make ```  To make sure the solver works correctly execute the test suite:  ```bash $ make test ```  One of the test cases will timeout as part of the testsuite and a number of tests will return with the result UNKNOWN, which is intended.    ## Usage  The most common use case for the solver is to solve formulas specified as a [QDIMACS](http://www.qbflib.org/qdimacs.html) file.",qdimacs,SOFTWARE
/cadet file.qdimacs ```  You can also pipe QDIMACS into the solver:  ```bash $ cat file.qdimacs | .,cadet,SOFTWARE
/cadet file.qdimacs ```  You can also pipe QDIMACS into the solver:  ```bash $ cat file.qdimacs | .,qdimacs,SOFTWARE
/cadet file.qdimacs ```  You can also pipe QDIMACS into the solver:  ```bash $ cat file.qdimacs | .,QDIMACS,SOFTWARE
/cadet file.qdimacs ```  You can also pipe QDIMACS into the solver:  ```bash $ cat file.qdimacs | .,qdimacs,SOFTWARE
/cadet ```  #### Input Formats  CADET reads files in both [QDIMACS](http://www.qbflib.org/qdimacs.html) and [QAIGER](https://github.com/ltentrup/QAIGER) format.,cadet,SOFTWARE
/cadet ```  #### Input Formats  CADET reads files in both [QDIMACS](http://www.qbflib.org/qdimacs.html) and [QAIGER](https://github.com/ltentrup/QAIGER) format.,CADET,SOFTWARE
/cadet ```  #### Input Formats  CADET reads files in both [QDIMACS](http://www.qbflib.org/qdimacs.html) and [QAIGER](https://github.com/ltentrup/QAIGER) format.,QDIMACS,SOFTWARE
/cadet ```  #### Input Formats  CADET reads files in both [QDIMACS](http://www.qbflib.org/qdimacs.html) and [QAIGER](https://github.com/ltentrup/QAIGER) format.,qdimacs,SOFTWARE
/cadet ```  #### Input Formats  CADET reads files in both [QDIMACS](http://www.qbflib.org/qdimacs.html) and [QAIGER](https://github.com/ltentrup/QAIGER) format.,QAIGER,SOFTWARE
/cadet ```  #### Input Formats  CADET reads files in both [QDIMACS](http://www.qbflib.org/qdimacs.html) and [QAIGER](https://github.com/ltentrup/QAIGER) format.,QAIGER,SOFTWARE
"Files can be zipped with gzip, but must then end with the file extension gz or gzip.    ## Proofs  CADET is able to prove (or _certify_) its results.",CADET,SOFTWARE
"As 2QBF formulas in QDIMACS have a forall-exists quantifier alternation, proofs for UNSAT results are given as an assignment to the universally quantified variables.",QDIMACS,SOFTWARE
Certificates for UNSAT results are written to stdout according to the [QDIMACS](http://www.qbflib.org/qdimacs.html) standard.,QDIMACS,SOFTWARE
Certificates for UNSAT results are written to stdout according to the [QDIMACS](http://www.qbflib.org/qdimacs.html) standard.,qdimacs,SOFTWARE
"To print output according to the QDIMACS standard, use the `--qdimacs_out` flag.",QDIMACS,SOFTWARE
"To print output according to the QDIMACS standard, use the `--qdimacs_out` flag.",qdimacs,SOFTWARE
CADET checks UNSAT certificates internally by default.,CADET,SOFTWARE
With the command line option `-c [file]` CADET writes the SAT certificate for true 2QBF.,CADET,SOFTWARE
You can either specify a file name to which the certificate should be written (ending in `.aag` or `.aig`) or you can specify `sdtout` to let CADET print the certificate on the terminal.,CADET,SOFTWARE
/cadet -c certificate.aag file.qdimacs ```  As soon as you work with certificates you may want to install the [AIGER tool set](http://fmv.jku.at/aiger/aiger-1.9.4.tar.gz) and the [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/).,cadet,SOFTWARE
/cadet -c certificate.aag file.qdimacs ```  As soon as you work with certificates you may want to install the [AIGER tool set](http://fmv.jku.at/aiger/aiger-1.9.4.tar.gz) and the [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/).,qdimacs,SOFTWARE
/cadet -c certificate.aag file.qdimacs ```  As soon as you work with certificates you may want to install the [AIGER tool set](http://fmv.jku.at/aiger/aiger-1.9.4.tar.gz) and the [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/).,AIGER,SOFTWARE
/cadet -c certificate.aag file.qdimacs ```  As soon as you work with certificates you may want to install the [AIGER tool set](http://fmv.jku.at/aiger/aiger-1.9.4.tar.gz) and the [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/).,aiger,SOFTWARE
/cadet -c certificate.aag file.qdimacs ```  As soon as you work with certificates you may want to install the [AIGER tool set](http://fmv.jku.at/aiger/aiger-1.9.4.tar.gz) and the [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/).,aiger-1.9.4,SOFTWARE
/cadet -c certificate.aag file.qdimacs ```  As soon as you work with certificates you may want to install the [AIGER tool set](http://fmv.jku.at/aiger/aiger-1.9.4.tar.gz) and the [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/).,ABC,SOFTWARE
/cadet -c certificate.aag file.qdimacs ```  As soon as you work with certificates you may want to install the [AIGER tool set](http://fmv.jku.at/aiger/aiger-1.9.4.tar.gz) and the [ABC](https://people.eecs.berkeley.edu/~alanmi/abc/).,abc,SOFTWARE
"The distribution of CADET comes with several scripts that demonstrate how to generate, simplify, and check certificates using ABC and the AIGER tool set.  #### Checking Certificates  UNSAT results are checked internally by default.",CADET,SOFTWARE
"The distribution of CADET comes with several scripts that demonstrate how to generate, simplify, and check certificates using ABC and the AIGER tool set.  #### Checking Certificates  UNSAT results are checked internally by default.",ABC,SOFTWARE
"The distribution of CADET comes with several scripts that demonstrate how to generate, simplify, and check certificates using ABC and the AIGER tool set.  #### Checking Certificates  UNSAT results are checked internally by default.",AIGER,SOFTWARE
You can double check them by asserting the assignment to the universals in the CNF of the QDIMACS and querying a SAT solver.,QDIMACS,SOFTWARE
"For checking SAT certificates you have two options: By default CADET produces certificates that can be checked by Certcheck, which was written by [Leander Tentrup](https://www.react.uni-saarland.de/people/tentrup.html).",CADET,SOFTWARE
Certcheck comes with the distribution of [CAQE](https://www.react.uni-saarland.de/tools/caqe/).,CAQE,SOFTWARE
Certcheck comes with the distribution of [CAQE](https://www.react.uni-saarland.de/tools/caqe/).,caqe,SOFTWARE
To produce certificates that are compatible with [QBFcert](http://fmv.jku.at/qbfcert/) add `--qbfcert` option to the command line.,QBFcert,SOFTWARE
To produce certificates that are compatible with [QBFcert](http://fmv.jku.at/qbfcert/) add `--qbfcert` option to the command line.,qbfcert,SOFTWARE
To produce certificates that are compatible with [QBFcert](http://fmv.jku.at/qbfcert/) add `--qbfcert` option to the command line.,qbfcert,SOFTWARE
"Note that QBFcert standard is only compatible with the ASCII format of the AIGER standard, so be sure that the certificate file name ends with `.aag`.",QBFcert,SOFTWARE
"Note that QBFcert standard is only compatible with the ASCII format of the AIGER standard, so be sure that the certificate file name ends with `.aag`.",AIGER,SOFTWARE
"Also, be aware that QBFcert certificates cannot be minimized by ABC.   ## Publications  [Learning Heuristics for 2QBF through Deep Reinforcement Learning](https://arxiv.org/abs/1807.08058).",QBFcert,SOFTWARE
"Also, be aware that QBFcert certificates cannot be minimized by ABC.   ## Publications  [Learning Heuristics for 2QBF through Deep Reinforcement Learning](https://arxiv.org/abs/1807.08058).",ABC,SOFTWARE
[Image](rule_gen.png) -->  ## Requirements  On Ubuntu:  - python(3.5.2) - pip(20.3.4) - pytorch(0.4.0) - [cuda](https://developer.nvidia.com/cuda-downloads?,python(3.5.2),SOFTWARE
[Image](rule_gen.png) -->  ## Requirements  On Ubuntu:  - python(3.5.2) - pip(20.3.4) - pytorch(0.4.0) - [cuda](https://developer.nvidia.com/cuda-downloads?,pip(20.3.4),SOFTWARE
[Image](rule_gen.png) -->  ## Requirements  On Ubuntu:  - python(3.5.2) - pip(20.3.4) - pytorch(0.4.0) - [cuda](https://developer.nvidia.com/cuda-downloads?,pytorch(0.4.0),SOFTWARE
[Image](rule_gen.png) -->  ## Requirements  On Ubuntu:  - python(3.5.2) - pip(20.3.4) - pytorch(0.4.0) - [cuda](https://developer.nvidia.com/cuda-downloads?,cuda,SOFTWARE
[Image](rule_gen.png) -->  ## Requirements  On Ubuntu:  - python(3.5.2) - pip(20.3.4) - pytorch(0.4.0) - [cuda](https://developer.nvidia.com/cuda-downloads?,cuda,SOFTWARE
"target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=18.04&target_type=deb_local) - [docker](https://docs.docker.com/engine/install/ubuntu/) - [nvidia-docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)  Sign up a docker account from [dockerhub](https://hub.docker.com/)   ## Installation and Execution ### <span id=""Download"">Download dataset and model</span>:   download the UI image [dataset](https://doi.org/10.5281/zenodo.5081242) and unzip:  ```shell unzip data.zip ```  data/images:   - *data/images/Base* : 132 screenshots of game1 & game2 with UI display issues from 466 test reports. - *data/images/Code* : 9,412 screenshots of game1 & game2 with UI display issues generated by our Code augmentation method. - *data/images/Normal*: 7,750 screenshots of game1 & game2 without UI display issues collected by randomly traversing the game scene. - *data/images/Rule(F)* : 7,750 screenshots of game1 & game2 with UI display issues generated by our Rule(F) augmentation method. - *data/images/Rule(R)* : 7,750 screenshots of game1 & game2 with UI display issues generated by our Rule(R) augmentation method. - *data/images/testDataSet* : 192 screenshots  with UI display issues from 466 test reports(exclude game1 & game2).",docker,SOFTWARE
"target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=18.04&target_type=deb_local) - [docker](https://docs.docker.com/engine/install/ubuntu/) - [nvidia-docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)  Sign up a docker account from [dockerhub](https://hub.docker.com/)   ## Installation and Execution ### <span id=""Download"">Download dataset and model</span>:   download the UI image [dataset](https://doi.org/10.5281/zenodo.5081242) and unzip:  ```shell unzip data.zip ```  data/images:   - *data/images/Base* : 132 screenshots of game1 & game2 with UI display issues from 466 test reports. - *data/images/Code* : 9,412 screenshots of game1 & game2 with UI display issues generated by our Code augmentation method. - *data/images/Normal*: 7,750 screenshots of game1 & game2 without UI display issues collected by randomly traversing the game scene. - *data/images/Rule(F)* : 7,750 screenshots of game1 & game2 with UI display issues generated by our Rule(F) augmentation method. - *data/images/Rule(R)* : 7,750 screenshots of game1 & game2 with UI display issues generated by our Rule(R) augmentation method. - *data/images/testDataSet* : 192 screenshots  with UI display issues from 466 test reports(exclude game1 & game2).",docker,SOFTWARE
"target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=18.04&target_type=deb_local) - [docker](https://docs.docker.com/engine/install/ubuntu/) - [nvidia-docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)  Sign up a docker account from [dockerhub](https://hub.docker.com/)   ## Installation and Execution ### <span id=""Download"">Download dataset and model</span>:   download the UI image [dataset](https://doi.org/10.5281/zenodo.5081242) and unzip:  ```shell unzip data.zip ```  data/images:   - *data/images/Base* : 132 screenshots of game1 & game2 with UI display issues from 466 test reports. - *data/images/Code* : 9,412 screenshots of game1 & game2 with UI display issues generated by our Code augmentation method. - *data/images/Normal*: 7,750 screenshots of game1 & game2 without UI display issues collected by randomly traversing the game scene. - *data/images/Rule(F)* : 7,750 screenshots of game1 & game2 with UI display issues generated by our Rule(F) augmentation method. - *data/images/Rule(R)* : 7,750 screenshots of game1 & game2 with UI display issues generated by our Rule(R) augmentation method. - *data/images/testDataSet* : 192 screenshots  with UI display issues from 466 test reports(exclude game1 & game2).",nvidia-docker,SOFTWARE
"target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=18.04&target_type=deb_local) - [docker](https://docs.docker.com/engine/install/ubuntu/) - [nvidia-docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)  Sign up a docker account from [dockerhub](https://hub.docker.com/)   ## Installation and Execution ### <span id=""Download"">Download dataset and model</span>:   download the UI image [dataset](https://doi.org/10.5281/zenodo.5081242) and unzip:  ```shell unzip data.zip ```  data/images:   - *data/images/Base* : 132 screenshots of game1 & game2 with UI display issues from 466 test reports. - *data/images/Code* : 9,412 screenshots of game1 & game2 with UI display issues generated by our Code augmentation method. - *data/images/Normal*: 7,750 screenshots of game1 & game2 without UI display issues collected by randomly traversing the game scene. - *data/images/Rule(F)* : 7,750 screenshots of game1 & game2 with UI display issues generated by our Rule(F) augmentation method. - *data/images/Rule(R)* : 7,750 screenshots of game1 & game2 with UI display issues generated by our Rule(R) augmentation method. - *data/images/testDataSet* : 192 screenshots  with UI display issues from 466 test reports(exclude game1 & game2).",docker,SOFTWARE
"target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=18.04&target_type=deb_local) - [docker](https://docs.docker.com/engine/install/ubuntu/) - [nvidia-docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)  Sign up a docker account from [dockerhub](https://hub.docker.com/)   ## Installation and Execution ### <span id=""Download"">Download dataset and model</span>:   download the UI image [dataset](https://doi.org/10.5281/zenodo.5081242) and unzip:  ```shell unzip data.zip ```  data/images:   - *data/images/Base* : 132 screenshots of game1 & game2 with UI display issues from 466 test reports. - *data/images/Code* : 9,412 screenshots of game1 & game2 with UI display issues generated by our Code augmentation method. - *data/images/Normal*: 7,750 screenshots of game1 & game2 without UI display issues collected by randomly traversing the game scene. - *data/images/Rule(F)* : 7,750 screenshots of game1 & game2 with UI display issues generated by our Rule(F) augmentation method. - *data/images/Rule(R)* : 7,750 screenshots of game1 & game2 with UI display issues generated by our Rule(R) augmentation method. - *data/images/testDataSet* : 192 screenshots  with UI display issues from 466 test reports(exclude game1 & game2).",docker,SOFTWARE
"We apply our approach to generate sentence analogies from sentence embeddings.  ## Quickstart  You can quickly install Vec2Sent using pip:  ```shell pip install ""vec2sent @ git+https://github.com/maruker/vec2sent.git"" ```  There are three entry points to **generate** and **evaluate** sentences, and to perform **arithmetic** in the vector space.  ### Vector Arithmetic  ```shell vec2sent_arithmetic -s infersent -c maruker/vec2sent-infersent ```  ```text Please enter sentence a (Or nothing if done):his name is robert Please enter sentence b (Or nothing if done):he is a doctor Please enter sentence c (Or nothing if done):her name is julia Please enter sentence d (Or nothing if done): Please enter an arithmetic expression (e.g.",Vec2Sent,SOFTWARE
"We apply our approach to generate sentence analogies from sentence embeddings.  ## Quickstart  You can quickly install Vec2Sent using pip:  ```shell pip install ""vec2sent @ git+https://github.com/maruker/vec2sent.git"" ```  There are three entry points to **generate** and **evaluate** sentences, and to perform **arithmetic** in the vector space.  ### Vector Arithmetic  ```shell vec2sent_arithmetic -s infersent -c maruker/vec2sent-infersent ```  ```text Please enter sentence a (Or nothing if done):his name is robert Please enter sentence b (Or nothing if done):he is a doctor Please enter sentence c (Or nothing if done):her name is julia Please enter sentence d (Or nothing if done): Please enter an arithmetic expression (e.g.",vec2sent,SOFTWARE
"We apply our approach to generate sentence analogies from sentence embeddings.  ## Quickstart  You can quickly install Vec2Sent using pip:  ```shell pip install ""vec2sent @ git+https://github.com/maruker/vec2sent.git"" ```  There are three entry points to **generate** and **evaluate** sentences, and to perform **arithmetic** in the vector space.  ### Vector Arithmetic  ```shell vec2sent_arithmetic -s infersent -c maruker/vec2sent-infersent ```  ```text Please enter sentence a (Or nothing if done):his name is robert Please enter sentence b (Or nothing if done):he is a doctor Please enter sentence c (Or nothing if done):her name is julia Please enter sentence d (Or nothing if done): Please enter an arithmetic expression (e.g.",vec2sent,SOFTWARE
"We apply our approach to generate sentence analogies from sentence embeddings.  ## Quickstart  You can quickly install Vec2Sent using pip:  ```shell pip install ""vec2sent @ git+https://github.com/maruker/vec2sent.git"" ```  There are three entry points to **generate** and **evaluate** sentences, and to perform **arithmetic** in the vector space.  ### Vector Arithmetic  ```shell vec2sent_arithmetic -s infersent -c maruker/vec2sent-infersent ```  ```text Please enter sentence a (Or nothing if done):his name is robert Please enter sentence b (Or nothing if done):he is a doctor Please enter sentence c (Or nothing if done):her name is julia Please enter sentence d (Or nothing if done): Please enter an arithmetic expression (e.g.",vec2sent,SOFTWARE
"We apply our approach to generate sentence analogies from sentence embeddings.  ## Quickstart  You can quickly install Vec2Sent using pip:  ```shell pip install ""vec2sent @ git+https://github.com/maruker/vec2sent.git"" ```  There are three entry points to **generate** and **evaluate** sentences, and to perform **arithmetic** in the vector space.  ### Vector Arithmetic  ```shell vec2sent_arithmetic -s infersent -c maruker/vec2sent-infersent ```  ```text Please enter sentence a (Or nothing if done):his name is robert Please enter sentence b (Or nothing if done):he is a doctor Please enter sentence c (Or nothing if done):her name is julia Please enter sentence d (Or nothing if done): Please enter an arithmetic expression (e.g.",vec2sent,SOFTWARE
"(a + b) * c / 2):b-a+c  she is a doctor ```  ### Sentence Generation  For example, generate outputs using the hierarchical sentence embedding  ```shell vec2sent_generate -s hier -c maruker/vec2sent-hier -d data/test.en.2008 -o hier.txt ```  ### Evaluation  The outputs from the previous step can now be evaluated.",vec2sent,SOFTWARE
"(a + b) * c / 2):b-a+c  she is a doctor ```  ### Sentence Generation  For example, generate outputs using the hierarchical sentence embedding  ```shell vec2sent_generate -s hier -c maruker/vec2sent-hier -d data/test.en.2008 -o hier.txt ```  ### Evaluation  The outputs from the previous step can now be evaluated.",vec2sent,SOFTWARE
"For example, the following command computes the bleu score  ```shell vec2sent_evaluate --metric BLEU --file hier.txt ```  The following metrics are available  | Parameter | Explanation                                                                       | |-----------|-----------------------------------------------------------------------------------| | ID        | Fraction of all sentences where the output is identical to the input              | | PERM      | Fraction of all output sentences that can be formed as a permutation of the input | | ID_PERM   | Fraction of all permuations that are identical to the input                       | | BLEU      | Document BLEU score                                                               | | MOVER     | Average Mover Score between input and output sentences                            |   > [!",vec2sent,SOFTWARE
TIP] > Vec2Sent needs to download several gigabites of sentence embedding models.,Vec2Sent,SOFTWARE
Those files can be deleted using the command `vec2sent_cleanup`  ## Available Models  We upload our models to the Hugging Face Hub.,vec2sent,SOFTWARE
"The following table shows, which parameters to set in order to load the sentence embeddings and corresponding Vec2Sent models",Vec2Sent,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",avg,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",vec2sent,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",BPEmb,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",hier,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",vec2sent,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",BPEmb,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",gem,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",vec2sent,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",Geometric Embeddings,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",sent2vec,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",vec2sent,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",Sent2Vec,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",infersent,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",vec2sent,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",InferSent,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",sbert-large,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",vec2sent,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",SBERT,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",vec2sent,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",python,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",venv,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",venv,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",venv,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",git,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",pip,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",Vec2Sent,SOFTWARE
"| Sentence embedding name `-s` | Checkpoint `-c`              | Explanation                                                                       | |------------------------------|------------------------------|-----------------------------------------------------------------------------------| | avg                          | maruker/vec2sent-avg         | Average pooling on [BPEmb](https://github.com/bheinzerling/bpemb) word embeddings | | hier                         | maruker/vec2sent-hier        | Hierarchical pooling on [BPEmb](https://github.com/bheinzerling/bpemb)            | | gem                          | maruker/vec2sent-gem         | [Geometric Embeddings](https://github.com/fursovia/geometric_embedding)           | | sent2vec                     | maruker/vec2sent-sent2vec    | [Sent2Vec](https://github.com/epfml/sent2vec)                                     | | infersent                    | maruker/vec2sent-infersent   | [InferSent](https://github.com/facebookresearch/InferSent)                        | | sbert-large                  | maruker/vec2sent-sbert-large | [SBERT](https://github.com/UKPLab/sentence-transformers)                          |  Additional sentence embeddings can be used by extending the class ``vec2sent.sentence_embeddings.abstract_sentence_embedding.AbstractEmbedding``.  ## Installation  #### (Optional) Setup Virtual Environment  ```shell python -m venv venv source venv/bin/activate ```  #### Download requirements ```shell # Download git submodules (MoS model and some sentence embeddings) git submodule update --init ```  #### Install  ```shell pip install . ```  ## Citation If you find Vec2Sent useful in your academic work, please consider citing ``` @inproceedings{kerscher-eger-2020-vec2sent,     title = ""{V}ec2{S}ent: Probing Sentence Embeddings with Natural Language Generation"",     author = ""Kerscher, Martin  and       Eger, Steffen"",     booktitle = ""Proceedings of the 28th International Conference on Computational Linguistics"",     month = dec,     year = ""2020"",     address = ""Barcelona, Spain (Online)"",     publisher = ""International Committee on Computational Linguistics"",     url = ""https://www.aclweb.org/anthology/2020.coling-main.152"",     pages = ""1729--1736"",     abstract = ""We introspect black-box sentence embeddings by conditionally generating from them with the objective to retrieve the underlying discrete sentence.",vec2sent,SOFTWARE
# ALTA - (A)utomatic (L)yrics (T)r(A)nscription  A Kaldi[1] recipe for automatic lyrics transcription.  ## Table of Contents  - [ALTA - (A)utomatic (L)yrics (T)r(A)nscription](#alta----a-utomatic--l-yrics--t-r-a-nscription)   * [System Details](#system-details)   * [Setup](#setup)       - [1) Kaldi](#1--kaldi)       - [2) Dependencies](#2--dependencies)   * [How to run](#how-to-run)     + [A) Data preparation:](#a--data-preparation-)       - [1) Retrieve the data:](#1--retrieve-the-data-)       - [1) Locate:](#1--locate-)         * [Step 1-a (Docker use ONLY):](#step-1-a--docker-use-only--)     + [B) Running the training pipeline](#b--running-the-training-pipeline)     + [C) (OPTIONAL) Extract frame-level Phoneme posteriorgrams:](#c---optional--extract-frame-level-phoneme-posteriorgrams-)     + [Future Work](#future-work)     + [Citation](#citation)     + [References](#references)    ## System Details  Automatic Lyrics Transcription is the task of translating singing voice into text.,Kaldi,SOFTWARE
# ALTA - (A)utomatic (L)yrics (T)r(A)nscription  A Kaldi[1] recipe for automatic lyrics transcription.  ## Table of Contents  - [ALTA - (A)utomatic (L)yrics (T)r(A)nscription](#alta----a-utomatic--l-yrics--t-r-a-nscription)   * [System Details](#system-details)   * [Setup](#setup)       - [1) Kaldi](#1--kaldi)       - [2) Dependencies](#2--dependencies)   * [How to run](#how-to-run)     + [A) Data preparation:](#a--data-preparation-)       - [1) Retrieve the data:](#1--retrieve-the-data-)       - [1) Locate:](#1--locate-)         * [Step 1-a (Docker use ONLY):](#step-1-a--docker-use-only--)     + [B) Running the training pipeline](#b--running-the-training-pipeline)     + [C) (OPTIONAL) Extract frame-level Phoneme posteriorgrams:](#c---optional--extract-frame-level-phoneme-posteriorgrams-)     + [Future Work](#future-work)     + [Citation](#citation)     + [References](#references)    ## System Details  Automatic Lyrics Transcription is the task of translating singing voice into text.,Kaldi,SOFTWARE
# ALTA - (A)utomatic (L)yrics (T)r(A)nscription  A Kaldi[1] recipe for automatic lyrics transcription.  ## Table of Contents  - [ALTA - (A)utomatic (L)yrics (T)r(A)nscription](#alta----a-utomatic--l-yrics--t-r-a-nscription)   * [System Details](#system-details)   * [Setup](#setup)       - [1) Kaldi](#1--kaldi)       - [2) Dependencies](#2--dependencies)   * [How to run](#how-to-run)     + [A) Data preparation:](#a--data-preparation-)       - [1) Retrieve the data:](#1--retrieve-the-data-)       - [1) Locate:](#1--locate-)         * [Step 1-a (Docker use ONLY):](#step-1-a--docker-use-only--)     + [B) Running the training pipeline](#b--running-the-training-pipeline)     + [C) (OPTIONAL) Extract frame-level Phoneme posteriorgrams:](#c---optional--extract-frame-level-phoneme-posteriorgrams-)     + [Future Work](#future-work)     + [Citation](#citation)     + [References](#references)    ## System Details  Automatic Lyrics Transcription is the task of translating singing voice into text.,kaldi,SOFTWARE
# ALTA - (A)utomatic (L)yrics (T)r(A)nscription  A Kaldi[1] recipe for automatic lyrics transcription.  ## Table of Contents  - [ALTA - (A)utomatic (L)yrics (T)r(A)nscription](#alta----a-utomatic--l-yrics--t-r-a-nscription)   * [System Details](#system-details)   * [Setup](#setup)       - [1) Kaldi](#1--kaldi)       - [2) Dependencies](#2--dependencies)   * [How to run](#how-to-run)     + [A) Data preparation:](#a--data-preparation-)       - [1) Retrieve the data:](#1--retrieve-the-data-)       - [1) Locate:](#1--locate-)         * [Step 1-a (Docker use ONLY):](#step-1-a--docker-use-only--)     + [B) Running the training pipeline](#b--running-the-training-pipeline)     + [C) (OPTIONAL) Extract frame-level Phoneme posteriorgrams:](#c---optional--extract-frame-level-phoneme-posteriorgrams-)     + [Future Work](#future-work)     + [Citation](#citation)     + [References](#references)    ## System Details  Automatic Lyrics Transcription is the task of translating singing voice into text.,Docker,SOFTWARE
# ALTA - (A)utomatic (L)yrics (T)r(A)nscription  A Kaldi[1] recipe for automatic lyrics transcription.  ## Table of Contents  - [ALTA - (A)utomatic (L)yrics (T)r(A)nscription](#alta----a-utomatic--l-yrics--t-r-a-nscription)   * [System Details](#system-details)   * [Setup](#setup)       - [1) Kaldi](#1--kaldi)       - [2) Dependencies](#2--dependencies)   * [How to run](#how-to-run)     + [A) Data preparation:](#a--data-preparation-)       - [1) Retrieve the data:](#1--retrieve-the-data-)       - [1) Locate:](#1--locate-)         * [Step 1-a (Docker use ONLY):](#step-1-a--docker-use-only--)     + [B) Running the training pipeline](#b--running-the-training-pipeline)     + [C) (OPTIONAL) Extract frame-level Phoneme posteriorgrams:](#c---optional--extract-frame-level-phoneme-posteriorgrams-)     + [Future Work](#future-work)     + [Citation](#citation)     + [References](#references)    ## System Details  Automatic Lyrics Transcription is the task of translating singing voice into text.,docker,SOFTWARE
"<p align=""center"">   <img src=""https://github.com/emirdemirel/ALTA/blob/master/img/img-git1.png"" width=""550"" height=""160""> </p>  **Acoustic Model**: Sequence discriminative training on LF-MMI criteria[2] (Kaldi-chain recipe).",Kaldi,SOFTWARE
MStre-Net[3] proposes three improvements over the standard Kaldi-chain recipe:  - The neural network is based on the multistream TDNN architecture with distinct TDNN streams.,Kaldi,SOFTWARE
"<p align=""center"">   <img src=""https://github.com/emirdemirel/ALTA/blob/master/img/arch_multi_diverse.png"" width=""400"" height=""200""> </p>   - Cross-domain Training:  <p align=""center"">   <img src=""https://github.com/emirdemirel/ALTA/blob/master/img/crossdomain.png"" width=""500"" height=""160""> </p>   - Music Informed Silence Modeling:  <p align=""center"">   <img src=""https://github.com/emirdemirel/ALTA/blob/master/img/musicsilence.png"" width=""300"" height=""90""> </p>   **Language Model**: The LM is a 4-gram MaxEnt trained using the SRILM toolkit, where Kneser-Ney smoothing applied.",MaxEnt,SOFTWARE
"<p align=""center"">   <img src=""https://github.com/emirdemirel/ALTA/blob/master/img/arch_multi_diverse.png"" width=""400"" height=""200""> </p>   - Cross-domain Training:  <p align=""center"">   <img src=""https://github.com/emirdemirel/ALTA/blob/master/img/crossdomain.png"" width=""500"" height=""160""> </p>   - Music Informed Silence Modeling:  <p align=""center"">   <img src=""https://github.com/emirdemirel/ALTA/blob/master/img/musicsilence.png"" width=""300"" height=""90""> </p>   **Language Model**: The LM is a 4-gram MaxEnt trained using the SRILM toolkit, where Kneser-Ney smoothing applied.",SRILM,SOFTWARE
"We use the singing adapted version presented here[5].   ## Setup    #### 1) Kaldi  This framework is built as a [Kaldi](http://kaldi-asr.org/)[1] recipe  For instructions on Kaldi installation, please visit https://github.com/kaldi-asr/kaldi  #### 2) Dependencies  ``` pip install -r requirements.txt ```   ## How to run  * Modify ```KALDI_ROOT``` in  ```path.sh``` according to where your Kaldi installation is.  ### A) Data preparation: #### 1) Retrieve the data:  * DAMP:  We use the Sing!",Kaldi,SOFTWARE
"We use the singing adapted version presented here[5].   ## Setup    #### 1) Kaldi  This framework is built as a [Kaldi](http://kaldi-asr.org/)[1] recipe  For instructions on Kaldi installation, please visit https://github.com/kaldi-asr/kaldi  #### 2) Dependencies  ``` pip install -r requirements.txt ```   ## How to run  * Modify ```KALDI_ROOT``` in  ```path.sh``` according to where your Kaldi installation is.  ### A) Data preparation: #### 1) Retrieve the data:  * DAMP:  We use the Sing!",Kaldi,SOFTWARE
"We use the singing adapted version presented here[5].   ## Setup    #### 1) Kaldi  This framework is built as a [Kaldi](http://kaldi-asr.org/)[1] recipe  For instructions on Kaldi installation, please visit https://github.com/kaldi-asr/kaldi  #### 2) Dependencies  ``` pip install -r requirements.txt ```   ## How to run  * Modify ```KALDI_ROOT``` in  ```path.sh``` according to where your Kaldi installation is.  ### A) Data preparation: #### 1) Retrieve the data:  * DAMP:  We use the Sing!",kaldi,SOFTWARE
"We use the singing adapted version presented here[5].   ## Setup    #### 1) Kaldi  This framework is built as a [Kaldi](http://kaldi-asr.org/)[1] recipe  For instructions on Kaldi installation, please visit https://github.com/kaldi-asr/kaldi  #### 2) Dependencies  ``` pip install -r requirements.txt ```   ## How to run  * Modify ```KALDI_ROOT``` in  ```path.sh``` according to where your Kaldi installation is.  ### A) Data preparation: #### 1) Retrieve the data:  * DAMP:  We use the Sing!",Kaldi,SOFTWARE
"We use the singing adapted version presented here[5].   ## Setup    #### 1) Kaldi  This framework is built as a [Kaldi](http://kaldi-asr.org/)[1] recipe  For instructions on Kaldi installation, please visit https://github.com/kaldi-asr/kaldi  #### 2) Dependencies  ``` pip install -r requirements.txt ```   ## How to run  * Modify ```KALDI_ROOT``` in  ```path.sh``` according to where your Kaldi installation is.  ### A) Data preparation: #### 1) Retrieve the data:  * DAMP:  We use the Sing!",kaldi,SOFTWARE
"We use the singing adapted version presented here[5].   ## Setup    #### 1) Kaldi  This framework is built as a [Kaldi](http://kaldi-asr.org/)[1] recipe  For instructions on Kaldi installation, please visit https://github.com/kaldi-asr/kaldi  #### 2) Dependencies  ``` pip install -r requirements.txt ```   ## How to run  * Modify ```KALDI_ROOT``` in  ```path.sh``` according to where your Kaldi installation is.  ### A) Data preparation: #### 1) Retrieve the data:  * DAMP:  We use the Sing!",kaldi,SOFTWARE
"We use the singing adapted version presented here[5].   ## Setup    #### 1) Kaldi  This framework is built as a [Kaldi](http://kaldi-asr.org/)[1] recipe  For instructions on Kaldi installation, please visit https://github.com/kaldi-asr/kaldi  #### 2) Dependencies  ``` pip install -r requirements.txt ```   ## How to run  * Modify ```KALDI_ROOT``` in  ```path.sh``` according to where your Kaldi installation is.  ### A) Data preparation: #### 1) Retrieve the data:  * DAMP:  We use the Sing!",pip,SOFTWARE
"We use the singing adapted version presented here[5].   ## Setup    #### 1) Kaldi  This framework is built as a [Kaldi](http://kaldi-asr.org/)[1] recipe  For instructions on Kaldi installation, please visit https://github.com/kaldi-asr/kaldi  #### 2) Dependencies  ``` pip install -r requirements.txt ```   ## How to run  * Modify ```KALDI_ROOT``` in  ```path.sh``` according to where your Kaldi installation is.  ### A) Data preparation: #### 1) Retrieve the data:  * DAMP:  We use the Sing!",KALDI,SOFTWARE
"We use the singing adapted version presented here[5].   ## Setup    #### 1) Kaldi  This framework is built as a [Kaldi](http://kaldi-asr.org/)[1] recipe  For instructions on Kaldi installation, please visit https://github.com/kaldi-asr/kaldi  #### 2) Dependencies  ``` pip install -r requirements.txt ```   ## How to run  * Modify ```KALDI_ROOT``` in  ```path.sh``` according to where your Kaldi installation is.  ### A) Data preparation: #### 1) Retrieve the data:  * DAMP:  We use the Sing!",Kaldi,SOFTWARE
"To retrieve the data, please refer to the relevant Github repository at:  https://github.com/gabolsgabs/DALI  According to the repository, you can download the audio files under 'Getting the audio' section.",Github,SOFTWARE
"""The Kaldi speech recognition toolkit.""",Kaldi,SOFTWARE
[Build Status](https://github.com/ben-manes/caffeine/workflows/build/badge.svg)](https://github.com/ben-manes/caffeine/actions?,caffeine,SOFTWARE
[Build Status](https://github.com/ben-manes/caffeine/workflows/build/badge.svg)](https://github.com/ben-manes/caffeine/actions?,caffeine,SOFTWARE
[Coverage Status](https://img.shields.io/coveralls/ben-manes/caffeine.svg)](https://coveralls.io/r/ben-manes/caffeine?,caffeine,SOFTWARE
[Coverage Status](https://img.shields.io/coveralls/ben-manes/caffeine.svg)](https://coveralls.io/r/ben-manes/caffeine?,caffeine,SOFTWARE
[Maven Central](https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine) [!,caffeine,SOFTWARE
[Maven Central](https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine) [!,caffeine,SOFTWARE
[Maven Central](https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine) [!,caffeine,SOFTWARE
[Maven Central](https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine) [!,caffeine,SOFTWARE
[JavaDoc](http://www.javadoc.io/badge/com.github.ben-manes.caffeine/caffeine.svg)](http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine) [!,caffeine,SOFTWARE
[JavaDoc](http://www.javadoc.io/badge/com.github.ben-manes.caffeine/caffeine.svg)](http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine) [!,caffeine,SOFTWARE
[JavaDoc](http://www.javadoc.io/badge/com.github.ben-manes.caffeine/caffeine.svg)](http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine) [!,caffeine,SOFTWARE
[JavaDoc](http://www.javadoc.io/badge/com.github.ben-manes.caffeine/caffeine.svg)](http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine) [!,caffeine,SOFTWARE
"[Stack Overflow](http://img.shields.io/:stack%20overflow-caffeine-brightgreen.svg)](http://stackoverflow.com/questions/tagged/caffeine) <a href=""https://github.com/ben-manes/caffeine/wiki""> <img align=""right"" height=""90px"" src=""https://raw.githubusercontent.com/ben-manes/caffeine/master/wiki/logo.png""> </a>  Caffeine is a [high performance][benchmarks], [near optimal][efficiency] caching library.",caffeine,SOFTWARE
"[Stack Overflow](http://img.shields.io/:stack%20overflow-caffeine-brightgreen.svg)](http://stackoverflow.com/questions/tagged/caffeine) <a href=""https://github.com/ben-manes/caffeine/wiki""> <img align=""right"" height=""90px"" src=""https://raw.githubusercontent.com/ben-manes/caffeine/master/wiki/logo.png""> </a>  Caffeine is a [high performance][benchmarks], [near optimal][efficiency] caching library.",caffeine,SOFTWARE
"[Stack Overflow](http://img.shields.io/:stack%20overflow-caffeine-brightgreen.svg)](http://stackoverflow.com/questions/tagged/caffeine) <a href=""https://github.com/ben-manes/caffeine/wiki""> <img align=""right"" height=""90px"" src=""https://raw.githubusercontent.com/ben-manes/caffeine/master/wiki/logo.png""> </a>  Caffeine is a [high performance][benchmarks], [near optimal][efficiency] caching library.",caffeine,SOFTWARE
"[Stack Overflow](http://img.shields.io/:stack%20overflow-caffeine-brightgreen.svg)](http://stackoverflow.com/questions/tagged/caffeine) <a href=""https://github.com/ben-manes/caffeine/wiki""> <img align=""right"" height=""90px"" src=""https://raw.githubusercontent.com/ben-manes/caffeine/master/wiki/logo.png""> </a>  Caffeine is a [high performance][benchmarks], [near optimal][efficiency] caching library.",caffeine,SOFTWARE
"[Stack Overflow](http://img.shields.io/:stack%20overflow-caffeine-brightgreen.svg)](http://stackoverflow.com/questions/tagged/caffeine) <a href=""https://github.com/ben-manes/caffeine/wiki""> <img align=""right"" height=""90px"" src=""https://raw.githubusercontent.com/ben-manes/caffeine/master/wiki/logo.png""> </a>  Caffeine is a [high performance][benchmarks], [near optimal][efficiency] caching library.",Caffeine,SOFTWARE
"For more details, see our [user's guide][users-guide] and browse the [API docs][javadoc] for the latest release.  ### Cache  Caffeine provides an in-memory cache using a Google Guava inspired API.",Caffeine,SOFTWARE
"For more details, see our [user's guide][users-guide] and browse the [API docs][javadoc] for the latest release.  ### Cache  Caffeine provides an in-memory cache using a Google Guava inspired API.",Google Guava,SOFTWARE
"* [Design of a Modern Cache: part #1][modern-cache-1], [part #2][modern-cache-2] ([slides][modern-cache-slides]) at [HighScalability][HighScalability]  * Caffeine is presented as part of research papers evaluating its novel eviction policy",Caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
[benchmarks]: https://github.com/ben-manes/caffeine/wiki/Benchmarks [users-guide]: https://github.com/ben-manes/caffeine/wiki [javadoc]: http://www.javadoc.io/doc/com.github.ben-manes.caffeine/caffeine [guava-cache]: https://github.com/google/guava/wiki/CachesExplained [clhm]: https://code.google.com/p/concurrentlinkedhashmap [population]: https://github.com/ben-manes/caffeine/wiki/Population [size]: https://github.com/ben-manes/caffeine/wiki/Eviction#size-based [time]: https://github.com/ben-manes/caffeine/wiki/Eviction#time-based [refresh]: https://github.com/ben-manes/caffeine/wiki/Refresh [reference]: https://github.com/ben-manes/caffeine/wiki/Eviction#reference-based [listener]: https://github.com/ben-manes/caffeine/wiki/Removal [compute]: https://github.com/ben-manes/caffeine/wiki/Compute [statistics]: https://github.com/ben-manes/caffeine/wiki/Statistics [simulator]: https://github.com/ben-manes/caffeine/wiki/Simulator [guava-adapter]: https://github.com/ben-manes/caffeine/wiki/Guava [jsr107]: https://github.com/ben-manes/caffeine/wiki/JCache [maven]: https://maven-badges.herokuapp.com/maven-central/com.github.ben-manes.caffeine/caffeine [releases]: https://github.com/ben-manes/caffeine/releases [snapshots]: https://oss.sonatype.org/content/repositories/snapshots/com/github/ben-manes/caffeine/ [efficiency]: https://github.com/ben-manes/caffeine/wiki/Efficiency [tinylfu]: https://dl.acm.org/authorize?,caffeine,SOFTWARE
# Detecting Corrupted Labels Without Training a Model to Predict  **[Update 5/17/2023]** SimiFeat is a module of [Docta](https://github.com/Docta-ai/docta) now!,SimiFeat,SOFTWARE
# Detecting Corrupted Labels Without Training a Model to Predict  **[Update 5/17/2023]** SimiFeat is a module of [Docta](https://github.com/Docta-ai/docta) now!,Docta,SOFTWARE
# Detecting Corrupted Labels Without Training a Model to Predict  **[Update 5/17/2023]** SimiFeat is a module of [Docta](https://github.com/Docta-ai/docta) now!,docta,SOFTWARE
This code is a PyTorch implementation of the [paper](https://arxiv.org/abs/2110.06283): Detecting Corrupted Labels Without Training a Model to Predict    ## Prerequisites  Python 3.6.9  PyTorch 1.7.1  Torchvision 0.8.2  Full list in .,PyTorch,SOFTWARE
This code is a PyTorch implementation of the [paper](https://arxiv.org/abs/2110.06283): Detecting Corrupted Labels Without Training a Model to Predict    ## Prerequisites  Python 3.6.9  PyTorch 1.7.1  Torchvision 0.8.2  Full list in .,Python 3.6.9,SOFTWARE
This code is a PyTorch implementation of the [paper](https://arxiv.org/abs/2110.06283): Detecting Corrupted Labels Without Training a Model to Predict    ## Prerequisites  Python 3.6.9  PyTorch 1.7.1  Torchvision 0.8.2  Full list in .,PyTorch 1.7.1,SOFTWARE
This code is a PyTorch implementation of the [paper](https://arxiv.org/abs/2110.06283): Detecting Corrupted Labels Without Training a Model to Predict    ## Prerequisites  Python 3.6.9  PyTorch 1.7.1  Torchvision 0.8.2  Full list in .,Torchvision 0.8.2,SOFTWARE
".. raw:: html   The COBRA Toolbox |br| COnstraint-Based Reconstruction and Analysis Toolbox ---------------------------------------------------------------------------  .. raw:: html     <table>      <tr>      <td><div align=""center""><a href=""https://opencobra.github.io/cobratoolbox/latest/tutorials/index.html""><img src=""https://img.shields.io/badge/COBRA-tutorials-blue.svg?",COBRA Toolbox,SOFTWARE
".. raw:: html   The COBRA Toolbox |br| COnstraint-Based Reconstruction and Analysis Toolbox ---------------------------------------------------------------------------  .. raw:: html     <table>      <tr>      <td><div align=""center""><a href=""https://opencobra.github.io/cobratoolbox/latest/tutorials/index.html""><img src=""https://img.shields.io/badge/COBRA-tutorials-blue.svg?",COnstraint-Based Reconstruction and Analysis Toolbox,SOFTWARE
".. raw:: html   The COBRA Toolbox |br| COnstraint-Based Reconstruction and Analysis Toolbox ---------------------------------------------------------------------------  .. raw:: html     <table>      <tr>      <td><div align=""center""><a href=""https://opencobra.github.io/cobratoolbox/latest/tutorials/index.html""><img src=""https://img.shields.io/badge/COBRA-tutorials-blue.svg?",cobratoolbox,SOFTWARE
"maxAge=0""></a>        <a href=""https://opencobra.github.io/cobratoolbox/latest""><img src=""https://img.shields.io/badge/COBRA-docs-blue.svg?",cobratoolbox,SOFTWARE
"forum/cobra-toolbox""><img src=""https://img.shields.io/badge/COBRA-forum-blue.svg?",cobra-toolbox,SOFTWARE
"maxAge=0""></a></div></td>        <td><div align=""center""><a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-linux/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/linux.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-macOS/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/macOS.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-windows7/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/windows.svg""></a>        <a href=""http://opencobra.github.io/cobratoolbox/docs/builds.html""><img src=""http://concordion.org/img/benefit-links.png?",COBRAToolbox,SOFTWARE
"maxAge=0""></a></div></td>        <td><div align=""center""><a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-linux/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/linux.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-macOS/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/macOS.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-windows7/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/windows.svg""></a>        <a href=""http://opencobra.github.io/cobratoolbox/docs/builds.html""><img src=""http://concordion.org/img/benefit-links.png?",cobratoolbox,SOFTWARE
"maxAge=0""></a></div></td>        <td><div align=""center""><a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-linux/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/linux.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-macOS/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/macOS.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-windows7/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/windows.svg""></a>        <a href=""http://opencobra.github.io/cobratoolbox/docs/builds.html""><img src=""http://concordion.org/img/benefit-links.png?",COBRAToolbox,SOFTWARE
"maxAge=0""></a></div></td>        <td><div align=""center""><a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-linux/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/linux.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-macOS/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/macOS.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-windows7/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/windows.svg""></a>        <a href=""http://opencobra.github.io/cobratoolbox/docs/builds.html""><img src=""http://concordion.org/img/benefit-links.png?",cobratoolbox,SOFTWARE
"maxAge=0""></a></div></td>        <td><div align=""center""><a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-linux/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/linux.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-macOS/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/macOS.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-windows7/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/windows.svg""></a>        <a href=""http://opencobra.github.io/cobratoolbox/docs/builds.html""><img src=""http://concordion.org/img/benefit-links.png?",COBRAToolbox,SOFTWARE
"maxAge=0""></a></div></td>        <td><div align=""center""><a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-linux/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/linux.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-macOS/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/macOS.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-windows7/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/windows.svg""></a>        <a href=""http://opencobra.github.io/cobratoolbox/docs/builds.html""><img src=""http://concordion.org/img/benefit-links.png?",cobratoolbox,SOFTWARE
"maxAge=0""></a></div></td>        <td><div align=""center""><a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-linux/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/linux.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-macOS/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/macOS.svg""></a>        <a href=""https://king.nuigalway.ie/jenkins/job/COBRAToolbox-branches-auto-windows7/""><img src=""https://king.nuigalway.ie/cobratoolbox/badges/windows.svg""></a>        <a href=""http://opencobra.github.io/cobratoolbox/docs/builds.html""><img src=""http://concordion.org/img/benefit-links.png?",cobratoolbox,SOFTWARE
"maxAge=0"" height=""20px"" alt=""All continuous integration builds""></a>        </div></td>        <td><div align=""center""><img src=""https://king.nuigalway.ie/cobratoolbox/codegrade/codegrade.svg"" alt=""Ratio of the number of inefficient code lines and the total number of lines of code (in percent).",cobratoolbox,SOFTWARE
"You may install ``TOMLAB``, ``IBM ILOG CPLEX``, ``GUROBI``, or ``MOSEK`` by following these `detailed instructions <https://opencobra.github.io/cobratoolbox/docs/solvers.html>`__.  .. end-requirements-marker  Installation ------------  .. begin-installation-marker  1.",TOMLAB,SOFTWARE
"You may install ``TOMLAB``, ``IBM ILOG CPLEX``, ``GUROBI``, or ``MOSEK`` by following these `detailed instructions <https://opencobra.github.io/cobratoolbox/docs/solvers.html>`__.  .. end-requirements-marker  Installation ------------  .. begin-installation-marker  1.",IBM ILOG CPLEX,SOFTWARE
"You may install ``TOMLAB``, ``IBM ILOG CPLEX``, ``GUROBI``, or ``MOSEK`` by following these `detailed instructions <https://opencobra.github.io/cobratoolbox/docs/solvers.html>`__.  .. end-requirements-marker  Installation ------------  .. begin-installation-marker  1.",GUROBI,SOFTWARE
"You may install ``TOMLAB``, ``IBM ILOG CPLEX``, ``GUROBI``, or ``MOSEK`` by following these `detailed instructions <https://opencobra.github.io/cobratoolbox/docs/solvers.html>`__.  .. end-requirements-marker  Installation ------------  .. begin-installation-marker  1.",MOSEK,SOFTWARE
"You may install ``TOMLAB``, ``IBM ILOG CPLEX``, ``GUROBI``, or ``MOSEK`` by following these `detailed instructions <https://opencobra.github.io/cobratoolbox/docs/solvers.html>`__.  .. end-requirements-marker  Installation ------------  .. begin-installation-marker  1.",cobratoolbox,SOFTWARE
/cobratoolbox/`` will be    created).,cobratoolbox,SOFTWARE
You can clone the repository using:     .. code-block:: console        $ git clone --depth=1 https://github.com/opencobra/cobratoolbox.git cobratoolbox      |warning| Please note the ``--depth=1`` in the clone command.,console,SOFTWARE
You can clone the repository using:     .. code-block:: console        $ git clone --depth=1 https://github.com/opencobra/cobratoolbox.git cobratoolbox      |warning| Please note the ``--depth=1`` in the clone command.,cobratoolbox,SOFTWARE
Run this command in ``Terminal`` (on |macos| and |linux|) or in ``Git Bash`` (on |windows|) -    **not** in |matlab|.,Terminal,SOFTWARE
Run this command in ``Terminal`` (on |macos| and |linux|) or in ``Git Bash`` (on |windows|) -    **not** in |matlab|.,Git Bash,SOFTWARE
Run this command in ``Terminal`` (on |macos| and |linux|) or in ``Git Bash`` (on |windows|) -    **not** in |matlab|.,matlab,SOFTWARE
"Although not recommended, you can download the    repository as a `compressed archive <https://king.nuigalway.ie/cobratoolbox/releases/theCOBRAToolbox.zip>`__.  2.",COBRAToolbox,SOFTWARE
"Change to the folder ``cobratoolbox/`` and run from |matlab|     .. code-block:: matlab        >> initCobraToolbox  .. end-installation-marker   Tutorials, Documentation, and Support -------------------------------------  -  Consult all tutorials in the section |tutorials|.",cobratoolbox,SOFTWARE
"Change to the folder ``cobratoolbox/`` and run from |matlab|     .. code-block:: matlab        >> initCobraToolbox  .. end-installation-marker   Tutorials, Documentation, and Support -------------------------------------  -  Consult all tutorials in the section |tutorials|.",matlab,SOFTWARE
"Change to the folder ``cobratoolbox/`` and run from |matlab|     .. code-block:: matlab        >> initCobraToolbox  .. end-installation-marker   Tutorials, Documentation, and Support -------------------------------------  -  Consult all tutorials in the section |tutorials|.",matlab,SOFTWARE
"Change to the folder ``cobratoolbox/`` and run from |matlab|     .. code-block:: matlab        >> initCobraToolbox  .. end-installation-marker   Tutorials, Documentation, and Support -------------------------------------  -  Consult all tutorials in the section |tutorials|.",CobraToolbox,SOFTWARE
All tutorials can be run from    the    `/tutorials <https://github.com/opencobra/cobratoolbox/tree/master/tutorials>`__    directory,cobratoolbox,SOFTWARE
"|tada| |thumbsup|  .. raw:: html     <p align=""center"">    <img src=""https://cdn.jsdelivr.net/gh/opencobra/MATLAB.devTools@e735bd91310e8ef10fab4d3c21833a85bf4b8159/docs/source/_static/img/logo_devTools.png"" height=""120px"" alt=""devTools""/>    </p>   You can install the `MATLAB.devTools <https://github.com/opencobra/MATLAB.devTools>`__ from within MATLAB by typing:  .. code-block:: matlab      >> installDevTools()  |bulb| Check out `MATLAB.devTools <https://github.com/opencobra/MATLAB.devTools>`__ - and contribute the smart way!",MATLAB.devTools,SOFTWARE
"|tada| |thumbsup|  .. raw:: html     <p align=""center"">    <img src=""https://cdn.jsdelivr.net/gh/opencobra/MATLAB.devTools@e735bd91310e8ef10fab4d3c21833a85bf4b8159/docs/source/_static/img/logo_devTools.png"" height=""120px"" alt=""devTools""/>    </p>   You can install the `MATLAB.devTools <https://github.com/opencobra/MATLAB.devTools>`__ from within MATLAB by typing:  .. code-block:: matlab      >> installDevTools()  |bulb| Check out `MATLAB.devTools <https://github.com/opencobra/MATLAB.devTools>`__ - and contribute the smart way!",MATLAB,SOFTWARE
"|tada| |thumbsup|  .. raw:: html     <p align=""center"">    <img src=""https://cdn.jsdelivr.net/gh/opencobra/MATLAB.devTools@e735bd91310e8ef10fab4d3c21833a85bf4b8159/docs/source/_static/img/logo_devTools.png"" height=""120px"" alt=""devTools""/>    </p>   You can install the `MATLAB.devTools <https://github.com/opencobra/MATLAB.devTools>`__ from within MATLAB by typing:  .. code-block:: matlab      >> installDevTools()  |bulb| Check out `MATLAB.devTools <https://github.com/opencobra/MATLAB.devTools>`__ - and contribute the smart way!",matlab,SOFTWARE
"|tada| |thumbsup|  .. raw:: html     <p align=""center"">    <img src=""https://cdn.jsdelivr.net/gh/opencobra/MATLAB.devTools@e735bd91310e8ef10fab4d3c21833a85bf4b8159/docs/source/_static/img/logo_devTools.png"" height=""120px"" alt=""devTools""/>    </p>   You can install the `MATLAB.devTools <https://github.com/opencobra/MATLAB.devTools>`__ from within MATLAB by typing:  .. code-block:: matlab      >> installDevTools()  |bulb| Check out `MATLAB.devTools <https://github.com/opencobra/MATLAB.devTools>`__ - and contribute the smart way!",MATLAB.devTools,SOFTWARE
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,matlab,SOFTWARE
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,cobratoolbox,SOFTWARE
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,COBRA,SOFTWARE
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,matlab,SOFTWARE
|thumbsup| Contribute to the ``opencobra/cobratoolbox`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#the-cobra-toolbox>`__:  .. code-block:: matlab      >> contribute('opencobra/cobratoolbox');  |thumbsup| Contribute to the ``opencobra/COBRA.tutorials`` repository by following `these instructions <https://opencobra.github.io/MATLAB.devTools/stable/contribute.html#cobra-tutorials>`__:  .. code-block:: matlab      >> contribute('opencobra/COBRA.tutorials');  -  Please follow the `Style    Guide <https://opencobra.github.io/cobratoolbox/docs/styleGuide.html>`__. -  More information on writing a **test** is    `here <https://opencobra.github.io/cobratoolbox/docs/testGuide.html>`__    and a template is    `here <https://opencobra.github.io/cobratoolbox/docs/testTemplate.html>`__. -  More information on formatting the documentation is    `here <https://opencobra.github.io/cobratoolbox/docs/documentationGuide.html>`__ -  A guide for reporting an **issue** is `here <https://opencobra.github.io/cobratoolbox/docs/issueGuide.html>`__.,COBRA,SOFTWARE
"If you want to use ``git`` via the command line interface and need help, this `guide <https://www.digitalocean.com/community/tutorials/how-to-create-a-pull-request-on-github>`__ or the official `GitHub guide <https://help.github.com/articles/creating-a-pull-request/>`__ come in handy.   .. end-how-to-contribute-marker  How to cite the COBRA Toolbox -----------------------------  .. begin-how-to-cite-marker  When citing the COBRA Toolbox, it is important to cite the original paper where an algorithm was first reported, as well as its implementation in the COBRA Toolbox.",git,SOFTWARE
"If you want to use ``git`` via the command line interface and need help, this `guide <https://www.digitalocean.com/community/tutorials/how-to-create-a-pull-request-on-github>`__ or the official `GitHub guide <https://help.github.com/articles/creating-a-pull-request/>`__ come in handy.   .. end-how-to-contribute-marker  How to cite the COBRA Toolbox -----------------------------  .. begin-how-to-cite-marker  When citing the COBRA Toolbox, it is important to cite the original paper where an algorithm was first reported, as well as its implementation in the COBRA Toolbox.",COBRA Toolbox,SOFTWARE
"If you want to use ``git`` via the command line interface and need help, this `guide <https://www.digitalocean.com/community/tutorials/how-to-create-a-pull-request-on-github>`__ or the official `GitHub guide <https://help.github.com/articles/creating-a-pull-request/>`__ come in handy.   .. end-how-to-contribute-marker  How to cite the COBRA Toolbox -----------------------------  .. begin-how-to-cite-marker  When citing the COBRA Toolbox, it is important to cite the original paper where an algorithm was first reported, as well as its implementation in the COBRA Toolbox.",COBRA Toolbox,SOFTWARE
"If you want to use ``git`` via the command line interface and need help, this `guide <https://www.digitalocean.com/community/tutorials/how-to-create-a-pull-request-on-github>`__ or the official `GitHub guide <https://help.github.com/articles/creating-a-pull-request/>`__ come in handy.   .. end-how-to-contribute-marker  How to cite the COBRA Toolbox -----------------------------  .. begin-how-to-cite-marker  When citing the COBRA Toolbox, it is important to cite the original paper where an algorithm was first reported, as well as its implementation in the COBRA Toolbox.",COBRA Toolbox,SOFTWARE
"This is important, because the objective of the COBRA Toolbox is to amalgamate and integrate the functionality of a wide range of COBRA algorithms and this will be undermined if contributors of new algorithms do not get their fair share of citations.",COBRA Toolbox,SOFTWARE
"The following is one example how to approach this within the methods section of a paper (**not** the supplemental material please):  *To generate a context-specific model the FASTCORE algorithm [1], implemented in The COBRA Toolbox v3.0 [2], was employed.*      [1] = Vlassis N, Pacheco MP, Sauter T (2014) Fast Reconstruction of     Compact Context-Specific Metabolic Network Models.",The COBRA Toolbox v3.0,SOFTWARE
"Fleming, **Creation and     analysis of biochemical constraint-based models: the COBRA Toolbox     v3.0**, Nature Protocols, volume 14, pages 639‚Äì702, 2019     `doi.org/10.1038/s41596-018-0098-2 <https://doi.org/10.1038/s41596-018-0098-2>`__.  .. end-how-to-cite-marker  Binaries and Compatibility --------------------------  |warning| Please make sure you install a compatible solver.",COBRA Toolbox     v3.0,SOFTWARE
"Check the compatibility `here <https://opencobra.github.io/cobratoolbox/docs/compatibility.html>`__.  .. begin-binaries-marker  For convenience, we provide `glpk_mex <https://github.com/blegat/glpkmex>`__ and `libSBML-5.17+ <http://sbml.org/Software/libSBML>`__ in ``/external``.",cobratoolbox,SOFTWARE
"Check the compatibility `here <https://opencobra.github.io/cobratoolbox/docs/compatibility.html>`__.  .. begin-binaries-marker  For convenience, we provide `glpk_mex <https://github.com/blegat/glpkmex>`__ and `libSBML-5.17+ <http://sbml.org/Software/libSBML>`__ in ``/external``.",glpk_mex,SOFTWARE
"Check the compatibility `here <https://opencobra.github.io/cobratoolbox/docs/compatibility.html>`__.  .. begin-binaries-marker  For convenience, we provide `glpk_mex <https://github.com/blegat/glpkmex>`__ and `libSBML-5.17+ <http://sbml.org/Software/libSBML>`__ in ``/external``.",glpkmex,SOFTWARE
"Check the compatibility `here <https://opencobra.github.io/cobratoolbox/docs/compatibility.html>`__.  .. begin-binaries-marker  For convenience, we provide `glpk_mex <https://github.com/blegat/glpkmex>`__ and `libSBML-5.17+ <http://sbml.org/Software/libSBML>`__ in ``/external``.",libSBML-5.17,SOFTWARE
"Check the compatibility `here <https://opencobra.github.io/cobratoolbox/docs/compatibility.html>`__.  .. begin-binaries-marker  For convenience, we provide `glpk_mex <https://github.com/blegat/glpkmex>`__ and `libSBML-5.17+ <http://sbml.org/Software/libSBML>`__ in ``/external``.",libSBML,SOFTWARE
"`Binaries <https://github.com/opencobra/COBRA.binary>`__ for these libraries are provided in a submodule for Mac OS X 10.6 or later (64-bit), GNU/Linux Ubuntu 14.0+ (64-bit), and Microsoft Windows 7+ (64-bit).",COBRA,SOFTWARE
"For unsupported OS, please refer to their respective building instructions (`glpk_mex <https://github.com/blegat/glpkmex#instructions-for-compiling-from-source>`__, `libSBML <http://sbml.org/Software/libSBML/5.17.0/docs//cpp-api/libsbml-installation.html>`__).",glpk_mex,SOFTWARE
"For unsupported OS, please refer to their respective building instructions (`glpk_mex <https://github.com/blegat/glpkmex#instructions-for-compiling-from-source>`__, `libSBML <http://sbml.org/Software/libSBML/5.17.0/docs//cpp-api/libsbml-installation.html>`__).",glpkmex,SOFTWARE
"For unsupported OS, please refer to their respective building instructions (`glpk_mex <https://github.com/blegat/glpkmex#instructions-for-compiling-from-source>`__, `libSBML <http://sbml.org/Software/libSBML/5.17.0/docs//cpp-api/libsbml-installation.html>`__).",libSBML,SOFTWARE
"For unsupported OS, please refer to their respective building instructions (`glpk_mex <https://github.com/blegat/glpkmex#instructions-for-compiling-from-source>`__, `libSBML <http://sbml.org/Software/libSBML/5.17.0/docs//cpp-api/libsbml-installation.html>`__).",libSBML,SOFTWARE
Read more on the compatibility with SBML-FBCv2 `here <https://opencobra.github.io/cobratoolbox/docs/notes.html>`__.  .. end-binaries-marker  Disclaimer ----------  *The software provided by the openCOBRA Project is distributed under the GNU GPLv3 or later.,cobratoolbox,SOFTWARE
"|macos| raw:: html     <img src=""https://king.nuigalway.ie/cobratoolbox/img/apple.png"" height=""20px"" width=""20px"" alt=""macOS"">   ..",cobratoolbox,SOFTWARE
"|macos| raw:: html     <img src=""https://king.nuigalway.ie/cobratoolbox/img/apple.png"" height=""20px"" width=""20px"" alt=""macOS"">   ..",macOS,SOFTWARE
"|linux| raw:: html     <img src=""https://king.nuigalway.ie/cobratoolbox/img/linux.png"" height=""20px"" width=""20px"" alt=""linux"">   ..",cobratoolbox,SOFTWARE
"|linux| raw:: html     <img src=""https://king.nuigalway.ie/cobratoolbox/img/linux.png"" height=""20px"" width=""20px"" alt=""linux"">   ..",linux,SOFTWARE
"|windows| raw:: html     <img src=""https://king.nuigalway.ie/cobratoolbox/img/windows.png"" height=""20px"" width=""20px"" alt=""windows"">   ..",cobratoolbox,SOFTWARE
"|windows| raw:: html     <img src=""https://king.nuigalway.ie/cobratoolbox/img/windows.png"" height=""20px"" width=""20px"" alt=""windows"">   ..",windows,SOFTWARE
"|warning| raw:: html     <img src=""https://king.nuigalway.ie/cobratoolbox/img/warning.png"" height=""20px"" width=""20px"" alt=""warning"">   ..",cobratoolbox,SOFTWARE
"|matlab| raw:: html     <img src=""https://king.nuigalway.ie/cobratoolbox/img/matlab.png"" height=""20px"" width=""20px"" alt=""matlab"">   ..",matlab,SOFTWARE
"|matlab| raw:: html     <img src=""https://king.nuigalway.ie/cobratoolbox/img/matlab.png"" height=""20px"" width=""20px"" alt=""matlab"">   ..",cobratoolbox,SOFTWARE
"|matlab| raw:: html     <img src=""https://king.nuigalway.ie/cobratoolbox/img/matlab.png"" height=""20px"" width=""20px"" alt=""matlab"">   ..",matlab,SOFTWARE
"|tada| raw:: html     <img src=""https://king.nuigalway.ie/cobratoolbox/img/tada.png"" height=""20px"" width=""20px"" alt=""tada"">   ..",cobratoolbox,SOFTWARE
"|thumbsup| raw:: html     <img src=""https://king.nuigalway.ie/cobratoolbox/img/thumbsUP.png"" height=""20px"" width=""20px"" alt=""thumbsup"">   ..",cobratoolbox,SOFTWARE
"|bulb| raw:: html     <img src=""https://king.nuigalway.ie/cobratoolbox/img/bulb.png"" height=""20px"" width=""20px"" alt=""bulb"">   ..",cobratoolbox,SOFTWARE
"|tutorials| raw:: html     <a href=""https://opencobra.github.io/cobratoolbox/latest/tutorials/index.html""><img src=""https://img.shields.io/badge/COBRA-tutorials-blue.svg?",cobratoolbox,SOFTWARE
"|latest| raw:: html     <a href=""https://opencobra.github.io/cobratoolbox/latest""><img src=""https://img.shields.io/badge/COBRA-docs-blue.svg?",cobratoolbox,SOFTWARE
"forum/cobra-toolbox""><img src=""https://img.shields.io/badge/COBRA-forum-blue.svg""></a>   ..",cobra-toolbox,SOFTWARE
git clone https://github.com/john-hewitt/vinfo-probing-internal/         cd vinfo-probing-internal          1.,git,SOFTWARE
conda create --name sp-env         conda activate sp-env  1.,conda,SOFTWARE
conda install --file requirements.txt  1.,conda,SOFTWARE
"This is crucial for running many experiments with Huggingface `transformers` models, since the tokenization and alignment of subword tokens to corpus tokens takes more time than running the experiment itself once loaded.  ``` cache: &id_cache !",transformers,SOFTWARE
"OntonotesReader &id_disk_reader   args:     device: cpu   train_path: *idtrainpath    dev_path: *iddevpath    test_path: *idtestpath  ``` The `args` bit here is sort of a vestigal part of earlier code design; its only member, the `device`, is used whenver PyTorch objects are involved, to put tensors on the right device.",PyTorch,SOFTWARE
"Now, run  ``` cd scripts/ontonotes_scripts ldc_ontonotes_path=/scr/corpora/ldc/2013/LDC2013T19/ontonotes-release-5.0/data/files/data/ bash prep_ontonotes_v4.sh $ldc_onotonotes_path ```  Nice.",bash,SOFTWARE
"The slides for our ACL presentation can be found [here](https://github.com/princeton-nlp/EvalConvQA/blob/main/ACL%202022%20Video%20talk.pdf).  ## Quick links * [Overview](#Overview) * [Human Evaluation Dataset](#Human-Evaluation-Dataset) * [Automatic model evaluation interface](#Automatic-model-evaluation-interface) * [Setup](#Setup)   * [Install dependencies](#Install-dependencies)   * [Download the datasets](#Download-the-datasets) * [Evaluating existing models](#Evaluating-existing-models)   * [BERT](#BERT)   * [GraphFlow](#GraphFlow)   * [HAM](#HAM)   * [ExCorD](#ExCorD) * [Evaluating your own model](#Evaluating-your-own-model) * [Citation](#Citation)  ## Overview  In this work, we conduct the first large-scale human evaluation of state-of-the-art conversational QA systems.",BERT,SOFTWARE
"The slides for our ACL presentation can be found [here](https://github.com/princeton-nlp/EvalConvQA/blob/main/ACL%202022%20Video%20talk.pdf).  ## Quick links * [Overview](#Overview) * [Human Evaluation Dataset](#Human-Evaluation-Dataset) * [Automatic model evaluation interface](#Automatic-model-evaluation-interface) * [Setup](#Setup)   * [Install dependencies](#Install-dependencies)   * [Download the datasets](#Download-the-datasets) * [Evaluating existing models](#Evaluating-existing-models)   * [BERT](#BERT)   * [GraphFlow](#GraphFlow)   * [HAM](#HAM)   * [ExCorD](#ExCorD) * [Evaluating your own model](#Evaluating-your-own-model) * [Citation](#Citation)  ## Overview  In this work, we conduct the first large-scale human evaluation of state-of-the-art conversational QA systems.",GraphFlow,SOFTWARE
"The slides for our ACL presentation can be found [here](https://github.com/princeton-nlp/EvalConvQA/blob/main/ACL%202022%20Video%20talk.pdf).  ## Quick links * [Overview](#Overview) * [Human Evaluation Dataset](#Human-Evaluation-Dataset) * [Automatic model evaluation interface](#Automatic-model-evaluation-interface) * [Setup](#Setup)   * [Install dependencies](#Install-dependencies)   * [Download the datasets](#Download-the-datasets) * [Evaluating existing models](#Evaluating-existing-models)   * [BERT](#BERT)   * [GraphFlow](#GraphFlow)   * [HAM](#HAM)   * [ExCorD](#ExCorD) * [Evaluating your own model](#Evaluating-your-own-model) * [Citation](#Citation)  ## Overview  In this work, we conduct the first large-scale human evaluation of state-of-the-art conversational QA systems.",HAM,SOFTWARE
"The slides for our ACL presentation can be found [here](https://github.com/princeton-nlp/EvalConvQA/blob/main/ACL%202022%20Video%20talk.pdf).  ## Quick links * [Overview](#Overview) * [Human Evaluation Dataset](#Human-Evaluation-Dataset) * [Automatic model evaluation interface](#Automatic-model-evaluation-interface) * [Setup](#Setup)   * [Install dependencies](#Install-dependencies)   * [Download the datasets](#Download-the-datasets) * [Evaluating existing models](#Evaluating-existing-models)   * [BERT](#BERT)   * [GraphFlow](#GraphFlow)   * [HAM](#HAM)   * [ExCorD](#ExCorD) * [Evaluating your own model](#Evaluating-your-own-model) * [Citation](#Citation)  ## Overview  In this work, we conduct the first large-scale human evaluation of state-of-the-art conversational QA systems.",ExCorD,SOFTWARE
"[Auto-rewrite](figs/autorewrite.png)  ## Setup  ### Install dependencies  Please install all dependency packages using the following command: ```bash pip install -r requirements.txt ```  ### Download the datasets  Our experiments use [QuAC dataset](https://quac.ai) for passages and conversations, and the test set of [CANARD dataset](https://sites.google.com/view/qanta/projects/canard) for context-independent questions in `Auto-Replace`.  ## Evaluating existing models  We provide our implementations for the four models that we used in our paper: BERT, [GraphFlow](https://www.ijcai.org/Proceedings/2020/171), [HAM](https://dl.acm.org/doi/abs/10.1145/3357384.3357905), [ExCorD](https://aclanthology.org/2021.acl-long.478/).",pip,SOFTWARE
"[Auto-rewrite](figs/autorewrite.png)  ## Setup  ### Install dependencies  Please install all dependency packages using the following command: ```bash pip install -r requirements.txt ```  ### Download the datasets  Our experiments use [QuAC dataset](https://quac.ai) for passages and conversations, and the test set of [CANARD dataset](https://sites.google.com/view/qanta/projects/canard) for context-independent questions in `Auto-Replace`.  ## Evaluating existing models  We provide our implementations for the four models that we used in our paper: BERT, [GraphFlow](https://www.ijcai.org/Proceedings/2020/171), [HAM](https://dl.acm.org/doi/abs/10.1145/3357384.3357905), [ExCorD](https://aclanthology.org/2021.acl-long.478/).",BERT,SOFTWARE
"[Auto-rewrite](figs/autorewrite.png)  ## Setup  ### Install dependencies  Please install all dependency packages using the following command: ```bash pip install -r requirements.txt ```  ### Download the datasets  Our experiments use [QuAC dataset](https://quac.ai) for passages and conversations, and the test set of [CANARD dataset](https://sites.google.com/view/qanta/projects/canard) for context-independent questions in `Auto-Replace`.  ## Evaluating existing models  We provide our implementations for the four models that we used in our paper: BERT, [GraphFlow](https://www.ijcai.org/Proceedings/2020/171), [HAM](https://dl.acm.org/doi/abs/10.1145/3357384.3357905), [ExCorD](https://aclanthology.org/2021.acl-long.478/).",GraphFlow,SOFTWARE
"[Auto-rewrite](figs/autorewrite.png)  ## Setup  ### Install dependencies  Please install all dependency packages using the following command: ```bash pip install -r requirements.txt ```  ### Download the datasets  Our experiments use [QuAC dataset](https://quac.ai) for passages and conversations, and the test set of [CANARD dataset](https://sites.google.com/view/qanta/projects/canard) for context-independent questions in `Auto-Replace`.  ## Evaluating existing models  We provide our implementations for the four models that we used in our paper: BERT, [GraphFlow](https://www.ijcai.org/Proceedings/2020/171), [HAM](https://dl.acm.org/doi/abs/10.1145/3357384.3357905), [ExCorD](https://aclanthology.org/2021.acl-long.478/).",HAM,SOFTWARE
"[Auto-rewrite](figs/autorewrite.png)  ## Setup  ### Install dependencies  Please install all dependency packages using the following command: ```bash pip install -r requirements.txt ```  ### Download the datasets  Our experiments use [QuAC dataset](https://quac.ai) for passages and conversations, and the test set of [CANARD dataset](https://sites.google.com/view/qanta/projects/canard) for context-independent questions in `Auto-Replace`.  ## Evaluating existing models  We provide our implementations for the four models that we used in our paper: BERT, [GraphFlow](https://www.ijcai.org/Proceedings/2020/171), [HAM](https://dl.acm.org/doi/abs/10.1145/3357384.3357905), [ExCorD](https://aclanthology.org/2021.acl-long.478/).",ExCorD,SOFTWARE
```bash # Run Training python run_quac_train.py \   --type bert \   --model_name_or_path bert-base-uncased \   --do_train \   --output_dir ${directory_to_save_model} \   --overwrite_output_dir \   --train_file ${path_to_quac_train_file} \   --train_batch_size 8 \   --gradient_accumulation_steps 4 \   --max_seq_length 512 \   --learning_rate 3e-5 \   --history_len 2 \   --warmup_proportion 0.1 \   --max_grad_norm -1 \   --weight_decay 0.01 \   --rationale_beta 0 \ # important for BERT  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type bert \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --match_metric f1 \   --add_background \   --skip_entity \   --rewrite \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### GraphFlow We did not find an uploaded model checkpoint so we trained our own using [their training script](https://github.com/hugochan/GraphFlow).,python,SOFTWARE
```bash # Run Training python run_quac_train.py \   --type bert \   --model_name_or_path bert-base-uncased \   --do_train \   --output_dir ${directory_to_save_model} \   --overwrite_output_dir \   --train_file ${path_to_quac_train_file} \   --train_batch_size 8 \   --gradient_accumulation_steps 4 \   --max_seq_length 512 \   --learning_rate 3e-5 \   --history_len 2 \   --warmup_proportion 0.1 \   --max_grad_norm -1 \   --weight_decay 0.01 \   --rationale_beta 0 \ # important for BERT  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type bert \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --match_metric f1 \   --add_background \   --skip_entity \   --rewrite \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### GraphFlow We did not find an uploaded model checkpoint so we trained our own using [their training script](https://github.com/hugochan/GraphFlow).,python,SOFTWARE
```bash # Run Training python run_quac_train.py \   --type bert \   --model_name_or_path bert-base-uncased \   --do_train \   --output_dir ${directory_to_save_model} \   --overwrite_output_dir \   --train_file ${path_to_quac_train_file} \   --train_batch_size 8 \   --gradient_accumulation_steps 4 \   --max_seq_length 512 \   --learning_rate 3e-5 \   --history_len 2 \   --warmup_proportion 0.1 \   --max_grad_norm -1 \   --weight_decay 0.01 \   --rationale_beta 0 \ # important for BERT  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type bert \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --match_metric f1 \   --add_background \   --skip_entity \   --rewrite \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### GraphFlow We did not find an uploaded model checkpoint so we trained our own using [their training script](https://github.com/hugochan/GraphFlow).,GraphFlow,SOFTWARE
"```bash  # Download Stanford CoreNLP package wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip unzip stanford-corenlp-latest.zip rm -f stanford-corenlp-latest.zip  # Start StanfordCoreNLP server java -mx4g -cp ""${directory_to_standford_corenlp_package}"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 &  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \     --type graphflow \     --predict_file ${path-to-annotated-dev-json-file} \     --output_dir ${directory-to-model-checkpoint} \     --saved_vocab_file ${directory-to-saved-model-vocab} \     --pretrained ${directory-to-model-checkpoint} \     --write_dir /n/fs/scratch/huihanl/unified/graphflow/write \     --match_metric f1 \     --add_background \     --skip_entity \     --rewrite \     --fix_vocab_embed \     --f_qem \     --f_pos \     --f_ner \     --use_ques_marker \     --use_gnn \     --temporal_gnn \     --use_bert \     --use_bert_weight \     --shuffle \     --out_predictions \     --predict_raw_text \     --out_pred_in_folder \     --optimizer adamax \     --start_i ${index_of_first_passage_to_eval} \     --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### HAM The orgininal model checkpoint can be downloaded from [CodaLab](https://worksheets.codalab.org/rest/bundles/0x5c08cb0fb90c4afd8a2811bb63023cce/contents/blob/)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type ham \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --do_lower_case \   --history_len 6 \   --match_metric f1 \   --add_background \   --skip_entity \   --replace \   --init_checkpoint ${directory-to-model-checkpoint}/model_52000.ckpt \   --bert_config_file ${directory-to-pretrained-bert-large-uncased}/bert_config.json \   --vocab_file ${directory-to-model-checkpoint}/vocab.txt \   --MTL_mu 0.8 \   --MTL_lambda 0.1 \   --mtl_input reduce_mean \   --max_answer_length 40 \   --max_considered_history_turns 4 \   --bert_hidden 1024 \   --fine_grained_attention \   --better_hae \   --MTL \   --use_history_answer_marker \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### ExCorD The original model checkpoint can be downloaded from [their repo](https://drive.google.com/file/d/1Xf0-XUvGi7jgiAAdA5BQLk7p5ikc_wOl/view?",Stanford CoreNLP,SOFTWARE
"```bash  # Download Stanford CoreNLP package wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip unzip stanford-corenlp-latest.zip rm -f stanford-corenlp-latest.zip  # Start StanfordCoreNLP server java -mx4g -cp ""${directory_to_standford_corenlp_package}"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 &  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \     --type graphflow \     --predict_file ${path-to-annotated-dev-json-file} \     --output_dir ${directory-to-model-checkpoint} \     --saved_vocab_file ${directory-to-saved-model-vocab} \     --pretrained ${directory-to-model-checkpoint} \     --write_dir /n/fs/scratch/huihanl/unified/graphflow/write \     --match_metric f1 \     --add_background \     --skip_entity \     --rewrite \     --fix_vocab_embed \     --f_qem \     --f_pos \     --f_ner \     --use_ques_marker \     --use_gnn \     --temporal_gnn \     --use_bert \     --use_bert_weight \     --shuffle \     --out_predictions \     --predict_raw_text \     --out_pred_in_folder \     --optimizer adamax \     --start_i ${index_of_first_passage_to_eval} \     --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### HAM The orgininal model checkpoint can be downloaded from [CodaLab](https://worksheets.codalab.org/rest/bundles/0x5c08cb0fb90c4afd8a2811bb63023cce/contents/blob/)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type ham \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --do_lower_case \   --history_len 6 \   --match_metric f1 \   --add_background \   --skip_entity \   --replace \   --init_checkpoint ${directory-to-model-checkpoint}/model_52000.ckpt \   --bert_config_file ${directory-to-pretrained-bert-large-uncased}/bert_config.json \   --vocab_file ${directory-to-model-checkpoint}/vocab.txt \   --MTL_mu 0.8 \   --MTL_lambda 0.1 \   --mtl_input reduce_mean \   --max_answer_length 40 \   --max_considered_history_turns 4 \   --bert_hidden 1024 \   --fine_grained_attention \   --better_hae \   --MTL \   --use_history_answer_marker \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### ExCorD The original model checkpoint can be downloaded from [their repo](https://drive.google.com/file/d/1Xf0-XUvGi7jgiAAdA5BQLk7p5ikc_wOl/view?",wget,SOFTWARE
"```bash  # Download Stanford CoreNLP package wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip unzip stanford-corenlp-latest.zip rm -f stanford-corenlp-latest.zip  # Start StanfordCoreNLP server java -mx4g -cp ""${directory_to_standford_corenlp_package}"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 &  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \     --type graphflow \     --predict_file ${path-to-annotated-dev-json-file} \     --output_dir ${directory-to-model-checkpoint} \     --saved_vocab_file ${directory-to-saved-model-vocab} \     --pretrained ${directory-to-model-checkpoint} \     --write_dir /n/fs/scratch/huihanl/unified/graphflow/write \     --match_metric f1 \     --add_background \     --skip_entity \     --rewrite \     --fix_vocab_embed \     --f_qem \     --f_pos \     --f_ner \     --use_ques_marker \     --use_gnn \     --temporal_gnn \     --use_bert \     --use_bert_weight \     --shuffle \     --out_predictions \     --predict_raw_text \     --out_pred_in_folder \     --optimizer adamax \     --start_i ${index_of_first_passage_to_eval} \     --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### HAM The orgininal model checkpoint can be downloaded from [CodaLab](https://worksheets.codalab.org/rest/bundles/0x5c08cb0fb90c4afd8a2811bb63023cce/contents/blob/)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type ham \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --do_lower_case \   --history_len 6 \   --match_metric f1 \   --add_background \   --skip_entity \   --replace \   --init_checkpoint ${directory-to-model-checkpoint}/model_52000.ckpt \   --bert_config_file ${directory-to-pretrained-bert-large-uncased}/bert_config.json \   --vocab_file ${directory-to-model-checkpoint}/vocab.txt \   --MTL_mu 0.8 \   --MTL_lambda 0.1 \   --mtl_input reduce_mean \   --max_answer_length 40 \   --max_considered_history_turns 4 \   --bert_hidden 1024 \   --fine_grained_attention \   --better_hae \   --MTL \   --use_history_answer_marker \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### ExCorD The original model checkpoint can be downloaded from [their repo](https://drive.google.com/file/d/1Xf0-XUvGi7jgiAAdA5BQLk7p5ikc_wOl/view?",stanford-corenlp,SOFTWARE
"```bash  # Download Stanford CoreNLP package wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip unzip stanford-corenlp-latest.zip rm -f stanford-corenlp-latest.zip  # Start StanfordCoreNLP server java -mx4g -cp ""${directory_to_standford_corenlp_package}"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 &  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \     --type graphflow \     --predict_file ${path-to-annotated-dev-json-file} \     --output_dir ${directory-to-model-checkpoint} \     --saved_vocab_file ${directory-to-saved-model-vocab} \     --pretrained ${directory-to-model-checkpoint} \     --write_dir /n/fs/scratch/huihanl/unified/graphflow/write \     --match_metric f1 \     --add_background \     --skip_entity \     --rewrite \     --fix_vocab_embed \     --f_qem \     --f_pos \     --f_ner \     --use_ques_marker \     --use_gnn \     --temporal_gnn \     --use_bert \     --use_bert_weight \     --shuffle \     --out_predictions \     --predict_raw_text \     --out_pred_in_folder \     --optimizer adamax \     --start_i ${index_of_first_passage_to_eval} \     --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### HAM The orgininal model checkpoint can be downloaded from [CodaLab](https://worksheets.codalab.org/rest/bundles/0x5c08cb0fb90c4afd8a2811bb63023cce/contents/blob/)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type ham \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --do_lower_case \   --history_len 6 \   --match_metric f1 \   --add_background \   --skip_entity \   --replace \   --init_checkpoint ${directory-to-model-checkpoint}/model_52000.ckpt \   --bert_config_file ${directory-to-pretrained-bert-large-uncased}/bert_config.json \   --vocab_file ${directory-to-model-checkpoint}/vocab.txt \   --MTL_mu 0.8 \   --MTL_lambda 0.1 \   --mtl_input reduce_mean \   --max_answer_length 40 \   --max_considered_history_turns 4 \   --bert_hidden 1024 \   --fine_grained_attention \   --better_hae \   --MTL \   --use_history_answer_marker \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### ExCorD The original model checkpoint can be downloaded from [their repo](https://drive.google.com/file/d/1Xf0-XUvGi7jgiAAdA5BQLk7p5ikc_wOl/view?",stanford-corenlp,SOFTWARE
"```bash  # Download Stanford CoreNLP package wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip unzip stanford-corenlp-latest.zip rm -f stanford-corenlp-latest.zip  # Start StanfordCoreNLP server java -mx4g -cp ""${directory_to_standford_corenlp_package}"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 &  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \     --type graphflow \     --predict_file ${path-to-annotated-dev-json-file} \     --output_dir ${directory-to-model-checkpoint} \     --saved_vocab_file ${directory-to-saved-model-vocab} \     --pretrained ${directory-to-model-checkpoint} \     --write_dir /n/fs/scratch/huihanl/unified/graphflow/write \     --match_metric f1 \     --add_background \     --skip_entity \     --rewrite \     --fix_vocab_embed \     --f_qem \     --f_pos \     --f_ner \     --use_ques_marker \     --use_gnn \     --temporal_gnn \     --use_bert \     --use_bert_weight \     --shuffle \     --out_predictions \     --predict_raw_text \     --out_pred_in_folder \     --optimizer adamax \     --start_i ${index_of_first_passage_to_eval} \     --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### HAM The orgininal model checkpoint can be downloaded from [CodaLab](https://worksheets.codalab.org/rest/bundles/0x5c08cb0fb90c4afd8a2811bb63023cce/contents/blob/)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type ham \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --do_lower_case \   --history_len 6 \   --match_metric f1 \   --add_background \   --skip_entity \   --replace \   --init_checkpoint ${directory-to-model-checkpoint}/model_52000.ckpt \   --bert_config_file ${directory-to-pretrained-bert-large-uncased}/bert_config.json \   --vocab_file ${directory-to-model-checkpoint}/vocab.txt \   --MTL_mu 0.8 \   --MTL_lambda 0.1 \   --mtl_input reduce_mean \   --max_answer_length 40 \   --max_considered_history_turns 4 \   --bert_hidden 1024 \   --fine_grained_attention \   --better_hae \   --MTL \   --use_history_answer_marker \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### ExCorD The original model checkpoint can be downloaded from [their repo](https://drive.google.com/file/d/1Xf0-XUvGi7jgiAAdA5BQLk7p5ikc_wOl/view?",stanford-corenlp,SOFTWARE
"```bash  # Download Stanford CoreNLP package wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip unzip stanford-corenlp-latest.zip rm -f stanford-corenlp-latest.zip  # Start StanfordCoreNLP server java -mx4g -cp ""${directory_to_standford_corenlp_package}"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 &  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \     --type graphflow \     --predict_file ${path-to-annotated-dev-json-file} \     --output_dir ${directory-to-model-checkpoint} \     --saved_vocab_file ${directory-to-saved-model-vocab} \     --pretrained ${directory-to-model-checkpoint} \     --write_dir /n/fs/scratch/huihanl/unified/graphflow/write \     --match_metric f1 \     --add_background \     --skip_entity \     --rewrite \     --fix_vocab_embed \     --f_qem \     --f_pos \     --f_ner \     --use_ques_marker \     --use_gnn \     --temporal_gnn \     --use_bert \     --use_bert_weight \     --shuffle \     --out_predictions \     --predict_raw_text \     --out_pred_in_folder \     --optimizer adamax \     --start_i ${index_of_first_passage_to_eval} \     --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### HAM The orgininal model checkpoint can be downloaded from [CodaLab](https://worksheets.codalab.org/rest/bundles/0x5c08cb0fb90c4afd8a2811bb63023cce/contents/blob/)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type ham \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --do_lower_case \   --history_len 6 \   --match_metric f1 \   --add_background \   --skip_entity \   --replace \   --init_checkpoint ${directory-to-model-checkpoint}/model_52000.ckpt \   --bert_config_file ${directory-to-pretrained-bert-large-uncased}/bert_config.json \   --vocab_file ${directory-to-model-checkpoint}/vocab.txt \   --MTL_mu 0.8 \   --MTL_lambda 0.1 \   --mtl_input reduce_mean \   --max_answer_length 40 \   --max_considered_history_turns 4 \   --bert_hidden 1024 \   --fine_grained_attention \   --better_hae \   --MTL \   --use_history_answer_marker \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### ExCorD The original model checkpoint can be downloaded from [their repo](https://drive.google.com/file/d/1Xf0-XUvGi7jgiAAdA5BQLk7p5ikc_wOl/view?",StanfordCoreNLP,SOFTWARE
"```bash  # Download Stanford CoreNLP package wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip unzip stanford-corenlp-latest.zip rm -f stanford-corenlp-latest.zip  # Start StanfordCoreNLP server java -mx4g -cp ""${directory_to_standford_corenlp_package}"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 &  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \     --type graphflow \     --predict_file ${path-to-annotated-dev-json-file} \     --output_dir ${directory-to-model-checkpoint} \     --saved_vocab_file ${directory-to-saved-model-vocab} \     --pretrained ${directory-to-model-checkpoint} \     --write_dir /n/fs/scratch/huihanl/unified/graphflow/write \     --match_metric f1 \     --add_background \     --skip_entity \     --rewrite \     --fix_vocab_embed \     --f_qem \     --f_pos \     --f_ner \     --use_ques_marker \     --use_gnn \     --temporal_gnn \     --use_bert \     --use_bert_weight \     --shuffle \     --out_predictions \     --predict_raw_text \     --out_pred_in_folder \     --optimizer adamax \     --start_i ${index_of_first_passage_to_eval} \     --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### HAM The orgininal model checkpoint can be downloaded from [CodaLab](https://worksheets.codalab.org/rest/bundles/0x5c08cb0fb90c4afd8a2811bb63023cce/contents/blob/)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type ham \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --do_lower_case \   --history_len 6 \   --match_metric f1 \   --add_background \   --skip_entity \   --replace \   --init_checkpoint ${directory-to-model-checkpoint}/model_52000.ckpt \   --bert_config_file ${directory-to-pretrained-bert-large-uncased}/bert_config.json \   --vocab_file ${directory-to-model-checkpoint}/vocab.txt \   --MTL_mu 0.8 \   --MTL_lambda 0.1 \   --mtl_input reduce_mean \   --max_answer_length 40 \   --max_considered_history_turns 4 \   --bert_hidden 1024 \   --fine_grained_attention \   --better_hae \   --MTL \   --use_history_answer_marker \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### ExCorD The original model checkpoint can be downloaded from [their repo](https://drive.google.com/file/d/1Xf0-XUvGi7jgiAAdA5BQLk7p5ikc_wOl/view?",standford_corenlp,SOFTWARE
"```bash  # Download Stanford CoreNLP package wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip unzip stanford-corenlp-latest.zip rm -f stanford-corenlp-latest.zip  # Start StanfordCoreNLP server java -mx4g -cp ""${directory_to_standford_corenlp_package}"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 &  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \     --type graphflow \     --predict_file ${path-to-annotated-dev-json-file} \     --output_dir ${directory-to-model-checkpoint} \     --saved_vocab_file ${directory-to-saved-model-vocab} \     --pretrained ${directory-to-model-checkpoint} \     --write_dir /n/fs/scratch/huihanl/unified/graphflow/write \     --match_metric f1 \     --add_background \     --skip_entity \     --rewrite \     --fix_vocab_embed \     --f_qem \     --f_pos \     --f_ner \     --use_ques_marker \     --use_gnn \     --temporal_gnn \     --use_bert \     --use_bert_weight \     --shuffle \     --out_predictions \     --predict_raw_text \     --out_pred_in_folder \     --optimizer adamax \     --start_i ${index_of_first_passage_to_eval} \     --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### HAM The orgininal model checkpoint can be downloaded from [CodaLab](https://worksheets.codalab.org/rest/bundles/0x5c08cb0fb90c4afd8a2811bb63023cce/contents/blob/)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type ham \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --do_lower_case \   --history_len 6 \   --match_metric f1 \   --add_background \   --skip_entity \   --replace \   --init_checkpoint ${directory-to-model-checkpoint}/model_52000.ckpt \   --bert_config_file ${directory-to-pretrained-bert-large-uncased}/bert_config.json \   --vocab_file ${directory-to-model-checkpoint}/vocab.txt \   --MTL_mu 0.8 \   --MTL_lambda 0.1 \   --mtl_input reduce_mean \   --max_answer_length 40 \   --max_considered_history_turns 4 \   --bert_hidden 1024 \   --fine_grained_attention \   --better_hae \   --MTL \   --use_history_answer_marker \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### ExCorD The original model checkpoint can be downloaded from [their repo](https://drive.google.com/file/d/1Xf0-XUvGi7jgiAAdA5BQLk7p5ikc_wOl/view?",StanfordCoreNLPServer,SOFTWARE
"```bash  # Download Stanford CoreNLP package wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip unzip stanford-corenlp-latest.zip rm -f stanford-corenlp-latest.zip  # Start StanfordCoreNLP server java -mx4g -cp ""${directory_to_standford_corenlp_package}"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 &  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \     --type graphflow \     --predict_file ${path-to-annotated-dev-json-file} \     --output_dir ${directory-to-model-checkpoint} \     --saved_vocab_file ${directory-to-saved-model-vocab} \     --pretrained ${directory-to-model-checkpoint} \     --write_dir /n/fs/scratch/huihanl/unified/graphflow/write \     --match_metric f1 \     --add_background \     --skip_entity \     --rewrite \     --fix_vocab_embed \     --f_qem \     --f_pos \     --f_ner \     --use_ques_marker \     --use_gnn \     --temporal_gnn \     --use_bert \     --use_bert_weight \     --shuffle \     --out_predictions \     --predict_raw_text \     --out_pred_in_folder \     --optimizer adamax \     --start_i ${index_of_first_passage_to_eval} \     --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### HAM The orgininal model checkpoint can be downloaded from [CodaLab](https://worksheets.codalab.org/rest/bundles/0x5c08cb0fb90c4afd8a2811bb63023cce/contents/blob/)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type ham \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --do_lower_case \   --history_len 6 \   --match_metric f1 \   --add_background \   --skip_entity \   --replace \   --init_checkpoint ${directory-to-model-checkpoint}/model_52000.ckpt \   --bert_config_file ${directory-to-pretrained-bert-large-uncased}/bert_config.json \   --vocab_file ${directory-to-model-checkpoint}/vocab.txt \   --MTL_mu 0.8 \   --MTL_lambda 0.1 \   --mtl_input reduce_mean \   --max_answer_length 40 \   --max_considered_history_turns 4 \   --bert_hidden 1024 \   --fine_grained_attention \   --better_hae \   --MTL \   --use_history_answer_marker \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### ExCorD The original model checkpoint can be downloaded from [their repo](https://drive.google.com/file/d/1Xf0-XUvGi7jgiAAdA5BQLk7p5ikc_wOl/view?",python,SOFTWARE
"```bash  # Download Stanford CoreNLP package wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip unzip stanford-corenlp-latest.zip rm -f stanford-corenlp-latest.zip  # Start StanfordCoreNLP server java -mx4g -cp ""${directory_to_standford_corenlp_package}"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 &  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \     --type graphflow \     --predict_file ${path-to-annotated-dev-json-file} \     --output_dir ${directory-to-model-checkpoint} \     --saved_vocab_file ${directory-to-saved-model-vocab} \     --pretrained ${directory-to-model-checkpoint} \     --write_dir /n/fs/scratch/huihanl/unified/graphflow/write \     --match_metric f1 \     --add_background \     --skip_entity \     --rewrite \     --fix_vocab_embed \     --f_qem \     --f_pos \     --f_ner \     --use_ques_marker \     --use_gnn \     --temporal_gnn \     --use_bert \     --use_bert_weight \     --shuffle \     --out_predictions \     --predict_raw_text \     --out_pred_in_folder \     --optimizer adamax \     --start_i ${index_of_first_passage_to_eval} \     --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### HAM The orgininal model checkpoint can be downloaded from [CodaLab](https://worksheets.codalab.org/rest/bundles/0x5c08cb0fb90c4afd8a2811bb63023cce/contents/blob/)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type ham \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --do_lower_case \   --history_len 6 \   --match_metric f1 \   --add_background \   --skip_entity \   --replace \   --init_checkpoint ${directory-to-model-checkpoint}/model_52000.ckpt \   --bert_config_file ${directory-to-pretrained-bert-large-uncased}/bert_config.json \   --vocab_file ${directory-to-model-checkpoint}/vocab.txt \   --MTL_mu 0.8 \   --MTL_lambda 0.1 \   --mtl_input reduce_mean \   --max_answer_length 40 \   --max_considered_history_turns 4 \   --bert_hidden 1024 \   --fine_grained_attention \   --better_hae \   --MTL \   --use_history_answer_marker \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### ExCorD The original model checkpoint can be downloaded from [their repo](https://drive.google.com/file/d/1Xf0-XUvGi7jgiAAdA5BQLk7p5ikc_wOl/view?",HAM,SOFTWARE
"```bash  # Download Stanford CoreNLP package wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip unzip stanford-corenlp-latest.zip rm -f stanford-corenlp-latest.zip  # Start StanfordCoreNLP server java -mx4g -cp ""${directory_to_standford_corenlp_package}"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 &  # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \     --type graphflow \     --predict_file ${path-to-annotated-dev-json-file} \     --output_dir ${directory-to-model-checkpoint} \     --saved_vocab_file ${directory-to-saved-model-vocab} \     --pretrained ${directory-to-model-checkpoint} \     --write_dir /n/fs/scratch/huihanl/unified/graphflow/write \     --match_metric f1 \     --add_background \     --skip_entity \     --rewrite \     --fix_vocab_embed \     --f_qem \     --f_pos \     --f_ner \     --use_ques_marker \     --use_gnn \     --temporal_gnn \     --use_bert \     --use_bert_weight \     --shuffle \     --out_predictions \     --predict_raw_text \     --out_pred_in_folder \     --optimizer adamax \     --start_i ${index_of_first_passage_to_eval} \     --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### HAM The orgininal model checkpoint can be downloaded from [CodaLab](https://worksheets.codalab.org/rest/bundles/0x5c08cb0fb90c4afd8a2811bb63023cce/contents/blob/)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type ham \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --do_lower_case \   --history_len 6 \   --match_metric f1 \   --add_background \   --skip_entity \   --replace \   --init_checkpoint ${directory-to-model-checkpoint}/model_52000.ckpt \   --bert_config_file ${directory-to-pretrained-bert-large-uncased}/bert_config.json \   --vocab_file ${directory-to-model-checkpoint}/vocab.txt \   --MTL_mu 0.8 \   --MTL_lambda 0.1 \   --mtl_input reduce_mean \   --max_answer_length 40 \   --max_considered_history_turns 4 \   --bert_hidden 1024 \   --fine_grained_attention \   --better_hae \   --MTL \   --use_history_answer_marker \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```   ### ExCorD The original model checkpoint can be downloaded from [their repo](https://drive.google.com/file/d/1Xf0-XUvGi7jgiAAdA5BQLk7p5ikc_wOl/view?",python,SOFTWARE
usp=sharing)  ```bash # Run Evaluation (Auto-Rewrite as example) python run_quac_eval.py \   --type excord \   --output_dir ${directory-to-model-checkpoint} \   --write_dir ${directory-to-write-evaluation-result} \   --predict_file val_v0.2.json \   --max_seq_length 512 \   --doc_stride 128 \   --max_query_length 64 \   --match_metric f1 \   --add_background \   --skip_entity \   --rewrite \   --start_i ${index_of_first_passage_to_eval} \   --end_i ${index_of_last_passage_to_eval_exclusive} \ ```  ## Evaluating your own model One can follow our existing implementations for the four models to implement evaluation for their own models.,python,SOFTWARE
"# Probabilistic Cross-Modal Embedding (PCME) CVPR 2021  Official Pytorch implementation of PCME | [Paper](https://arxiv.org/abs/2101.05068)  [Sanghyuk Chun](https://sanghyukchun.github.io/home/)<sup>1</sup> [Seong Joon Oh](https://seongjoonoh.com/)<sup>1</sup> Rafael Sampaio de Rezende<sup>2</sup> [Yannis Kalantidis](https://www.skamalas.com/)<sup>2</sup> Diane Larlus<sup>2</sup>  <sup>1</sup><sub>[NAVER AI LAB](https://naver-career.gitbook.io/en/teams/clova-cic)</sub><br> <sup>2</sup><sub>[NAVER LABS Europe](https://europe.naverlabs.com/)</sub>   <a href=""https://www.youtube.com/watch?",Pytorch,SOFTWARE
"- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [ECCV Caption](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.  ``` pip install cython && pip install -r requirements.txt python -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```  ### Dockerfile  You can use my docker image as well ``` docker pull sanghyukchun/pcme:torch1.2-apex-dali ```  Please Add `--model__cache_dir /vector_cache` when you run the code  ## Configuration  All experiments are based on configuration files (see [config/coco](config/coco) and [config/cub](config/cub)).",pip,SOFTWARE
"- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [ECCV Caption](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.  ``` pip install cython && pip install -r requirements.txt python -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```  ### Dockerfile  You can use my docker image as well ``` docker pull sanghyukchun/pcme:torch1.2-apex-dali ```  Please Add `--model__cache_dir /vector_cache` when you run the code  ## Configuration  All experiments are based on configuration files (see [config/coco](config/coco) and [config/cub](config/cub)).",cython,SOFTWARE
"- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [ECCV Caption](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.  ``` pip install cython && pip install -r requirements.txt python -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```  ### Dockerfile  You can use my docker image as well ``` docker pull sanghyukchun/pcme:torch1.2-apex-dali ```  Please Add `--model__cache_dir /vector_cache` when you run the code  ## Configuration  All experiments are based on configuration files (see [config/coco](config/coco) and [config/cub](config/cub)).",pip,SOFTWARE
"- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [ECCV Caption](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.  ``` pip install cython && pip install -r requirements.txt python -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```  ### Dockerfile  You can use my docker image as well ``` docker pull sanghyukchun/pcme:torch1.2-apex-dali ```  Please Add `--model__cache_dir /vector_cache` when you run the code  ## Configuration  All experiments are based on configuration files (see [config/coco](config/coco) and [config/cub](config/cub)).",python,SOFTWARE
"- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [ECCV Caption](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.  ``` pip install cython && pip install -r requirements.txt python -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```  ### Dockerfile  You can use my docker image as well ``` docker pull sanghyukchun/pcme:torch1.2-apex-dali ```  Please Add `--model__cache_dir /vector_cache` when you run the code  ## Configuration  All experiments are based on configuration files (see [config/coco](config/coco) and [config/cub](config/cub)).",pip,SOFTWARE
"- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [ECCV Caption](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.  ``` pip install cython && pip install -r requirements.txt python -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```  ### Dockerfile  You can use my docker image as well ``` docker pull sanghyukchun/pcme:torch1.2-apex-dali ```  Please Add `--model__cache_dir /vector_cache` when you run the code  ## Configuration  All experiments are based on configuration files (see [config/coco](config/coco) and [config/cub](config/cub)).",docker,SOFTWARE
"- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [ECCV Caption](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.  ``` pip install cython && pip install -r requirements.txt python -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```  ### Dockerfile  You can use my docker image as well ``` docker pull sanghyukchun/pcme:torch1.2-apex-dali ```  Please Add `--model__cache_dir /vector_cache` when you run the code  ## Configuration  All experiments are based on configuration files (see [config/coco](config/coco) and [config/cub](config/cub)).",docker,SOFTWARE
"- 16 Jul, 2022: Add PCME CutMix-pretrained weight (used for [ECCV Caption](https://github.com/naver-ai/eccv-caption) paper) - 23 Jun, 2021: Initial upload.  ## Installation  Install dependencies using the following command.  ``` pip install cython && pip install -r requirements.txt python -c 'import nltk; nltk.download(""punkt"", download_dir=""/opt/conda/nltk_data"")' git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./ ```  ### Dockerfile  You can use my docker image as well ``` docker pull sanghyukchun/pcme:torch1.2-apex-dali ```  Please Add `--model__cache_dir /vector_cache` when you run the code  ## Configuration  All experiments are based on configuration files (see [config/coco](config/coco) and [config/cub](config/cub)).",torch1.2,SOFTWARE
"If you want to change only a few options, instead of re-writing a new configuration file, you can override the configuration as the follows:  ``` python <train | eval>.py --dataloader__batch_size 32 --dataloader__eval_batch_size 8 --model__eval_method matching_prob ```  See [config/parser.py](config/parser.py) for details  ## Dataset preparation  ### COCO Caption  We followed the same split provided by [VSE++](http://www.cs.toronto.edu/~faghri/vsepp/data.tar).",python,SOFTWARE
<br> We are planning to re-implement efficient PMRP as soon as possible.  ### COCO Caption  ``` # Compute recall metrics python evaluate_recall_coco.py .,python,SOFTWARE
/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image ```  ``` # Compute plausible match R-Precision (PMRP) metric python extract_rankings_coco.py .,python,SOFTWARE
"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",pcme,SOFTWARE
"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",python,SOFTWARE
"/config/coco/pcme_coco.yaml \     --dataset_root <your_dataset_path> \     --model_path model_last.pth \     --dump_to <dumped_ranking_file> \     # --model__cache_dir /vector_cache # if you use my docker image  python evaluate_pmrp_coco.py --ranking_file <dumped_ranking_file> ```  | Method   | I2T 1K PMRP | I2T 1K R@1 | I2T ECCV mAP@R | T2I 1K PMRP | T2I 1K R@1 | T2I ECCV mAP@R | Model file | |----------|----------|---------|----------|----------|---------|----------|------------| | PCME     | 45.0     | 68.8    |   26.2   | 46.0     | 54.6    |   48.0   | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_coco.pth) | | PCME (CutMix-pretrained) | 46.2 | 68.3 | 28.6 | 47.1 | 56.7 | 54.9 | [link](https://github.com/naver-ai/pcme/releases/download/v1.0.0/pcme_cutmix_coco.pth) | | PVSE K=1 | 40.3     | 66.7    |   23.4   | 41.8     | 53.5    |   44.6   | -          | | PVSE K=2 | 42.8     | 69.2    |   26.7   | 43.6     | 55.2    |   53.8   | -          | | VSRN     | 41.2     | 76.2    |   30.8   | 42.4     | 62.8    |   53.8   | -          | | VSRN + AOQ | 44.7   | 77.5    |   30.7   | 45.6     | 63.5    |   51.2   | -          |  Check [ECCV Caption dataset](https://github.com/naver-ai/eccv-caption) for more details of ""ECCV mAP@R"". - Paper: [ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359) - GitHub: [naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)  ### CUB Caption  ``` python evaluate_cub.py .",python,SOFTWARE
"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --model_path model_last.pth \     # --model__cache_dir /vector_cache # if you use my docker image ```  NOTE: If you just download file from [reedscot/cvpr2016](https://github.com/reedscot/cvpr2016), then `caption_root` will be `cvpr2016_cub/text_c10`  If you want to test other probabilistic distances, such as Wasserstein distance or KL-divergence, try the following command:  ``` python evaluate_cub.py .",python,SOFTWARE
"<br> Please note that, hence, the results couldn't be reproduced if you use smaller hardware than V100.  ### COCO Caption  ``` python train_coco.py .",python,SOFTWARE
"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.  #### hyperparameter search  We additionally use cross-validation splits by (Xian, et el. 2017), namely using 100 classes for training and 50 classes for validation.   ``` python train_cub.py .",python,SOFTWARE
"/config/cub/pcme_cub.yaml \     --dataset_root <your_dataset_path> \     --caption_root <your_caption_path> \     --dataset_name cub_trainval1 \     # --model__cache_dir /vector_cache # if you use my docker image ```  Similarly, you can use `cub_trainval2` and `cub_trainval3` as well.  #### training with full training classes  ``` python train_cub.py .",python,SOFTWARE
/data/F1.png)  ## System requirement  ### OS Requirements This package is supported for *Linux*.,Linux,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",Linux,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",Ubuntu 20.04,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",numpy,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",pandas,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",matplotlib,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",scipy,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",sklearn,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",lightgbm,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",shap,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",joblib,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",seaborn,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",medGAN,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",medBGAN,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",EMR-WGAN,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",WGAN,SOFTWARE
"The package has been tested on the following systems: + Linux: Ubuntu 20.04  ### Python Dependencies  ``` numpy pandas matplotlib scipy sklearn lightgbm shap joblib seaborn ```  ## Synthetic EHR data generation --- We used this framework to evaluate five GAN models that were designed to synthesize structured EHR profiles of patients: 1) medGAN, 2) medBGAN, 3) EMR-WGAN, 4) WGAN, and 5) DPGAN.",DPGAN,SOFTWARE
Fine-tuning pre-trained BERT produces classifiers with modest performance compared to the state of the art for sequence classification.,BERT,SOFTWARE
We use the same test sets to compute the ROC AUC of the trained BERT model and average those scores as well.,BERT,SOFTWARE
"As we can see, for all attributes other than 'sarcastic' the BERT model outperforms a randomly selected human annotator, indicating that it has sufficiently captured the semantic and syntactic structures for these attributes.",BERT,SOFTWARE
"For 'sarcastic', the gap between the BERT model and human annotators indicates a rich area for studying whether model performance can be improved",BERT,SOFTWARE
"We also make use of some code from [eICU Benchmarks](https://github.com/mostafaalishahi/eICU_Benchmark).   ## To replicate the experiments in the paper:  ### Step 0: Environment and Prerequisites Run the following commands to clone this repo and create the Conda environment:  ``` git clone https://github.com/MLforHealth/ClinicalDG.git cd ClinicalDG/ conda env create -f environment.yml conda activate clinicaldg ```  ### Step 1: Obtaining the Data See [DataSources.md](DataSources.md) for detailed instructions.  ### Step 2: Running Experiments  Experiments can be ran using the same procedure as for the [DomainBed framework](https://github.com/facebookresearch/DomainBed), with a few additional adjustable data hyperparameters which should be passed in as a JSON formatted dictionary.",eICU Benchmarks,SOFTWARE
"We also make use of some code from [eICU Benchmarks](https://github.com/mostafaalishahi/eICU_Benchmark).   ## To replicate the experiments in the paper:  ### Step 0: Environment and Prerequisites Run the following commands to clone this repo and create the Conda environment:  ``` git clone https://github.com/MLforHealth/ClinicalDG.git cd ClinicalDG/ conda env create -f environment.yml conda activate clinicaldg ```  ### Step 1: Obtaining the Data See [DataSources.md](DataSources.md) for detailed instructions.  ### Step 2: Running Experiments  Experiments can be ran using the same procedure as for the [DomainBed framework](https://github.com/facebookresearch/DomainBed), with a few additional adjustable data hyperparameters which should be passed in as a JSON formatted dictionary.",eICU_Benchmark,SOFTWARE
"We also make use of some code from [eICU Benchmarks](https://github.com/mostafaalishahi/eICU_Benchmark).   ## To replicate the experiments in the paper:  ### Step 0: Environment and Prerequisites Run the following commands to clone this repo and create the Conda environment:  ``` git clone https://github.com/MLforHealth/ClinicalDG.git cd ClinicalDG/ conda env create -f environment.yml conda activate clinicaldg ```  ### Step 1: Obtaining the Data See [DataSources.md](DataSources.md) for detailed instructions.  ### Step 2: Running Experiments  Experiments can be ran using the same procedure as for the [DomainBed framework](https://github.com/facebookresearch/DomainBed), with a few additional adjustable data hyperparameters which should be passed in as a JSON formatted dictionary.",git,SOFTWARE
"We also make use of some code from [eICU Benchmarks](https://github.com/mostafaalishahi/eICU_Benchmark).   ## To replicate the experiments in the paper:  ### Step 0: Environment and Prerequisites Run the following commands to clone this repo and create the Conda environment:  ``` git clone https://github.com/MLforHealth/ClinicalDG.git cd ClinicalDG/ conda env create -f environment.yml conda activate clinicaldg ```  ### Step 1: Obtaining the Data See [DataSources.md](DataSources.md) for detailed instructions.  ### Step 2: Running Experiments  Experiments can be ran using the same procedure as for the [DomainBed framework](https://github.com/facebookresearch/DomainBed), with a few additional adjustable data hyperparameters which should be passed in as a JSON formatted dictionary.",ClinicalDG,SOFTWARE
"We also make use of some code from [eICU Benchmarks](https://github.com/mostafaalishahi/eICU_Benchmark).   ## To replicate the experiments in the paper:  ### Step 0: Environment and Prerequisites Run the following commands to clone this repo and create the Conda environment:  ``` git clone https://github.com/MLforHealth/ClinicalDG.git cd ClinicalDG/ conda env create -f environment.yml conda activate clinicaldg ```  ### Step 1: Obtaining the Data See [DataSources.md](DataSources.md) for detailed instructions.  ### Step 2: Running Experiments  Experiments can be ran using the same procedure as for the [DomainBed framework](https://github.com/facebookresearch/DomainBed), with a few additional adjustable data hyperparameters which should be passed in as a JSON formatted dictionary.",conda,SOFTWARE
"We also make use of some code from [eICU Benchmarks](https://github.com/mostafaalishahi/eICU_Benchmark).   ## To replicate the experiments in the paper:  ### Step 0: Environment and Prerequisites Run the following commands to clone this repo and create the Conda environment:  ``` git clone https://github.com/MLforHealth/ClinicalDG.git cd ClinicalDG/ conda env create -f environment.yml conda activate clinicaldg ```  ### Step 1: Obtaining the Data See [DataSources.md](DataSources.md) for detailed instructions.  ### Step 2: Running Experiments  Experiments can be ran using the same procedure as for the [DomainBed framework](https://github.com/facebookresearch/DomainBed), with a few additional adjustable data hyperparameters which should be passed in as a JSON formatted dictionary.",conda,SOFTWARE
"We also make use of some code from [eICU Benchmarks](https://github.com/mostafaalishahi/eICU_Benchmark).   ## To replicate the experiments in the paper:  ### Step 0: Environment and Prerequisites Run the following commands to clone this repo and create the Conda environment:  ``` git clone https://github.com/MLforHealth/ClinicalDG.git cd ClinicalDG/ conda env create -f environment.yml conda activate clinicaldg ```  ### Step 1: Obtaining the Data See [DataSources.md](DataSources.md) for detailed instructions.  ### Step 2: Running Experiments  Experiments can be ran using the same procedure as for the [DomainBed framework](https://github.com/facebookresearch/DomainBed), with a few additional adjustable data hyperparameters which should be passed in as a JSON formatted dictionary.",DomainBed,SOFTWARE
"We also make use of some code from [eICU Benchmarks](https://github.com/mostafaalishahi/eICU_Benchmark).   ## To replicate the experiments in the paper:  ### Step 0: Environment and Prerequisites Run the following commands to clone this repo and create the Conda environment:  ``` git clone https://github.com/MLforHealth/ClinicalDG.git cd ClinicalDG/ conda env create -f environment.yml conda activate clinicaldg ```  ### Step 1: Obtaining the Data See [DataSources.md](DataSources.md) for detailed instructions.  ### Step 2: Running Experiments  Experiments can be ran using the same procedure as for the [DomainBed framework](https://github.com/facebookresearch/DomainBed), with a few additional adjustable data hyperparameters which should be passed in as a JSON formatted dictionary.",DomainBed,SOFTWARE
"For example, to train a single model: ``` python -m clinicaldg.scripts.train\        --algorithm ERM\        --dataset eICUSubsampleUnobs\        --es_method val\        --hparams  '{""eicu_architecture"": ""GRU"", ""eicu_subsample_g1_mean"": 0.5, ""eicu_subsample_g2_mean"": 0.05}'\        --output_dir /path/to/output ```  To sweep a range of datasets, algorithms, and hyperparameters: ``` python -m clinicaldg.scripts.sweep launch\        --output_dir=/my/sweep/output/path\        --command_launcher slurm\        --algorithms ERMID ERM IRM VREx RVP IGA CORAL MLDG GroupDRO \        --datasets CXR CXRBinary\        --n_hparams 10\        --n_trials 5\        --es_method train\        --hparams '{""cxr_augment"": 1}' ```  A detailed list of `hparams` available for each dataset can be found [here](hparams.md).",python,SOFTWARE
"For example, to train a single model: ``` python -m clinicaldg.scripts.train\        --algorithm ERM\        --dataset eICUSubsampleUnobs\        --es_method val\        --hparams  '{""eicu_architecture"": ""GRU"", ""eicu_subsample_g1_mean"": 0.5, ""eicu_subsample_g2_mean"": 0.05}'\        --output_dir /path/to/output ```  To sweep a range of datasets, algorithms, and hyperparameters: ``` python -m clinicaldg.scripts.sweep launch\        --output_dir=/my/sweep/output/path\        --command_launcher slurm\        --algorithms ERMID ERM IRM VREx RVP IGA CORAL MLDG GroupDRO \        --datasets CXR CXRBinary\        --n_hparams 10\        --n_trials 5\        --es_method train\        --hparams '{""cxr_augment"": 1}' ```  A detailed list of `hparams` available for each dataset can be found [here](hparams.md).",python,SOFTWARE
"# VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation  <div align=""center"">   <img src=""VISOLO.png""/> </div>  ## Paper [VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation](https://arxiv.org/abs/2112.04177)  ### Note * Based on [detectron2](https://github.com/facebookresearch/detectron2). * The codes are under [projects/](projects/) folder, which follows the convention of detectron2. * You can easily import our project to the latest detectron2 by following below",detectron2,SOFTWARE
"# VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation  <div align=""center"">   <img src=""VISOLO.png""/> </div>  ## Paper [VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation](https://arxiv.org/abs/2112.04177)  ### Note * Based on [detectron2](https://github.com/facebookresearch/detectron2). * The codes are under [projects/](projects/) folder, which follows the convention of detectron2. * You can easily import our project to the latest detectron2 by following below",detectron2,SOFTWARE
"# VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation  <div align=""center"">   <img src=""VISOLO.png""/> </div>  ## Paper [VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation](https://arxiv.org/abs/2112.04177)  ### Note * Based on [detectron2](https://github.com/facebookresearch/detectron2). * The codes are under [projects/](projects/) folder, which follows the convention of detectron2. * You can easily import our project to the latest detectron2 by following below",detectron2,SOFTWARE
"# VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation  <div align=""center"">   <img src=""VISOLO.png""/> </div>  ## Paper [VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation](https://arxiv.org/abs/2112.04177)  ### Note * Based on [detectron2](https://github.com/facebookresearch/detectron2). * The codes are under [projects/](projects/) folder, which follows the convention of detectron2. * You can easily import our project to the latest detectron2 by following below",detectron2,SOFTWARE
- inserting [projects/VISOLO](projects/VISOLO) folder     - updating [detectron2/projects/__init\_\_.py](detectron2/projects/__init__.py)     - updating [setup.py](.,detectron2,SOFTWARE
- inserting [projects/VISOLO](projects/VISOLO) folder     - updating [detectron2/projects/__init\_\_.py](detectron2/projects/__init__.py)     - updating [setup.py](.,detectron2,SOFTWARE
"Installation. * Run a docker container and install the repository. * Compile a docker image following [docker/README.md](docker/README.md). * On top of the docker container, install the repository by the following command.",docker,SOFTWARE
"Installation. * Run a docker container and install the repository. * Compile a docker image following [docker/README.md](docker/README.md). * On top of the docker container, install the repository by the following command.",docker,SOFTWARE
"Installation. * Run a docker container and install the repository. * Compile a docker image following [docker/README.md](docker/README.md). * On top of the docker container, install the repository by the following command.",docker,SOFTWARE
"Installation. * Run a docker container and install the repository. * Compile a docker image following [docker/README.md](docker/README.md). * On top of the docker container, install the repository by the following command.",docker,SOFTWARE
"Installation. * Run a docker container and install the repository. * Compile a docker image following [docker/README.md](docker/README.md). * On top of the docker container, install the repository by the following command.",docker,SOFTWARE
"```bash git clone https://github.com/SuHoHan95/VISOLO.git cd VISOLO pip install -e . pip install -r requirements.txt pip install git+https://github.com/youtubevos/cocoapi.git#""egg=pycocotools&subdirectory=PythonAPI"" ```  2.",git,SOFTWARE
"```bash git clone https://github.com/SuHoHan95/VISOLO.git cd VISOLO pip install -e . pip install -r requirements.txt pip install git+https://github.com/youtubevos/cocoapi.git#""egg=pycocotools&subdirectory=PythonAPI"" ```  2.",pip,SOFTWARE
"```bash git clone https://github.com/SuHoHan95/VISOLO.git cd VISOLO pip install -e . pip install -r requirements.txt pip install git+https://github.com/youtubevos/cocoapi.git#""egg=pycocotools&subdirectory=PythonAPI"" ```  2.",pip,SOFTWARE
"```bash git clone https://github.com/SuHoHan95/VISOLO.git cd VISOLO pip install -e . pip install -r requirements.txt pip install git+https://github.com/youtubevos/cocoapi.git#""egg=pycocotools&subdirectory=PythonAPI"" ```  2.",pip,SOFTWARE
* Training using 4 GPUS(TESLA V100-PCIE-32GB) * Pre-training on COCO dataset ```bash python train_net.py --num-gpus 4 --config-file .,python,SOFTWARE
usp=sharing)) ```bash python train_net.py --num-gpus 4 --config-file .,python,SOFTWARE
Evaluating on YTVIS 2019 ```bash python train_net.py --eval-only --num-gpus 1 --config-file .,python,SOFTWARE
"```BibTex @inproceedings{han2022visolo,   title={VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation},   author={Han, Su Ho and Hwang, Sukjun and Oh, Seoung Wug and Park, Yeonchool and Kim, Hyunwoo and Kim, Min-Jung and Kim, Seon Joo},   booktitle =  {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   year={2022} } ```  ## Acknowledgement We highly appreciate all previous works that influenced our project.\ Special thanks to facebookresearch and IFC authors for their wonderful codes that have been publicly released ([detectron2](https://github.com/facebookresearch/detectron2), [IFC](https://github.com/sukjunhwang/IFC)).",IFC,SOFTWARE
"```BibTex @inproceedings{han2022visolo,   title={VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation},   author={Han, Su Ho and Hwang, Sukjun and Oh, Seoung Wug and Park, Yeonchool and Kim, Hyunwoo and Kim, Min-Jung and Kim, Seon Joo},   booktitle =  {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   year={2022} } ```  ## Acknowledgement We highly appreciate all previous works that influenced our project.\ Special thanks to facebookresearch and IFC authors for their wonderful codes that have been publicly released ([detectron2](https://github.com/facebookresearch/detectron2), [IFC](https://github.com/sukjunhwang/IFC)).",detectron2,SOFTWARE
"```BibTex @inproceedings{han2022visolo,   title={VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation},   author={Han, Su Ho and Hwang, Sukjun and Oh, Seoung Wug and Park, Yeonchool and Kim, Hyunwoo and Kim, Min-Jung and Kim, Seon Joo},   booktitle =  {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   year={2022} } ```  ## Acknowledgement We highly appreciate all previous works that influenced our project.\ Special thanks to facebookresearch and IFC authors for their wonderful codes that have been publicly released ([detectron2](https://github.com/facebookresearch/detectron2), [IFC](https://github.com/sukjunhwang/IFC)).",detectron2,SOFTWARE
"```BibTex @inproceedings{han2022visolo,   title={VISOLO: Grid-Based Space-Time Aggregation for Efficient Online Video Instance Segmentation},   author={Han, Su Ho and Hwang, Sukjun and Oh, Seoung Wug and Park, Yeonchool and Kim, Hyunwoo and Kim, Min-Jung and Kim, Seon Joo},   booktitle =  {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   year={2022} } ```  ## Acknowledgement We highly appreciate all previous works that influenced our project.\ Special thanks to facebookresearch and IFC authors for their wonderful codes that have been publicly released ([detectron2](https://github.com/facebookresearch/detectron2), [IFC](https://github.com/sukjunhwang/IFC)).",IFC,SOFTWARE
"```bibtex @inproceedings{gutierrez2021thermal,    title={Thermal image super-resolution using second-order channel attention with varying receptive fields},   author={Gutierrez, Nolan B and Beksi, William J},   booktitle={Proceedings of the International Conference on Computer Vision Systems (ICVS)},   pages={3--13},   year={2021} } ```   ### Installation  To run the experiments within this repository, `opencv`, `numpy`, `tensorflow`, and `pillow` need to be installed along with their dependencies.",opencv,SOFTWARE
"```bibtex @inproceedings{gutierrez2021thermal,    title={Thermal image super-resolution using second-order channel attention with varying receptive fields},   author={Gutierrez, Nolan B and Beksi, William J},   booktitle={Proceedings of the International Conference on Computer Vision Systems (ICVS)},   pages={3--13},   year={2021} } ```   ### Installation  To run the experiments within this repository, `opencv`, `numpy`, `tensorflow`, and `pillow` need to be installed along with their dependencies.",numpy,SOFTWARE
"```bibtex @inproceedings{gutierrez2021thermal,    title={Thermal image super-resolution using second-order channel attention with varying receptive fields},   author={Gutierrez, Nolan B and Beksi, William J},   booktitle={Proceedings of the International Conference on Computer Vision Systems (ICVS)},   pages={3--13},   year={2021} } ```   ### Installation  To run the experiments within this repository, `opencv`, `numpy`, `tensorflow`, and `pillow` need to be installed along with their dependencies.",tensorflow,SOFTWARE
"```bibtex @inproceedings{gutierrez2021thermal,    title={Thermal image super-resolution using second-order channel attention with varying receptive fields},   author={Gutierrez, Nolan B and Beksi, William J},   booktitle={Proceedings of the International Conference on Computer Vision Systems (ICVS)},   pages={3--13},   year={2021} } ```   ### Installation  To run the experiments within this repository, `opencv`, `numpy`, `tensorflow`, and `pillow` need to be installed along with their dependencies.",pillow,SOFTWARE
The experiments were conducted on an Ubuntu 20.04 machine using an Anaconda 3 environment.,Ubuntu 20.04,SOFTWARE
The experiments were conducted on an Ubuntu 20.04 machine using an Anaconda 3 environment.,Anaconda 3,SOFTWARE
The experiments were run with an NVIDIA GPU and CUDA.,CUDA,SOFTWARE
"To create the environment, run the following commands from a terminal with Anaconda 3 installed.",Anaconda,SOFTWARE
Create the Anaconda environment:   `conda create -y -n myenv tensorflow==2.4.1 numpy==1.19.2  pip  pillow==7.2.0 tensorflow-gpu==2.5.0`   Activate the environment:   `conda activate myenv`   Install OpenCV:   `conda install -c conda-forge opencv`   ### Usage   Our driver file is equipped with a simple GUI that allows you to select datasets for training and testing.,Anaconda,SOFTWARE
Create the Anaconda environment:   `conda create -y -n myenv tensorflow==2.4.1 numpy==1.19.2  pip  pillow==7.2.0 tensorflow-gpu==2.5.0`   Activate the environment:   `conda activate myenv`   Install OpenCV:   `conda install -c conda-forge opencv`   ### Usage   Our driver file is equipped with a simple GUI that allows you to select datasets for training and testing.,conda,SOFTWARE
Create the Anaconda environment:   `conda create -y -n myenv tensorflow==2.4.1 numpy==1.19.2  pip  pillow==7.2.0 tensorflow-gpu==2.5.0`   Activate the environment:   `conda activate myenv`   Install OpenCV:   `conda install -c conda-forge opencv`   ### Usage   Our driver file is equipped with a simple GUI that allows you to select datasets for training and testing.,tensorflow==2.4.1,SOFTWARE
Create the Anaconda environment:   `conda create -y -n myenv tensorflow==2.4.1 numpy==1.19.2  pip  pillow==7.2.0 tensorflow-gpu==2.5.0`   Activate the environment:   `conda activate myenv`   Install OpenCV:   `conda install -c conda-forge opencv`   ### Usage   Our driver file is equipped with a simple GUI that allows you to select datasets for training and testing.,numpy==1.19.2,SOFTWARE
Create the Anaconda environment:   `conda create -y -n myenv tensorflow==2.4.1 numpy==1.19.2  pip  pillow==7.2.0 tensorflow-gpu==2.5.0`   Activate the environment:   `conda activate myenv`   Install OpenCV:   `conda install -c conda-forge opencv`   ### Usage   Our driver file is equipped with a simple GUI that allows you to select datasets for training and testing.,pip,SOFTWARE
Create the Anaconda environment:   `conda create -y -n myenv tensorflow==2.4.1 numpy==1.19.2  pip  pillow==7.2.0 tensorflow-gpu==2.5.0`   Activate the environment:   `conda activate myenv`   Install OpenCV:   `conda install -c conda-forge opencv`   ### Usage   Our driver file is equipped with a simple GUI that allows you to select datasets for training and testing.,pillow==7.2.0,SOFTWARE
Create the Anaconda environment:   `conda create -y -n myenv tensorflow==2.4.1 numpy==1.19.2  pip  pillow==7.2.0 tensorflow-gpu==2.5.0`   Activate the environment:   `conda activate myenv`   Install OpenCV:   `conda install -c conda-forge opencv`   ### Usage   Our driver file is equipped with a simple GUI that allows you to select datasets for training and testing.,tensorflow-gpu==2.5.0,SOFTWARE
Create the Anaconda environment:   `conda create -y -n myenv tensorflow==2.4.1 numpy==1.19.2  pip  pillow==7.2.0 tensorflow-gpu==2.5.0`   Activate the environment:   `conda activate myenv`   Install OpenCV:   `conda install -c conda-forge opencv`   ### Usage   Our driver file is equipped with a simple GUI that allows you to select datasets for training and testing.,conda,SOFTWARE
Create the Anaconda environment:   `conda create -y -n myenv tensorflow==2.4.1 numpy==1.19.2  pip  pillow==7.2.0 tensorflow-gpu==2.5.0`   Activate the environment:   `conda activate myenv`   Install OpenCV:   `conda install -c conda-forge opencv`   ### Usage   Our driver file is equipped with a simple GUI that allows you to select datasets for training and testing.,OpenCV,SOFTWARE
Create the Anaconda environment:   `conda create -y -n myenv tensorflow==2.4.1 numpy==1.19.2  pip  pillow==7.2.0 tensorflow-gpu==2.5.0`   Activate the environment:   `conda activate myenv`   Install OpenCV:   `conda install -c conda-forge opencv`   ### Usage   Our driver file is equipped with a simple GUI that allows you to select datasets for training and testing.,conda,SOFTWARE
Create the Anaconda environment:   `conda create -y -n myenv tensorflow==2.4.1 numpy==1.19.2  pip  pillow==7.2.0 tensorflow-gpu==2.5.0`   Activate the environment:   `conda activate myenv`   Install OpenCV:   `conda install -c conda-forge opencv`   ### Usage   Our driver file is equipped with a simple GUI that allows you to select datasets for training and testing.,opencv,SOFTWARE
"Next, to quickly set the directories which you will be using run the following command:  `python train.py 2`  You will be presented with a GUI which allows you to select the proper dataset.",python,SOFTWARE
"For example, to begin an experiment using the x4 upscaling factor, run the following command in an Anaconda terminal:   `python train.py 4`  You now have the option of training, creating results, and evaluating, as determined by the Booleans in lines 53-55.",Anaconda,SOFTWARE
"For example, to begin an experiment using the x4 upscaling factor, run the following command in an Anaconda terminal:   `python train.py 4`  You now have the option of training, creating results, and evaluating, as determined by the Booleans in lines 53-55.",python,SOFTWARE
"/model_name/dataset/scale`.  ### Running Experiments  Our experiments require that `modelnames` is set as follows:   `modelnames = ['rcan','rcan_dd','rcan_dd_comp','rcan_DDSOCA', 'rcan_soca']`  If you want to run experiments to reproduce our results, then run the following commands:  ``` python train.py 2 python train.py 3 python train.py 4 ```  All of the results will be stored in the `.",python,SOFTWARE
"/model_name/dataset/scale`.  ### Running Experiments  Our experiments require that `modelnames` is set as follows:   `modelnames = ['rcan','rcan_dd','rcan_dd_comp','rcan_DDSOCA', 'rcan_soca']`  If you want to run experiments to reproduce our results, then run the following commands:  ``` python train.py 2 python train.py 3 python train.py 4 ```  All of the results will be stored in the `.",python,SOFTWARE
"/model_name/dataset/scale`.  ### Running Experiments  Our experiments require that `modelnames` is set as follows:   `modelnames = ['rcan','rcan_dd','rcan_dd_comp','rcan_DDSOCA', 'rcan_soca']`  If you want to run experiments to reproduce our results, then run the following commands:  ``` python train.py 2 python train.py 3 python train.py 4 ```  All of the results will be stored in the `.",python,SOFTWARE
"To reproduce these results (assuming that the datasets are set correctly), simply run the following command:   `python test_bicubic.py`  ### License  [!",python,SOFTWARE
Our best performing model with XLNet achieves a Macro F1 score of only 78.18% and an accuracy of 78.23% showing that there is substantial room for improvement.  ## Dataset Description We derive [SciNLI](https://drive.google.com/drive/folders/1kjBTVBV1HlMWW5xK8V096LahsU3pULHU?,XLNet,SOFTWARE
"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions.",numpy==1.21.6,SOFTWARE
"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions.",pandas==1.4.1,SOFTWARE
"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions.",scikit_learn==1.1.1,SOFTWARE
"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions.",torch==1.9.0+cu111,SOFTWARE
"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions.",transformers==4.20.1,SOFTWARE
"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions.",transformers,SOFTWARE
"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions.",python,SOFTWARE
"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions.",BERT,SOFTWARE
"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions.",Sci_BERT,SOFTWARE
"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions.",RoBERTa,SOFTWARE
"* Total: 107,412.  ## Model Training & Testing ### Requirements ``` numpy==1.21.6 pandas==1.4.1 scikit_learn==1.1.1 torch==1.9.0+cu111 transformers==4.20.1 transformers.egg==info ```  ### Fine-tuning Pre-trained Language Models  ``` python Train_and_test_models.py --base <location of a directory containing the train, test and dev files in CSV format> --model_type <'BERT', 'Sci_BERT', 'RoBERTa' or 'XLNet'> --batch_size <batch size> --max_length <combined max length of sentence1 and sentence2> --num_epochs <number of epochs to train the model for> --epoch_patience <patience for early stopping> --device <device to run your experiment on> --random_seed <some random seed> ```  ## Citation If you use this dataset, please cite our paper:  ``` @inproceedings{sadat-caragea-2022-scinli,     title = ""{S}ci{NLI}: A Corpus for Natural Language Inference on Scientific Text"",     author = ""Sadat, Mobashir  and       Caragea, Cornelia"",     booktitle = ""Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"",     month = may,     year = ""2022"",     address = ""Dublin, Ireland"",     publisher = ""Association for Computational Linguistics"",     url = ""https://aclanthology.org/2022.acl-long.511"",     pages = ""7399--7409"", } ``` ## License SciNLI is licensed with Attribution-ShareAlike 4.0 International [(CC BY-SA 4.0)](https://creativecommons.org/licenses/by-sa/4.0/).  ## Contact Please contact us at msadat3@uic.edu with any questions.",XLNet,SOFTWARE
"The paper can be found at https://arxiv.org/abs/1901.06803 and will appear at [AAMAS 2019](http://aamas2019.encs.concordia.ca/).  ## Installation  ### Requirements:  * [GPytorch](https://github.com/cornellius-gp/gpytorch) (Beta Release) * [Networkx](https://networkx.github.io/) * [Seaborn](https://seaborn.pydata.org/) * [Pandas](https://pandas.pydata.org/)  After installing the listed dependencies, simply clone this package to run experiments.  ## Getting Started  See `run.py` script to setup a simulation environment and run an agent to adapively collect data and learn the target distribution.",GPytorch,SOFTWARE
"The paper can be found at https://arxiv.org/abs/1901.06803 and will appear at [AAMAS 2019](http://aamas2019.encs.concordia.ca/).  ## Installation  ### Requirements:  * [GPytorch](https://github.com/cornellius-gp/gpytorch) (Beta Release) * [Networkx](https://networkx.github.io/) * [Seaborn](https://seaborn.pydata.org/) * [Pandas](https://pandas.pydata.org/)  After installing the listed dependencies, simply clone this package to run experiments.  ## Getting Started  See `run.py` script to setup a simulation environment and run an agent to adapively collect data and learn the target distribution.",gpytorch,SOFTWARE
"The paper can be found at https://arxiv.org/abs/1901.06803 and will appear at [AAMAS 2019](http://aamas2019.encs.concordia.ca/).  ## Installation  ### Requirements:  * [GPytorch](https://github.com/cornellius-gp/gpytorch) (Beta Release) * [Networkx](https://networkx.github.io/) * [Seaborn](https://seaborn.pydata.org/) * [Pandas](https://pandas.pydata.org/)  After installing the listed dependencies, simply clone this package to run experiments.  ## Getting Started  See `run.py` script to setup a simulation environment and run an agent to adapively collect data and learn the target distribution.",Networkx,SOFTWARE
"The paper can be found at https://arxiv.org/abs/1901.06803 and will appear at [AAMAS 2019](http://aamas2019.encs.concordia.ca/).  ## Installation  ### Requirements:  * [GPytorch](https://github.com/cornellius-gp/gpytorch) (Beta Release) * [Networkx](https://networkx.github.io/) * [Seaborn](https://seaborn.pydata.org/) * [Pandas](https://pandas.pydata.org/)  After installing the listed dependencies, simply clone this package to run experiments.  ## Getting Started  See `run.py` script to setup a simulation environment and run an agent to adapively collect data and learn the target distribution.",networkx,SOFTWARE
"The paper can be found at https://arxiv.org/abs/1901.06803 and will appear at [AAMAS 2019](http://aamas2019.encs.concordia.ca/).  ## Installation  ### Requirements:  * [GPytorch](https://github.com/cornellius-gp/gpytorch) (Beta Release) * [Networkx](https://networkx.github.io/) * [Seaborn](https://seaborn.pydata.org/) * [Pandas](https://pandas.pydata.org/)  After installing the listed dependencies, simply clone this package to run experiments.  ## Getting Started  See `run.py` script to setup a simulation environment and run an agent to adapively collect data and learn the target distribution.",Seaborn,SOFTWARE
"The paper can be found at https://arxiv.org/abs/1901.06803 and will appear at [AAMAS 2019](http://aamas2019.encs.concordia.ca/).  ## Installation  ### Requirements:  * [GPytorch](https://github.com/cornellius-gp/gpytorch) (Beta Release) * [Networkx](https://networkx.github.io/) * [Seaborn](https://seaborn.pydata.org/) * [Pandas](https://pandas.pydata.org/)  After installing the listed dependencies, simply clone this package to run experiments.  ## Getting Started  See `run.py` script to setup a simulation environment and run an agent to adapively collect data and learn the target distribution.",seaborn,SOFTWARE
"The paper can be found at https://arxiv.org/abs/1901.06803 and will appear at [AAMAS 2019](http://aamas2019.encs.concordia.ca/).  ## Installation  ### Requirements:  * [GPytorch](https://github.com/cornellius-gp/gpytorch) (Beta Release) * [Networkx](https://networkx.github.io/) * [Seaborn](https://seaborn.pydata.org/) * [Pandas](https://pandas.pydata.org/)  After installing the listed dependencies, simply clone this package to run experiments.  ## Getting Started  See `run.py` script to setup a simulation environment and run an agent to adapively collect data and learn the target distribution.",Pandas,SOFTWARE
"The paper can be found at https://arxiv.org/abs/1901.06803 and will appear at [AAMAS 2019](http://aamas2019.encs.concordia.ca/).  ## Installation  ### Requirements:  * [GPytorch](https://github.com/cornellius-gp/gpytorch) (Beta Release) * [Networkx](https://networkx.github.io/) * [Seaborn](https://seaborn.pydata.org/) * [Pandas](https://pandas.pydata.org/)  After installing the listed dependencies, simply clone this package to run experiments.  ## Getting Started  See `run.py` script to setup a simulation environment and run an agent to adapively collect data and learn the target distribution.",pandas,SOFTWARE
"For example,   ``` python run.py --eval_only --render ``` simulates an agent moving in the field to gather samples in an informative manner.",python,SOFTWARE
"# *Fast* R-CNN: Fast Region-based Convolutional Networks for object detection  Created by Ross Girshick at Microsoft Research, Redmond.  ### Introduction  **Fast R-CNN** is a fast framework for object detection with deep ConvNets.",*Fast* R-CNN,SOFTWARE
"# *Fast* R-CNN: Fast Region-based Convolutional Networks for object detection  Created by Ross Girshick at Microsoft Research, Redmond.  ### Introduction  **Fast R-CNN** is a fast framework for object detection with deep ConvNets.",Fast R-CNN,SOFTWARE
"Fast R-CNN  - trains state-of-the-art models, like VGG16, 9x faster than traditional R-CNN and 3x faster than SPPnet,  - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time,  - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet,  - and is written in Python and C++/Caffe.",Fast R-CNN,SOFTWARE
"Fast R-CNN  - trains state-of-the-art models, like VGG16, 9x faster than traditional R-CNN and 3x faster than SPPnet,  - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time,  - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet,  - and is written in Python and C++/Caffe.",VGG16,SOFTWARE
"Fast R-CNN  - trains state-of-the-art models, like VGG16, 9x faster than traditional R-CNN and 3x faster than SPPnet,  - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time,  - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet,  - and is written in Python and C++/Caffe.",R-CNN,SOFTWARE
"Fast R-CNN  - trains state-of-the-art models, like VGG16, 9x faster than traditional R-CNN and 3x faster than SPPnet,  - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time,  - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet,  - and is written in Python and C++/Caffe.",SPPnet,SOFTWARE
"Fast R-CNN  - trains state-of-the-art models, like VGG16, 9x faster than traditional R-CNN and 3x faster than SPPnet,  - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time,  - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet,  - and is written in Python and C++/Caffe.",R-CNN,SOFTWARE
"Fast R-CNN  - trains state-of-the-art models, like VGG16, 9x faster than traditional R-CNN and 3x faster than SPPnet,  - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time,  - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet,  - and is written in Python and C++/Caffe.",SPPnet,SOFTWARE
"Fast R-CNN  - trains state-of-the-art models, like VGG16, 9x faster than traditional R-CNN and 3x faster than SPPnet,  - runs 200x faster than R-CNN and 10x faster than SPPnet at test-time,  - has a significantly higher mAP on PASCAL VOC than both R-CNN and SPPnet,  - and is written in Python and C++/Caffe.",Caffe,SOFTWARE
"Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083).  ### License  Fast R-CNN is released under the MIT License (refer to the LICENSE file for details).  ### Citing Fast R-CNN  If you find Fast R-CNN useful in your research, please consider citing:      @article{girshick15fastrcnn,         Author = {Ross Girshick},         Title = {Fast R-CNN},         Journal = {arXiv preprint arXiv:1504.08083},         Year = {2015}     }      ### Contents 1.",Fast R-CNN,SOFTWARE
"Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083).  ### License  Fast R-CNN is released under the MIT License (refer to the LICENSE file for details).  ### Citing Fast R-CNN  If you find Fast R-CNN useful in your research, please consider citing:      @article{girshick15fastrcnn,         Author = {Ross Girshick},         Title = {Fast R-CNN},         Journal = {arXiv preprint arXiv:1504.08083},         Year = {2015}     }      ### Contents 1.",Fast R-CNN,SOFTWARE
"Fast R-CNN was initially described in an [arXiv tech report](http://arxiv.org/abs/1504.08083).  ### License  Fast R-CNN is released under the MIT License (refer to the LICENSE file for details).  ### Citing Fast R-CNN  If you find Fast R-CNN useful in your research, please consider citing:      @article{girshick15fastrcnn,         Author = {Ross Girshick},         Title = {Fast R-CNN},         Journal = {arXiv preprint arXiv:1504.08083},         Year = {2015}     }      ### Contents 1.",Fast R-CNN,SOFTWARE
Requirements for `Caffe` and `pycaffe` (see: [Caffe installation instructions](http://caffe.berkeleyvision.org/installation.html))    **Note:** Caffe *must* be built with support for Python layers!,Caffe,SOFTWARE
Requirements for `Caffe` and `pycaffe` (see: [Caffe installation instructions](http://caffe.berkeleyvision.org/installation.html))    **Note:** Caffe *must* be built with support for Python layers!,pycaffe,SOFTWARE
Requirements for `Caffe` and `pycaffe` (see: [Caffe installation instructions](http://caffe.berkeleyvision.org/installation.html))    **Note:** Caffe *must* be built with support for Python layers!,Caffe,SOFTWARE
Requirements for `Caffe` and `pycaffe` (see: [Caffe installation instructions](http://caffe.berkeleyvision.org/installation.html))    **Note:** Caffe *must* be built with support for Python layers!,caffe,SOFTWARE
Requirements for `Caffe` and `pycaffe` (see: [Caffe installation instructions](http://caffe.berkeleyvision.org/installation.html))    **Note:** Caffe *must* be built with support for Python layers!,Caffe,SOFTWARE
"Python packages you might not have: `cython`, `python-opencv`, `easydict` 3.",Python,SOFTWARE
"Python packages you might not have: `cython`, `python-opencv`, `easydict` 3.",python-opencv,SOFTWARE
"Python packages you might not have: `cython`, `python-opencv`, `easydict` 3.",easydict,SOFTWARE
[optional] MATLAB (required for PASCAL VOC evaluation only)  ### Requirements: hardware  1.,MATLAB,SOFTWARE
"For training smaller networks (CaffeNet, VGG_CNN_M_1024) a good GPU (e.g., Titan, K20, K40, ...) with at least 3G of memory suffices 2.",CaffeNet,SOFTWARE
"For training with VGG16, you'll need a K40 (~11G of memory)  ### Installation (sufficient for the demo)  1.",VGG16,SOFTWARE
Clone the Fast R-CNN repository   ```Shell   # Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/fast-rcnn.git   ```    2.,Fast R-CNN,SOFTWARE
Clone the Fast R-CNN repository   ```Shell   # Make sure to clone with --recursive   git clone --recursive https://github.com/rbgirshick/fast-rcnn.git   ```    2.,git,SOFTWARE
"We'll call the directory that you cloned Fast R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*        **Note 1:** If you didn't clone Fast R-CNN with the `--recursive` flag, then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `fast-rcnn` branch (or equivalent detached state).",Fast R-CNN,SOFTWARE
"We'll call the directory that you cloned Fast R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*        **Note 1:** If you didn't clone Fast R-CNN with the `--recursive` flag, then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `fast-rcnn` branch (or equivalent detached state).",Fast R-CNN,SOFTWARE
"We'll call the directory that you cloned Fast R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*        **Note 1:** If you didn't clone Fast R-CNN with the `--recursive` flag, then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `fast-rcnn` branch (or equivalent detached state).",caffe,SOFTWARE
"We'll call the directory that you cloned Fast R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*        **Note 1:** If you didn't clone Fast R-CNN with the `--recursive` flag, then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `fast-rcnn` branch (or equivalent detached state).",git,SOFTWARE
"We'll call the directory that you cloned Fast R-CNN into `FRCN_ROOT`     *Ignore notes 1 and 2 if you followed step 1 above.*        **Note 1:** If you didn't clone Fast R-CNN with the `--recursive` flag, then you'll need to manually clone the `caffe-fast-rcnn` submodule:     ```Shell     git submodule update --init --recursive     ```     **Note 2:** The `caffe-fast-rcnn` submodule needs to be on the `fast-rcnn` branch (or equivalent detached state).",caffe,SOFTWARE
"Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     # Now follow the Caffe installation instructions here:     #   http://caffe.berkeleyvision.org/installation.html      # If you're experienced with Caffe and have all of the requirements installed     # and your Makefile.config in place, then simply do:     make -j8 && make pycaffe     ```      5.",Caffe,SOFTWARE
"Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     # Now follow the Caffe installation instructions here:     #   http://caffe.berkeleyvision.org/installation.html      # If you're experienced with Caffe and have all of the requirements installed     # and your Makefile.config in place, then simply do:     make -j8 && make pycaffe     ```      5.",pycaffe,SOFTWARE
"Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     # Now follow the Caffe installation instructions here:     #   http://caffe.berkeleyvision.org/installation.html      # If you're experienced with Caffe and have all of the requirements installed     # and your Makefile.config in place, then simply do:     make -j8 && make pycaffe     ```      5.",Caffe,SOFTWARE
"Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     # Now follow the Caffe installation instructions here:     #   http://caffe.berkeleyvision.org/installation.html      # If you're experienced with Caffe and have all of the requirements installed     # and your Makefile.config in place, then simply do:     make -j8 && make pycaffe     ```      5.",caffe,SOFTWARE
"Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     # Now follow the Caffe installation instructions here:     #   http://caffe.berkeleyvision.org/installation.html      # If you're experienced with Caffe and have all of the requirements installed     # and your Makefile.config in place, then simply do:     make -j8 && make pycaffe     ```      5.",Caffe,SOFTWARE
"Build Caffe and pycaffe     ```Shell     cd $FRCN_ROOT/caffe-fast-rcnn     # Now follow the Caffe installation instructions here:     #   http://caffe.berkeleyvision.org/installation.html      # If you're experienced with Caffe and have all of the requirements installed     # and your Makefile.config in place, then simply do:     make -j8 && make pycaffe     ```      5.",pycaffe,SOFTWARE
Download pre-computed Fast R-CNN detectors     ```Shell     cd $FRCN_ROOT     .,Fast R-CNN,SOFTWARE
Download pre-computed Fast R-CNN detectors     ```Shell     cd $FRCN_ROOT     .,Shell,SOFTWARE
**Python**  To run the demo ```Shell cd $FRCN_ROOT .,Shell,SOFTWARE
"**Note:** If the demo crashes Caffe because your GPU doesn't have enough memory, try running the demo with a small network, e.g., `.",Caffe,SOFTWARE
"**MATLAB**  There's also a *basic* MATLAB demo, though it's missing some minor bells and whistles compared to the Python version.",MATLAB,SOFTWARE
"**MATLAB**  There's also a *basic* MATLAB demo, though it's missing some minor bells and whistles compared to the Python version.",MATLAB,SOFTWARE
```Shell cd $FRCN_ROOT/matlab matlab # wait for matlab to start,matlab,SOFTWARE
```Shell cd $FRCN_ROOT/matlab matlab # wait for matlab to start,matlab,SOFTWARE
"# At the matlab prompt, run the script: >> fast_rcnn_demo ```  Fast R-CNN training is implemented in Python only, but test-time detection functionality also exists in MATLAB.",matlab,SOFTWARE
"# At the matlab prompt, run the script: >> fast_rcnn_demo ```  Fast R-CNN training is implemented in Python only, but test-time detection functionality also exists in MATLAB.",Fast R-CNN,SOFTWARE
"# At the matlab prompt, run the script: >> fast_rcnn_demo ```  Fast R-CNN training is implemented in Python only, but test-time detection functionality also exists in MATLAB.",MATLAB,SOFTWARE
EdgeBoxes: [matlab code](https://github.com/pdollar/edges) 3.,EdgeBoxes,SOFTWARE
GOP and LPO: [python code](http://www.philkr.net/) 4.,GOP,SOFTWARE
GOP and LPO: [python code](http://www.philkr.net/) 4.,LPO,SOFTWARE
"Download the training, validation, test data and VOCdevkit   ```Shell  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtrainval_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtest_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCdevkit_08-Jun-2007.tar  ```   2.",VOCdevkit,SOFTWARE
"Download the training, validation, test data and VOCdevkit   ```Shell  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtrainval_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtest_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCdevkit_08-Jun-2007.tar  ```   2.",Shell,SOFTWARE
"Download the training, validation, test data and VOCdevkit   ```Shell  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtrainval_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCtest_06-Nov-2007.tar  wget http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007/VOCdevkit_08-Jun-2007.tar  ```   2.",wget,SOFTWARE
Extract all of these tars into one directory named `VOCdevkit`   ```Shell  tar xvf VOCtrainval_06-Nov-2007.tar  tar xvf VOCtest_06-Nov-2007.tar  tar xvf VOCdevkit_08-Jun-2007.tar  ```  3.,Shell,SOFTWARE
Extract all of these tars into one directory named `VOCdevkit`   ```Shell  tar xvf VOCtrainval_06-Nov-2007.tar  tar xvf VOCtest_06-Nov-2007.tar  tar xvf VOCdevkit_08-Jun-2007.tar  ```  3.,tar,SOFTWARE
Extract all of these tars into one directory named `VOCdevkit`   ```Shell  tar xvf VOCtrainval_06-Nov-2007.tar  tar xvf VOCtest_06-Nov-2007.tar  tar xvf VOCdevkit_08-Jun-2007.tar  ```  3.,tar,SOFTWARE
Extract all of these tars into one directory named `VOCdevkit`   ```Shell  tar xvf VOCtrainval_06-Nov-2007.tar  tar xvf VOCtest_06-Nov-2007.tar  tar xvf VOCdevkit_08-Jun-2007.tar  ```  3.,tar,SOFTWARE
"It should have this basic structure   ```Shell    $VOCdevkit/                           # development kit    $VOCdevkit/VOCcode/                   # VOC utility code    $VOCdevkit/VOC2007                    # image sets, annotations, etc",Shell,SOFTWARE
"It should have this basic structure   ```Shell    $VOCdevkit/                           # development kit    $VOCdevkit/VOCcode/                   # VOC utility code    $VOCdevkit/VOC2007                    # image sets, annotations, etc",V,SOFTWARE
"/data/scripts/fetch_selective_search_data.sh ```  This will populate the `$FRCN_ROOT/data` folder with `selective_selective_data`.  ### Download pre-trained ImageNet models  Pre-trained ImageNet models can be downloaded for the three networks described in the paper: CaffeNet (model **S**), VGG_CNN_M_1024 (model **M**), and VGG16 (model **L**).",CaffeNet,SOFTWARE
"/data/scripts/fetch_selective_search_data.sh ```  This will populate the `$FRCN_ROOT/data` folder with `selective_selective_data`.  ### Download pre-trained ImageNet models  Pre-trained ImageNet models can be downloaded for the three networks described in the paper: CaffeNet (model **S**), VGG_CNN_M_1024 (model **M**), and VGG16 (model **L**).",VGG_CNN_M_1024,SOFTWARE
"/data/scripts/fetch_selective_search_data.sh ```  This will populate the `$FRCN_ROOT/data` folder with `selective_selective_data`.  ### Download pre-trained ImageNet models  Pre-trained ImageNet models can be downloaded for the three networks described in the paper: CaffeNet (model **S**), VGG_CNN_M_1024 (model **M**), and VGG16 (model **L**).",VGG16,SOFTWARE
"/data/scripts/fetch_imagenet_models.sh ``` These models are all available in the [Caffe Model Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo), but are provided here for your convenience.  ### Usage  **Train** a Fast R-CNN detector.",Caffe,SOFTWARE
"/data/scripts/fetch_imagenet_models.sh ``` These models are all available in the [Caffe Model Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo), but are provided here for your convenience.  ### Usage  **Train** a Fast R-CNN detector.",caffe,SOFTWARE
"/data/scripts/fetch_imagenet_models.sh ``` These models are all available in the [Caffe Model Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo), but are provided here for your convenience.  ### Usage  **Train** a Fast R-CNN detector.",Fast R-CNN,SOFTWARE
"For example, train a VGG16 network on VOC 2007 trainval:  ```Shell .",VGG16,SOFTWARE
/tools/train_net.py --gpu 0 --solver models/VGG16/solver.prototxt \  --weights data/imagenet_models/VGG16.v2.caffemodel ```  If you see this error  ``` EnvironmentError: MATLAB command 'matlab' not found.,MATLAB,SOFTWARE
/tools/train_net.py --gpu 0 --solver models/VGG16/solver.prototxt \  --weights data/imagenet_models/VGG16.v2.caffemodel ```  If you see this error  ``` EnvironmentError: MATLAB command 'matlab' not found.,matlab,SOFTWARE
Please add 'matlab' to your PATH. ```  then you need to make sure the `matlab` binary is in your `$PATH`.,matlab,SOFTWARE
Please add 'matlab' to your PATH. ```  then you need to make sure the `matlab` binary is in your `$PATH`.,matlab,SOFTWARE
MATLAB is currently required for PASCAL VOC evaluation.,MATLAB,SOFTWARE
**Test** a Fast R-CNN detector.,Fast R-CNN,SOFTWARE
/tools/test_net.py --gpu 1 --def models/VGG16/test.prototxt \  --net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel ```  Test output is written underneath `$FRCN_ROOT/output`.,fast_rcnn,SOFTWARE
/tools/compress_net.py --def models/VGG16/test.prototxt \  --def-svd models/VGG16/compressed/test.prototxt \     --net output/default/voc_2007_trainval/vgg16_fast_rcnn_iter_40000.caffemodel # Test the model you just compressed .,fast_rcnn,SOFTWARE
"**Note:** Until recently (commit a566e39), the RNG seed for Caffe was not fixed during training.",Caffe,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",fast-rcnn,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",fast_rcnn,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",ast-rcnn,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",voc_2007,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",fast_rcnn,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",fast-rcnn,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",fast_rcnn,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",fast_rcnn,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",ast-rcnn,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",fast_rcnn,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",fast_rcnn,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",fast-rcnn,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",fast_rcnn,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",fast-rcnn,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",voc_2012,SOFTWARE
"Results generated before this commit will have some stochastic variation.  ### Extra downloads  - [Experiment logs](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/fast_rcnn_experiments.tgz) - PASCAL VOC test set detections     - [voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_caffenet_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg16_trained_on_2007_trainval.tgz)     - [voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2007_test_results_fast_rcnn_vgg_cnn_m_1024_trained_on_2007_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2007_trainvaltest_2012_trainval.tgz)     - [voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc_2012_test_results_fast_rcnn_vgg16_trained_on_2012_trainval.tgz) - [Fast R-CNN VGG16 model](http://www.cs.berkeley.edu/~rbg/fast-rcnn-data/voc12_submission.tgz) trained on VOC07 train,val,test union with VOC12 train,val",Fast R-CNN,SOFTWARE
The models are available on the Hugging Face model hub:  - [avsolatorio/GIST-large-Embedding-v0](https://huggingface.co/avsolatorio/GIST-large-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,avsolatorio/GIST-large-Embedding-v0,SOFTWARE
The models are available on the Hugging Face model hub:  - [avsolatorio/GIST-large-Embedding-v0](https://huggingface.co/avsolatorio/GIST-large-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,avsolatorio/GIST-large-Embedding-v0,SOFTWARE
The base model used is the [`BAAI/bge-large-en-v1.5`](https://huggingface.co/BAAI/bge-large-en-v1.5). - [avsolatorio/GIST-Embedding-v0](https://huggingface.co/avsolatorio/GIST-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,BAAI/bge-large-en-v1.5,SOFTWARE
The base model used is the [`BAAI/bge-large-en-v1.5`](https://huggingface.co/BAAI/bge-large-en-v1.5). - [avsolatorio/GIST-Embedding-v0](https://huggingface.co/avsolatorio/GIST-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,BAAI/bge-large-en-v1.5,SOFTWARE
The base model used is the [`BAAI/bge-large-en-v1.5`](https://huggingface.co/BAAI/bge-large-en-v1.5). - [avsolatorio/GIST-Embedding-v0](https://huggingface.co/avsolatorio/GIST-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,avsolatorio/GIST-Embedding-v0,SOFTWARE
The base model used is the [`BAAI/bge-large-en-v1.5`](https://huggingface.co/BAAI/bge-large-en-v1.5). - [avsolatorio/GIST-Embedding-v0](https://huggingface.co/avsolatorio/GIST-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,avsolatorio/GIST-Embedding-v0,SOFTWARE
The base model used is the [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5). - [avsolatorio/GIST-small-Embedding-v0](https://huggingface.co/avsolatorio/GIST-small-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,BAAI/bge-base-en-v1.5,SOFTWARE
The base model used is the [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5). - [avsolatorio/GIST-small-Embedding-v0](https://huggingface.co/avsolatorio/GIST-small-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,BAAI/bge-base-en-v1.5,SOFTWARE
The base model used is the [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5). - [avsolatorio/GIST-small-Embedding-v0](https://huggingface.co/avsolatorio/GIST-small-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,avsolatorio/GIST-small-Embedding-v0,SOFTWARE
The base model used is the [`BAAI/bge-base-en-v1.5`](https://huggingface.co/BAAI/bge-base-en-v1.5). - [avsolatorio/GIST-small-Embedding-v0](https://huggingface.co/avsolatorio/GIST-small-Embedding-v0): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,avsolatorio/GIST-small-Embedding-v0,SOFTWARE
The base model used is the [`BAAI/bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-small-en-v1.5). - [avsolatorio/GIST-all-MiniLM-L6-v2](https://huggingface.co/avsolatorio/GIST-all-MiniLM-L6-v2): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,BAAI/bge-small-en-v1.5,SOFTWARE
The base model used is the [`BAAI/bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-small-en-v1.5). - [avsolatorio/GIST-all-MiniLM-L6-v2](https://huggingface.co/avsolatorio/GIST-all-MiniLM-L6-v2): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,BAAI/bge-small-en-v1.5,SOFTWARE
The base model used is the [`BAAI/bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-small-en-v1.5). - [avsolatorio/GIST-all-MiniLM-L6-v2](https://huggingface.co/avsolatorio/GIST-all-MiniLM-L6-v2): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,avsolatorio/GIST-all-MiniLM-L6-v2,SOFTWARE
The base model used is the [`BAAI/bge-small-en-v1.5`](https://huggingface.co/BAAI/bge-small-en-v1.5). - [avsolatorio/GIST-all-MiniLM-L6-v2](https://huggingface.co/avsolatorio/GIST-all-MiniLM-L6-v2): The model fine-tuned using the GISTEmbed framework and the MEDI+MTEBcls dataset.,avsolatorio/GIST-all-MiniLM-L6-v2,SOFTWARE
The base model used is the [`sentence-transformers/all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2),sentence-transformers/all-MiniLM-L6-v2,SOFTWARE
The base model used is the [`sentence-transformers/all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2),sentence-transformers/all-MiniLM-L6-v2,SOFTWARE
# Usage  The model can be easily loaded using the Sentence Transformers library.,Sentence Transformers,SOFTWARE
```Python import torch.nn.functional as F from sentence_transformers import SentenceTransformer  revision = None  # Replace with the specific revision to ensure reproducibility in  case the model is updated.,Python,SOFTWARE
```Python import torch.nn.functional as F from sentence_transformers import SentenceTransformer  revision = None  # Replace with the specific revision to ensure reproducibility in  case the model is updated.,torch,SOFTWARE
```Python import torch.nn.functional as F from sentence_transformers import SentenceTransformer  revision = None  # Replace with the specific revision to ensure reproducibility in  case the model is updated.,sentence_transformers,SOFTWARE
```Python import torch.nn.functional as F from sentence_transformers import SentenceTransformer  revision = None  # Replace with the specific revision to ensure reproducibility in  case the model is updated.,SentenceTransformer,SOFTWARE
"model = SentenceTransformer(""avsolatorio/GIST-Embedding-v0"", revision=revision)  texts = [     ""Illustration of the REaLTabFormer model.",avsolatorio/GIST-Embedding-v0,SOFTWARE
"model = SentenceTransformer(""avsolatorio/GIST-Embedding-v0"", revision=revision)  texts = [     ""Illustration of the REaLTabFormer model.",REaLTabFormer,SOFTWARE
The left block shows the non-relational tabular data model using GPT-2 with a causal LM head.,GPT-2,SOFTWARE
"The trained GPT-2 model on the parent table, with weights frozen, is also used as the encoder in the Seq2Seq model."",     ""Predicting human mobility holds significant practical value, with applications ranging from enhancing disaster risk planning to simulating epidemic spread.",GPT-2,SOFTWARE
"The following steps are necessary to reproduce the results:   First, create a new conda environment and install poetry.  ``` conda create -n GISTEmbed python=3.10  conda activate GISTEmbed  pip install poetry ```  Next, clone the repository and install the dependencies.  ``` git clone https://github.com/avsolatorio/GISTEmbed.git  cd GISTEmbed  poetry install ```  To reduce the likelihood of encountering issues and unexpected training runs, we set up a convention that would validate the intended parameters and configurations.",conda,SOFTWARE
"The following steps are necessary to reproduce the results:   First, create a new conda environment and install poetry.  ``` conda create -n GISTEmbed python=3.10  conda activate GISTEmbed  pip install poetry ```  Next, clone the repository and install the dependencies.  ``` git clone https://github.com/avsolatorio/GISTEmbed.git  cd GISTEmbed  poetry install ```  To reduce the likelihood of encountering issues and unexpected training runs, we set up a convention that would validate the intended parameters and configurations.",poetry,SOFTWARE
"The following steps are necessary to reproduce the results:   First, create a new conda environment and install poetry.  ``` conda create -n GISTEmbed python=3.10  conda activate GISTEmbed  pip install poetry ```  Next, clone the repository and install the dependencies.  ``` git clone https://github.com/avsolatorio/GISTEmbed.git  cd GISTEmbed  poetry install ```  To reduce the likelihood of encountering issues and unexpected training runs, we set up a convention that would validate the intended parameters and configurations.",conda,SOFTWARE
"The following steps are necessary to reproduce the results:   First, create a new conda environment and install poetry.  ``` conda create -n GISTEmbed python=3.10  conda activate GISTEmbed  pip install poetry ```  Next, clone the repository and install the dependencies.  ``` git clone https://github.com/avsolatorio/GISTEmbed.git  cd GISTEmbed  poetry install ```  To reduce the likelihood of encountering issues and unexpected training runs, we set up a convention that would validate the intended parameters and configurations.",conda,SOFTWARE
"The following steps are necessary to reproduce the results:   First, create a new conda environment and install poetry.  ``` conda create -n GISTEmbed python=3.10  conda activate GISTEmbed  pip install poetry ```  Next, clone the repository and install the dependencies.  ``` git clone https://github.com/avsolatorio/GISTEmbed.git  cd GISTEmbed  poetry install ```  To reduce the likelihood of encountering issues and unexpected training runs, we set up a convention that would validate the intended parameters and configurations.",poetry,SOFTWARE
"The following steps are necessary to reproduce the results:   First, create a new conda environment and install poetry.  ``` conda create -n GISTEmbed python=3.10  conda activate GISTEmbed  pip install poetry ```  Next, clone the repository and install the dependencies.  ``` git clone https://github.com/avsolatorio/GISTEmbed.git  cd GISTEmbed  poetry install ```  To reduce the likelihood of encountering issues and unexpected training runs, we set up a convention that would validate the intended parameters and configurations.",git,SOFTWARE
"The following steps are necessary to reproduce the results:   First, create a new conda environment and install poetry.  ``` conda create -n GISTEmbed python=3.10  conda activate GISTEmbed  pip install poetry ```  Next, clone the repository and install the dependencies.  ``` git clone https://github.com/avsolatorio/GISTEmbed.git  cd GISTEmbed  poetry install ```  To reduce the likelihood of encountering issues and unexpected training runs, we set up a convention that would validate the intended parameters and configurations.",poetry,SOFTWARE
The script also uses WANDB for logging.,WANDB,SOFTWARE
Ensure to set the `WANDB_API_KEY` environment variable to enable logging to WANDB,WANDB,SOFTWARE
Ensure to set the `WANDB_API_KEY` environment variable to enable logging to WANDB,WANDB,SOFTWARE
Sentence Transformers library to support the GISTEmbed framework.,Sentence Transformers,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",conda,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",conda,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",conda,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",python=3.9,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",conda,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pip,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",rdkit-pypi,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",selfies,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pip,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",easydict,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pip,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",PyYAML,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pip,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",torch==1.13.1+cu116,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",torchvision==0.14.1+cu116,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pip,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",dgl,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pip,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",dglgo,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",dgllife,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",conda,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pytorch,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",faiss-gpu,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pip,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",info-nce-pytorch,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pip,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",networkx==2.4,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pip,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pytorch-lightning==1.6.4,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pip,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",rxn-chem-utils,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",rxn-utils,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pip,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",tensorboard,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",tensorboardX,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",cython-npm,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",numpy,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pandas,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",tqdm,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",git,SOFTWARE
"The first one is to create conda environment base on the our config, this might create conflicts because of different operating systems or conda sources on your machine:  `conda env create -f environment.yml`  The second option is to simply run the following commands to install the basic dependencies manually (recommended):  ``` conda create -n unirxn python=3.9 conda activate unirxn pip install rdkit-pypi selfies pip install easydict pip install PyYAML  pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116  pip install  dgl -f https://data.dgl.ai/wheels/cu116/repo.html pip install  dglgo -f https://data.dgl.ai/wheels-test/repo.html pip install dgllife  conda install -c pytorch faiss-gpu pip install info-nce-pytorch  pip install networkx==2.4  pip install pytorch-lightning==1.6.4 pip install rxn-chem-utils rxn-utils  pip install tensorboard tensorboardX pip install cython-npm numpy pandas tqdm  #this package requires to compile from source #run in a different directory from the unirxn project git clone https://github.com/rxn4chemistry/rxnmapper.git  cd rxnmapper pip install -e .    ```  ## Pretrained weights and Preprocessed Data Download  The pretrained weight are provided [here](https://zenodo.org/records/10938189).",pip,SOFTWARE
"If you are intereseted in using our generated drug-like structures direclty, you can download it from [here](https://doi.org/10.5281/zenodo.10970615)  ## Reaction Featurization  First, prepare your reaction rxn file to a processed pickle file by running ``` INPUT=/place/of/your/input/rxn #we provide a example here TMP=/place/to_store/your/tmp_file  python generation/prepare_rxn_for_feat.py --input_file $INPUT --output_file $TMP  ```  Then run the featurization scripts ``` python generation/featurize.py --input_file $TMP --model_dir ckpt/uni_rxn_base.ckpt ``` it will generate a unirxnfp pickled file in your TMP path  ## Molecule Generation  Before using our model for molecule generation to design your chemical library, you need to build a library of reactants and reagents for our model to retrieve molecules at each generative step.",python,SOFTWARE
"If you are intereseted in using our generated drug-like structures direclty, you can download it from [here](https://doi.org/10.5281/zenodo.10970615)  ## Reaction Featurization  First, prepare your reaction rxn file to a processed pickle file by running ``` INPUT=/place/of/your/input/rxn #we provide a example here TMP=/place/to_store/your/tmp_file  python generation/prepare_rxn_for_feat.py --input_file $INPUT --output_file $TMP  ```  Then run the featurization scripts ``` python generation/featurize.py --input_file $TMP --model_dir ckpt/uni_rxn_base.ckpt ``` it will generate a unirxnfp pickled file in your TMP path  ## Molecule Generation  Before using our model for molecule generation to design your chemical library, you need to build a library of reactants and reagents for our model to retrieve molecules at each generative step.",python,SOFTWARE
"It is also easy to build your own library, you only need to create a pickled file of a dictionary as:     {         'reactant' :[List of smiles]         'reagent' :[List of smiles]     } You can run this to generate the representation for this library: ``` python generation/build_react_lib.py --input_file $YOUR_LIB_PATH --model_dir ckpt/uni_rxn_gen.ckpt ``` Run this script with the default configuration will create a representation library using provided reactants (from ZINC subset) and reagents (from USPTO).",python,SOFTWARE
"Then you are able to run the generation process based on your library ```  #generate structure analogues  INPUT=/place/of/your/input/seed #support sdf file and raw smiles file OUTPUT=/place/to_store/your/output  python generation/generate_paths_reb.py --model_dir ckpt/uni_rxn_gen.ckpt --input_file $INPUT --react_lib_file dataset/data/react_lib_smi_rep.pkl --output_dir $OUTPUT ```  The output file is a list of chemical reaction paths, each path is formulated as: input seed --> reactants, reagents --> intermediate product --> reactants, reagents --> intermediate product --> ... --> final product  See more configuration guide by running `python generation/generate_paths_reb.py -h`    ## Train From Scratch (Optional) ### Dataset Construction  First download the USTPO_MIT dataset from (https://github.com/wengong-jin/nips17-rexgen/blob/master/USPTO/data.zip) and save it to `dataset/raw/`, also unzip it.",python,SOFTWARE
"Then you are able to run the generation process based on your library ```  #generate structure analogues  INPUT=/place/of/your/input/seed #support sdf file and raw smiles file OUTPUT=/place/to_store/your/output  python generation/generate_paths_reb.py --model_dir ckpt/uni_rxn_gen.ckpt --input_file $INPUT --react_lib_file dataset/data/react_lib_smi_rep.pkl --output_dir $OUTPUT ```  The output file is a list of chemical reaction paths, each path is formulated as: input seed --> reactants, reagents --> intermediate product --> reactants, reagents --> intermediate product --> ... --> final product  See more configuration guide by running `python generation/generate_paths_reb.py -h`    ## Train From Scratch (Optional) ### Dataset Construction  First download the USTPO_MIT dataset from (https://github.com/wengong-jin/nips17-rexgen/blob/master/USPTO/data.zip) and save it to `dataset/raw/`, also unzip it.",python,SOFTWARE
"Then you are able to run the generation process based on your library ```  #generate structure analogues  INPUT=/place/of/your/input/seed #support sdf file and raw smiles file OUTPUT=/place/to_store/your/output  python generation/generate_paths_reb.py --model_dir ckpt/uni_rxn_gen.ckpt --input_file $INPUT --react_lib_file dataset/data/react_lib_smi_rep.pkl --output_dir $OUTPUT ```  The output file is a list of chemical reaction paths, each path is formulated as: input seed --> reactants, reagents --> intermediate product --> reactants, reagents --> intermediate product --> ... --> final product  See more configuration guide by running `python generation/generate_paths_reb.py -h`    ## Train From Scratch (Optional) ### Dataset Construction  First download the USTPO_MIT dataset from (https://github.com/wengong-jin/nips17-rexgen/blob/master/USPTO/data.zip) and save it to `dataset/raw/`, also unzip it.",USTPO,SOFTWARE
Run the following scripts to create your own dataset.  ``` cd dataset #create data for pretraining python clean_all_reaction_for_pretrain.py  #create data for generative model training python create_dataset.py  ```  ### Training ``` python trainer_pretrainig_graph_pl.py python trainer_set_vae.py ```    ## License  This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.,python,SOFTWARE
Run the following scripts to create your own dataset.  ``` cd dataset #create data for pretraining python clean_all_reaction_for_pretrain.py  #create data for generative model training python create_dataset.py  ```  ### Training ``` python trainer_pretrainig_graph_pl.py python trainer_set_vae.py ```    ## License  This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.,python,SOFTWARE
Run the following scripts to create your own dataset.  ``` cd dataset #create data for pretraining python clean_all_reaction_for_pretrain.py  #create data for generative model training python create_dataset.py  ```  ### Training ``` python trainer_pretrainig_graph_pl.py python trainer_set_vae.py ```    ## License  This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.,python,SOFTWARE
Run the following scripts to create your own dataset.  ``` cd dataset #create data for pretraining python clean_all_reaction_for_pretrain.py  #create data for generative model training python create_dataset.py  ```  ### Training ``` python trainer_pretrainig_graph_pl.py python trainer_set_vae.py ```    ## License  This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.,python,SOFTWARE
- Please first install our modified pysindy package archived at [this OneDrive link](https://1drv.ms/u/c/39cecf604f8b30de/Ed4wi09gz84ggDl7AAAAAAABgZ89ebMdSRESd2a8jiF01w?,pysindy,SOFTWARE
- Please first install our modified pysindy package archived at [this OneDrive link](https://1drv.ms/u/c/39cecf604f8b30de/Ed4wi09gz84ggDl7AAAAAAABgZ89ebMdSRESd2a8jiF01w?,OneDrive,SOFTWARE
"The more updated version is avaiable at [this repository](https://github.com/Pongpisit-Thanasutives/pysindy). - To use the L0BnB best-subset solver, please install [the package](https://github.com/Pongpisit-Thanasutives/l0bnb).",L0BnB,SOFTWARE
"# Efficient and performance-portable vector software  [//]: # (placeholder, do not remove)  Highway is a C++ library that provides portable SIMD/vector intrinsics.",Highway,SOFTWARE
"[Documentation](https://google.github.io/highway/en/master/)  Previously licensed under Apache 2, now dual-licensed as Apache 2 / BSD-3.  ## Why  We are passionate about high-performance software.",highway,SOFTWARE
Highway is for engineers who want to reliably and economically push the boundaries of what is possible in software.  ## How  CPUs provide SIMD/vector instructions that apply the same operation to multiple data items.,Highway,SOFTWARE
Highway makes SIMD/vector programming practical and workable according to these guiding principles:  **Does what you expect**: Highway is a C++ library with carefully-chosen functions that map well to CPU instructions without extensive compiler transformations.,Highway,SOFTWARE
Highway makes SIMD/vector programming practical and workable according to these guiding principles:  **Does what you expect**: Highway is a C++ library with carefully-chosen functions that map well to CPU instructions without extensive compiler transformations.,Highway,SOFTWARE
"**Works on widely-used platforms**: Highway supports five architectures; the same application code can target various instruction sets, including those with 'scalable' vectors (size unknown at compile time).",Highway,SOFTWARE
Highway only requires C++11 and supports four families of compilers.,Highway,SOFTWARE
"If you would like to use Highway on other platforms, please raise an issue.",Highway,SOFTWARE
"**Flexible to deploy**: Applications using Highway can run on heterogeneous clouds or client devices, choosing the best available instruction set at runtime.",Highway,SOFTWARE
"**Suitable for a variety of domains**: Highway provides an extensive set of operations, used for image processing (floating-point), compression, video analysis, linear algebra, cryptography, sorting and random generation.",Highway,SOFTWARE
"**Rewards data-parallel design**: Highway provides tools such as Gather, MaskedLoad, and FixedTag to enable speedups for legacy data structures.",Highway,SOFTWARE
"**Rewards data-parallel design**: Highway provides tools such as Gather, MaskedLoad, and FixedTag to enable speedups for legacy data structures.",Gather,SOFTWARE
"**Rewards data-parallel design**: Highway provides tools such as Gather, MaskedLoad, and FixedTag to enable speedups for legacy data structures.",MaskedLoad,SOFTWARE
"**Rewards data-parallel design**: Highway provides tools such as Gather, MaskedLoad, and FixedTag to enable speedups for legacy data structures.",FixedTag,SOFTWARE
Most are GitHub repositories.,GitHub,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",Chromium,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",Vivaldi,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",Firefox,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",floorp,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",foxhound,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",librewolf,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",RNA analysis,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",BPCells,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",Sparse voxel renderer,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",voxl,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",bkille,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",BitLib,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",eustas,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",2im,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",Grok,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",grok,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",JPEG XL,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",libjxl,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",JPEGenc,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",JPEGenc,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",Jpegli,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",jpegli,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",OpenHTJ2K,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",cloudinary,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",ssimulacra2,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",m-ab-s,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",media-autobuild_suite,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",libvips,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",libvips,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",AlienCowEatCake,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",ImageViewer,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",diffractor,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",diffractor,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",mirillis,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",jpegxl-wic,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",Lux panorama,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",image viewer,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",iresearch database index,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",iresearch,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",michaeljclark,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",zvec,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",nebula interactive analytics,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",OLAP,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",nebula,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",ScaNN Scalable Nearest Neighbors,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",scann,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",vectorlite vector search,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",vectorlite,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",gemma.cpp,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",gemma.cpp,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",Tensorflow,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",Numpy,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",zpye,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",SimpleInfer,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",MIT Model-Based Design and Verification,SOFTWARE
"*   Audio: [Zimtohrli perceptual metric](https://github.com/google/zimtohrli) *   Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf /     Waterfox) *   Computational biology: [RNA analysis](https://github.com/bnprks/BPCells) *   Computer graphics: [Sparse voxel renderer](https://github.com/rools/voxl) *   Cryptography: google/distributed_point_functions, google/shell-encryption *   Data structures: bkille/BitLib *   Image codecs: eustas/2im,     [Grok JPEG 2000](https://github.com/GrokImageCompression/grok),     [JPEG XL](https://github.com/libjxl/libjxl),     [JPEGenc](https://github.com/osamu620/JPEGenc),     [Jpegli](https://github.com/google/jpegli), OpenHTJ2K *   Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite,     [libvips](https://github.com/libvips/libvips) *   Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor,     mirillis/jpegxl-wic,     [Lux panorama/image viewer](https://bitbucket.org/kfj/pv/) *   Information retrieval:     [iresearch database index](https://github.com/iresearch-toolkit/iresearch),     michaeljclark/zvec,     [nebula interactive analytics / OLAP](https://github.com/varchar-io/nebula),     [ScaNN Scalable Nearest Neighbors](https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann),     [vectorlite vector search](https://github.com/1yefuwang1/vectorlite/) *   Machine learning: [gemma.cpp](https://github.com/google/gemma.cpp),     Tensorflow, Numpy, zpye/SimpleInfer *   Robotics:     [MIT Model-Based Design and Verification](https://github.com/RobotLocomotion/drake)  Other  *   [Evaluation of C++ SIMD Libraries](https://www.mnm-team.org/pub/Fopras/rock23/):     ""Highway excelled with a strong performance across multiple SIMD extensions     [..].",Highway,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",Highway,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",zimt,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",zimt,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",vectorized Quicksort,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",Git,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",alpinelinux,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",conan-io,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",conda-forge,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",DragonFlyBSD,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",fd00,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",yacp,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",freebsd,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",getsolus,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",packages,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",ghostbsd,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",microsoft,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",vcpkg,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",MidnightBSD,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",MSYS2,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",NetBSD,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",openSUSE,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",opnsense,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",Xilinx,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",Vitis_Libraries,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",xmake-io,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",xmake-repo,SOFTWARE
"Thus, Highway may currently be the most suitable SIMD library for many     software projects."" *   [zimt](https://github.com/kfjahnke/zimt): C++11 template library to process n-dimensional arrays with multi-threaded SIMD code *   [vectorized Quicksort](https://github.com/google/highway/tree/master/hwy/contrib/sort) ([paper](https://arxiv.org/abs/2205.05982))  If you'd like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:  *   alpinelinux *   conan-io *   conda-forge *   DragonFlyBSD, *   fd00/yacp *   freebsd *   getsolus/packages *   ghostbsd *   microsoft/vcpkg *   MidnightBSD *   MSYS2 *   NetBSD *   openSUSE *   opnsense *   Xilinx/Vitis_Libraries *   xmake-io/xmake-repo  See also the list at https://repology.org/project/highway-simd-library/versions .  ## Current status  ### Targets  Highway supports 24 targets, listed in alphabetical order of platform:  -   Any: `EMU128`, `SCALAR`; -   Armv7+: `NEON_WITHOUT_AES`, `NEON`, `NEON_BF16`, `SVE`, `SVE2`, `SVE_256`,     `SVE2_128`; -   IBM Z: `Z14`, `Z15`; -   POWER: `PPC8` (v2.07), `PPC9` (v3.0), `PPC10` (v3.1B, not yet supported due     to compiler bugs, see #1207; also requires QEMU 7.2); -   RISC-V: `RVV` (1.0); -   WebAssembly: `WASM`, `WASM_EMU256` (a 2x unrolled version of wasm128,     enabled if `HWY_WANT_WASM2` is defined.",highway-simd-library,SOFTWARE
"-   `AVX2` (~Haswell, also includes BMI2 + F16 + FMA)     -   `AVX3` (~Skylake, AVX-512F/BW/CD/DQ/VL)     -   `AVX3_DL` (~Icelake, includes BitAlg + CLMUL + GFNI + VAES + VBMI +         VBMI2 + VNNI + VPOPCNT; requires opt-in by defining `HWY_WANT_AVX3_DL`         unless compiling for static dispatch),     -   `AVX3_ZEN4` (like AVX3_DL but optimized for AMD Zen4; requires opt-in by         defining `HWY_WANT_AVX3_ZEN4` if compiling for static dispatch, but         enabled by default for runtime dispatch),     -   `AVX3_SPR` (~Sapphire Rapids, includes AVX-512FP16)  Our policy is that unless otherwise specified, targets will remain supported as long as they can be (cross-)compiled with currently supported Clang or GCC, and tested using QEMU.",Clang,SOFTWARE
"-   `AVX2` (~Haswell, also includes BMI2 + F16 + FMA)     -   `AVX3` (~Skylake, AVX-512F/BW/CD/DQ/VL)     -   `AVX3_DL` (~Icelake, includes BitAlg + CLMUL + GFNI + VAES + VBMI +         VBMI2 + VNNI + VPOPCNT; requires opt-in by defining `HWY_WANT_AVX3_DL`         unless compiling for static dispatch),     -   `AVX3_ZEN4` (like AVX3_DL but optimized for AMD Zen4; requires opt-in by         defining `HWY_WANT_AVX3_ZEN4` if compiling for static dispatch, but         enabled by default for runtime dispatch),     -   `AVX3_SPR` (~Sapphire Rapids, includes AVX-512FP16)  Our policy is that unless otherwise specified, targets will remain supported as long as they can be (cross-)compiled with currently supported Clang or GCC, and tested using QEMU.",GCC,SOFTWARE
"-   `AVX2` (~Haswell, also includes BMI2 + F16 + FMA)     -   `AVX3` (~Skylake, AVX-512F/BW/CD/DQ/VL)     -   `AVX3_DL` (~Icelake, includes BitAlg + CLMUL + GFNI + VAES + VBMI +         VBMI2 + VNNI + VPOPCNT; requires opt-in by defining `HWY_WANT_AVX3_DL`         unless compiling for static dispatch),     -   `AVX3_ZEN4` (like AVX3_DL but optimized for AMD Zen4; requires opt-in by         defining `HWY_WANT_AVX3_ZEN4` if compiling for static dispatch, but         enabled by default for runtime dispatch),     -   `AVX3_SPR` (~Sapphire Rapids, includes AVX-512FP16)  Our policy is that unless otherwise specified, targets will remain supported as long as they can be (cross-)compiled with currently supported Clang or GCC, and tested using QEMU.",QEMU,SOFTWARE
"If the target can be compiled with LLVM trunk and tested using our version of QEMU without extra flags, then it is eligible for inclusion in our continuous testing infrastructure.",LLVM,SOFTWARE
"If the target can be compiled with LLVM trunk and tested using our version of QEMU without extra flags, then it is eligible for inclusion in our continuous testing infrastructure.",QEMU,SOFTWARE
"Otherwise, the target will be manually tested before releases with selected versions/configurations of Clang and GCC.",Clang,SOFTWARE
"Otherwise, the target will be manually tested before releases with selected versions/configurations of Clang and GCC.",GCC,SOFTWARE
"Before releases, we also test on x86 with Clang and GCC, and Armv7/8 via GCC cross-compile.",Clang,SOFTWARE
"Before releases, we also test on x86 with Clang and GCC, and Armv7/8 via GCC cross-compile.",GCC,SOFTWARE
"Before releases, we also test on x86 with Clang and GCC, and Armv7/8 via GCC cross-compile.",GCC,SOFTWARE
"See the [testing process](g3doc/release_testing_process.md) for details.  ### Related modules  The `contrib` directory contains SIMD-related utilities: an image class with aligned rows, a math library (16 functions already implemented, mostly trigonometry), and functions for computing dot products and sorting.  ### Other libraries  If you only require x86 support, you may also use Agner Fog's [VCL vector class library](https://github.com/vectorclass).",VCL,SOFTWARE
"See the [testing process](g3doc/release_testing_process.md) for details.  ### Related modules  The `contrib` directory contains SIMD-related utilities: an image class with aligned rows, a math library (16 functions already implemented, mostly trigonometry), and functions for computing dot products and sorting.  ### Other libraries  If you only require x86 support, you may also use Agner Fog's [VCL vector class library](https://github.com/vectorclass).",vector class library,SOFTWARE
"See the [testing process](g3doc/release_testing_process.md) for details.  ### Related modules  The `contrib` directory contains SIMD-related utilities: an image class with aligned rows, a math library (16 functions already implemented, mostly trigonometry), and functions for computing dot products and sorting.  ### Other libraries  If you only require x86 support, you may also use Agner Fog's [VCL vector class library](https://github.com/vectorclass).",vectorclass,SOFTWARE
"If you have existing code using x86/NEON intrinsics, you may be interested in [SIMDe](https://github.com/simd-everywhere/simde), which emulates those intrinsics using other platforms' intrinsics or autovectorization.  ## Installation  This project uses CMake to generate and build.",SIMDe,SOFTWARE
"If you have existing code using x86/NEON intrinsics, you may be interested in [SIMDe](https://github.com/simd-everywhere/simde), which emulates those intrinsics using other platforms' intrinsics or autovectorization.  ## Installation  This project uses CMake to generate and build.",simde,SOFTWARE
"If you have existing code using x86/NEON intrinsics, you may be interested in [SIMDe](https://github.com/simd-everywhere/simde), which emulates those intrinsics using other platforms' intrinsics or autovectorization.  ## Installation  This project uses CMake to generate and build.",CMake,SOFTWARE
In a Debian-based system you can install it via:  ```bash sudo apt install cmake ```  Highway's unit tests use [googletest](https://github.com/google/googletest).,Debian,SOFTWARE
In a Debian-based system you can install it via:  ```bash sudo apt install cmake ```  Highway's unit tests use [googletest](https://github.com/google/googletest).,cmake,SOFTWARE
"By default, Highway's CMake downloads this dependency at configuration time.",CMake,SOFTWARE
"To build Highway as a shared or static library (depending on BUILD_SHARED_LIBS), the standard CMake workflow can be used:  ```bash mkdir -p build && cd build cmake .. make -j && make test ```  Or you can run `run_tests.sh` (`run_tests.bat` on Windows).",Highway,SOFTWARE
"To build Highway as a shared or static library (depending on BUILD_SHARED_LIBS), the standard CMake workflow can be used:  ```bash mkdir -p build && cd build cmake .. make -j && make test ```  Or you can run `run_tests.sh` (`run_tests.bat` on Windows).",CMake,SOFTWARE
"To build Highway as a shared or static library (depending on BUILD_SHARED_LIBS), the standard CMake workflow can be used:  ```bash mkdir -p build && cd build cmake .. make -j && make test ```  Or you can run `run_tests.sh` (`run_tests.bat` on Windows).",cmake,SOFTWARE
"To build Highway as a shared or static library (depending on BUILD_SHARED_LIBS), the standard CMake workflow can be used:  ```bash mkdir -p build && cd build cmake .. make -j && make test ```  Or you can run `run_tests.sh` (`run_tests.bat` on Windows).",Windows,SOFTWARE
"Note that johnplatts has successfully built and run the Highway tests on 32-bit x86, including AVX2/3, on GCC 7/8 and Clang 8/11/12.",GCC,SOFTWARE
"Note that johnplatts has successfully built and run the Highway tests on 32-bit x86, including AVX2/3, on GCC 7/8 and Clang 8/11/12.",Clang,SOFTWARE
See #1279.  ## Building highway - Using vcpkg  highway is now available in [vcpkg](https://github.com/Microsoft/vcpkg)  ```bash vcpkg install highway ```  The highway port in vcpkg is kept up to date by Microsoft team members and community contributors.,highway,SOFTWARE
See #1279.  ## Building highway - Using vcpkg  highway is now available in [vcpkg](https://github.com/Microsoft/vcpkg)  ```bash vcpkg install highway ```  The highway port in vcpkg is kept up to date by Microsoft team members and community contributors.,vcpkg,SOFTWARE
See #1279.  ## Building highway - Using vcpkg  highway is now available in [vcpkg](https://github.com/Microsoft/vcpkg)  ```bash vcpkg install highway ```  The highway port in vcpkg is kept up to date by Microsoft team members and community contributors.,highway,SOFTWARE
See #1279.  ## Building highway - Using vcpkg  highway is now available in [vcpkg](https://github.com/Microsoft/vcpkg)  ```bash vcpkg install highway ```  The highway port in vcpkg is kept up to date by Microsoft team members and community contributors.,vcpkg,SOFTWARE
See #1279.  ## Building highway - Using vcpkg  highway is now available in [vcpkg](https://github.com/Microsoft/vcpkg)  ```bash vcpkg install highway ```  The highway port in vcpkg is kept up to date by Microsoft team members and community contributors.,vcpkg,SOFTWARE
See #1279.  ## Building highway - Using vcpkg  highway is now available in [vcpkg](https://github.com/Microsoft/vcpkg)  ```bash vcpkg install highway ```  The highway port in vcpkg is kept up to date by Microsoft team members and community contributors.,vcpkg,SOFTWARE
See #1279.  ## Building highway - Using vcpkg  highway is now available in [vcpkg](https://github.com/Microsoft/vcpkg)  ```bash vcpkg install highway ```  The highway port in vcpkg is kept up to date by Microsoft team members and community contributors.,vcpkg,SOFTWARE
"If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.  ## Quick start  You can use the `benchmark` inside examples/ as a starting point.",vcpkg,SOFTWARE
"If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.  ## Quick start  You can use the `benchmark` inside examples/ as a starting point.",vcpkg,SOFTWARE
"For clang and GCC, `-O2` is generally sufficient.",clang,SOFTWARE
"For clang and GCC, `-O2` is generally sufficient.",GCC,SOFTWARE
"For MSVC, we recommend compiling with `/Gv` to allow non-inlined functions to pass vector arguments in registers.",MSVC,SOFTWARE
"If intending to use the AVX2 target together with half-width vectors (e.g. for `PromoteTo`), it is also important to compile with `/arch:AVX2`.",AVX2,SOFTWARE
"If intending to use the AVX2 target together with half-width vectors (e.g. for `PromoteTo`), it is also important to compile with `/arch:AVX2`.",AVX2,SOFTWARE
This seems to be the only way to reliably generate VEX-encoded SSE instructions on MSVC.,MSVC,SOFTWARE
"Sometimes MSVC generates VEX-encoded SSE instructions, if they are mixed with AVX, but not always, see [DevCom-10618264](https://developercommunity.visualstudio.com/t/10618264).",MSVC,SOFTWARE
"Sometimes MSVC generates VEX-encoded SSE instructions, if they are mixed with AVX, but not always, see [DevCom-10618264](https://developercommunity.visualstudio.com/t/10618264).",AVX,SOFTWARE
"This dataset serves as the backbone for many of the studies in lung nodule detection. - **LUNA16**: Derived from LIDC-IDRI, LUNA16 focuses on nodule detection, providing 888 high-quality CT scans with standardized metrics for evaluating algorithms. - **Additional Datasets**: Datasets such as ELCAP, NSCLC, and ANODE09 are also referenced to supplement research efforts.  ## Key Techniques and Models This project explores multiple deep learning models tailored to different aspects of lung nodule detection, segmentation, and classification:  ### Detection Models - **CNN-based detection**: A variety of CNN architectures are used, ranging from lightweight models like Light CNN to more advanced networks such as U-Net++ and EfficientNet. - **YOLOv8**: A real-time detection model, effective for fast nodule identification with minimal false positives. - **Hybrid Approaches**: Fusion of 3D imaging techniques and biomarker data to enhance detection precision.  ### Segmentation Models - **U-Net Variants**: Models like Wavelet U-Net++ and 3D DenseUNet provide robust segmentation of nodules with a focus on improving Dice Similarity Coefficient (DSC). - **Attention Mechanisms**: Self-attention networks (e.g., HSNet) improve segmentation accuracy by focusing on relevant features in CT scans. - **Multi-task Learning**: Approaches that integrate both nodule detection and segmentation into a single pipeline for better performance.  ### Classification Models - **Deep Learning Classifiers**: CNN-based classifiers, such as those built on ResNet and DenseNet architectures, are employed to distinguish between benign and malignant nodules. - **Ensemble Learning**: Hybrid deep learning models that combine multiple classifiers to enhance accuracy, sensitivity, and specificity. - **SVM and Traditional Approaches**: For comparative purposes, traditional machine learning methods such as Support Vector Machines (SVM) are also tested against deep learning models.  ## Performance Metrics The performance of the models is evaluated using the following key metrics: - **Sensitivity (True Positive Rate)**: Measures the model's ability to correctly identify nodules. - **Specificity (True Negative Rate)**: Measures the model's ability to correctly identify non-nodules. - **Dice Similarity Coefficient (DSC)**: Used in segmentation tasks to evaluate the overlap between predicted and ground truth nodule regions. - **Area Under the Curve (AUC)**: Commonly used in classification tasks to evaluate the overall model performance. - **Competition Performance Metric (CPM)**: Specific to lung nodule detection, measuring sensitivity at varying false-positive rates.  ## Future Work This project highlights the potential of deep learning models in improving the accuracy of lung nodule detection and classification.",YOLOv8,SOFTWARE
"# ActionFormer: Localizing Moments of Actions with Transformers  ## Introduction This code repo implements Actionformer, one of the first Transformer-based model for temporal action localization --- detecting the onsets and offsets of action instances and recognizing their action categories.",ActionFormer,SOFTWARE
"# ActionFormer: Localizing Moments of Actions with Transformers  ## Introduction This code repo implements Actionformer, one of the first Transformer-based model for temporal action localization --- detecting the onsets and offsets of action instances and recognizing their action categories.",Actionformer,SOFTWARE
"Without bells and whistles, ActionFormer achieves 71.0% mAP at tIoU=0.5 on THUMOS14, outperforming the best prior model by 14.1 absolute percentage points and crossing the 60% mAP for the first time.",ActionFormer,SOFTWARE
"Further, ActionFormer demonstrates strong results on ActivityNet 1.3 (36.56% average mAP) and the more challenging EPIC-Kitchens 100 (+13.5% average mAP over prior works).",ActionFormer,SOFTWARE
"In addition, ActionFormer is the backbone for many winning solutions in the Ego4D Moment Queries Challenge 2022.",ActionFormer,SOFTWARE
"<div align=""center"">   <img src=""teaser.jpg"" width=""600px""/> </div>  Specifically, we adopt a minimalist design and develop a Transformer based model for temporal action localization, inspired by the recent success of Transformers in NLP and vision.",Transformers,SOFTWARE
"/libs/utils: Utility functions for training, inference, and postprocessing.  ## Installation * Follow INSTALL.md for installing necessary dependencies and compiling the code.  ## Frequently Asked Questions * See FAQ.md.   ## To Reproduce Our Results on THUMOS14 **Download Features and Annotations** * Download *thumos.tar.gz* (`md5sum 375f76ffbf7447af1035e694971ec9b2`) from [this Box link](https://uwmadison.box.com/s/glpuxadymf3gd01m1cj6g5c3bn39qbgr) or [this Google Drive link](https://drive.google.com/file/d/1zt2eoldshf99vJMDuu8jqxda55dCyhZP/view?",Google Drive,SOFTWARE
usp=sharing) or [this BaiduYun link](https://pan.baidu.com/s/1TgS91LVV-vzFTgIHl1AEGA?,BaiduYun,SOFTWARE
‚îÇ ‚îî‚îÄ‚îÄ‚îÄdata/ ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄthumos/ ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄannotations ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄi3d_features    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ... | ‚îî‚îÄ‚îÄ‚îÄlibs ‚îÇ ‚îÇ   ... ```  **Training and Evaluation** * Train our ActionFormer with I3D features.,ActionFormer,SOFTWARE
```shell python .,python,SOFTWARE
/configs/thumos_i3d.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.,TensorBoard,SOFTWARE
/configs/thumos_i3d.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.,tensorboard,SOFTWARE
```shell python .,python,SOFTWARE
The model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/1isG3bc1dG5-llBRFCivJwz_7c_b0XDcY/view?,Google Drive,SOFTWARE
```shell python .,python,SOFTWARE
/pretrained/thumos_i3d_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method            |  0.3  |  0.4  |  0.5  |  0.6  |  0.7  |  Avg  | |-------------------|-------|-------|-------|-------|-------|-------| | ActionFormer      | 82.13 | 77.80 | 70.95 | 59.40 | 43.87 | 66.83 |   ## To Reproduce Our Results on ActivityNet 1.3 **Download Features and Annotations** * Download *anet_1.3.tar.gz* (`md5sum c415f50120b9425ee1ede9ac3ce11203`) from [this Box link](https://uwmadison.box.com/s/aisdoymowukc99zoc7gpqegxbb4whikx) or [this Google Drive Link](https://drive.google.com/file/d/1VW8px1Nz9A17i0wMVUfxh6YsPCLVqL-S/view?,ActionFormer,SOFTWARE
/pretrained/thumos_i3d_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method            |  0.3  |  0.4  |  0.5  |  0.6  |  0.7  |  Avg  | |-------------------|-------|-------|-------|-------|-------|-------| | ActionFormer      | 82.13 | 77.80 | 70.95 | 59.40 | 43.87 | 66.83 |   ## To Reproduce Our Results on ActivityNet 1.3 **Download Features and Annotations** * Download *anet_1.3.tar.gz* (`md5sum c415f50120b9425ee1ede9ac3ce11203`) from [this Box link](https://uwmadison.box.com/s/aisdoymowukc99zoc7gpqegxbb4whikx) or [this Google Drive Link](https://drive.google.com/file/d/1VW8px1Nz9A17i0wMVUfxh6YsPCLVqL-S/view?,Google Drive,SOFTWARE
usp=sharing) or [this BaiduYun Link](https://pan.baidu.com/s/1tw5W8B5YqDvfl-mrlWQvnQ?,BaiduYun,SOFTWARE
```shell python .,python,SOFTWARE
/configs/anet_tsp.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.,TensorBoard,SOFTWARE
/configs/anet_tsp.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.,tensorboard,SOFTWARE
```shell python .,python,SOFTWARE
The model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/1JKh3w14ngAjgzuuP22BnjhkhIcBSqteJ/view?,Google Drive,SOFTWARE
```shell python .,python,SOFTWARE
/pretrained/anet_tsp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.67 | 37.81 |  8.36 | 36.56 |   **[Optional] Reproducing Our Results with I3D Features**  * Download *anet_1.3_i3d.tar.gz* (`md5sum e649425954e0123401650312dd0d56a7`) from [this Google Drive Link](https://drive.google.com/file/d/16239kUT2Z-j6S6PXIT1b_31OJi35QW_o/view?,ActionFormer,SOFTWARE
/pretrained/anet_tsp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.67 | 37.81 |  8.36 | 36.56 |   **[Optional] Reproducing Our Results with I3D Features**  * Download *anet_1.3_i3d.tar.gz* (`md5sum e649425954e0123401650312dd0d56a7`) from [this Google Drive Link](https://drive.google.com/file/d/16239kUT2Z-j6S6PXIT1b_31OJi35QW_o/view?,Google Drive,SOFTWARE
* Train our ActionFormer with I3D features.,ActionFormer,SOFTWARE
```shell python .,python,SOFTWARE
```shell python .,python,SOFTWARE
/ckpt/anet_i3d_reproduce ```  * The pre-trained model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/152dw2JDoNPssSnaQDaNolQUSFgcHlxe3/view?,Google Drive,SOFTWARE
"/pretrained*), and run ```shell python .",python,SOFTWARE
/pretrained/anet_i3d_reproduce/ ```  * The results (mAP at tIoUs) with I3D features should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.29 | 36.71 |  8.24 | 36.03 |  ## To Reproduce Our Results on EPIC Kitchens 100 **Download Features and Annotations** * Download *epic_kitchens.tar.gz* (`md5sum add9803756afd9a023bc9a9c547e0229`) from [this Box link](https://uwmadison.box.com/s/vdha47qnce6jhqktz9g4mq1gc40w82yj) or [this Google Drive Link](https://drive.google.com/file/d/1Z4U_dLuu6_cV5NBIrSzsSDOOj2Uar85X/view?,ActionFormer,SOFTWARE
/pretrained/anet_i3d_reproduce/ ```  * The results (mAP at tIoUs) with I3D features should be  | Method            |  0.5  |  0.75 |  0.95 |  Avg  | |-------------------|-------|-------|-------|-------| | ActionFormer      | 54.29 | 36.71 |  8.24 | 36.03 |  ## To Reproduce Our Results on EPIC Kitchens 100 **Download Features and Annotations** * Download *epic_kitchens.tar.gz* (`md5sum add9803756afd9a023bc9a9c547e0229`) from [this Box link](https://uwmadison.box.com/s/vdha47qnce6jhqktz9g4mq1gc40w82yj) or [this Google Drive Link](https://drive.google.com/file/d/1Z4U_dLuu6_cV5NBIrSzsSDOOj2Uar85X/view?,Google Drive,SOFTWARE
usp=sharing) or [this BaiduYun Link](https://pan.baidu.com/s/15tOdX6Yp4AJ9lFGjbQ8dgg?,BaiduYun,SOFTWARE
"‚îÇ ‚îî‚îÄ‚îÄ‚îÄdata/ ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄepic_kitchens/ ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄannotations ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄfeatures    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ... | ‚îî‚îÄ‚îÄ‚îÄlibs ‚îÇ ‚îÇ   ... ```  **Training and Evaluation** * On EPIC Kitchens, we train separate models for nouns and verbs. * To train our ActionFormer on verbs with SlowFast features, use ```shell python .",ActionFormer,SOFTWARE
"‚îÇ ‚îî‚îÄ‚îÄ‚îÄdata/ ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄepic_kitchens/ ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄannotations ‚îÇ    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄfeatures    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ... | ‚îî‚îÄ‚îÄ‚îÄlibs ‚îÇ ‚îÇ   ... ```  **Training and Evaluation** * On EPIC Kitchens, we train separate models for nouns and verbs. * To train our ActionFormer on verbs with SlowFast features, use ```shell python .",python,SOFTWARE
"/configs/epic_slowfast_verb.yaml --output reproduce ``` * To train our ActionFormer on nouns with SlowFast features, use ```shell python .",python,SOFTWARE
```shell python .,python,SOFTWARE
```shell python .,python,SOFTWARE
The model with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/file/d/1Ta4ggKSj2YcszSrDbePlHe1ECF1CFKK4/view?,Google Drive,SOFTWARE
"usp=sharing) (verb), and from this [Google Drive link](https://drive.google.com/file/d/1OTlxeiWj8JE9n1-LsRYogHmqgUdsE5PR/view?",Google Drive,SOFTWARE
```shell python .,python,SOFTWARE
```shell python .,python,SOFTWARE
/pretrained/epic_slowfast_noun_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method              |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |---------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (verb) | 26.58 | 25.42 | 24.15 | 22.29 | 19.09 | 23.51 | | ActionFormer (noun) | 25.21 | 24.11 | 22.66 | 20.47 | 16.97 | 21.88 |  ## To Reproduce Our Results on Ego4D Moment Queries Benchmark **Download Features and Annotations** * Download the official SlowFast and Omnivore features from [the Ego4D website](https://ego4d-data.org/#download) and the official EgoVLP features from [this link](https://github.com/showlab/EgoVLP/issues/1#issuecomment-1219076014).,ActionFormer,SOFTWARE
‚îÇ ‚îî‚îÄ‚îÄ‚îÄdata/ ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄego4d/ ‚îÇ    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄannotations ‚îÇ    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄslowfast_features ‚îÇ    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄomnivore_features ‚îÇ    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄegovlp_features   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ... | ‚îî‚îÄ‚îÄ‚îÄlibs ‚îÇ ‚îÇ   ... ```  **Training and Evaluation** * We provide config files for training ActionFormer with different feature combinations.,ActionFormer,SOFTWARE
```shell python .,python,SOFTWARE
/configs/ego4d_omnivore_egovlp.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.,TensorBoard,SOFTWARE
/configs/ego4d_omnivore_egovlp.yaml --output reproduce ``` * [Optional] Monitor the training using TensorBoard ```shell tensorboard --logdir=.,tensorboard,SOFTWARE
```shell python .,python,SOFTWARE
The models with all training logs can be downloaded from [this Google Drive link](https://drive.google.com/drive/folders/1NpAECS0ZhcCuehXkF9OhLQDPFrNdStJb?,Google Drive,SOFTWARE
```shell python .,python,SOFTWARE
/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,ActionFormer,SOFTWARE
/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,ActionFormer,SOFTWARE
/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,ActionFormer,SOFTWARE
/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,ActionFormer,SOFTWARE
/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,ActionFormer,SOFTWARE
/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,ActionFormer,SOFTWARE
/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,ActionFormer,SOFTWARE
/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,ActionFormer,SOFTWARE
/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,ActionFormer,SOFTWARE
/pretrained/ego4d_omnivore_egovlp_reproduce/ ``` * The results (mAP at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 20.09 | 17.45 | 14.44 | 12.46 | 10.00 | 14.89 | | ActionFormer (O)      | 23.87 | 20.78 | 18.39 | 15.33 | 12.65 | 18.20 | | ActionFormer (E)      | 26.84 | 23.86 | 20.57 | 17.19 | 14.54 | 20.60 | | ActionFormer (S+E)    | 27.98 | 24.46 | 21.21 | 18.56 | 15.60 | 21.56 | | ActionFormer (O+E)    | 27.99 | 24.94 | 21.94 | 19.05 | 15.98 | 21.98 | | ActionFormer (S+O+E)  | 28.26 | 24.69 | 21.88 | 19.35 | 16.28 | 22.09 |  * The results (Recall@1x at tIoUs) should be  | Method                |  0.1  |  0.2  |  0.3  |  0.4  |  0.5  |  Avg  | |-----------------------|-------|-------|-------|-------|-------|-------| | ActionFormer (S)      | 52.25 | 45.84 | 40.60 | 36.58 | 31.33 | 41.32 | | ActionFormer (O)      | 54.63 | 48.72 | 43.03 | 37.76 | 33.57 | 43.54 | | ActionFormer (E)      | 59.53 | 54.39 | 48.97 | 42.75 | 37.12 | 48.55 | | ActionFormer (S+E)    | 59.96 | 53.75 | 48.76 | 44.00 | 38.96 | 49.09 | | ActionFormer (O+E)    | 61.03 | 54.15 | 49.79 | 45.17 | 39.88 | 49.99 | | ActionFormer (S+O+E)  | 60.85 | 54.16 | 49.60 | 45.12 | 39.87 | 49.92 |  ## Training and Evaluating Your Own Dataset Work in progress.,ActionFormer,SOFTWARE
"Please open an issue if you would like to contribute.  ### Citation  ``` @article{gilbert2022reward,   title={Reward reports for reinforcement learning},   author={Gilbert, Thomas Krendl and Dean, Sarah and Lambert, Nathan and Zick, Tom and Snoswell, Aaron},   journal={arXiv preprint arXiv:2204.10817},   year={2022} } ```  ### Running  To run the web tool, run the following command: ``` cd builder npm start ```  Then visit [localhost:8080/](http://localhost:8080/) in the browser.",npm,SOFTWARE
See the DartMinHash paper https://arxiv.org/abs/2005.11547 for a description of the algorithm and further results of experiments.  ## Requirements The BagMinHash algorithm uses XXHash64 which must be installed:  1.,XXHash64,SOFTWARE
"Get xxhash from https://github.com/Cyan4973/xxHash, e.g. using `git clone https://github.com/Cyan4973/xxHash.git` 2.",xxhash,SOFTWARE
"Get xxhash from https://github.com/Cyan4973/xxHash, e.g. using `git clone https://github.com/Cyan4973/xxHash.git` 2.",xxHash,SOFTWARE
"Get xxhash from https://github.com/Cyan4973/xxHash, e.g. using `git clone https://github.com/Cyan4973/xxHash.git` 2.",git,SOFTWARE
"Get xxhash from https://github.com/Cyan4973/xxHash, e.g. using `git clone https://github.com/Cyan4973/xxHash.git` 2.",xxHash,SOFTWARE
Place xxhash.h and libxxhash.a into the directory /bagminhash/xxhash  The code compiles under GCC version 7.5.0 https://gcc.gnu.org/ with relevant commands in the makefile https://www.gnu.org/software/make/.  ## Commands  `make run` compiles and executes the main function in main.cpp.,GCC version 7.5.0,SOFTWARE
BagMinHash2 is essentially always faster and is what we compare against. * DartMinHash: Optimized implementation following the pseudocode in the paper.  ### Performance timings  | id | L0   | log2_L1 | t    | ICWS    | FastICWS | ICWS_xxhash | BagMinHash1 | BagMinHash2 | DartMinHash | |----|------|---------|------|---------|----------|-------------|-------------|-------------|-------------| | 0  | 64   | 0.000   | 64   | 0.899   | 0.060    | 0.538       | 2.439       | 0.628       | 0.042       | | 1  | 1024 | 0.000   | 64   | 11.565  | 0.515    | 9.604       | 4.374       | 1.706       | 0.145       | | 2  | 64   | 0.000   | 1024 | 19.296  | 2.885    | 8.083       | 48.248      | 13.279      | 0.592       | | 3  | 1024 | 0.000   | 1024 | 187.661 | 12.643   | 120.135     | 79.775      | 16.586      | 0.824       | | 4  | 256  | 0.000   | 1    | 0.040   | 0.008    | 0.040       | 0.112       | 0.103       | 0.021       | | 5  | 256  | 0.000   | 256  | 14.645  | 0.939    | 7.716       | 13.687      | 3.270       | 0.187       | | 6  | 1024 | 0.000   | 256  | 45.239  | 2.703    | 30.127      | 18.175      | 4.296       | 0.274       | | 7  | 1024 | 64.000  | 256  | 46.717  | 2.720    | 30.122      | 18.241      | 4.250       | 2.632       | | 8  | 1024 | -64.000 | 256  | 47.677  | 2.719    | 30.117      | 18.096      | 4.192       | 2.333       |  ### Jaccard similarity estimates  | sim_j | t  | ICWS_xxhash | FastICWS | BagMinHash2 | DartMinHash | |-------|----|-------------|----------|-------------|-------------| | 0.500 | 1  | 1.000       | 1.000    | 0.000       | 1.000       | | 0.500 | 2  | 0.500       | 0.500    | 0.000       | 0.500       | | 0.500 | 3  | 0.333       | 0.333    | 0.000       | 0.333       | | 0.500 | 4  | 0.500       | 0.250    | 0.750       | 0.750       | | 0.500 | 5  | 0.000       | 0.400    | 0.600       | 0.200       | | 0.500 | 6  | 0.667       | 0.500    | 0.500       | 0.000       | | 0.500 | 7  | 0.571       | 0.714    | 0.429       | 0.429       | | 0.500 | 8  | 0.250       | 0.375    | 0.625       | 0.500       | | 0.500 | 9  | 0.889       | 0.222    | 0.556       | 0.444       | | 0.500 | 10 | 0.600       | 0.400    | 0.700       | 0.400       |  ## Tests We use Catch2 https://github.com/catchorg/Catch2 for unit testing.,Catch2,SOFTWARE
BagMinHash2 is essentially always faster and is what we compare against. * DartMinHash: Optimized implementation following the pseudocode in the paper.  ### Performance timings  | id | L0   | log2_L1 | t    | ICWS    | FastICWS | ICWS_xxhash | BagMinHash1 | BagMinHash2 | DartMinHash | |----|------|---------|------|---------|----------|-------------|-------------|-------------|-------------| | 0  | 64   | 0.000   | 64   | 0.899   | 0.060    | 0.538       | 2.439       | 0.628       | 0.042       | | 1  | 1024 | 0.000   | 64   | 11.565  | 0.515    | 9.604       | 4.374       | 1.706       | 0.145       | | 2  | 64   | 0.000   | 1024 | 19.296  | 2.885    | 8.083       | 48.248      | 13.279      | 0.592       | | 3  | 1024 | 0.000   | 1024 | 187.661 | 12.643   | 120.135     | 79.775      | 16.586      | 0.824       | | 4  | 256  | 0.000   | 1    | 0.040   | 0.008    | 0.040       | 0.112       | 0.103       | 0.021       | | 5  | 256  | 0.000   | 256  | 14.645  | 0.939    | 7.716       | 13.687      | 3.270       | 0.187       | | 6  | 1024 | 0.000   | 256  | 45.239  | 2.703    | 30.127      | 18.175      | 4.296       | 0.274       | | 7  | 1024 | 64.000  | 256  | 46.717  | 2.720    | 30.122      | 18.241      | 4.250       | 2.632       | | 8  | 1024 | -64.000 | 256  | 47.677  | 2.719    | 30.117      | 18.096      | 4.192       | 2.333       |  ### Jaccard similarity estimates  | sim_j | t  | ICWS_xxhash | FastICWS | BagMinHash2 | DartMinHash | |-------|----|-------------|----------|-------------|-------------| | 0.500 | 1  | 1.000       | 1.000    | 0.000       | 1.000       | | 0.500 | 2  | 0.500       | 0.500    | 0.000       | 0.500       | | 0.500 | 3  | 0.333       | 0.333    | 0.000       | 0.333       | | 0.500 | 4  | 0.500       | 0.250    | 0.750       | 0.750       | | 0.500 | 5  | 0.000       | 0.400    | 0.600       | 0.200       | | 0.500 | 6  | 0.667       | 0.500    | 0.500       | 0.000       | | 0.500 | 7  | 0.571       | 0.714    | 0.429       | 0.429       | | 0.500 | 8  | 0.250       | 0.375    | 0.625       | 0.500       | | 0.500 | 9  | 0.889       | 0.222    | 0.556       | 0.444       | | 0.500 | 10 | 0.600       | 0.400    | 0.700       | 0.400       |  ## Tests We use Catch2 https://github.com/catchorg/Catch2 for unit testing.,Catch2,SOFTWARE
"<p align=""center"">   <picture>     <source media=""(prefers-color-scheme: dark)"" srcset=""https://raw.githubusercontent.com/HPAI-BSC/prompt_engine/main/images/prompt_engine_logo.png"">     <img alt=""prompt_engine"" src=""https://raw.githubusercontent.com/HPAI-BSC/prompt_engine/main/images/prompt_engine_logo.png"" width=55%>   </picture> </p> <h2 align=""center""> prompt_engine: Evaluate your model using advanced prompt strategies </h2>  <p align=""center""> | <a href=""https://arxiv.org/abs/2409.15127""><b>Paper</b></a> | <a href=""https://huggingface.co/collections/HPAI-BSC/healthcare-llms-aloe-family-6701b6a777f7e874a2123363""><b>Aloe Family Models</b></a> | <a href=""https://hpai.bsc.es/""><b>HPAI Website</b></a> | </p>  *Latest News* üî•  - [2024/09] [**Aloe-Beta**](https://huggingface.co/collections/HPAI-BSC/healthcare-llms-aloe-family-6701b6a777f7e874a2123363) is out!",Aloe-Beta,SOFTWARE
- [2024/04] [**Aloe-Alpha-8B**](https://huggingface.co/HPAI-BSC/Llama3-Aloe-8B-Alpha) is now available in Hugginface!,Aloe-Alpha-8B,SOFTWARE
- [2024/04] [**Aloe-Alpha-8B**](https://huggingface.co/HPAI-BSC/Llama3-Aloe-8B-Alpha) is now available in Hugginface!,Llama3-Aloe-8B-Alpha,SOFTWARE
This repo was first created to support the [Aloe](https://huggingface.co/HPAI-BSC/Llama3-Aloe-8B-Alpha) model.,Llama3-Aloe-8B-Alpha,SOFTWARE
"We integrate attribute scores from ArmoRM-Llama3-8B, a reward model along with the critique model‚Äôs reflection as an external feedback to guide answer generation.   ## Implementation  To effectively implement these novel techniques, we developed a specialized framework centered around accelerated inference speed and efficient data storage & retrieval mechanisms.",ArmoRM-Llama3-8B,SOFTWARE
"Specifically, the architecture employs: - [**VLLM**](https://github.com/vllm-project/vllm): Fast Inference Very Large Language Model ibrary to facilitate rapid generation of responses in a efficient way. - Vector database:  Vector database solution to facilitate the storage and computation of vector similarities required for setting up the Medprompt technique.",VLLM,SOFTWARE
"Specifically, the architecture employs: - [**VLLM**](https://github.com/vllm-project/vllm): Fast Inference Very Large Language Model ibrary to facilitate rapid generation of responses in a efficient way. - Vector database:  Vector database solution to facilitate the storage and computation of vector similarities required for setting up the Medprompt technique.",vllm,SOFTWARE
We integrated two different vector database solutions:     - [**ChromaDB**](https://github.com/chroma-core/chroma):.,ChromaDB,SOFTWARE
- [**Qdrant**](https://qdrant.tech/): Embedding database focused on production-ready service with a convenient API.,Qdrant,SOFTWARE
- [**Qdrant**](https://qdrant.tech/): Embedding database focused on production-ready service with a convenient API.,qdrant,SOFTWARE
Users can easily configure a wide array of parameters according to their unique experimental designs or preferences by modifying simple YAML configuration files found within the designated config directory.   ## Usage guide  ### Installation A requirements file with the necessary packages is provided to install and execute this repo.  ``` pip install -r requirements.txt ```  To execute the test.,pip,SOFTWARE
# reachml  [!,reachml,SOFTWARE
[CI](https://github.com/ustunb/reachml/actions/workflows/ci.yml/badge.svg?,reachml,SOFTWARE
branch=main)](https://github.com/ustunb/reachml/actions/workflows/ci.yml)  `reachml` is a library for recourse verification.  ## Background  *Recourse* is the ability of a decision subject to change the prediction of a machine learning model through actions on their features.,reachml,SOFTWARE
branch=main)](https://github.com/ustunb/reachml/actions/workflows/ci.yml)  `reachml` is a library for recourse verification.  ## Background  *Recourse* is the ability of a decision subject to change the prediction of a machine learning model through actions on their features.,reachml,SOFTWARE
"*Recourse verification* aims to tell if a decision subject is assigned a prediction that is fixed.  ## Installation  You can install the library as follows: ``` pip install ""git+https://github.com/ustunb/reachml#egg=reachml[cplex]"" ```  Many of the functions in `reach-ml` will require [CPLEX](https://www.ibm.com/products/ilog-cplex-optimization-studio) to run properly.",pip,SOFTWARE
"*Recourse verification* aims to tell if a decision subject is assigned a prediction that is fixed.  ## Installation  You can install the library as follows: ``` pip install ""git+https://github.com/ustunb/reachml#egg=reachml[cplex]"" ```  Many of the functions in `reach-ml` will require [CPLEX](https://www.ibm.com/products/ilog-cplex-optimization-studio) to run properly.",reachml,SOFTWARE
"*Recourse verification* aims to tell if a decision subject is assigned a prediction that is fixed.  ## Installation  You can install the library as follows: ``` pip install ""git+https://github.com/ustunb/reachml#egg=reachml[cplex]"" ```  Many of the functions in `reach-ml` will require [CPLEX](https://www.ibm.com/products/ilog-cplex-optimization-studio) to run properly.",reachml,SOFTWARE
"*Recourse verification* aims to tell if a decision subject is assigned a prediction that is fixed.  ## Installation  You can install the library as follows: ``` pip install ""git+https://github.com/ustunb/reachml#egg=reachml[cplex]"" ```  Many of the functions in `reach-ml` will require [CPLEX](https://www.ibm.com/products/ilog-cplex-optimization-studio) to run properly.",cplex,SOFTWARE
"*Recourse verification* aims to tell if a decision subject is assigned a prediction that is fixed.  ## Installation  You can install the library as follows: ``` pip install ""git+https://github.com/ustunb/reachml#egg=reachml[cplex]"" ```  Many of the functions in `reach-ml` will require [CPLEX](https://www.ibm.com/products/ilog-cplex-optimization-studio) to run properly.",reach-ml,SOFTWARE
"*Recourse verification* aims to tell if a decision subject is assigned a prediction that is fixed.  ## Installation  You can install the library as follows: ``` pip install ""git+https://github.com/ustunb/reachml#egg=reachml[cplex]"" ```  Many of the functions in `reach-ml` will require [CPLEX](https://www.ibm.com/products/ilog-cplex-optimization-studio) to run properly.",CPLEX,SOFTWARE
The command above will install CPLEX Community Edition.,CPLEX Community Edition,SOFTWARE
"To avoid these, you will want install reachml without the cplex option, and download and install the full version of IBM CPLEX [following these instructions](https://github.com/ustunb/risk-slim/blob/master/docs/cplex_instructions.md).  ## Quickstart  The following example shows how to specify actionability constraints using `ActionSet` and to build a database of `ReachableSet` for each point.",reachml,SOFTWARE
"To avoid these, you will want install reachml without the cplex option, and download and install the full version of IBM CPLEX [following these instructions](https://github.com/ustunb/risk-slim/blob/master/docs/cplex_instructions.md).  ## Quickstart  The following example shows how to specify actionability constraints using `ActionSet` and to build a database of `ReachableSet` for each point.",cplex,SOFTWARE
"To avoid these, you will want install reachml without the cplex option, and download and install the full version of IBM CPLEX [following these instructions](https://github.com/ustunb/risk-slim/blob/master/docs/cplex_instructions.md).  ## Quickstart  The following example shows how to specify actionability constraints using `ActionSet` and to build a database of `ReachableSet` for each point.",CPLEX,SOFTWARE
"```python import pandas as pd from reachml import ActionSet, ReachableSet, ReachableDatabase from reachml.constraints import OneHotEncoding, DirectionalLinkage  # feature matrix with 3 points X = pd.DataFrame(     {         ""age"": [32, 19, 52],         ""marital_status"": [1, 0, 0],         ""years_since_last_default"": [5, 0, 21],         ""job_type_a"": [0, 1, 1], # categorical feature with one-hot encoding         ""job_type_b"": [1, 0, 0],         ""job_type_c"": [0, 0, 0],     } )  # Create an action set action_set = ActionSet(X)  # `ActionSet` infers the type and bounds on each feature from `X`.",pandas,SOFTWARE
"```python import pandas as pd from reachml import ActionSet, ReachableSet, ReachableDatabase from reachml.constraints import OneHotEncoding, DirectionalLinkage  # feature matrix with 3 points X = pd.DataFrame(     {         ""age"": [32, 19, 52],         ""marital_status"": [1, 0, 0],         ""years_since_last_default"": [5, 0, 21],         ""job_type_a"": [0, 1, 1], # categorical feature with one-hot encoding         ""job_type_b"": [1, 0, 0],         ""job_type_c"": [0, 0, 0],     } )  # Create an action set action_set = ActionSet(X)  # `ActionSet` infers the type and bounds on each feature from `X`.",reachml,SOFTWARE
"```python import pandas as pd from reachml import ActionSet, ReachableSet, ReachableDatabase from reachml.constraints import OneHotEncoding, DirectionalLinkage  # feature matrix with 3 points X = pd.DataFrame(     {         ""age"": [32, 19, 52],         ""marital_status"": [1, 0, 0],         ""years_since_last_default"": [5, 0, 21],         ""job_type_a"": [0, 1, 1], # categorical feature with one-hot encoding         ""job_type_b"": [1, 0, 0],         ""job_type_c"": [0, 0, 0],     } )  # Create an action set action_set = ActionSet(X)  # `ActionSet` infers the type and bounds on each feature from `X`.",reachml,SOFTWARE
"/reachable_db.h5` db.generate(data, overwrite=True)  # Pull reachable set for first point in dataset x = data.iloc[0] reachable_set = db[x] print(reachable_set) # should return the following output: ##    age  marital_status  years_since_last_default  job_type_a  job_type_b  job_type_c ## 0  32.0             1.0                       5.0         0.0         1.0         0.0 ## 1  32.0             1.0                       5.0         0.0         0.0         1.0 ## 2  32.0             1.0                       5.0         1.0         0.0         0.0 ## 3  33.0             1.0                       6.0         0.0         0.0         1.0 ## 4  33.0             1.0                       6.0         0.0         1.0         0.0 ## 5  33.0             1.0                       6.0         1.0         0.0         0.0 ``` Given a classifier `clf` with a predict method, you can test if a point has recourse as `np.any(clf.predict(reachable_set.X))`  For more examples, check out [this script](https://github.com/ustunb/reachml/blob/main/research/iclr2024/scripts/setup_dataset_actionset_fico.py) which sets up the action set for the FICO dataset.  ### Resources and Citation  For more about recourse verification, check out our paper ICLR 2024 spotlight paper: [Prediction without Preclusion](https://openreview.net/forum?",reachml,SOFTWARE
id=SCQfYpdoGE} } ```  The code for the paper is available under [research/iclr2024](https://github.com/ustunb/reachml/tree/main/research/iclr2024/).,reachml,SOFTWARE
"SLURM commands used for model training take this form: ```bash sbatch --job-name=frac_100u --nodes=1 --time=24:00:00 -p gpua100 --gres=gpu:1 --mem-per-cpu=32G --cpus-per-task=8     --output=frac_100u.out     --error=frac_100u.err     --wrap=""python model_training/finetune.py       --train_data_path data/fractial_code/fractial_code_100_train.jsonl      --output_dir models/fractial_code_100       --micro_batch_size 32       --num_epochs 2       --cutoff_len 512       --val_data_path data/fractial_code/fractial_code_1000_validation.jsonl       --test_data_path data/fractial_code/fractial_code_only_test.jsonl"" ``` and are partially available in the `data/fractial_mixes` folder.  ### Evaluation  Evaluation contains all code (called by scripts in `scripts/result_aggregation`) to compute prediction scores for all scorers.",SLURM,SOFTWARE
"SLURM commands used for model training take this form: ```bash sbatch --job-name=frac_100u --nodes=1 --time=24:00:00 -p gpua100 --gres=gpu:1 --mem-per-cpu=32G --cpus-per-task=8     --output=frac_100u.out     --error=frac_100u.err     --wrap=""python model_training/finetune.py       --train_data_path data/fractial_code/fractial_code_100_train.jsonl      --output_dir models/fractial_code_100       --micro_batch_size 32       --num_epochs 2       --cutoff_len 512       --val_data_path data/fractial_code/fractial_code_1000_validation.jsonl       --test_data_path data/fractial_code/fractial_code_only_test.jsonl"" ``` and are partially available in the `data/fractial_mixes` folder.  ### Evaluation  Evaluation contains all code (called by scripts in `scripts/result_aggregation`) to compute prediction scores for all scorers.",python,SOFTWARE
"Scorers include: - Any scorer available through HuggingFace Evaluate - BertScore - SentenceBert - A Reward model trained by OpenAssistant - API-access LLMs like GPT4 or GPT3.5 - Support for Custom heuristics on custom datasets  ### Contact  As this is a repository intended to support reproducing the work in the EMNLP paper, it is not going to be further developped moving onwards.",OpenAssistant,SOFTWARE
"Scorers include: - Any scorer available through HuggingFace Evaluate - BertScore - SentenceBert - A Reward model trained by OpenAssistant - API-access LLMs like GPT4 or GPT3.5 - Support for Custom heuristics on custom datasets  ### Contact  As this is a repository intended to support reproducing the work in the EMNLP paper, it is not going to be further developped moving onwards.",GPT4,SOFTWARE
"Scorers include: - Any scorer available through HuggingFace Evaluate - BertScore - SentenceBert - A Reward model trained by OpenAssistant - API-access LLMs like GPT4 or GPT3.5 - Support for Custom heuristics on custom datasets  ### Contact  As this is a repository intended to support reproducing the work in the EMNLP paper, it is not going to be further developped moving onwards.",GPT3.5,SOFTWARE
"It is in the process of being open-sourced, in the meantime, simply replacing it's use with classic API calls with the  OpenAI library yields the same results (lm-evaluator.py).",OpenAI,SOFTWARE
"The paper is available at https://arxiv.org/abs/2303.17646  # Instructions for running Phase1-Cosearch  ## Running the Co-search Run the Code using  ``` python Phase1_VGG16_backbone.py with the variables described below. ``` ## Variable Description   ``` --lr: Learning Rate --hw_params: Tuple of #PEs per Tile, #Crossbars per PE, #Tiles, Crossbar Size --target_latency: Target Latency --target_area: Target Area Constraint (mm^2) --epochs: Total Number of Search Epochs --wt_prec: Weight Precision --cellbit: Number of bits per NVM device --area_tolerance: The uncerainity in the on-chip area tolerated in the searched model.  ``` # Instructions for XPertSim C++ Evaluation  ## Step 1: Create the Network_custom.csv file  The Network_custom.csv file contains the DNN information required for evaluation.",python,SOFTWARE
"## Installation  The code has been tested with Python 3.8, CUDA 11.3, pytorch 1.10.1 and pytorch-lighting 1.4.8.",CUDA 11.3,SOFTWARE
"## Installation  The code has been tested with Python 3.8, CUDA 11.3, pytorch 1.10.1 and pytorch-lighting 1.4.8.",pytorch 1.10.1,SOFTWARE
"## Installation  The code has been tested with Python 3.8, CUDA 11.3, pytorch 1.10.1 and pytorch-lighting 1.4.8.",pytorch-lighting 1.4.8,SOFTWARE
"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",Conda,SOFTWARE
"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",Pytorch 1.10.1,SOFTWARE
"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",pytorch,SOFTWARE
"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",Minkowski Engine,SOFTWARE
"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",MinkowskiEngine,SOFTWARE
"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",Pytorch-Lighting 1.4.8,SOFTWARE
"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",pytorchlightning,SOFTWARE
"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",torchmetrics=0.7.2,SOFTWARE
"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",Scipy 1.7.3,SOFTWARE
"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",scipy,SOFTWARE
"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",Wandb,SOFTWARE
"Any other version may require to update the code for compatibility.  ### Conda To run the code, you need to install: - [Pytorch 1.10.1](https://pytorch.org/get-started/previous-versions/) - [Minkowski Engine](https://github.com/NVIDIA/MinkowskiEngine) - [Pytorch-Lighting 1.4.8](https://www.pytorchlightning.ai) (be sure to install torchmetrics=0.7.2) - [Scipy 1.7.3](https://scipy.org/install/) - [Wandb](https://docs.wandb.ai/quickstart)  ## Data preparation To download the data follow the instructions provided by [SemanticKITTI](http://www.semantic-kitti.org) and [SemanticPOSS](http://www.poss.pku.edu.cn/semanticposs.html).",wandb,SOFTWARE
"‚îÇ   ‚îî‚îÄ‚îÄ labels/              |          ‚îú‚îÄ‚îÄ 000000.label             |          ‚îú‚îÄ‚îÄ 000001.label             |          ‚îî‚îÄ‚îÄ ...             ‚îî‚îÄ‚îÄ ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union‚Äôs Horizon Europe research and innovation programme under grant agreement No 101058589.",python,SOFTWARE
"‚îÇ   ‚îî‚îÄ‚îÄ labels/              |          ‚îú‚îÄ‚îÄ 000000.label             |          ‚îú‚îÄ‚îÄ 000001.label             |          ‚îî‚îÄ‚îÄ ...             ‚îî‚îÄ‚îÄ ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union‚Äôs Horizon Europe research and innovation programme under grant agreement No 101058589.",python,SOFTWARE
"‚îÇ   ‚îî‚îÄ‚îÄ labels/              |          ‚îú‚îÄ‚îÄ 000000.label             |          ‚îú‚îÄ‚îÄ 000001.label             |          ‚îî‚îÄ‚îÄ ...             ‚îî‚îÄ‚îÄ ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union‚Äôs Horizon Europe research and innovation programme under grant agreement No 101058589.",python,SOFTWARE
"‚îÇ   ‚îî‚îÄ‚îÄ labels/              |          ‚îú‚îÄ‚îÄ 000000.label             |          ‚îú‚îÄ‚îÄ 000001.label             |          ‚îî‚îÄ‚îÄ ...             ‚îî‚îÄ‚îÄ ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union‚Äôs Horizon Europe research and innovation programme under grant agreement No 101058589.",python,SOFTWARE
"‚îÇ   ‚îî‚îÄ‚îÄ labels/              |          ‚îú‚îÄ‚îÄ 000000.label             |          ‚îú‚îÄ‚îÄ 000001.label             |          ‚îî‚îÄ‚îÄ ...             ‚îî‚îÄ‚îÄ ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union‚Äôs Horizon Europe research and innovation programme under grant agreement No 101058589.",python,SOFTWARE
"‚îÇ   ‚îî‚îÄ‚îÄ labels/              |          ‚îú‚îÄ‚îÄ 000000.label             |          ‚îú‚îÄ‚îÄ 000001.label             |          ‚îî‚îÄ‚îÄ ...             ‚îî‚îÄ‚îÄ ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union‚Äôs Horizon Europe research and innovation programme under grant agreement No 101058589.",python,SOFTWARE
"‚îÇ   ‚îî‚îÄ‚îÄ labels/              |          ‚îú‚îÄ‚îÄ 000000.label             |          ‚îú‚îÄ‚îÄ 000001.label             |          ‚îî‚îÄ‚îÄ ...             ‚îî‚îÄ‚îÄ ... ```  ## Commands ### Pretraining To run the pretraining: ``` python main_pretrain.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_pretrain.py -h ```  ### Discovery To run the discovery step (pretraining is not mandatory): ``` python main_discover.py -s [SPLIT NUMBER] --dataset [SemanticPOSS, SemanticKITTI] ``` For additional command line arguments, run: ``` python main_discover.py -h ``` To reproduce the paper results run: ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticPOSS --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.3 ``` ``` python main_discover.py -s [SPLIT NUMBER] --dataset SemanticKITTI --dataset_config [CONFIG_PATH] --num_heads=5 --overcluster_factor=3 --use_scheduler --adapting_epsilon_sk --use_uncertainty_queue --use_uncertainty_loss --uncertainty_percentile=0.5 ```  To test with a checkpoint, run: ``` python main_discover_test.py -s [SPLIT NUMBER] --checkpoint=[PATH TO LIGHTNING CHECKPOINT] ```  ## Citing our work  Please cite the following paper if you use our code:  ```latex @inproceedings{riz2023novel,   author={Riz, Luigi and Saltori, Cristiano and Ricci, Elisa and Poiesi, Fabio},   booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},    title={Novel Class Discovery for 3D Point Cloud Semantic Segmentation},    year={2023},   volume={},   number={},   pages={9393-9402},   doi={10.1109/CVPR52729.2023.00906}} ```  ## Acknowledgements  This project has received funding from the European Union‚Äôs Horizon Europe research and innovation programme under grant agreement No 101058589.",python,SOFTWARE
raw=true)  ## Code The code has been written in Python using the Pytorch framework.,Pytorch,SOFTWARE
"We provide a Jupyter Notebook, which can be run in Google Colab, containing the algorithm in a usable version.",Jupyter Notebook,SOFTWARE
"We provide a Jupyter Notebook, which can be run in Google Colab, containing the algorithm in a usable version.",Google Colab,SOFTWARE
[`CMake`](https://cmake.org/) is required to build the code.,CMake,SOFTWARE
[`CMake`](https://cmake.org/) is required to build the code.,cmake,SOFTWARE
"The code has few external dependencies (for benchmarking and serialization), so clone the repository with   git clone --recursive https://github.com/jermp/rdf_indexes.git  If you have cloned the repository without `--recursive`, you will need to perform the following commands before compiling:      git submodule init     git submodule update  To compile the code for a release environment (see file `CMakeLists.txt` for the used compilation flags), it is sufficient to do the following:      mkdir build     cd build     cmake ..",git,SOFTWARE
"The code has few external dependencies (for benchmarking and serialization), so clone the repository with   git clone --recursive https://github.com/jermp/rdf_indexes.git  If you have cloned the repository without `--recursive`, you will need to perform the following commands before compiling:      git submodule init     git submodule update  To compile the code for a release environment (see file `CMakeLists.txt` for the used compilation flags), it is sufficient to do the following:      mkdir build     cd build     cmake ..",CMake,SOFTWARE
"The code has few external dependencies (for benchmarking and serialization), so clone the repository with   git clone --recursive https://github.com/jermp/rdf_indexes.git  If you have cloned the repository without `--recursive`, you will need to perform the following commands before compiling:      git submodule init     git submodule update  To compile the code for a release environment (see file `CMakeLists.txt` for the used compilation flags), it is sufficient to do the following:      mkdir build     cd build     cmake ..",cmake,SOFTWARE
"make -j  For a testing environment, use the following instead:      mkdir debug_build     cd debug_build     cmake ..",cmake,SOFTWARE
(This dataset has been downloaded from [http://www.rdfhdt.org/datasets](http://www.rdfhdt.org/datasets) and extracted using the HDT [2] software at [http://www.rdfhdt.org/downloads](http://www.rdfhdt.org/downloads).,HDT,SOFTWARE
**NOTE** - The scripts require the modules `mmh3` and `numpy` that can be easily installed with `pip3 install mmh3 numpy`.  1.,mmh3,SOFTWARE
**NOTE** - The scripts require the modules `mmh3` and `numpy` that can be easily installed with `pip3 install mmh3 numpy`.  1.,numpy,SOFTWARE
**NOTE** - The scripts require the modules `mmh3` and `numpy` that can be easily installed with `pip3 install mmh3 numpy`.  1.,pip3,SOFTWARE
**NOTE** - The scripts require the modules `mmh3` and `numpy` that can be easily installed with `pip3 install mmh3 numpy`.  1.,mmh3,SOFTWARE
**NOTE** - The scripts require the modules `mmh3` and `numpy` that can be easily installed with `pip3 install mmh3 numpy`.  1.,numpy,SOFTWARE
python3 extract_vocabs.py ..,python3,SOFTWARE
python3 map_dataset.py ..,python3,SOFTWARE
python3 sort.py ..,python3,SOFTWARE
python3 build_stats.py wordnet31.mapped.sorted   This script will create the file `wordnet31.mapped.sorted.stats`.,python3,SOFTWARE
python ..,python,SOFTWARE
"# BTM **Reviving Undersampling for Long-Tailed Learning**  **Authors**: Hao Yu, Yingxiao Du, Jianxin Wu  [[`arXiv`](https://arxiv.org/pdf/2401.16811.pdf)] [[`bibtex`](#Citation)]   **Introduction**: This repository provides an implementation for the paper: ""[Reviving Undersampling for Long-Tailed Learning](https://arxiv.org/pdf/2401.16811.pdf)"" based on [MiSLAS](https://github.com/dvlab-research/MiSLAS).",BTM,SOFTWARE
"We revive the balanced undersampling produces a more equitable distribution of accuracy across categories, and devise a straightforward model ensemble strategy, which does not result in any additional overhead and achieves improved harmonic and geometric mean while keeping the average accuracy.* BTM is a simple, and efficient framework for long-tailed recognition.  ## Installation  **Requirements**  * Python 3.8 * torchvision 0.13.0 * Pytorch 1.12.0  **Dataset Preparation** * [ImageNet-LT](http://image-net.org/index) * [iNaturalist 2018](https://github.com/visipedia/inat_comp/tree/master/2018) * [Places-LT](http://places2.csail.mit.edu/download.html)  Change the `data_path` in `config/*/*.yaml` accordingly.  ## Training  **Stage-1**:  To get a model of Stage-1, you can directly download from [MiSLAS](https://github.com/dvlab-research/MiSLAS), or run:  ``` python train_stage1.py --cfg .",BTM,SOFTWARE
"We revive the balanced undersampling produces a more equitable distribution of accuracy across categories, and devise a straightforward model ensemble strategy, which does not result in any additional overhead and achieves improved harmonic and geometric mean while keeping the average accuracy.* BTM is a simple, and efficient framework for long-tailed recognition.  ## Installation  **Requirements**  * Python 3.8 * torchvision 0.13.0 * Pytorch 1.12.0  **Dataset Preparation** * [ImageNet-LT](http://image-net.org/index) * [iNaturalist 2018](https://github.com/visipedia/inat_comp/tree/master/2018) * [Places-LT](http://places2.csail.mit.edu/download.html)  Change the `data_path` in `config/*/*.yaml` accordingly.  ## Training  **Stage-1**:  To get a model of Stage-1, you can directly download from [MiSLAS](https://github.com/dvlab-research/MiSLAS), or run:  ``` python train_stage1.py --cfg .",Python 3.8,SOFTWARE
"We revive the balanced undersampling produces a more equitable distribution of accuracy across categories, and devise a straightforward model ensemble strategy, which does not result in any additional overhead and achieves improved harmonic and geometric mean while keeping the average accuracy.* BTM is a simple, and efficient framework for long-tailed recognition.  ## Installation  **Requirements**  * Python 3.8 * torchvision 0.13.0 * Pytorch 1.12.0  **Dataset Preparation** * [ImageNet-LT](http://image-net.org/index) * [iNaturalist 2018](https://github.com/visipedia/inat_comp/tree/master/2018) * [Places-LT](http://places2.csail.mit.edu/download.html)  Change the `data_path` in `config/*/*.yaml` accordingly.  ## Training  **Stage-1**:  To get a model of Stage-1, you can directly download from [MiSLAS](https://github.com/dvlab-research/MiSLAS), or run:  ``` python train_stage1.py --cfg .",torchvision 0.13.0,SOFTWARE
"We revive the balanced undersampling produces a more equitable distribution of accuracy across categories, and devise a straightforward model ensemble strategy, which does not result in any additional overhead and achieves improved harmonic and geometric mean while keeping the average accuracy.* BTM is a simple, and efficient framework for long-tailed recognition.  ## Installation  **Requirements**  * Python 3.8 * torchvision 0.13.0 * Pytorch 1.12.0  **Dataset Preparation** * [ImageNet-LT](http://image-net.org/index) * [iNaturalist 2018](https://github.com/visipedia/inat_comp/tree/master/2018) * [Places-LT](http://places2.csail.mit.edu/download.html)  Change the `data_path` in `config/*/*.yaml` accordingly.  ## Training  **Stage-1**:  To get a model of Stage-1, you can directly download from [MiSLAS](https://github.com/dvlab-research/MiSLAS), or run:  ``` python train_stage1.py --cfg .",Pytorch 1.12.0,SOFTWARE
"We revive the balanced undersampling produces a more equitable distribution of accuracy across categories, and devise a straightforward model ensemble strategy, which does not result in any additional overhead and achieves improved harmonic and geometric mean while keeping the average accuracy.* BTM is a simple, and efficient framework for long-tailed recognition.  ## Installation  **Requirements**  * Python 3.8 * torchvision 0.13.0 * Pytorch 1.12.0  **Dataset Preparation** * [ImageNet-LT](http://image-net.org/index) * [iNaturalist 2018](https://github.com/visipedia/inat_comp/tree/master/2018) * [Places-LT](http://places2.csail.mit.edu/download.html)  Change the `data_path` in `config/*/*.yaml` accordingly.  ## Training  **Stage-1**:  To get a model of Stage-1, you can directly download from [MiSLAS](https://github.com/dvlab-research/MiSLAS), or run:  ``` python train_stage1.py --cfg .",python,SOFTWARE
"**BTM**:  To training a model with undersamping, run: ``` python train_stage1_bl_10_classifier.py --cfg .",BTM,SOFTWARE
"**BTM**:  To training a model with undersamping, run: ``` python train_stage1_bl_10_classifier.py --cfg .",python,SOFTWARE
Then run  ``` python merge.py ```  for getting the fusion model.,python,SOFTWARE
"**Stage-2**:  To train a model for Stage-2, run:  ``` python train_stage2.py --cfg .",python,SOFTWARE
"/config/DATASETNAME/DATASETNAME_ARCH_stage2_mislas.yaml resume /path/to/checkpoint/BTM ```  The saved folder (including logs and checkpoints) is organized as follows. ``` MiSLAS ‚îú‚îÄ‚îÄ saved ‚îÇ   ‚îú‚îÄ‚îÄ modelname_date ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ckps ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ current.pth.tar ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ model_best.pth.tar ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logs ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ modelname.txt ‚îÇ   ...    ``` ## Evaluation  To evaluate a trained model, run:  ``` python eval.py --cfg .",python,SOFTWARE
/config/DATASETNAME/DATASETNAME_ARCH_stage1_mixup.yaml  resume /path/to/checkpoint/stage1 python eval.py --cfg .,python,SOFTWARE
Team DETR: Guide Queries as a Professional Team in Detection Transformers ========  [!,Team DETR,SOFTWARE
"To alleviate this issue, we propose Team DETR, which leverages query collaboration and position constraints to embrace objects of interest more precisely.",Team DETR,SOFTWARE
"In addition, the proposed Team DETR is flexible enough to be adapted to other existing DETR variants without increasing parameters and calculations.",Team DETR,SOFTWARE
"In addition, the proposed Team DETR is flexible enough to be adapted to other existing DETR variants without increasing parameters and calculations.",DETR,SOFTWARE
"Extensive experiments on the COCO dataset show that Team DETR achieves remarkable gains, especially for small and large objects.  ## Framework  !",Team DETR,SOFTWARE
/figures/framework.png)  The proposed Team DETR is based on the basic architecture of DAB-DETR.,Team DETR,SOFTWARE
"Furthermore, the prediction preferences of each query are dynamically extracted, and the anchor is updated accordingly.  ## Model Zoo  Without increasing parameters and calculations, our query teamwork can be easily integrated into DAB-based DETRs, including DAB-DETR, DN-DETR and the single-stage DINO.",DAB-DETR,SOFTWARE
"Furthermore, the prediction preferences of each query are dynamically extracted, and the anchor is updated accordingly.  ## Model Zoo  Without increasing parameters and calculations, our query teamwork can be easily integrated into DAB-based DETRs, including DAB-DETR, DN-DETR and the single-stage DINO.",DN-DETR,SOFTWARE
"Furthermore, the prediction preferences of each query are dynamically extracted, and the anchor is updated accordingly.  ## Model Zoo  Without increasing parameters and calculations, our query teamwork can be easily integrated into DAB-based DETRs, including DAB-DETR, DN-DETR and the single-stage DINO.",single-stage DINO,SOFTWARE
"usp=share_link) |  **Note:** The result of DAB-DETR-R50 w/ Team-DETR under the 50-epoch setting is different from which we report in the paper because we lost this checkpoint, and here is the one we retrained.  ## Usage  ### Installation  Our code contains three projects, Team-DAB-DETR, Team-DN-DETR, and Team-DINO, based on DAB-DETR, DN-DETR, and DINO, respectively, and no extra dependency is needed.",Team-DETR,SOFTWARE
"usp=share_link) |  **Note:** The result of DAB-DETR-R50 w/ Team-DETR under the 50-epoch setting is different from which we report in the paper because we lost this checkpoint, and here is the one we retrained.  ## Usage  ### Installation  Our code contains three projects, Team-DAB-DETR, Team-DN-DETR, and Team-DINO, based on DAB-DETR, DN-DETR, and DINO, respectively, and no extra dependency is needed.",DAB-DETR,SOFTWARE
"usp=share_link) |  **Note:** The result of DAB-DETR-R50 w/ Team-DETR under the 50-epoch setting is different from which we report in the paper because we lost this checkpoint, and here is the one we retrained.  ## Usage  ### Installation  Our code contains three projects, Team-DAB-DETR, Team-DN-DETR, and Team-DINO, based on DAB-DETR, DN-DETR, and DINO, respectively, and no extra dependency is needed.",DN-DETR,SOFTWARE
"usp=share_link) |  **Note:** The result of DAB-DETR-R50 w/ Team-DETR under the 50-epoch setting is different from which we report in the paper because we lost this checkpoint, and here is the one we retrained.  ## Usage  ### Installation  Our code contains three projects, Team-DAB-DETR, Team-DN-DETR, and Team-DINO, based on DAB-DETR, DN-DETR, and DINO, respectively, and no extra dependency is needed.",DINO,SOFTWARE
[[Installation of DAB-DETR]](https://github.com/IDEA-Research/DAB-DETR)  [[Installation of DN-DETR]](https://github.com/IDEA-Research/DN-DETR)  [[Installation of DINO]](https://github.com/IDEA-Research/DINO)  Our experimental environment is `python 3.7 & pytorch 1.11.0+cu113`.,DAB-DETR,SOFTWARE
[[Installation of DAB-DETR]](https://github.com/IDEA-Research/DAB-DETR)  [[Installation of DN-DETR]](https://github.com/IDEA-Research/DN-DETR)  [[Installation of DINO]](https://github.com/IDEA-Research/DINO)  Our experimental environment is `python 3.7 & pytorch 1.11.0+cu113`.,DAB-DETR,SOFTWARE
[[Installation of DAB-DETR]](https://github.com/IDEA-Research/DAB-DETR)  [[Installation of DN-DETR]](https://github.com/IDEA-Research/DN-DETR)  [[Installation of DINO]](https://github.com/IDEA-Research/DINO)  Our experimental environment is `python 3.7 & pytorch 1.11.0+cu113`.,DN-DETR,SOFTWARE
[[Installation of DAB-DETR]](https://github.com/IDEA-Research/DAB-DETR)  [[Installation of DN-DETR]](https://github.com/IDEA-Research/DN-DETR)  [[Installation of DINO]](https://github.com/IDEA-Research/DINO)  Our experimental environment is `python 3.7 & pytorch 1.11.0+cu113`.,DN-DETR,SOFTWARE
[[Installation of DAB-DETR]](https://github.com/IDEA-Research/DAB-DETR)  [[Installation of DN-DETR]](https://github.com/IDEA-Research/DN-DETR)  [[Installation of DINO]](https://github.com/IDEA-Research/DINO)  Our experimental environment is `python 3.7 & pytorch 1.11.0+cu113`.,DINO,SOFTWARE
[[Installation of DAB-DETR]](https://github.com/IDEA-Research/DAB-DETR)  [[Installation of DN-DETR]](https://github.com/IDEA-Research/DN-DETR)  [[Installation of DINO]](https://github.com/IDEA-Research/DINO)  Our experimental environment is `python 3.7 & pytorch 1.11.0+cu113`.,DINO,SOFTWARE
[[Installation of DAB-DETR]](https://github.com/IDEA-Research/DAB-DETR)  [[Installation of DN-DETR]](https://github.com/IDEA-Research/DN-DETR)  [[Installation of DINO]](https://github.com/IDEA-Research/DINO)  Our experimental environment is `python 3.7 & pytorch 1.11.0+cu113`.,python 3.7,SOFTWARE
[[Installation of DAB-DETR]](https://github.com/IDEA-Research/DAB-DETR)  [[Installation of DN-DETR]](https://github.com/IDEA-Research/DN-DETR)  [[Installation of DINO]](https://github.com/IDEA-Research/DINO)  Our experimental environment is `python 3.7 & pytorch 1.11.0+cu113`.,pytorch 1.11.0+cu113,SOFTWARE
We strongly recommend you use `pytorch >= 1.11.0` for its less GPU memory consumption.  ### Dataset  [COCO2017](https://cocodataset.org/) is used to validate our method.,pytorch >= 1.11.0,SOFTWARE
pwd=team) / [Google Drive](https://drive.google.com/drive/folders/1vL7XnQ37W7wNF1flTm9r8vVjx_YbBZko?,Google Drive,SOFTWARE
"```bash # Team-DAB-DETR and Team-DN-DETR # multi-gpu python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --resume /path/to/your/checkpoint \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --matcher team \   --q_splits 65 20 15 \   --eval  # single-gpu python main.py \   --coco_path /path/to/your/COCODIR \   --resume /path/to/your/checkpoint \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --matcher team \   --q_splits 65 20 15 \   --eval  # --------------------------------------------  # Team-DINO # You need to write config and .sh files in advance. # multi-gpu bash scripts/DINO_4scale_1stage_team_r50_e12_eval.sh /path/to/your/COCODIR /path/to/your/output/dir /path/to/your/checkpoint ```  ### Training  In default, we divide the queries into three groups, with the proportions of 65%, 20%, and 15%, corresponding to the relative scales of (0, 0.2], (0.2, 0.4], and (0.4, 1], respectively.",python,SOFTWARE
"```bash # Team-DAB-DETR and Team-DN-DETR # multi-gpu python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --resume /path/to/your/checkpoint \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --matcher team \   --q_splits 65 20 15 \   --eval  # single-gpu python main.py \   --coco_path /path/to/your/COCODIR \   --resume /path/to/your/checkpoint \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --matcher team \   --q_splits 65 20 15 \   --eval  # --------------------------------------------  # Team-DINO # You need to write config and .sh files in advance. # multi-gpu bash scripts/DINO_4scale_1stage_team_r50_e12_eval.sh /path/to/your/COCODIR /path/to/your/output/dir /path/to/your/checkpoint ```  ### Training  In default, we divide the queries into three groups, with the proportions of 65%, 20%, and 15%, corresponding to the relative scales of (0, 0.2], (0.2, 0.4], and (0.4, 1], respectively.",python,SOFTWARE
"If you want to change the responsible scale range of each group, you can modify matcher.py for Team-DAB-DETR and Team-DN-DETR or the config file for Team-DINO.",Team-DINO,SOFTWARE
"```bash # Team-DAB-DETR and Team-DN-DETR # multi-gpu (12-epoch setting / 1x setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # multi-gpu (50-epoch setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # single-gpu (12-epoch setting / 1x setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # single-gpu (50-epoch setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # --------------------------------------------  # Team-DINO # You need to write config and .sh files in advance. # multi-gpu bash scripts/DINO_4scale_1stage_team_r50_e12.sh /path/to/your/COCODIR /path/to/your/output/dir ```  ## How to integrate query teamwork into your model  The query teamwork contains three parts: scale-wise grouping, position constraint, and preference extraction.",python,SOFTWARE
"```bash # Team-DAB-DETR and Team-DN-DETR # multi-gpu (12-epoch setting / 1x setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # multi-gpu (50-epoch setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # single-gpu (12-epoch setting / 1x setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # single-gpu (50-epoch setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # --------------------------------------------  # Team-DINO # You need to write config and .sh files in advance. # multi-gpu bash scripts/DINO_4scale_1stage_team_r50_e12.sh /path/to/your/COCODIR /path/to/your/output/dir ```  ## How to integrate query teamwork into your model  The query teamwork contains three parts: scale-wise grouping, position constraint, and preference extraction.",python,SOFTWARE
"```bash # Team-DAB-DETR and Team-DN-DETR # multi-gpu (12-epoch setting / 1x setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # multi-gpu (50-epoch setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # single-gpu (12-epoch setting / 1x setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # single-gpu (50-epoch setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # --------------------------------------------  # Team-DINO # You need to write config and .sh files in advance. # multi-gpu bash scripts/DINO_4scale_1stage_team_r50_e12.sh /path/to/your/COCODIR /path/to/your/output/dir ```  ## How to integrate query teamwork into your model  The query teamwork contains three parts: scale-wise grouping, position constraint, and preference extraction.",python,SOFTWARE
"```bash # Team-DAB-DETR and Team-DN-DETR # multi-gpu (12-epoch setting / 1x setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # multi-gpu (50-epoch setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # single-gpu (12-epoch setting / 1x setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # single-gpu (50-epoch setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # --------------------------------------------  # Team-DINO # You need to write config and .sh files in advance. # multi-gpu bash scripts/DINO_4scale_1stage_team_r50_e12.sh /path/to/your/COCODIR /path/to/your/output/dir ```  ## How to integrate query teamwork into your model  The query teamwork contains three parts: scale-wise grouping, position constraint, and preference extraction.",python,SOFTWARE
"```bash # Team-DAB-DETR and Team-DN-DETR # multi-gpu (12-epoch setting / 1x setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # multi-gpu (50-epoch setting) python -m torch.distributed.launch --nproc_per_node=2 main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # single-gpu (12-epoch setting / 1x setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 12 \   --lr_drop 8 \   --matcher team \   --q_splits 65 20 15  # single-gpu (50-epoch setting) python main.py \   --coco_path /path/to/your/COCODIR \   --output_dir /path/to/your/output/dir \   --batch_size 8 \   --epochs 50 \   --lr_drop 40 \   --matcher team \   --q_splits 65 20 15  # --------------------------------------------  # Team-DINO # You need to write config and .sh files in advance. # multi-gpu bash scripts/DINO_4scale_1stage_team_r50_e12.sh /path/to/your/COCODIR /path/to/your/output/dir ```  ## How to integrate query teamwork into your model  The query teamwork contains three parts: scale-wise grouping, position constraint, and preference extraction.",bash,SOFTWARE
"Based on the source code of DAB-DETR / DN-DETR / DINO, every change in our code is clearly marked with ""# qt ..."".",DAB-DETR,SOFTWARE
"Based on the source code of DAB-DETR / DN-DETR / DINO, every change in our code is clearly marked with ""# qt ..."".",DN-DETR,SOFTWARE
"Based on the source code of DAB-DETR / DN-DETR / DINO, every change in our code is clearly marked with ""# qt ..."".",DINO,SOFTWARE
"The changes involve main.py, [DABDETR.py,] *transformer.py, matcher.py and engine.py.  ## Links  Our Team DETR is based on the basic architecture of DAB-DETR and is flexible enough to be adapted to DAB-based DETRs:  - **DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR**     Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang      International Conference on Learning Representations (ICLR) 2022     [[Paper]](https://arxiv.org/abs/2201.12329) [[Code]](https://github.com/SlongLiu/DAB-DETR) - **DN-DETR: Accelerate DETR Training by Introducing Query DeNoising**     Feng Li*, Hao Zhang*, Shilong Liu, Jian Guo, Lionel M.",DABDETR,SOFTWARE
"The changes involve main.py, [DABDETR.py,] *transformer.py, matcher.py and engine.py.  ## Links  Our Team DETR is based on the basic architecture of DAB-DETR and is flexible enough to be adapted to DAB-based DETRs:  - **DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR**     Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang      International Conference on Learning Representations (ICLR) 2022     [[Paper]](https://arxiv.org/abs/2201.12329) [[Code]](https://github.com/SlongLiu/DAB-DETR) - **DN-DETR: Accelerate DETR Training by Introducing Query DeNoising**     Feng Li*, Hao Zhang*, Shilong Liu, Jian Guo, Lionel M.",Team DETR,SOFTWARE
"The changes involve main.py, [DABDETR.py,] *transformer.py, matcher.py and engine.py.  ## Links  Our Team DETR is based on the basic architecture of DAB-DETR and is flexible enough to be adapted to DAB-based DETRs:  - **DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR**     Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang      International Conference on Learning Representations (ICLR) 2022     [[Paper]](https://arxiv.org/abs/2201.12329) [[Code]](https://github.com/SlongLiu/DAB-DETR) - **DN-DETR: Accelerate DETR Training by Introducing Query DeNoising**     Feng Li*, Hao Zhang*, Shilong Liu, Jian Guo, Lionel M.",DAB-DETR,SOFTWARE
"To recap, it is a mix of keyword search via SemanticScholar, and manual curation.",SemanticScholar,SOFTWARE
The code used to interface with SemanticScholar uses the unofficial [semanticscholar](https://github.com/danielnsilva/semanticscholar) python library.,SemanticScholar,SOFTWARE
The code used to interface with SemanticScholar uses the unofficial [semanticscholar](https://github.com/danielnsilva/semanticscholar) python library.,semanticscholar,SOFTWARE
The code used to interface with SemanticScholar uses the unofficial [semanticscholar](https://github.com/danielnsilva/semanticscholar) python library.,semanticscholar,SOFTWARE
Install with `pip install semanticscholar`.,pip,SOFTWARE
Install with `pip install semanticscholar`.,semanticscholar,SOFTWARE
"Each paper has the standard fields given by the SemanticScholar API: * `paperId` * `externalIds` * `url` * `title` * `abstract` * `venue` * `year` * `referenceCount` * `citationCount` * `influentialCitationCount` * `isOpenAccess` * `fieldsOfStudy` * `s2FieldsOfStudy` * `tldr` * `publicationTypes` * `publicationDate` * `journal` * `authors`  Note: Some of the papers are missing some of the fields, or they are marked as empty or `None`.",SemanticScholar,SOFTWARE
The following fields are missing in `xai-scholar.json` and need to be retrieved from SemanticScholar due to their large size (expect around 1 GB without `embedding`): * `embedding` * `citations` * `references`  The code to do so is in the jupyter notebook - but it simply calls `semanticscholar.get_paper(paperId)` for each `paperId`.,jupyter notebook,SOFTWARE
The following fields are missing in `xai-scholar.json` and need to be retrieved from SemanticScholar due to their large size (expect around 1 GB without `embedding`): * `embedding` * `citations` * `references`  The code to do so is in the jupyter notebook - but it simply calls `semanticscholar.get_paper(paperId)` for each `paperId`.,semanticscholar,SOFTWARE
"# OntoED and OntoEvent  <p align=""center"">     <font size=4><strong>OntoED: A Model for Low-resource Event Detection with Ontology Embedding</strong></font> </p>   üçé  The project is an official implementation for [**OntoED**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoED) model and a repository for [**OntoEvent**](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) dataset, which has firstly been proposed in the paper [OntoED: Low-resource Event Detection with Ontology Embedding](https://arxiv.org/pdf/2105.10922.pdf) accepted by ACL 2021",OntoED,SOFTWARE
ü§ó  The implementations are based on [Huggingface's Transformers](https://github.com/huggingface/transformers) and remanagement is referred to [MAVEN's baselines](https://github.com/THU-KEG/MAVEN-dataset/) & [DeepKE](https://github.com/zjunlp/DeepKE),Transformers,SOFTWARE
ü§ó  The implementations are based on [Huggingface's Transformers](https://github.com/huggingface/transformers) and remanagement is referred to [MAVEN's baselines](https://github.com/THU-KEG/MAVEN-dataset/) & [DeepKE](https://github.com/zjunlp/DeepKE),transformers,SOFTWARE
ü§ó  The implementations are based on [Huggingface's Transformers](https://github.com/huggingface/transformers) and remanagement is referred to [MAVEN's baselines](https://github.com/THU-KEG/MAVEN-dataset/) & [DeepKE](https://github.com/zjunlp/DeepKE),MAVEN's baselines,SOFTWARE
"You can directly download the archive, or run ```git clone https://github.com/231sm/Reasoning_In_EE.git``` at your teminal.   ``` cd [LOCAL_PROJECT_PATH]  git clone https://github.com/231sm/Reasoning_In_EE.git ```  **2.",git,SOFTWARE
"You can directly download the archive, or run ```git clone https://github.com/231sm/Reasoning_In_EE.git``` at your teminal.   ``` cd [LOCAL_PROJECT_PATH]  git clone https://github.com/231sm/Reasoning_In_EE.git ```  **2.",git,SOFTWARE
"Please refer to [OntoEvent](https://github.com/231sm/Reasoning_In_EE/tree/main/OntoEvent) for details.   ### Statistics The statistics of OntoEvent are shown below, and the detailed data schema can be referred to our paper.",OntoEvent,SOFTWARE
"([PDF](https://arxiv.org/pdf/1812.02664.pdf))  ``` @InProceedings{niu2019recursive,     author = {Niu, Yulei and Zhang, Hanwang and Zhang, Manli and Zhang, Jianhong and Lu, Zhiwu and Wen, Ji-Rong},     title = {Recursive Visual Attention in Visual Dialog},     booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},     month = {June},     year = {2019} }  ```  This code is reimplemented as a fork of [batra-mlp-lab/visdial-challenge-starter-pytorch][6].",pytorch,SOFTWARE
"Setup and Dependencies ----------------------  This code is implemented using PyTorch v1.0, and provides out of the box support with CUDA 9 and CuDNN 7.",PyTorch v1.0,SOFTWARE
"Setup and Dependencies ----------------------  This code is implemented using PyTorch v1.0, and provides out of the box support with CUDA 9 and CuDNN 7.",CUDA 9,SOFTWARE
"Setup and Dependencies ----------------------  This code is implemented using PyTorch v1.0, and provides out of the box support with CUDA 9 and CuDNN 7.",CuDNN 7,SOFTWARE
Anaconda/Miniconda is the recommended to set up this codebase:   ### Anaconda or Miniconda  1.,Anaconda,SOFTWARE
Anaconda/Miniconda is the recommended to set up this codebase:   ### Anaconda or Miniconda  1.,Miniconda,SOFTWARE
Anaconda/Miniconda is the recommended to set up this codebase:   ### Anaconda or Miniconda  1.,Anaconda,SOFTWARE
Anaconda/Miniconda is the recommended to set up this codebase:   ### Anaconda or Miniconda  1.,Miniconda,SOFTWARE
Install Anaconda or Miniconda distribution based on Python3+ from their [downloads' site][1]. 2.,Anaconda,SOFTWARE
Install Anaconda or Miniconda distribution based on Python3+ from their [downloads' site][1]. 2.,Miniconda,SOFTWARE
Clone this repository and create an environment:  ```shell git clone https://www.github.com/yuleiniu/rva conda create -n visdial-ch python=3.6  # activate the environment and install all dependencies conda activate visdial-ch cd rva/ pip install -r requirements.txt  # install this codebase as a package in development version python setup.py develop ```   Download Data -------------  1.,git,SOFTWARE
Clone this repository and create an environment:  ```shell git clone https://www.github.com/yuleiniu/rva conda create -n visdial-ch python=3.6  # activate the environment and install all dependencies conda activate visdial-ch cd rva/ pip install -r requirements.txt  # install this codebase as a package in development version python setup.py develop ```   Download Data -------------  1.,conda,SOFTWARE
Clone this repository and create an environment:  ```shell git clone https://www.github.com/yuleiniu/rva conda create -n visdial-ch python=3.6  # activate the environment and install all dependencies conda activate visdial-ch cd rva/ pip install -r requirements.txt  # install this codebase as a package in development version python setup.py develop ```   Download Data -------------  1.,conda,SOFTWARE
Clone this repository and create an environment:  ```shell git clone https://www.github.com/yuleiniu/rva conda create -n visdial-ch python=3.6  # activate the environment and install all dependencies conda activate visdial-ch cd rva/ pip install -r requirements.txt  # install this codebase as a package in development version python setup.py develop ```   Download Data -------------  1.,pip,SOFTWARE
Clone this repository and create an environment:  ```shell git clone https://www.github.com/yuleiniu/rva conda create -n visdial-ch python=3.6  # activate the environment and install all dependencies conda activate visdial-ch cd rva/ pip install -r requirements.txt  # install this codebase as a package in development version python setup.py develop ```   Download Data -------------  1.,python,SOFTWARE
"Extracting Features (Optional) -------------  ### With Docker (Optional) For Dockerfile, please refer to [batra-mlp-lab/visdial-challenge-starter-pytorch][8].  ### Without Docker (Optional)  0.",Docker,SOFTWARE
"Extracting Features (Optional) -------------  ### With Docker (Optional) For Dockerfile, please refer to [batra-mlp-lab/visdial-challenge-starter-pytorch][8].  ### Without Docker (Optional)  0.",pytorch,SOFTWARE
"Extracting Features (Optional) -------------  ### With Docker (Optional) For Dockerfile, please refer to [batra-mlp-lab/visdial-challenge-starter-pytorch][8].  ### Without Docker (Optional)  0.",Docker,SOFTWARE
"Set up opencv, [cocoapi][9] and [Detectron][10].  1.",opencv,SOFTWARE
"Set up opencv, [cocoapi][9] and [Detectron][10].  1.",cocoapi,SOFTWARE
"Set up opencv, [cocoapi][9] and [Detectron][10].  1.",Detectron,SOFTWARE
```shell python .,python,SOFTWARE
/data/extract_features_detectron.py --image-root /path/to/MSCOCO/train2014/ /path/to/MSCOCO/val2014/ --save-path /path/to/feature --split train # Bottom-up features of 36 proposals from images of train split. python .,python,SOFTWARE
/data/extract_features_detectron.py --image-root /path/to/Flickr/VisualDialog_val2018 --save-path /path/to/feature --split val # Bottom-up features of 36 proposals from images of val split. python .,python,SOFTWARE
/data/extract_features_detectron.py --image-root /path/to/Flickr/VisualDialog_test2018 --save-path /path/to/feature --split test # Bottom-up features of 36 proposals from images of test split. ```  Initializing GloVe Word Embeddings -------------- Simply run  ```shell python data/init_glove.py ```   Training --------  Train the model provided in this repository as:  ```shell python train.py --config-yml configs/rva.yml --gpu-ids 0 # provide more ids for multi-GPU execution other args... ```  ### Saving model checkpoints  This script will save model checkpoints at every epoch as per path specified by `--save-dirpath`.,shell,SOFTWARE
/data/extract_features_detectron.py --image-root /path/to/Flickr/VisualDialog_test2018 --save-path /path/to/feature --split test # Bottom-up features of 36 proposals from images of test split. ```  Initializing GloVe Word Embeddings -------------- Simply run  ```shell python data/init_glove.py ```   Training --------  Train the model provided in this repository as:  ```shell python train.py --config-yml configs/rva.yml --gpu-ids 0 # provide more ids for multi-GPU execution other args... ```  ### Saving model checkpoints  This script will save model checkpoints at every epoch as per path specified by `--save-dirpath`.,python,SOFTWARE
/data/extract_features_detectron.py --image-root /path/to/Flickr/VisualDialog_test2018 --save-path /path/to/feature --split test # Bottom-up features of 36 proposals from images of test split. ```  Initializing GloVe Word Embeddings -------------- Simply run  ```shell python data/init_glove.py ```   Training --------  Train the model provided in this repository as:  ```shell python train.py --config-yml configs/rva.yml --gpu-ids 0 # provide more ids for multi-GPU execution other args... ```  ### Saving model checkpoints  This script will save model checkpoints at every epoch as per path specified by `--save-dirpath`.,python,SOFTWARE
Refer [visdialch/utils/checkpointing.py][7] for more details on how checkpointing is managed.  ### Logging  We use [Tensorboard][2] for logging training progress.,Tensorboard,SOFTWARE
Recommended: execute `tensorboard --logdir /path/to/save_dir --port 8008` and visit `localhost:8008` in the browser.,tensorboard,SOFTWARE
"Evaluation ----------  Evaluation of a trained model checkpoint can be done as follows:  ```shell python evaluate.py --config-yml /path/to/config.yml --load-pthpath /path/to/checkpoint.pth --split val --gpu-ids 0 ```  This will generate an EvalAI submission file, and report metrics from the [Visual Dialog paper][5] (Mean reciprocal rank, R@{1, 5, 10}, Mean rank), and Normalized Discounted Cumulative Gain (NDCG), introduced in the first Visual Dialog Challenge (in 2018).",python,SOFTWARE
[1]: https://conda.io/docs/user-guide/install/download.html [2]: https://www.github.com/lanpa/tensorboardX [3]: https://visualdialog.org/data [4]: https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/visdial_1.0_word_counts_train.json [5]: https://arxiv.org/abs/1611.08669 [6]: https://www.github.com/batra-mlp-lab/visdial-challenge-starter-pytorch [7]: https://www.github.com/yuleiniu/rva/blob/master/visdialch/utils/checkpointing.py [8]: https://www.github.com/batra-mlp-lab/visdial-challenge-starter-pytorch#docker [9]: https://www.github.com/cocodataset/cocoapi [10]: https://www.github.com/facebookresearch/Detectron [11]: http://cocodataset.org/#download [12]: http://nlp.stanford.edu/data/glove.6B.zip,conda,SOFTWARE
[1]: https://conda.io/docs/user-guide/install/download.html [2]: https://www.github.com/lanpa/tensorboardX [3]: https://visualdialog.org/data [4]: https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/visdial_1.0_word_counts_train.json [5]: https://arxiv.org/abs/1611.08669 [6]: https://www.github.com/batra-mlp-lab/visdial-challenge-starter-pytorch [7]: https://www.github.com/yuleiniu/rva/blob/master/visdialch/utils/checkpointing.py [8]: https://www.github.com/batra-mlp-lab/visdial-challenge-starter-pytorch#docker [9]: https://www.github.com/cocodataset/cocoapi [10]: https://www.github.com/facebookresearch/Detectron [11]: http://cocodataset.org/#download [12]: http://nlp.stanford.edu/data/glove.6B.zip,tensorboardX,SOFTWARE
[1]: https://conda.io/docs/user-guide/install/download.html [2]: https://www.github.com/lanpa/tensorboardX [3]: https://visualdialog.org/data [4]: https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/visdial_1.0_word_counts_train.json [5]: https://arxiv.org/abs/1611.08669 [6]: https://www.github.com/batra-mlp-lab/visdial-challenge-starter-pytorch [7]: https://www.github.com/yuleiniu/rva/blob/master/visdialch/utils/checkpointing.py [8]: https://www.github.com/batra-mlp-lab/visdial-challenge-starter-pytorch#docker [9]: https://www.github.com/cocodataset/cocoapi [10]: https://www.github.com/facebookresearch/Detectron [11]: http://cocodataset.org/#download [12]: http://nlp.stanford.edu/data/glove.6B.zip,pytorch,SOFTWARE
[1]: https://conda.io/docs/user-guide/install/download.html [2]: https://www.github.com/lanpa/tensorboardX [3]: https://visualdialog.org/data [4]: https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/visdial_1.0_word_counts_train.json [5]: https://arxiv.org/abs/1611.08669 [6]: https://www.github.com/batra-mlp-lab/visdial-challenge-starter-pytorch [7]: https://www.github.com/yuleiniu/rva/blob/master/visdialch/utils/checkpointing.py [8]: https://www.github.com/batra-mlp-lab/visdial-challenge-starter-pytorch#docker [9]: https://www.github.com/cocodataset/cocoapi [10]: https://www.github.com/facebookresearch/Detectron [11]: http://cocodataset.org/#download [12]: http://nlp.stanford.edu/data/glove.6B.zip,pytorch,SOFTWARE
[1]: https://conda.io/docs/user-guide/install/download.html [2]: https://www.github.com/lanpa/tensorboardX [3]: https://visualdialog.org/data [4]: https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/visdial_1.0_word_counts_train.json [5]: https://arxiv.org/abs/1611.08669 [6]: https://www.github.com/batra-mlp-lab/visdial-challenge-starter-pytorch [7]: https://www.github.com/yuleiniu/rva/blob/master/visdialch/utils/checkpointing.py [8]: https://www.github.com/batra-mlp-lab/visdial-challenge-starter-pytorch#docker [9]: https://www.github.com/cocodataset/cocoapi [10]: https://www.github.com/facebookresearch/Detectron [11]: http://cocodataset.org/#download [12]: http://nlp.stanford.edu/data/glove.6B.zip,docker,SOFTWARE
"-- TABLE OF CONTENTS --> <ol>   <li>     <a href=""#about-the-project"">About The Project</a>   </li>   <li><a href=""#citing-abides"">Citing ABIDES</a></li>   <li>     <a href=""#getting-started"">Getting Started</a>     <ul>       <li><a href=""#installation"">Installation</a></li>     </ul>   </li>   <li>     <a href=""#usage-regular"">Usage (regular)</a>     <ul>       <li><a href=""#using-a-python-script"">Using a Python Script</a></li>       <li><a href=""#using-the-abides-command"">Using the `abides` Command</a></li>     </ul>   </li>   <li><a href=""#usage-gym"">Usage (Gym)</a></li>   <li><a href=""#default-available-markets-configurations"">Default Available Markets Configurations</a></li>   <li><a href=""#contributing"">Contributing</a></li>   <li><a href=""#license"">License</a></li>   <li><a href=""#acknowledgments"">Acknowledgments</a></li> </ol>  <!",abides,SOFTWARE
"Download the ABIDES source code, either directly from GitHub or with git:      ```bash     git clone https://github.com/jpmorganchase/abides-jpmc-public     ```      **Note:** The latest stable version is contained within the `main` branch.  2.",ABIDES,SOFTWARE
"Download the ABIDES source code, either directly from GitHub or with git:      ```bash     git clone https://github.com/jpmorganchase/abides-jpmc-public     ```      **Note:** The latest stable version is contained within the `main` branch.  2.",git,SOFTWARE
"Run the install script to install the ABIDES packages and their dependencies:      ```     sh install.sh     ```   <p align=""right"">(<a href=""#top"">back to top</a>)</p>  <!",ABIDES,SOFTWARE
"-- USAGE EXAMPLES --> ## Usage (regular) Regular ABIDES simulations can be run either directly in python or through the command line  _For more examples, please refer to the [Documentation](https://example.com)_  ### Using a Python Script:  ```python from abides_markets.configs import rmsc04 from abides_core import abides  config_state = rmsc04.build_config(seed = 0, end_time = '10:00:00') end_state = abides.run(config_state) ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ### Using the abides Command:  The config can be loaded and the simulation run using the `abides` command in the terminal (from directory root):  ``` $ abides abides-markets/abides_markets/configs/rmsc04.py --end_time ""10:00:00"" ```  The first argument is a path to a valid ABIDES configuration file.",ABIDES,SOFTWARE
"-- USAGE EXAMPLES --> ## Usage (regular) Regular ABIDES simulations can be run either directly in python or through the command line  _For more examples, please refer to the [Documentation](https://example.com)_  ### Using a Python Script:  ```python from abides_markets.configs import rmsc04 from abides_core import abides  config_state = rmsc04.build_config(seed = 0, end_time = '10:00:00') end_state = abides.run(config_state) ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ### Using the abides Command:  The config can be loaded and the simulation run using the `abides` command in the terminal (from directory root):  ``` $ abides abides-markets/abides_markets/configs/rmsc04.py --end_time ""10:00:00"" ```  The first argument is a path to a valid ABIDES configuration file.",abides,SOFTWARE
"-- USAGE EXAMPLES --> ## Usage (regular) Regular ABIDES simulations can be run either directly in python or through the command line  _For more examples, please refer to the [Documentation](https://example.com)_  ### Using a Python Script:  ```python from abides_markets.configs import rmsc04 from abides_core import abides  config_state = rmsc04.build_config(seed = 0, end_time = '10:00:00') end_state = abides.run(config_state) ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ### Using the abides Command:  The config can be loaded and the simulation run using the `abides` command in the terminal (from directory root):  ``` $ abides abides-markets/abides_markets/configs/rmsc04.py --end_time ""10:00:00"" ```  The first argument is a path to a valid ABIDES configuration file.",abides,SOFTWARE
"-- USAGE EXAMPLES --> ## Usage (regular) Regular ABIDES simulations can be run either directly in python or through the command line  _For more examples, please refer to the [Documentation](https://example.com)_  ### Using a Python Script:  ```python from abides_markets.configs import rmsc04 from abides_core import abides  config_state = rmsc04.build_config(seed = 0, end_time = '10:00:00') end_state = abides.run(config_state) ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ### Using the abides Command:  The config can be loaded and the simulation run using the `abides` command in the terminal (from directory root):  ``` $ abides abides-markets/abides_markets/configs/rmsc04.py --end_time ""10:00:00"" ```  The first argument is a path to a valid ABIDES configuration file.",abides,SOFTWARE
"-- USAGE EXAMPLES --> ## Usage (regular) Regular ABIDES simulations can be run either directly in python or through the command line  _For more examples, please refer to the [Documentation](https://example.com)_  ### Using a Python Script:  ```python from abides_markets.configs import rmsc04 from abides_core import abides  config_state = rmsc04.build_config(seed = 0, end_time = '10:00:00') end_state = abides.run(config_state) ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ### Using the abides Command:  The config can be loaded and the simulation run using the `abides` command in the terminal (from directory root):  ``` $ abides abides-markets/abides_markets/configs/rmsc04.py --end_time ""10:00:00"" ```  The first argument is a path to a valid ABIDES configuration file.",abides,SOFTWARE
"-- USAGE EXAMPLES --> ## Usage (regular) Regular ABIDES simulations can be run either directly in python or through the command line  _For more examples, please refer to the [Documentation](https://example.com)_  ### Using a Python Script:  ```python from abides_markets.configs import rmsc04 from abides_core import abides  config_state = rmsc04.build_config(seed = 0, end_time = '10:00:00') end_state = abides.run(config_state) ``` <p align=""right"">(<a href=""#top"">back to top</a>)</p>  ### Using the abides Command:  The config can be loaded and the simulation run using the `abides` command in the terminal (from directory root):  ``` $ abides abides-markets/abides_markets/configs/rmsc04.py --end_time ""10:00:00"" ```  The first argument is a path to a valid ABIDES configuration file.",ABIDES,SOFTWARE
"<p align=""right"">(<a href=""#top"">back to top</a>)</p>  ## Usage (Gym) ABIDES can also be run through a Gym interface using ABIDES-Gym environments.",ABIDES,SOFTWARE
"```python import gym import abides_gym  env = gym.make(     ""markets-daily_investor-v0"",     background_config=""rmsc04"", )  env.seed(0) initial_state = env.reset() for i in range(5):     state, reward, done, info = env.step(0) ```  ## Default Available Markets Configurations  ABIDES currently has the following available background Market Simulation Configuration:  * RMSC03: 1 Exchange Agent, 1 POV Market Maker Agent, 100 Value Agents, 25 Momentum Agents, 5000 Noise Agents   * RMSC04: 1 Exchange Agent, 2 Market Maker Agents, 102 Value Agents, 12 Momentum Agents, 1000  Noise Agents  <p align=""right"">(<a href=""#top"">back to top</a>)</p>  <!",ABIDES,SOFTWARE
Create your Feature Branch (`git checkout -b feature/AmazingFeature`) 3.,git,SOFTWARE
Commit your Changes (`git commit -m 'Add some AmazingFeature'`) 4.,git,SOFTWARE
"-- ACKNOWLEDGMENTS --> ## Acknowledgments ABIDES was originally developed by David Byrd and Tucker Balch: https://github.com/abides-sim/abides ABIDES is currently developed and maintained by [Jared Vann](https://github.com/jaredvann) (aka @jaredvann), [Selim Amrouni](https://github.com/selimamrouni) (aka @selimamrouni), and [Aymeric Moulin](https://github.com/AymericCAMoulin) (@AymericCAMoulin).",ABIDES,SOFTWARE
"-- ACKNOWLEDGMENTS --> ## Acknowledgments ABIDES was originally developed by David Byrd and Tucker Balch: https://github.com/abides-sim/abides ABIDES is currently developed and maintained by [Jared Vann](https://github.com/jaredvann) (aka @jaredvann), [Selim Amrouni](https://github.com/selimamrouni) (aka @selimamrouni), and [Aymeric Moulin](https://github.com/AymericCAMoulin) (@AymericCAMoulin).",ABIDES,SOFTWARE
"The network is trained under fully unsupervised manner._**  __Official pytorch implementation of ""Rethinking the Truly Unsupervised Image-to-Image Translation""__  > __[Rethinking the Truly Unsupervised Image-to-Image Translation](https://arxiv.org/abs/2006.06500)__    > Kyungjune Baek<sup>1</sup>*, Yunjey Choi<sup>2</sup>, Youngjung Uh<sup>1</sup>, Jaejun Yoo<sup>3</sup>, Hyunjung Shim<sup>1</sup>   > \* Work done during his internship at Clova AI Research   > <sup>1</sup> Yonsei University   > <sup>2</sup> NAVER AI Lab",pytorch,SOFTWARE
"Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data._  ### Requirement  __Library__ ``` pip install -r requirements.txt  * pytorch==1.1.0 or 1.2.0   * tqdm   * opencv-python   * scipy   * sklearn * matplotlib   * pillow   * tensorboardX  ```  __Dataset__   * This repo. utilizes the variant of ""ImageFolder"". * Download : [AFHQ (StarGANv2)](https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?",pip,SOFTWARE
"Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data._  ### Requirement  __Library__ ``` pip install -r requirements.txt  * pytorch==1.1.0 or 1.2.0   * tqdm   * opencv-python   * scipy   * sklearn * matplotlib   * pillow   * tensorboardX  ```  __Dataset__   * This repo. utilizes the variant of ""ImageFolder"". * Download : [AFHQ (StarGANv2)](https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?",pytorch==1.1.0,SOFTWARE
"Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data._  ### Requirement  __Library__ ``` pip install -r requirements.txt  * pytorch==1.1.0 or 1.2.0   * tqdm   * opencv-python   * scipy   * sklearn * matplotlib   * pillow   * tensorboardX  ```  __Dataset__   * This repo. utilizes the variant of ""ImageFolder"". * Download : [AFHQ (StarGANv2)](https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?",tqdm,SOFTWARE
"Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data._  ### Requirement  __Library__ ``` pip install -r requirements.txt  * pytorch==1.1.0 or 1.2.0   * tqdm   * opencv-python   * scipy   * sklearn * matplotlib   * pillow   * tensorboardX  ```  __Dataset__   * This repo. utilizes the variant of ""ImageFolder"". * Download : [AFHQ (StarGANv2)](https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?",opencv-python,SOFTWARE
"Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data._  ### Requirement  __Library__ ``` pip install -r requirements.txt  * pytorch==1.1.0 or 1.2.0   * tqdm   * opencv-python   * scipy   * sklearn * matplotlib   * pillow   * tensorboardX  ```  __Dataset__   * This repo. utilizes the variant of ""ImageFolder"". * Download : [AFHQ (StarGANv2)](https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?",scipy,SOFTWARE
"Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data._  ### Requirement  __Library__ ``` pip install -r requirements.txt  * pytorch==1.1.0 or 1.2.0   * tqdm   * opencv-python   * scipy   * sklearn * matplotlib   * pillow   * tensorboardX  ```  __Dataset__   * This repo. utilizes the variant of ""ImageFolder"". * Download : [AFHQ (StarGANv2)](https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?",sklearn,SOFTWARE
"Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data._  ### Requirement  __Library__ ``` pip install -r requirements.txt  * pytorch==1.1.0 or 1.2.0   * tqdm   * opencv-python   * scipy   * sklearn * matplotlib   * pillow   * tensorboardX  ```  __Dataset__   * This repo. utilizes the variant of ""ImageFolder"". * Download : [AFHQ (StarGANv2)](https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?",matplotlib,SOFTWARE
"Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data._  ### Requirement  __Library__ ``` pip install -r requirements.txt  * pytorch==1.1.0 or 1.2.0   * tqdm   * opencv-python   * scipy   * sklearn * matplotlib   * pillow   * tensorboardX  ```  __Dataset__   * This repo. utilizes the variant of ""ImageFolder"". * Download : [AFHQ (StarGANv2)](https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?",pillow,SOFTWARE
"Furthermore, TUNIT can be easily extended to semi-supervised learning with a few labeled data._  ### Requirement  __Library__ ``` pip install -r requirements.txt  * pytorch==1.1.0 or 1.2.0   * tqdm   * opencv-python   * scipy   * sklearn * matplotlib   * pillow   * tensorboardX  ```  __Dataset__   * This repo. utilizes the variant of ""ImageFolder"". * Download : [AFHQ (StarGANv2)](https://www.dropbox.com/s/t9l9o3vsx2jai3z/afhq.zip?",tensorboardX,SOFTWARE
__Train on local__ ``` Supervised python main.py --gpu $GPU_TO_USE --p_semi 1.0 --dataset animal_faces --data_path='..,python,SOFTWARE
/data'  Semi-supervised python main.py --gpu $GPU_TO_USE --p_semi 0.5 --dataset animal_faces --data_path='..,python,SOFTWARE
/data'  Unsupervised python main.py --gpu $GPU_TO_USE --p_semi 0.0 --dataset animal_faces --data_path='..,python,SOFTWARE
/data' ```  __Test on local__ ``` python main.py --gpu $GPU_TO_USE --validation --load_model $DIR_TO_LOAD --dataset animal_faces ```  __Monitoring__ ``` tensorboard --logdir=$DIR/events --port=$PORT ```  __Actual example__ ``` Train python main.py --gpu 0 --dataset animal_faces --output_k 10 --data_path '..,python,SOFTWARE
/data' ```  __Test on local__ ``` python main.py --gpu $GPU_TO_USE --validation --load_model $DIR_TO_LOAD --dataset animal_faces ```  __Monitoring__ ``` tensorboard --logdir=$DIR/events --port=$PORT ```  __Actual example__ ``` Train python main.py --gpu 0 --dataset animal_faces --output_k 10 --data_path '..,tensorboard,SOFTWARE
/data' ```  __Test on local__ ``` python main.py --gpu $GPU_TO_USE --validation --load_model $DIR_TO_LOAD --dataset animal_faces ```  __Monitoring__ ``` tensorboard --logdir=$DIR/events --port=$PORT ```  __Actual example__ ``` Train python main.py --gpu 0 --dataset animal_faces --output_k 10 --data_path '..,python,SOFTWARE
/data' --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --data_path '..,python,SOFTWARE
/data' --p_semi 0.2 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --data_path '..,python,SOFTWARE
/data' --p_semi 0.0 python main.py --gpu 1 --dataset animal_faces --data_path '..,python,SOFTWARE
"/data' --p_semi 1.0 python main.py --gpu 0,1 --dataset summer2winter --output_k 2 --data_path '..",python,SOFTWARE
/data' --p_semi 0.0 --img_size 256 --batch_size 16 --ddp  Test python main.py --gpu 0 --dataset animal_faces --output_k 10 --data_path '..,python,SOFTWARE
/data' --validation --load_model GAN_20190101_101010 python main.py --gpu 1 --dataset afhq_cat --output_k 10 --data_path '..,python,SOFTWARE
/data' --validation --load_model GAN_20190101_101010 python main.py --gpu 2 --dataset summer2winter --output_k 2 --data_path '..,python,SOFTWARE
/tunit/logs tensorboard --logdir=.,tensorboard,SOFTWARE
/GAN_20200101_101010/events ```  ## Pretrained Model ### Download [One Drive](https://1drv.ms/u/s!,One Drive,SOFTWARE
"e=IYsUwX) * Download folders to load, then place the folder under 'logs'. * Links of google drive are deprecated. ``` Project |--- tunit |          |--- main.py |          |--- logs |                 |--- animalFaces10_0_00 |                               |--- checkpoint.txt |                               |--- model_4568.ckpt |          |--- train |                 |--- train_unsupervised.py |                 |--- ...",google drive,SOFTWARE
"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",python,SOFTWARE
"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",python,SOFTWARE
"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",python,SOFTWARE
"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",python,SOFTWARE
"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",python,SOFTWARE
"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",python,SOFTWARE
"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",python,SOFTWARE
"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",python,SOFTWARE
"Then, RUN python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 ``` ### How to run ``` AFHQ Cat python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_cat_128 python main.py --gpu 0 --dataset afhq_cat --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_cat_256 ``` ``` AFHQ Dog python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_dog_128 python main.py --gpu 0 --dataset afhq_dog --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_dog_256  ``` ``` AFHQ Wild python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model afhq_wild_128 python main.py --gpu 0 --dataset afhq_wild --output_k 10 --img_size 256 --data_path $DATAPATH --validation --load_model afhq_wild_256 ``` ``` AnimalFaces-10 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_00 --p_semi 0.0 python main.py --gpu 0 --dataset animal_faces --output_k 10 --img_size 128 --data_path $DATAPATH --validation --load_model animalFaces10_0_20 --p_semi 0.2 ```  ## Explanation for codes The validation generates 200 images per args.iters iterations.",python,SOFTWARE
"* For more classes on AnimalFaces, change the list at main.py#L227 then, set args.output_k to len(args.att_to_use)     * ex) args.att_to_use = \[i for i in range(100)\] then, run: python main.py --output_k 100 ...  ### Arguments * batch_size, img_size, data_path and p_semi are frequently speified. * Please refer ""help"" of the arguments in main.py.  ### Code Structure * main.py     * Execute main.py to run the codes",python,SOFTWARE
"The code can be used to generate synthetic ELIZA training data, train and evaluate Transformers on ELIZA transcripts, and conduct some analysis of the learned mechanisms.",ELIZA,SOFTWARE
"The code can be used to generate synthetic ELIZA training data, train and evaluate Transformers on ELIZA transcripts, and conduct some analysis of the learned mechanisms.",ELIZA,SOFTWARE
(#Questions) * [Citation](#Citation)  ## Setup Install [PyTorch](https://pytorch.org/get-started/locally/) and then install the remaining requirements: `pip install -r requirements.txt`.,PyTorch,SOFTWARE
(#Questions) * [Citation](#Citation)  ## Setup Install [PyTorch](https://pytorch.org/get-started/locally/) and then install the remaining requirements: `pip install -r requirements.txt`.,pip,SOFTWARE
This code was tested using Python 3.12 and PyTorch version 2.2.2.  ## Generating data  The code we used to generate the data is in [src/generate_data.py](src/generate_data.py).,PyTorch version 2.2.2,SOFTWARE
"The [scripts](scripts/) directory contains the configurations for the datasets we used in our paper. - Multi-turn conversations: [scripts/generate_multi_turn_data.sh](scripts/generate_multi_turn_data.sh) - Single-turn conversations, varying the amount of repetition in the copying segments: [scripts/generate_single_turn_data.sh](scripts/generate_single_turn_data.sh)  The datasets we used in our experiments can also be downloaded directly from HuggingFace via this link: https://huggingface.co/datasets/danf0/eliza.  ## Training models  To train Transformers on ELIZA conversations, see [src/run.py](src/run.py).",ELIZA,SOFTWARE
# Anti-Backdoor Learning  PyTorch Code for NeurIPS 2021 paper **[Anti-Backdoor Learning: Training Clean Models on Poisoned Data](https://arxiv.org/pdf/2110.11571.pdf)**.  !,PyTorch,SOFTWARE
[CUDA 10.0](https://img.shields.io/badge/cuda-10.0-DodgerBlue.svg?,CUDA 10.0,SOFTWARE
[CUDA 10.0](https://img.shields.io/badge/cuda-10.0-DodgerBlue.svg?,cuda-10.0,SOFTWARE
Run the following command to verify the unlearning effect:  ```bash $ python quick_unlearning_demo.py  ``` The training logs are shown below. 1% isolation = 500 images from poisoned CIFAR-10.,python,SOFTWARE
"```python Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss 0,82.77777777777777,99.9888888888889,0.9145596397187975,0.0007119161817762587 Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss 1,82.97777777777777,47.13333333333333,0.9546798907385932,4.189897534688313 Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss 2,82.46666666666667,5.766666666666667,1.034722186088562,15.361101960923937 Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss 3,82.15555555555555,1.5222222222222221,1.0855470676422119,22.175255742390952 Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss 4,82.0111111111111,0.7111111111111111,1.1183592330084906,26.754894670274524 Epoch,Test_clean_acc,Test_bad_acc,Test_clean_loss,Test_bad_loss 5,81.86666666666666,0.4777777777777778,1.1441074348025853,30.429284422132703 ```  The unlearned model will be saved to `'weight/ABL_results/<model_name>.tar'`  Please read `quick_unlearning_demo.py` to adjust the default parameters for your experiment.  ---  ## More defense results on BadNets model trained with Data Augmentation   ```python [Logs for our ABL against Badnet Attacks]  ----------- Model Fine-tuning -------------- epoch: 40  lr: 0.0100 Epoch[41]:[200/774] loss:0.1456(0.1240)  prec@1:98.44(95.84)  prec@5:98.44(99.96) Epoch[41]:[400/774] loss:0.0553(0.1080)  prec@1:98.44(96.38)  prec@5:100.00(99.97) Epoch[41]:[600/774] loss:0.0693(0.1015)  prec@1:96.88(96.63)  prec@5:100.00(99.97) [Clean] Prec@1: 92.23, Loss: 0.2408 [Bad] Prec@1: 100.00, Loss: 0.0001 epoch: 41  lr: 0.0100 Epoch[42]:[200/774] loss:0.0532(0.0653)  prec@1:98.44(97.89)  prec@5:100.00(100.00) Epoch[42]:[400/774] loss:0.0534(0.0659)  prec@1:98.44(97.76)  prec@5:100.00(100.00) Epoch[42]:[600/774] loss:0.0514(0.0659)  prec@1:96.88(97.76)  prec@5:100.00(99.99) [Clean] Prec@1: 92.60, Loss: 0.2390 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 42  lr: 0.0100 Epoch[43]:[200/774] loss:0.0054(0.0499)  prec@1:100.00(98.33)  prec@5:100.00(99.99) Epoch[43]:[400/774] loss:0.0429(0.0525)  prec@1:98.44(98.21)  prec@5:100.00(99.99) Epoch[43]:[600/774] loss:0.0448(0.0537)  prec@1:98.44(98.19)  prec@5:100.00(99.99) [Clean] Prec@1: 92.52, Loss: 0.2409 [Bad] Prec@1: 100.00, Loss: 0.0001 epoch: 43  lr: 0.0100 Epoch[44]:[200/774] loss:0.0253(0.0472)  prec@1:98.44(98.41)  prec@5:100.00(99.99) Epoch[44]:[400/774] loss:0.0104(0.0463)  prec@1:100.00(98.43)  prec@5:100.00(99.99) Epoch[44]:[600/774] loss:0.0200(0.0452)  prec@1:100.00(98.46)  prec@5:100.00(99.99) [Clean] Prec@1: 92.60, Loss: 0.2459 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 44  lr: 0.0100 Epoch[45]:[200/774] loss:0.0510(0.0385)  prec@1:98.44(98.79)  prec@5:100.00(99.99) Epoch[45]:[400/774] loss:0.0244(0.0381)  prec@1:98.44(98.82)  prec@5:100.00(100.00) Epoch[45]:[600/774] loss:0.0203(0.0391)  prec@1:100.00(98.83)  prec@5:100.00(99.99) [Clean] Prec@1: 92.81, Loss: 0.2484 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 45  lr: 0.0100 Epoch[46]:[200/774] loss:0.0110(0.0374)  prec@1:100.00(98.75)  prec@5:100.00(99.99) Epoch[46]:[400/774] loss:0.0204(0.0371)  prec@1:98.44(98.79)  prec@5:100.00(99.99) Epoch[46]:[600/774] loss:0.0183(0.0369)  prec@1:100.00(98.76)  prec@5:100.00(99.99) [Clean] Prec@1: 92.99, Loss: 0.2495 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 46  lr: 0.0100 Epoch[47]:[200/774] loss:0.0452(0.0315)  prec@1:98.44(98.97)  prec@5:100.00(100.00) Epoch[47]:[400/774] loss:0.0315(0.0310)  prec@1:98.44(98.98)  prec@5:100.00(100.00) Epoch[47]:[600/774] loss:0.0298(0.0303)  prec@1:100.00(99.01)  prec@5:100.00(100.00) [Clean] Prec@1: 92.82, Loss: 0.2563 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 47  lr: 0.0100 Epoch[48]:[200/774] loss:0.0397(0.0269)  prec@1:98.44(99.12)  prec@5:100.00(100.00) Epoch[48]:[400/774] loss:0.0617(0.0262)  prec@1:98.44(99.16)  prec@5:100.00(100.00) Epoch[48]:[600/774] loss:0.0630(0.0270)  prec@1:98.44(99.16)  prec@5:100.00(100.00) [Clean] Prec@1: 92.81, Loss: 0.2678 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 48  lr: 0.0100 Epoch[49]:[200/774] loss:0.0251(0.0267)  prec@1:100.00(99.15)  prec@5:100.00(100.00) Epoch[49]:[400/774] loss:0.0298(0.0262)  prec@1:98.44(99.14)  prec@5:100.00(100.00) Epoch[49]:[600/774] loss:0.0384(0.0258)  prec@1:98.44(99.15)  prec@5:100.00(100.00) [Clean] Prec@1: 93.09, Loss: 0.2586 [Bad] Prec@1: 100.00, Loss: 0.0002 epoch: 49  lr: 0.0100 Epoch[50]:[200/774] loss:0.0359(0.0203)  prec@1:98.44(99.30)  prec@5:100.00(100.00) Epoch[50]:[400/774] loss:0.0062(0.0214)  prec@1:100.00(99.27)  prec@5:100.00(100.00) Epoch[50]:[600/774] loss:0.0418(0.0222)  prec@1:98.44(99.25)  prec@5:100.00(100.00) [Clean] Prec@1: 93.03, Loss: 0.2626 [Bad] Prec@1: 100.00, Loss: 0.0001 epoch: 50  lr: 0.0100 Epoch[51]:[200/774] loss:0.0040(0.0222)  prec@1:100.00(99.27)  prec@5:100.00(100.00) Epoch[51]:[400/774] loss:0.0269(0.0236)  prec@1:98.44(99.21)  prec@5:100.00(100.00) Epoch[51]:[600/774] loss:0.0219(0.0234)  prec@1:100.00(99.23)  prec@5:100.00(100.00) [Clean] Prec@1: 93.19, Loss: 0.2604 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 51  lr: 0.0100 Epoch[52]:[200/774] loss:0.0154(0.0201)  prec@1:98.44(99.34)  prec@5:100.00(100.00) Epoch[52]:[400/774] loss:0.0328(0.0200)  prec@1:98.44(99.38)  prec@5:100.00(100.00) Epoch[52]:[600/774] loss:0.0220(0.0204)  prec@1:98.44(99.36)  prec@5:100.00(100.00) [Clean] Prec@1: 93.27, Loss: 0.2652 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 52  lr: 0.0100 Epoch[53]:[200/774] loss:0.0090(0.0194)  prec@1:100.00(99.39)  prec@5:100.00(100.00) Epoch[53]:[400/774] loss:0.0019(0.0195)  prec@1:100.00(99.41)  prec@5:100.00(100.00) Epoch[53]:[600/774] loss:0.0402(0.0190)  prec@1:98.44(99.45)  prec@5:100.00(100.00) [Clean] Prec@1: 93.04, Loss: 0.2735 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 53  lr: 0.0100 Epoch[54]:[200/774] loss:0.0154(0.0186)  prec@1:100.00(99.38)  prec@5:100.00(100.00) Epoch[54]:[400/774] loss:0.0124(0.0182)  prec@1:100.00(99.40)  prec@5:100.00(100.00) Epoch[54]:[600/774] loss:0.0144(0.0181)  prec@1:100.00(99.45)  prec@5:100.00(100.00) [Clean] Prec@1: 93.17, Loss: 0.2693 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 54  lr: 0.0100 Epoch[55]:[200/774] loss:0.0119(0.0168)  prec@1:100.00(99.43)  prec@5:100.00(100.00) Epoch[55]:[400/774] loss:0.0228(0.0170)  prec@1:98.44(99.42)  prec@5:100.00(100.00) Epoch[55]:[600/774] loss:0.0096(0.0164)  prec@1:100.00(99.47)  prec@5:100.00(100.00) [Clean] Prec@1: 92.84, Loss: 0.2786 [Bad] Prec@1: 100.00, Loss: 0.0001 epoch: 55  lr: 0.0100 Epoch[56]:[200/774] loss:0.0307(0.0146)  prec@1:98.44(99.51)  prec@5:100.00(100.00) Epoch[56]:[400/774] loss:0.0065(0.0149)  prec@1:100.00(99.52)  prec@5:100.00(100.00) Epoch[56]:[600/774] loss:0.0348(0.0155)  prec@1:98.44(99.50)  prec@5:100.00(100.00) [Clean] Prec@1: 93.12, Loss: 0.2794 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 56  lr: 0.0100 Epoch[57]:[200/774] loss:0.0014(0.0134)  prec@1:100.00(99.59)  prec@5:100.00(100.00) Epoch[57]:[400/774] loss:0.0060(0.0133)  prec@1:100.00(99.59)  prec@5:100.00(100.00) Epoch[57]:[600/774] loss:0.0400(0.0133)  prec@1:95.31(99.61)  prec@5:100.00(100.00) [Clean] Prec@1: 93.13, Loss: 0.2819 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 57  lr: 0.0100 Epoch[58]:[200/774] loss:0.0062(0.0122)  prec@1:100.00(99.60)  prec@5:100.00(100.00) Epoch[58]:[400/774] loss:0.0065(0.0134)  prec@1:100.00(99.56)  prec@5:100.00(100.00) Epoch[58]:[600/774] loss:0.0198(0.0134)  prec@1:100.00(99.59)  prec@5:100.00(100.00) [Clean] Prec@1: 93.11, Loss: 0.2795 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 58  lr: 0.0100 Epoch[59]:[200/774] loss:0.0053(0.0094)  prec@1:100.00(99.73)  prec@5:100.00(100.00) Epoch[59]:[400/774] loss:0.0064(0.0105)  prec@1:100.00(99.70)  prec@5:100.00(100.00) Epoch[59]:[600/774] loss:0.0068(0.0112)  prec@1:100.00(99.67)  prec@5:100.00(100.00) [Clean] Prec@1: 93.04, Loss: 0.2900 [Bad] Prec@1: 100.00, Loss: 0.0000 epoch: 59  lr: 0.0100 Epoch[60]:[200/774] loss:0.0039(0.0147)  prec@1:100.00(99.55)  prec@5:100.00(99.99) Epoch[60]:[400/774] loss:0.0399(0.0142)  prec@1:96.88(99.58)  prec@5:100.00(100.00) Epoch[60]:[600/774] loss:0.0030(0.0134)  prec@1:100.00(99.59)  prec@5:100.00(100.00) [Clean] Prec@1: 93.24, Loss: 0.2905 [Bad] Prec@1: 100.00, Loss: 0.0000  ----------- Model unlearning -------------- epoch: 0  lr: 0.0005 [Clean] Prec@1: 93.24, Loss: 0.2905 [Bad] Prec@1: 100.00, Loss: 0.0000 testing the ascended model......",python,SOFTWARE
"The following is an example:  ```python from data_loader import *     if opt.load_fixed_data:         # load the fixed poisoned data of numpy format, e.g.",python,SOFTWARE
"# Note that the load data type is a pytorch tensor         poisoned_data = np.load(opt.poisoned_data_path, allow_pickle=True)         poisoned_data_loader = DataLoader(dataset=poisoned_data,                                             batch_size=opt.batch_size,                                             shuffle=True,                                             )     else:         poisoned_data, poisoned_data_loader = get_backdoor_loader(opt)      test_clean_loader, test_bad_loader = get_test_loader(opt) ``` Note that, for attacks `Dynamic, DFTS, FC, etc`, it is hard to include them in the `get_backdoor_loader()`.",pytorch,SOFTWARE
"Please feel free to read `create_poisoned_data.py` and `get_backdoor_loader` and adjust the parameters for your experiment.    ## ABL - Stage One: Backdoor Isolation To isolate 1% potentially backdoored examples and an isolation model, you can run the following command:  ```bash $ python backdoor_isolation.py  ```  After that, you will get an `isolation model` and use it to isolate `1% poisoned data` of the lowest training loss.",python,SOFTWARE
"Please check more details of the experimental settings in Section 4 and Appendix A of our paper, and adjust the parameters in `config.py` for your experiment.    ## ABL - Stage Two: Backdoor Unlearning With the 1% isolated data and the isolation model, we can then continue with the later training of unlearning using the following code:  ```bash $ python backdoor_unlearning.py  ```  At this stage, the backdoor has already been learned into the isolation model.",python,SOFTWARE
"> arXiv preprint arXiv:1902.11237     superimposed sinusoidal backdoor signal with default parameters     """"""     alpha = 0.2     img = np.float32(img)     pattern = np.zeros_like(img)     m = pattern.shape[1]     for i in range(img.shape[0]):         for j in range(img.shape[1]):             for k in range(img.shape[2]):                 pattern[i, j] = delta * np.sin(2 * np.pi * j * f / m)      img = alpha * np.uint32(img) + (1 - alpha) * pattern     img = np.uint8(np.clip(img, 0, 255))      #     if debug:     #         cv2.imshow('planted image', img)     #         cv2.waitKey()      return img ```  **Dynamic:** Input-aware Dynamic Backdoor Attack  - [paper](https://papers.nips.cc/paper/2020/hash/234e691320c0ad5b45ee3c96d0d7b8f8-Abstract.html) - [pytorch implementation](https://github.com/VinAIResearch/input-aware-backdoor-attack-release)  **FC:** Poison Frogs!",pytorch,SOFTWARE
Targeted Clean-Label Poisoning Attacks on Neural Networks  - [paper](file/22722a343513ed45f14905eb07621686-Paper.pdf) - [pytorch implementation](https://github.com/FlouriteJ/PoisonFrogs)  **DFST:** Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification  - [paper](https://arxiv.org/abs/2012.11212) - [tensorflow implementation](https://github.com/Megum1/DFST)  **LBA:** Latent Backdoor Attacks on Deep Neural Networks  - [paper](https://people.cs.uchicago.edu/~ravenben/publications/pdf/pbackdoor-ccs19.pdf) - [tensorflow implementation](http://sandlab.cs.uchicago.edu/latent/)  **CBA:** Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features  - [paper](https://dl.acm.org/doi/abs/10.1145/3372297.3423362) - [pytorch implementation](https://github.com/TemporaryAcc0unt/composite-attack)  #### Feature space attack benchmark  `Note`: This repository is the official implementation of [Just How Toxic is Data Poisoning?,pytorch,SOFTWARE
Targeted Clean-Label Poisoning Attacks on Neural Networks  - [paper](file/22722a343513ed45f14905eb07621686-Paper.pdf) - [pytorch implementation](https://github.com/FlouriteJ/PoisonFrogs)  **DFST:** Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification  - [paper](https://arxiv.org/abs/2012.11212) - [tensorflow implementation](https://github.com/Megum1/DFST)  **LBA:** Latent Backdoor Attacks on Deep Neural Networks  - [paper](https://people.cs.uchicago.edu/~ravenben/publications/pdf/pbackdoor-ccs19.pdf) - [tensorflow implementation](http://sandlab.cs.uchicago.edu/latent/)  **CBA:** Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features  - [paper](https://dl.acm.org/doi/abs/10.1145/3372297.3423362) - [pytorch implementation](https://github.com/TemporaryAcc0unt/composite-attack)  #### Feature space attack benchmark  `Note`: This repository is the official implementation of [Just How Toxic is Data Poisoning?,tensorflow,SOFTWARE
Targeted Clean-Label Poisoning Attacks on Neural Networks  - [paper](file/22722a343513ed45f14905eb07621686-Paper.pdf) - [pytorch implementation](https://github.com/FlouriteJ/PoisonFrogs)  **DFST:** Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification  - [paper](https://arxiv.org/abs/2012.11212) - [tensorflow implementation](https://github.com/Megum1/DFST)  **LBA:** Latent Backdoor Attacks on Deep Neural Networks  - [paper](https://people.cs.uchicago.edu/~ravenben/publications/pdf/pbackdoor-ccs19.pdf) - [tensorflow implementation](http://sandlab.cs.uchicago.edu/latent/)  **CBA:** Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features  - [paper](https://dl.acm.org/doi/abs/10.1145/3372297.3423362) - [pytorch implementation](https://github.com/TemporaryAcc0unt/composite-attack)  #### Feature space attack benchmark  `Note`: This repository is the official implementation of [Just How Toxic is Data Poisoning?,tensorflow,SOFTWARE
Targeted Clean-Label Poisoning Attacks on Neural Networks  - [paper](file/22722a343513ed45f14905eb07621686-Paper.pdf) - [pytorch implementation](https://github.com/FlouriteJ/PoisonFrogs)  **DFST:** Deep Feature Space Trojan Attack of Neural Networks by Controlled Detoxification  - [paper](https://arxiv.org/abs/2012.11212) - [tensorflow implementation](https://github.com/Megum1/DFST)  **LBA:** Latent Backdoor Attacks on Deep Neural Networks  - [paper](https://people.cs.uchicago.edu/~ravenben/publications/pdf/pbackdoor-ccs19.pdf) - [tensorflow implementation](http://sandlab.cs.uchicago.edu/latent/)  **CBA:** Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features  - [paper](https://dl.acm.org/doi/abs/10.1145/3372297.3423362) - [pytorch implementation](https://github.com/TemporaryAcc0unt/composite-attack)  #### Feature space attack benchmark  `Note`: This repository is the official implementation of [Just How Toxic is Data Poisoning?,pytorch,SOFTWARE
- [pytorch implementation](https://github.com/aks2203/poisoning-benchmark)  #### Library  `Note`: TrojanZoo provides a universal pytorch platform to conduct security researches (especially backdoor attacks/defenses) of image classification in deep learning.,pytorch,SOFTWARE
- [pytorch implementation](https://github.com/aks2203/poisoning-benchmark)  #### Library  `Note`: TrojanZoo provides a universal pytorch platform to conduct security researches (especially backdoor attacks/defenses) of image classification in deep learning.,TrojanZoo,SOFTWARE
- [pytorch implementation](https://github.com/aks2203/poisoning-benchmark)  #### Library  `Note`: TrojanZoo provides a universal pytorch platform to conduct security researches (especially backdoor attacks/defenses) of image classification in deep learning.,pytorch,SOFTWARE
Backdoors 101 ‚Äî is a PyTorch framework for state-of-the-art backdoor defenses and attacks on deep learning models,Backdoors 101,SOFTWARE
Backdoors 101 ‚Äî is a PyTorch framework for state-of-the-art backdoor defenses and attacks on deep learning models,PyTorch,SOFTWARE
"- [trojanzoo](https://github.com/ain-soph/trojanzoo) - [backdoors101](https://github.com/ebagdasa/backdoors101)    ## Citation  If you find the code is useful for your research, please cite our work:  ``` @inproceedings{li2021anti,   title={Anti-Backdoor Learning: Training Clean Models on Poisoned Data},   author={Li, Yige and Lyu, Xixiang and Koren, Nodens and Lyu, Lingjuan and Li, Bo and Ma, Xingjun},   booktitle={NeurIPS},   year={2021} } ```  ## Contacts  Please feel free to drop a message here if you have any questions.",trojanzoo,SOFTWARE
"- [trojanzoo](https://github.com/ain-soph/trojanzoo) - [backdoors101](https://github.com/ebagdasa/backdoors101)    ## Citation  If you find the code is useful for your research, please cite our work:  ``` @inproceedings{li2021anti,   title={Anti-Backdoor Learning: Training Clean Models on Poisoned Data},   author={Li, Yige and Lyu, Xixiang and Koren, Nodens and Lyu, Lingjuan and Li, Bo and Ma, Xingjun},   booktitle={NeurIPS},   year={2021} } ```  ## Contacts  Please feel free to drop a message here if you have any questions.",trojanzoo,SOFTWARE
"- [trojanzoo](https://github.com/ain-soph/trojanzoo) - [backdoors101](https://github.com/ebagdasa/backdoors101)    ## Citation  If you find the code is useful for your research, please cite our work:  ``` @inproceedings{li2021anti,   title={Anti-Backdoor Learning: Training Clean Models on Poisoned Data},   author={Li, Yige and Lyu, Xixiang and Koren, Nodens and Lyu, Lingjuan and Li, Bo and Ma, Xingjun},   booktitle={NeurIPS},   year={2021} } ```  ## Contacts  Please feel free to drop a message here if you have any questions.",backdoors101,SOFTWARE
"- [trojanzoo](https://github.com/ain-soph/trojanzoo) - [backdoors101](https://github.com/ebagdasa/backdoors101)    ## Citation  If you find the code is useful for your research, please cite our work:  ``` @inproceedings{li2021anti,   title={Anti-Backdoor Learning: Training Clean Models on Poisoned Data},   author={Li, Yige and Lyu, Xixiang and Koren, Nodens and Lyu, Lingjuan and Li, Bo and Ma, Xingjun},   booktitle={NeurIPS},   year={2021} } ```  ## Contacts  Please feel free to drop a message here if you have any questions.",backdoors101,SOFTWARE
AryzSDJYB5TxnDWZtpb3ZjL3xBny) to `input/` - Untar the file `tar -xvjf input.tar.bz2`   #### Reproduce Preprocess Steps  1.,tar,SOFTWARE
Download data from [WikiSQL](https://github.com/salesforce/WikiSQL).   ``` $ cd wikisql_data $ wget https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2 $ tar -xvjf data.tar.bz2 ``` 2.,wget,SOFTWARE
Download data from [WikiSQL](https://github.com/salesforce/WikiSQL).   ``` $ cd wikisql_data $ wget https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2 $ tar -xvjf data.tar.bz2 ``` 2.,tar,SOFTWARE
"+ Meta + Sum loss: `model_zoo/meta_sum`   + Base Sum loss: `model_zoo/base_sum`   # Requirements  - Tensorflow 1.4 - python 3.6 - [Stanza](https://github.com/stanfordnlp/stanza)   # Citation  If you use the code in your paper, then please cite it as:    ``` @inproceedings{pshuang2018PT-MAML,   author    = {Po{-}Sen Huang and                Chenglong Wang and                Rishabh Singh and                Wen-tau Yih and                Xiaodong He},   title     = {Natural Language to Structured Query Generation via Meta-Learning},   booktitle = {NAACL},   year      = {2018}, } ```   ``` @inproceedings{2018executionguided,   author    = {Chenglong Wang and                Po{-}Sen Huang and                Alex Polozov and                Marc Brockschmidt and                 Rishabh Singh},   title = ""{Execution-Guided Neural Program Decoding}"",   booktitle = {ICML workshop on Neural Abstract Machines & Program Induction v2 (NAMPI)},   year = {2018} } ```   and   ``` @techreport{chenglong,   author = {Wang, Chenglong and Brockschmidt, Marc and Singh, Rishabh},   title = {Pointing Out {SQL} Queries From Text},   number = {MSR-TR-2017-45},   year = {2017},   month = {November},   url = {https://www.microsoft.com/en-us/research/publication/pointing-sql-queries-text/}, } ```    # Contributing  This project welcomes contributions and suggestions.",Tensorflow 1.4,SOFTWARE
"+ Meta + Sum loss: `model_zoo/meta_sum`   + Base Sum loss: `model_zoo/base_sum`   # Requirements  - Tensorflow 1.4 - python 3.6 - [Stanza](https://github.com/stanfordnlp/stanza)   # Citation  If you use the code in your paper, then please cite it as:    ``` @inproceedings{pshuang2018PT-MAML,   author    = {Po{-}Sen Huang and                Chenglong Wang and                Rishabh Singh and                Wen-tau Yih and                Xiaodong He},   title     = {Natural Language to Structured Query Generation via Meta-Learning},   booktitle = {NAACL},   year      = {2018}, } ```   ``` @inproceedings{2018executionguided,   author    = {Chenglong Wang and                Po{-}Sen Huang and                Alex Polozov and                Marc Brockschmidt and                 Rishabh Singh},   title = ""{Execution-Guided Neural Program Decoding}"",   booktitle = {ICML workshop on Neural Abstract Machines & Program Induction v2 (NAMPI)},   year = {2018} } ```   and   ``` @techreport{chenglong,   author = {Wang, Chenglong and Brockschmidt, Marc and Singh, Rishabh},   title = {Pointing Out {SQL} Queries From Text},   number = {MSR-TR-2017-45},   year = {2017},   month = {November},   url = {https://www.microsoft.com/en-us/research/publication/pointing-sql-queries-text/}, } ```    # Contributing  This project welcomes contributions and suggestions.",Stanza,SOFTWARE
"+ Meta + Sum loss: `model_zoo/meta_sum`   + Base Sum loss: `model_zoo/base_sum`   # Requirements  - Tensorflow 1.4 - python 3.6 - [Stanza](https://github.com/stanfordnlp/stanza)   # Citation  If you use the code in your paper, then please cite it as:    ``` @inproceedings{pshuang2018PT-MAML,   author    = {Po{-}Sen Huang and                Chenglong Wang and                Rishabh Singh and                Wen-tau Yih and                Xiaodong He},   title     = {Natural Language to Structured Query Generation via Meta-Learning},   booktitle = {NAACL},   year      = {2018}, } ```   ``` @inproceedings{2018executionguided,   author    = {Chenglong Wang and                Po{-}Sen Huang and                Alex Polozov and                Marc Brockschmidt and                 Rishabh Singh},   title = ""{Execution-Guided Neural Program Decoding}"",   booktitle = {ICML workshop on Neural Abstract Machines & Program Induction v2 (NAMPI)},   year = {2018} } ```   and   ``` @techreport{chenglong,   author = {Wang, Chenglong and Brockschmidt, Marc and Singh, Rishabh},   title = {Pointing Out {SQL} Queries From Text},   number = {MSR-TR-2017-45},   year = {2017},   month = {November},   url = {https://www.microsoft.com/en-us/research/publication/pointing-sql-queries-text/}, } ```    # Contributing  This project welcomes contributions and suggestions.",stanza,SOFTWARE
"When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment).",CLA-bot,SOFTWARE
"# tcn3r  Reconciliation rules for n-ary relations encoded in RDF triples  A detailed description of the motivation and the algorithms of tcn3r is available in [the related article](https://arxiv.org/pdf/2002.08103.pdf).  ## Citing tcn3r  When citing tcn3r, please use the following reference:  Pierre Monnin, Miguel Couceiro, Amedeo Napoli, and Adrien Coulet.",tcn3r,SOFTWARE
"# tcn3r  Reconciliation rules for n-ary relations encoded in RDF triples  A detailed description of the motivation and the algorithms of tcn3r is available in [the related article](https://arxiv.org/pdf/2002.08103.pdf).  ## Citing tcn3r  When citing tcn3r, please use the following reference:  Pierre Monnin, Miguel Couceiro, Amedeo Napoli, and Adrien Coulet.",tcn3r,SOFTWARE
"# tcn3r  Reconciliation rules for n-ary relations encoded in RDF triples  A detailed description of the motivation and the algorithms of tcn3r is available in [the related article](https://arxiv.org/pdf/2002.08103.pdf).  ## Citing tcn3r  When citing tcn3r, please use the following reference:  Pierre Monnin, Miguel Couceiro, Amedeo Napoli, and Adrien Coulet.",tcn3r,SOFTWARE
"# tcn3r  Reconciliation rules for n-ary relations encoded in RDF triples  A detailed description of the motivation and the algorithms of tcn3r is available in [the related article](https://arxiv.org/pdf/2002.08103.pdf).  ## Citing tcn3r  When citing tcn3r, please use the following reference:  Pierre Monnin, Miguel Couceiro, Amedeo Napoli, and Adrien Coulet.",tcn3r,SOFTWARE
"Springer, 2020, pp. 48‚Äì56. doi: 10.1007/978-3-030- 57855-8_4. url: https://doi.org/10.1007/978-3-030-57855-8_4.  ``` @inproceedings{Monnin2020,   author    = {Pierre Monnin and                Miguel Couceiro and                Amedeo Napoli and                Adrien Coulet},   editor    = {Mehwish Alam and                Tanya Braun and                Bruno Yun},   title     = {Knowledge-Based Matching of n-ary Tuples},   booktitle = {Ontologies and Concepts in Mind and Machine - 25th International Conference                on Conceptual Structures, {ICCS} 2020, Bolzano, Italy, September 18-20,                2020, Proceedings},   series    = {Lecture Notes in Computer Science},   volume    = {12277},   pages     = {48--56},   publisher = {Springer},   year      = {2020},   url       = {https://doi.org/10.1007/978-3-030-57855-8_4},   doi       = {10.1007/978-3-030-57855-8_4}, } ```  ## Execution  ### ``batch`` mode  Executes reconciliation rules on every pair of relationships in the triplestore.  #### Execution (without Docker)  ```bash tcn3r --configuration conf.json -o output.ttl --simlimit SL --complimit CL --dimensionlimit DL --max-rows MR -t threads ```  where:  * *conf.json*: is the configuration file needed to configure the scripts -- see below * *output.ttl*: is the path to the output TTL file where the generated links between relationships will be stored * *SL*: Minimum similarity on non-empty aggregated dimensions to consider relations as related (< 0 to disable) * *CL*: Minimum number of non-empty comparable aggregated dimensions to consider relations as related (< 0 to disable) * *DL*: Minimum number of non-empty aggregated dimensions to apply simlimit or complimit (< 0 to disable) * *MR*: Max number of rows the SPARQL endpoint can return for a query * *threads*: number of threads to use when comparing relations (e.g., 8)  #### Execution (in Docker)  You can use the target ``run`` of the provided Makefile that calls the Docker image with:  ```bash docker run --rm $(MAPUSER) -v ${PWD}/data:/data $(INAME):$(VERSION) --configuration /data/conf.json.example -o /data/output.ttl --simlimit 0.8 --complimit 2 --dimensionlimit 2 --max-rows 10000 --explain false -t 4 ```  The ``data`` subdirectory of the current directory is shared with the Docker container as ``/data``.",Docker,SOFTWARE
"Springer, 2020, pp. 48‚Äì56. doi: 10.1007/978-3-030- 57855-8_4. url: https://doi.org/10.1007/978-3-030-57855-8_4.  ``` @inproceedings{Monnin2020,   author    = {Pierre Monnin and                Miguel Couceiro and                Amedeo Napoli and                Adrien Coulet},   editor    = {Mehwish Alam and                Tanya Braun and                Bruno Yun},   title     = {Knowledge-Based Matching of n-ary Tuples},   booktitle = {Ontologies and Concepts in Mind and Machine - 25th International Conference                on Conceptual Structures, {ICCS} 2020, Bolzano, Italy, September 18-20,                2020, Proceedings},   series    = {Lecture Notes in Computer Science},   volume    = {12277},   pages     = {48--56},   publisher = {Springer},   year      = {2020},   url       = {https://doi.org/10.1007/978-3-030-57855-8_4},   doi       = {10.1007/978-3-030-57855-8_4}, } ```  ## Execution  ### ``batch`` mode  Executes reconciliation rules on every pair of relationships in the triplestore.  #### Execution (without Docker)  ```bash tcn3r --configuration conf.json -o output.ttl --simlimit SL --complimit CL --dimensionlimit DL --max-rows MR -t threads ```  where:  * *conf.json*: is the configuration file needed to configure the scripts -- see below * *output.ttl*: is the path to the output TTL file where the generated links between relationships will be stored * *SL*: Minimum similarity on non-empty aggregated dimensions to consider relations as related (< 0 to disable) * *CL*: Minimum number of non-empty comparable aggregated dimensions to consider relations as related (< 0 to disable) * *DL*: Minimum number of non-empty aggregated dimensions to apply simlimit or complimit (< 0 to disable) * *MR*: Max number of rows the SPARQL endpoint can return for a query * *threads*: number of threads to use when comparing relations (e.g., 8)  #### Execution (in Docker)  You can use the target ``run`` of the provided Makefile that calls the Docker image with:  ```bash docker run --rm $(MAPUSER) -v ${PWD}/data:/data $(INAME):$(VERSION) --configuration /data/conf.json.example -o /data/output.ttl --simlimit 0.8 --complimit 2 --dimensionlimit 2 --max-rows 10000 --explain false -t 4 ```  The ``data`` subdirectory of the current directory is shared with the Docker container as ``/data``.",tcn3r,SOFTWARE
"Springer, 2020, pp. 48‚Äì56. doi: 10.1007/978-3-030- 57855-8_4. url: https://doi.org/10.1007/978-3-030-57855-8_4.  ``` @inproceedings{Monnin2020,   author    = {Pierre Monnin and                Miguel Couceiro and                Amedeo Napoli and                Adrien Coulet},   editor    = {Mehwish Alam and                Tanya Braun and                Bruno Yun},   title     = {Knowledge-Based Matching of n-ary Tuples},   booktitle = {Ontologies and Concepts in Mind and Machine - 25th International Conference                on Conceptual Structures, {ICCS} 2020, Bolzano, Italy, September 18-20,                2020, Proceedings},   series    = {Lecture Notes in Computer Science},   volume    = {12277},   pages     = {48--56},   publisher = {Springer},   year      = {2020},   url       = {https://doi.org/10.1007/978-3-030-57855-8_4},   doi       = {10.1007/978-3-030-57855-8_4}, } ```  ## Execution  ### ``batch`` mode  Executes reconciliation rules on every pair of relationships in the triplestore.  #### Execution (without Docker)  ```bash tcn3r --configuration conf.json -o output.ttl --simlimit SL --complimit CL --dimensionlimit DL --max-rows MR -t threads ```  where:  * *conf.json*: is the configuration file needed to configure the scripts -- see below * *output.ttl*: is the path to the output TTL file where the generated links between relationships will be stored * *SL*: Minimum similarity on non-empty aggregated dimensions to consider relations as related (< 0 to disable) * *CL*: Minimum number of non-empty comparable aggregated dimensions to consider relations as related (< 0 to disable) * *DL*: Minimum number of non-empty aggregated dimensions to apply simlimit or complimit (< 0 to disable) * *MR*: Max number of rows the SPARQL endpoint can return for a query * *threads*: number of threads to use when comparing relations (e.g., 8)  #### Execution (in Docker)  You can use the target ``run`` of the provided Makefile that calls the Docker image with:  ```bash docker run --rm $(MAPUSER) -v ${PWD}/data:/data $(INAME):$(VERSION) --configuration /data/conf.json.example -o /data/output.ttl --simlimit 0.8 --complimit 2 --dimensionlimit 2 --max-rows 10000 --explain false -t 4 ```  The ``data`` subdirectory of the current directory is shared with the Docker container as ``/data``.",Docker,SOFTWARE
"Springer, 2020, pp. 48‚Äì56. doi: 10.1007/978-3-030- 57855-8_4. url: https://doi.org/10.1007/978-3-030-57855-8_4.  ``` @inproceedings{Monnin2020,   author    = {Pierre Monnin and                Miguel Couceiro and                Amedeo Napoli and                Adrien Coulet},   editor    = {Mehwish Alam and                Tanya Braun and                Bruno Yun},   title     = {Knowledge-Based Matching of n-ary Tuples},   booktitle = {Ontologies and Concepts in Mind and Machine - 25th International Conference                on Conceptual Structures, {ICCS} 2020, Bolzano, Italy, September 18-20,                2020, Proceedings},   series    = {Lecture Notes in Computer Science},   volume    = {12277},   pages     = {48--56},   publisher = {Springer},   year      = {2020},   url       = {https://doi.org/10.1007/978-3-030-57855-8_4},   doi       = {10.1007/978-3-030-57855-8_4}, } ```  ## Execution  ### ``batch`` mode  Executes reconciliation rules on every pair of relationships in the triplestore.  #### Execution (without Docker)  ```bash tcn3r --configuration conf.json -o output.ttl --simlimit SL --complimit CL --dimensionlimit DL --max-rows MR -t threads ```  where:  * *conf.json*: is the configuration file needed to configure the scripts -- see below * *output.ttl*: is the path to the output TTL file where the generated links between relationships will be stored * *SL*: Minimum similarity on non-empty aggregated dimensions to consider relations as related (< 0 to disable) * *CL*: Minimum number of non-empty comparable aggregated dimensions to consider relations as related (< 0 to disable) * *DL*: Minimum number of non-empty aggregated dimensions to apply simlimit or complimit (< 0 to disable) * *MR*: Max number of rows the SPARQL endpoint can return for a query * *threads*: number of threads to use when comparing relations (e.g., 8)  #### Execution (in Docker)  You can use the target ``run`` of the provided Makefile that calls the Docker image with:  ```bash docker run --rm $(MAPUSER) -v ${PWD}/data:/data $(INAME):$(VERSION) --configuration /data/conf.json.example -o /data/output.ttl --simlimit 0.8 --complimit 2 --dimensionlimit 2 --max-rows 10000 --explain false -t 4 ```  The ``data`` subdirectory of the current directory is shared with the Docker container as ``/data``.",Docker,SOFTWARE
"Springer, 2020, pp. 48‚Äì56. doi: 10.1007/978-3-030- 57855-8_4. url: https://doi.org/10.1007/978-3-030-57855-8_4.  ``` @inproceedings{Monnin2020,   author    = {Pierre Monnin and                Miguel Couceiro and                Amedeo Napoli and                Adrien Coulet},   editor    = {Mehwish Alam and                Tanya Braun and                Bruno Yun},   title     = {Knowledge-Based Matching of n-ary Tuples},   booktitle = {Ontologies and Concepts in Mind and Machine - 25th International Conference                on Conceptual Structures, {ICCS} 2020, Bolzano, Italy, September 18-20,                2020, Proceedings},   series    = {Lecture Notes in Computer Science},   volume    = {12277},   pages     = {48--56},   publisher = {Springer},   year      = {2020},   url       = {https://doi.org/10.1007/978-3-030-57855-8_4},   doi       = {10.1007/978-3-030-57855-8_4}, } ```  ## Execution  ### ``batch`` mode  Executes reconciliation rules on every pair of relationships in the triplestore.  #### Execution (without Docker)  ```bash tcn3r --configuration conf.json -o output.ttl --simlimit SL --complimit CL --dimensionlimit DL --max-rows MR -t threads ```  where:  * *conf.json*: is the configuration file needed to configure the scripts -- see below * *output.ttl*: is the path to the output TTL file where the generated links between relationships will be stored * *SL*: Minimum similarity on non-empty aggregated dimensions to consider relations as related (< 0 to disable) * *CL*: Minimum number of non-empty comparable aggregated dimensions to consider relations as related (< 0 to disable) * *DL*: Minimum number of non-empty aggregated dimensions to apply simlimit or complimit (< 0 to disable) * *MR*: Max number of rows the SPARQL endpoint can return for a query * *threads*: number of threads to use when comparing relations (e.g., 8)  #### Execution (in Docker)  You can use the target ``run`` of the provided Makefile that calls the Docker image with:  ```bash docker run --rm $(MAPUSER) -v ${PWD}/data:/data $(INAME):$(VERSION) --configuration /data/conf.json.example -o /data/output.ttl --simlimit 0.8 --complimit 2 --dimensionlimit 2 --max-rows 10000 --explain false -t 4 ```  The ``data`` subdirectory of the current directory is shared with the Docker container as ``/data``.",docker,SOFTWARE
"Springer, 2020, pp. 48‚Äì56. doi: 10.1007/978-3-030- 57855-8_4. url: https://doi.org/10.1007/978-3-030-57855-8_4.  ``` @inproceedings{Monnin2020,   author    = {Pierre Monnin and                Miguel Couceiro and                Amedeo Napoli and                Adrien Coulet},   editor    = {Mehwish Alam and                Tanya Braun and                Bruno Yun},   title     = {Knowledge-Based Matching of n-ary Tuples},   booktitle = {Ontologies and Concepts in Mind and Machine - 25th International Conference                on Conceptual Structures, {ICCS} 2020, Bolzano, Italy, September 18-20,                2020, Proceedings},   series    = {Lecture Notes in Computer Science},   volume    = {12277},   pages     = {48--56},   publisher = {Springer},   year      = {2020},   url       = {https://doi.org/10.1007/978-3-030-57855-8_4},   doi       = {10.1007/978-3-030-57855-8_4}, } ```  ## Execution  ### ``batch`` mode  Executes reconciliation rules on every pair of relationships in the triplestore.  #### Execution (without Docker)  ```bash tcn3r --configuration conf.json -o output.ttl --simlimit SL --complimit CL --dimensionlimit DL --max-rows MR -t threads ```  where:  * *conf.json*: is the configuration file needed to configure the scripts -- see below * *output.ttl*: is the path to the output TTL file where the generated links between relationships will be stored * *SL*: Minimum similarity on non-empty aggregated dimensions to consider relations as related (< 0 to disable) * *CL*: Minimum number of non-empty comparable aggregated dimensions to consider relations as related (< 0 to disable) * *DL*: Minimum number of non-empty aggregated dimensions to apply simlimit or complimit (< 0 to disable) * *MR*: Max number of rows the SPARQL endpoint can return for a query * *threads*: number of threads to use when comparing relations (e.g., 8)  #### Execution (in Docker)  You can use the target ``run`` of the provided Makefile that calls the Docker image with:  ```bash docker run --rm $(MAPUSER) -v ${PWD}/data:/data $(INAME):$(VERSION) --configuration /data/conf.json.example -o /data/output.ttl --simlimit 0.8 --complimit 2 --dimensionlimit 2 --max-rows 10000 --explain false -t 4 ```  The ``data`` subdirectory of the current directory is shared with the Docker container as ``/data``.",Docker,SOFTWARE
"``simlimit`` is set to 0.8, ``complimit`` to 2 and ``dimensionlimit`` is set to 2. 4 threads will be used.  ### ``explain`` mode  #### Execution (without Docker)  ```bash tcn3r --configuration conf.json -o output.ttl --simlimit SL --complimit CL --dimensionlimit DL --max-rows MR --explain true ```  where:  * *conf.json*: is the configuration file needed to configure the scripts -- see below * *output.txt*: is the path to the output text file where explanations of links between relationships will be stored * *SL*: Minimum similarity on non-empty aggregated dimensions to consider relations as related (< 0 to disable) * *CL*: Minimum number of non-empty comparable aggregated dimensions to consider relations as related (< 0 to disable) * *DL*: Minimum number of non-empty aggregated dimensions to apply simlimit or complimit (< 0 to disable) * *MR*: Max number of rows the SPARQL endpoint can return for a query  URIs of relations to compare will be asked interactively.  #### Execution (in Docker)  Not available.  ## Input  ### Configuration JSON file  A configuration JSON file is needed to configure the scripts.",Docker,SOFTWARE
"``simlimit`` is set to 0.8, ``complimit`` to 2 and ``dimensionlimit`` is set to 2. 4 threads will be used.  ### ``explain`` mode  #### Execution (without Docker)  ```bash tcn3r --configuration conf.json -o output.ttl --simlimit SL --complimit CL --dimensionlimit DL --max-rows MR --explain true ```  where:  * *conf.json*: is the configuration file needed to configure the scripts -- see below * *output.txt*: is the path to the output text file where explanations of links between relationships will be stored * *SL*: Minimum similarity on non-empty aggregated dimensions to consider relations as related (< 0 to disable) * *CL*: Minimum number of non-empty comparable aggregated dimensions to consider relations as related (< 0 to disable) * *DL*: Minimum number of non-empty aggregated dimensions to apply simlimit or complimit (< 0 to disable) * *MR*: Max number of rows the SPARQL endpoint can return for a query  URIs of relations to compare will be asked interactively.  #### Execution (in Docker)  Not available.  ## Input  ### Configuration JSON file  A configuration JSON file is needed to configure the scripts.",tcn3r,SOFTWARE
"``simlimit`` is set to 0.8, ``complimit`` to 2 and ``dimensionlimit`` is set to 2. 4 threads will be used.  ### ``explain`` mode  #### Execution (without Docker)  ```bash tcn3r --configuration conf.json -o output.ttl --simlimit SL --complimit CL --dimensionlimit DL --max-rows MR --explain true ```  where:  * *conf.json*: is the configuration file needed to configure the scripts -- see below * *output.txt*: is the path to the output text file where explanations of links between relationships will be stored * *SL*: Minimum similarity on non-empty aggregated dimensions to consider relations as related (< 0 to disable) * *CL*: Minimum number of non-empty comparable aggregated dimensions to consider relations as related (< 0 to disable) * *DL*: Minimum number of non-empty aggregated dimensions to apply simlimit or complimit (< 0 to disable) * *MR*: Max number of rows the SPARQL endpoint can return for a query  URIs of relations to compare will be asked interactively.  #### Execution (in Docker)  Not available.  ## Input  ### Configuration JSON file  A configuration JSON file is needed to configure the scripts.",Docker,SOFTWARE
Subproperties will be considered as well. * _output-pred-equal_: URI of a predicate to use to identify equal relationships * _output-pred-equiv_: URI of a predicate to use to identify equivalent relationships * _output-pred-leq_: URI of a predicate to use to identify lower or equal relationships * _output-pred-geq_: URI of a predicate to use to identify greater or equal relationships * _output-pred-comparable_: URI of a predicate to use to identify comparable relationships * _output-do-related-predicate_: URI of a predicate to use to identify relationships that are related  ## Tests  A [test/pgxo+test.owl](test/pgxo+test.owl) file is available to test tcn3r.,tcn3r,SOFTWARE
"After the import, run tcn3r using the test configuration provided in [test/test-conf.json](test/test-conf.json) and the following parameters ``--simlimit 0.85 --complimit 3 --dimensionlimit 3``.",tcn3r,SOFTWARE
The (sorted) expected results can be found in [test/expected-output.ttl](test/expected-output.ttl).  ## Dependencies  * C++17 * boost * libcurl * OpenMP,C++17,SOFTWARE
The (sorted) expected results can be found in [test/expected-output.ttl](test/expected-output.ttl).  ## Dependencies  * C++17 * boost * libcurl * OpenMP,boost,SOFTWARE
The (sorted) expected results can be found in [test/expected-output.ttl](test/expected-output.ttl).  ## Dependencies  * C++17 * boost * libcurl * OpenMP,libcurl,SOFTWARE
The (sorted) expected results can be found in [test/expected-output.ttl](test/expected-output.ttl).  ## Dependencies  * C++17 * boost * libcurl * OpenMP,OpenMP,SOFTWARE
# OnML (Ontology-based Interpretable Machine Learning for Textual Data)  This is the official implementation of the [Ontology-based Interpretable Machine Learning for Textual Data](https://arxiv.org/pdf/2004.00204.pdf) in Tensorflow.,OnML,SOFTWARE
# OnML (Ontology-based Interpretable Machine Learning for Textual Data)  This is the official implementation of the [Ontology-based Interpretable Machine Learning for Textual Data](https://arxiv.org/pdf/2004.00204.pdf) in Tensorflow.,Tensorflow,SOFTWARE
"If you use this code or our results in your research, please cite as appropriate:  ``` @article{lai2020ontology,   title={Ontology-based Interpretable Machine Learning for Textual Data},   author={Lai, Phung and Phan, NhatHai and Hu, Han and Badeti, Anuja and Newman, David and Dou, Dejing},   journal={International Joint Conference on Neural Networks},   year={2020} } ```   ## Software Requirements  Python 3.5 is used for the current codebase.",Python 3.5,SOFTWARE
Tensorflow 1.1 or later (Tensor 1 only)  Prot√©g√© for Ontology   ## Experiments The repository comes with instructions to reproduce the results in the paper or to train the model from scratch:  To view Ontology: + ConSo and DrugAO ontologies used in the paper are provided in folder `ontologies`. + Open ontology locally by [Prot√©g√©](https://protege.stanford.edu/products.php). + Open ontology online by importing the ontology on [WebVOWL](http://vowl.visualdataweb.org/webvowl.html).,Tensorflow 1.1,SOFTWARE
Tensorflow 1.1 or later (Tensor 1 only)  Prot√©g√© for Ontology   ## Experiments The repository comes with instructions to reproduce the results in the paper or to train the model from scratch:  To view Ontology: + ConSo and DrugAO ontologies used in the paper are provided in folder `ontologies`. + Open ontology locally by [Prot√©g√©](https://protege.stanford.edu/products.php). + Open ontology online by importing the ontology on [WebVOWL](http://vowl.visualdataweb.org/webvowl.html).,Prot√©g√©,SOFTWARE
Tensorflow 1.1 or later (Tensor 1 only)  Prot√©g√© for Ontology   ## Experiments The repository comes with instructions to reproduce the results in the paper or to train the model from scratch:  To view Ontology: + ConSo and DrugAO ontologies used in the paper are provided in folder `ontologies`. + Open ontology locally by [Prot√©g√©](https://protege.stanford.edu/products.php). + Open ontology online by importing the ontology on [WebVOWL](http://vowl.visualdataweb.org/webvowl.html).,Prot√©g√©,SOFTWARE
Tensorflow 1.1 or later (Tensor 1 only)  Prot√©g√© for Ontology   ## Experiments The repository comes with instructions to reproduce the results in the paper or to train the model from scratch:  To view Ontology: + ConSo and DrugAO ontologies used in the paper are provided in folder `ontologies`. + Open ontology locally by [Prot√©g√©](https://protege.stanford.edu/products.php). + Open ontology online by importing the ontology on [WebVOWL](http://vowl.visualdataweb.org/webvowl.html).,protege,SOFTWARE
Tensorflow 1.1 or later (Tensor 1 only)  Prot√©g√© for Ontology   ## Experiments The repository comes with instructions to reproduce the results in the paper or to train the model from scratch:  To view Ontology: + ConSo and DrugAO ontologies used in the paper are provided in folder `ontologies`. + Open ontology locally by [Prot√©g√©](https://protege.stanford.edu/products.php). + Open ontology online by importing the ontology on [WebVOWL](http://vowl.visualdataweb.org/webvowl.html).,WebVOWL,SOFTWARE
Tensorflow 1.1 or later (Tensor 1 only)  Prot√©g√© for Ontology   ## Experiments The repository comes with instructions to reproduce the results in the paper or to train the model from scratch:  To view Ontology: + ConSo and DrugAO ontologies used in the paper are provided in folder `ontologies`. + Open ontology locally by [Prot√©g√©](https://protege.stanford.edu/products.php). + Open ontology online by importing the ontology on [WebVOWL](http://vowl.visualdataweb.org/webvowl.html).,webvowl,SOFTWARE
"To reproduce the results: + Clone or download the folder from this repository. + Some large-size data or pretrained models of any file needed to run, if you cannot find it here, please find it on [Google Drive folder](https://drive.google.com/drive/folders/17w6RLR5pTG8BfXN-039YWBMnJWrYGKmK?",Google Drive,SOFTWARE
"The common errors and how to solve it are as follows:  + `module has no attribute ""placeholder""`: This is because you are using Tensorflow 2, while the code is using Tensorflow 1.",Tensorflow 2,SOFTWARE
"The common errors and how to solve it are as follows:  + `module has no attribute ""placeholder""`: This is because you are using Tensorflow 2, while the code is using Tensorflow 1.",Tensorflow 1,SOFTWARE
"+ `object of type ""NoneType"" has no len`: I guess you are using Python 3.7.",Python 3.7,SOFTWARE
"Note that this code uses part of [LIME](https://github.com/marcotcr/lime) for visualization, and that visualization does not work with Python 3.7  ([error](https://github.com/marcotcr/lime/issues/294)).",LIME,SOFTWARE
"Note that this code uses part of [LIME](https://github.com/marcotcr/lime) for visualization, and that visualization does not work with Python 3.7  ([error](https://github.com/marcotcr/lime/issues/294)).",lime,SOFTWARE
"Note that this code uses part of [LIME](https://github.com/marcotcr/lime) for visualization, and that visualization does not work with Python 3.7  ([error](https://github.com/marcotcr/lime/issues/294)).",Python 3.7,SOFTWARE
"Note that this code uses part of [LIME](https://github.com/marcotcr/lime) for visualization, and that visualization does not work with Python 3.7  ([error](https://github.com/marcotcr/lime/issues/294)).",lime,SOFTWARE
"So I recommend you to use Python3 < 3.7 to be able to run the whole code; otherwise, you can run OnML but no visualization.",Python3,SOFTWARE
"So I recommend you to use Python3 < 3.7 to be able to run the whole code; otherwise, you can run OnML but no visualization.",OnML,SOFTWARE
"Create an environment by: `conda create -n name-of-environment python=version-of-python`, e.g., conda create -n my_py37 python=3.7 2.",conda,SOFTWARE
"Create an environment by: `conda create -n name-of-environment python=version-of-python`, e.g., conda create -n my_py37 python=3.7 2.",python,SOFTWARE
"Create an environment by: `conda create -n name-of-environment python=version-of-python`, e.g., conda create -n my_py37 python=3.7 2.",python,SOFTWARE
"Create an environment by: `conda create -n name-of-environment python=version-of-python`, e.g., conda create -n my_py37 python=3.7 2.",conda,SOFTWARE
"Create an environment by: `conda create -n name-of-environment python=version-of-python`, e.g., conda create -n my_py37 python=3.7 2.",python=3.7 2,SOFTWARE
"Activate the created environment by: `conda activate name-of-environment`, e.g., conda activate my_py37 3.",conda,SOFTWARE
"Activate the created environment by: `conda activate name-of-environment`, e.g., conda activate my_py37 3.",conda,SOFTWARE
"Deactivate the environment after using: `conda deactivate`  ## Potential issues  If you have any issues while running the code or further information, please send email directly to the first author of this paper (`tl353@njit.edu`).",conda,SOFTWARE
More information about the categories available in **annotation_framework.pdf**  ## Pretrained Model The SciBert baseline classifier trained in section 3 of the paper can be used for fine-tuning or inference following the instructions at [SciTweets_SciBert @ Huggingface](https://huggingface.co/sschellhammer/SciTweets_SciBert).  ## Publication: <!,SciTweets_SciBert,SOFTWARE
More information about the categories available in **annotation_framework.pdf**  ## Pretrained Model The SciBert baseline classifier trained in section 3 of the paper can be used for fine-tuning or inference following the instructions at [SciTweets_SciBert @ Huggingface](https://huggingface.co/sschellhammer/SciTweets_SciBert).  ## Publication: <!,sschellhammer/SciTweets_SciBert,SOFTWARE
"v=VAuz25w0a5k**    # ML-Quadrat (ML2)  ML2 is a Computer-Aided Software Engineering (CASE) tool, based on the Model-Driven Software Engineering (MDSE) paradigm, specifically the Domain-Specific Modeling (DSM) methodology with full code generation.",ML-Quadrat,SOFTWARE
"v=VAuz25w0a5k**    # ML-Quadrat (ML2)  ML2 is a Computer-Aided Software Engineering (CASE) tool, based on the Model-Driven Software Engineering (MDSE) paradigm, specifically the Domain-Specific Modeling (DSM) methodology with full code generation.",ML2,SOFTWARE
"v=VAuz25w0a5k**    # ML-Quadrat (ML2)  ML2 is a Computer-Aided Software Engineering (CASE) tool, based on the Model-Driven Software Engineering (MDSE) paradigm, specifically the Domain-Specific Modeling (DSM) methodology with full code generation.",ML2,SOFTWARE
ML2 is focused on creating smart services for the Internet of Things (IoT) and Cyber-Physical Systems (CPS).,ML2,SOFTWARE
"Similar to ThingML/HEADS, ML2 is built using the [Eclipse Modeling Framework (EMF)](https://www.eclipse.org/modeling/emf/), as well as the [Xtext framework](https://www.eclipse.org/Xtext/), and is released under the terms of the Apache 2.0 permissive open source license.",ML2,SOFTWARE
"However, in what follows, we refer to the project name as ML2 for simplicity.    ## Why ML2?",ML2,SOFTWARE
"In other words, the practitioner has access to the APIs of ML libraries and frameworks, such as [Scikit-Learn](https://scikit-learn.org/stable/), [Keras](https://keras.io)/[TensorFlow](https://www.tensorflow.org), [PyTorch](https://pytorch.org) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) at the modeling layer.",Scikit-Learn,SOFTWARE
"In other words, the practitioner has access to the APIs of ML libraries and frameworks, such as [Scikit-Learn](https://scikit-learn.org/stable/), [Keras](https://keras.io)/[TensorFlow](https://www.tensorflow.org), [PyTorch](https://pytorch.org) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) at the modeling layer.",scikit-learn,SOFTWARE
"In other words, the practitioner has access to the APIs of ML libraries and frameworks, such as [Scikit-Learn](https://scikit-learn.org/stable/), [Keras](https://keras.io)/[TensorFlow](https://www.tensorflow.org), [PyTorch](https://pytorch.org) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) at the modeling layer.",Keras,SOFTWARE
"In other words, the practitioner has access to the APIs of ML libraries and frameworks, such as [Scikit-Learn](https://scikit-learn.org/stable/), [Keras](https://keras.io)/[TensorFlow](https://www.tensorflow.org), [PyTorch](https://pytorch.org) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) at the modeling layer.",keras,SOFTWARE
"In other words, the practitioner has access to the APIs of ML libraries and frameworks, such as [Scikit-Learn](https://scikit-learn.org/stable/), [Keras](https://keras.io)/[TensorFlow](https://www.tensorflow.org), [PyTorch](https://pytorch.org) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) at the modeling layer.",TensorFlow,SOFTWARE
"In other words, the practitioner has access to the APIs of ML libraries and frameworks, such as [Scikit-Learn](https://scikit-learn.org/stable/), [Keras](https://keras.io)/[TensorFlow](https://www.tensorflow.org), [PyTorch](https://pytorch.org) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) at the modeling layer.",tensorflow,SOFTWARE
"In other words, the practitioner has access to the APIs of ML libraries and frameworks, such as [Scikit-Learn](https://scikit-learn.org/stable/), [Keras](https://keras.io)/[TensorFlow](https://www.tensorflow.org), [PyTorch](https://pytorch.org) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) at the modeling layer.",PyTorch,SOFTWARE
"In other words, the practitioner has access to the APIs of ML libraries and frameworks, such as [Scikit-Learn](https://scikit-learn.org/stable/), [Keras](https://keras.io)/[TensorFlow](https://www.tensorflow.org), [PyTorch](https://pytorch.org) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) at the modeling layer.",pytorch,SOFTWARE
"In other words, the practitioner has access to the APIs of ML libraries and frameworks, such as [Scikit-Learn](https://scikit-learn.org/stable/), [Keras](https://keras.io)/[TensorFlow](https://www.tensorflow.org), [PyTorch](https://pytorch.org) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) at the modeling layer.",WEKA,SOFTWARE
"In other words, the practitioner has access to the APIs of ML libraries and frameworks, such as [Scikit-Learn](https://scikit-learn.org/stable/), [Keras](https://keras.io)/[TensorFlow](https://www.tensorflow.org), [PyTorch](https://pytorch.org) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) at the modeling layer.",weka,SOFTWARE
This mode offers a lot of flexibility since the practitioner is not limited to the ML models/algorithms/methods/techniques that are already supported by the DSML of ML2.,ML2,SOFTWARE
[How to Cite ML2 in Your Publications](#citation)  2.,ML2,SOFTWARE
"How to Cite ML2 in Your Publications  Please cite the following journal paper if you are using ML2 or referring to it in your papers:    [**A model-driven approach to machine learning and software modeling for the IoT:** Generating full source code for smart Internet of Things (IoT) services and cyber-physical systems (CPS), Armin Moin, Moharram Challenger, Atta Badii and Stephan G√ºnnemann, Software and Systems Modeling (SoSyM), January 2022.]",ML2,SOFTWARE
"How to Cite ML2 in Your Publications  Please cite the following journal paper if you are using ML2 or referring to it in your papers:    [**A model-driven approach to machine learning and software modeling for the IoT:** Generating full source code for smart Internet of Things (IoT) services and cyber-physical systems (CPS), Armin Moin, Moharram Challenger, Atta Badii and Stephan G√ºnnemann, Software and Systems Modeling (SoSyM), January 2022.]",ML2,SOFTWARE
"(https://link.springer.com/article/10.1007/s10270-021-00967-x)  BibTeX:  ```  @article{Moin+2022-SoSyM,   author = {Armin Moin and Moharram Challenger and Atta Badii and Stephan G{\""u}nnemann},   date = {2022/01/19},   doi = {10.1007/s10270-021-00967-x},   isbn = {1619-1374},   journal = {Software and Systems Modeling (SoSyM)},   title = {A model-driven approach to machine learning and software modeling for the {IoT}},   url = {https://doi.org/10.1007/s10270-021-00967-x},   year = {2022},  }  ```    <a name=""issues""></a>  ## 2.",BibTeX,SOFTWARE
Reporting Issues/Bugs and Requests  ML2 is a research prototype.,ML2,SOFTWARE
"If you find any issues/bugs or have any feature request, please kindly report that through our issue tracking system: https://github.com/arminmoin/ML-Quadrat/issues    [Back to top](#toc)    <a name=""user-doc-quick""></a>  ## 3.",ML-Quadrat,SOFTWARE
"Quick (15 mins) Tutorial  Here, we provide a quick tutorial to get familiar with ML2.",ML2,SOFTWARE
"However, in order to read the full documentation, please go to the section [ML2 Users' Documentation (Full Tutorial)](#user-doc) below.    ### How to install ML2?",ML2,SOFTWARE
"However, in order to read the full documentation, please go to the section [ML2 Users' Documentation (Full Tutorial)](#user-doc) below.    ### How to install ML2?",ML2,SOFTWARE
"In this tutorial, we use a x86_64 Linux system with the Ubuntu 20.04.2 LTS (focal) operating system.",Linux,SOFTWARE
"In this tutorial, we use a x86_64 Linux system with the Ubuntu 20.04.2 LTS (focal) operating system.",Ubuntu 20.04.2 LTS,SOFTWARE
Please install the following software before proceeding with the installation of ML2 (use ```sudo su``` to run the commands as root and at the end ```exit``` in order to get back to the non-root user):    1.,ML2,SOFTWARE
[Git](https://git-scm.com/):   ```  sudo su  apt-get install git  ```  2.,Git,SOFTWARE
[Git](https://git-scm.com/):   ```  sudo su  apt-get install git  ```  2.,git,SOFTWARE
[Git](https://git-scm.com/):   ```  sudo su  apt-get install git  ```  2.,sudo,SOFTWARE
[Git](https://git-scm.com/):   ```  sudo su  apt-get install git  ```  2.,git,SOFTWARE
[The Java Runtime Environment (JRE) and the Java Development Kit (JDK)](https://www.java.com/en/download/):   ```  apt-get install default-jre default-jdk  ```  3.,JRE,SOFTWARE
[The Java Runtime Environment (JRE) and the Java Development Kit (JDK)](https://www.java.com/en/download/):   ```  apt-get install default-jre default-jdk  ```  3.,Java Development Kit,SOFTWARE
[The Java Runtime Environment (JRE) and the Java Development Kit (JDK)](https://www.java.com/en/download/):   ```  apt-get install default-jre default-jdk  ```  3.,JDK,SOFTWARE
[The Java Runtime Environment (JRE) and the Java Development Kit (JDK)](https://www.java.com/en/download/):   ```  apt-get install default-jre default-jdk  ```  3.,java,SOFTWARE
"[Apache Maven](https://maven.apache.org/):   ```  apt-get install maven  exit  ```    **Now, please follow the steps below in the Linux terminal / shell / command line:**    1.",Apache Maven,SOFTWARE
"[Apache Maven](https://maven.apache.org/):   ```  apt-get install maven  exit  ```    **Now, please follow the steps below in the Linux terminal / shell / command line:**    1.",maven,SOFTWARE
"[Apache Maven](https://maven.apache.org/):   ```  apt-get install maven  exit  ```    **Now, please follow the steps below in the Linux terminal / shell / command line:**    1.",maven,SOFTWARE
"[Apache Maven](https://maven.apache.org/):   ```  apt-get install maven  exit  ```    **Now, please follow the steps below in the Linux terminal / shell / command line:**    1.",Linux,SOFTWARE
Check out the source code from the Git repository:  ```bash  git clone https://github.com/arminmoin/ML-Quadrat/  ```    2.,Git,SOFTWARE
Check out the source code from the Git repository:  ```bash  git clone https://github.com/arminmoin/ML-Quadrat/  ```    2.,git,SOFTWARE
Check out the source code from the Git repository:  ```bash  git clone https://github.com/arminmoin/ML-Quadrat/  ```    2.,ML-Quadrat,SOFTWARE
"Install ML2 using the Apache Maven:  ```bash  cd ML-Quadrat  mvn clean install -DskipTests  cd ML2/language  mvn clean install -DskipTests  cd ../..  ```    Note that the -DskipTests option lets us skip running the tests, thus saving more time.",ML2,SOFTWARE
"Install ML2 using the Apache Maven:  ```bash  cd ML-Quadrat  mvn clean install -DskipTests  cd ML2/language  mvn clean install -DskipTests  cd ../..  ```    Note that the -DskipTests option lets us skip running the tests, thus saving more time.",Apache Maven,SOFTWARE
"Install ML2 using the Apache Maven:  ```bash  cd ML-Quadrat  mvn clean install -DskipTests  cd ML2/language  mvn clean install -DskipTests  cd ../..  ```    Note that the -DskipTests option lets us skip running the tests, thus saving more time.",ML-Quadrat,SOFTWARE
"Install ML2 using the Apache Maven:  ```bash  cd ML-Quadrat  mvn clean install -DskipTests  cd ML2/language  mvn clean install -DskipTests  cd ../..  ```    Note that the -DskipTests option lets us skip running the tests, thus saving more time.",ML2,SOFTWARE
"If you want to see a more detailed output to debug, use the option -X:    ```bash  mvn clean install -DskipTests -X  ```    Moreover, if you want to use Maven in the offline mode, e.g., in the case that your machine is behind a firewall that prohibits the Internet access, you should use the option -o, but, remember that you would need to first copy the .m2 directory, which includes the Maven cache from another computer behind the firewall (on which you have already cached ¬¥the required dependencies) to this computer.",Maven,SOFTWARE
"If you want to see a more detailed output to debug, use the option -X:    ```bash  mvn clean install -DskipTests -X  ```    Moreover, if you want to use Maven in the offline mode, e.g., in the case that your machine is behind a firewall that prohibits the Internet access, you should use the option -o, but, remember that you would need to first copy the .m2 directory, which includes the Maven cache from another computer behind the firewall (on which you have already cached ¬¥the required dependencies) to this computer.",Maven,SOFTWARE
"There exist a number of sample model instances with the .thingml extension at this location: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml    Let's choose [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) for this quick tutorial, and generate, e.g., the Python and Java source code out of it using the Python_Java model-to-code transformation (a.k.a. code generator or ""compiler"").",ML-Quadrat,SOFTWARE
"There exist a number of sample model instances with the .thingml extension at this location: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml    Let's choose [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) for this quick tutorial, and generate, e.g., the Python and Java source code out of it using the Python_Java model-to-code transformation (a.k.a. code generator or ""compiler"").",ML2,SOFTWARE
"There exist a number of sample model instances with the .thingml extension at this location: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml    Let's choose [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) for this quick tutorial, and generate, e.g., the Python and Java source code out of it using the Python_Java model-to-code transformation (a.k.a. code generator or ""compiler"").",ML2,SOFTWARE
"There exist a number of sample model instances with the .thingml extension at this location: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml    Let's choose [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) for this quick tutorial, and generate, e.g., the Python and Java source code out of it using the Python_Java model-to-code transformation (a.k.a. code generator or ""compiler"").",ML-Quadrat,SOFTWARE
"There exist a number of sample model instances with the .thingml extension at this location: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml    Let's choose [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) for this quick tutorial, and generate, e.g., the Python and Java source code out of it using the Python_Java model-to-code transformation (a.k.a. code generator or ""compiler"").",ML2,SOFTWARE
"However, in ML2, this example is enhanced in order to make the server a bit ""smarter""!",ML2,SOFTWARE
"Note that the software model instances in ML2 have the .thingml extension, similar to the ThingML/HEADS projects, although the meta-models/grammars of the DSMLs are different (ML2 is backward-compatible).      ### How to generate code out of the sample model instance?",ML2,SOFTWARE
Run the following commands in the Linux terminal:  ```bash  cd ML2/compilers/registry/target  java -jar mlquadrat.compilers.registry-2.0.0-SNAPSHOT-jar-with-dependencies.jar -c auto -s ../../..,Linux,SOFTWARE
Run the following commands in the Linux terminal:  ```bash  cd ML2/compilers/registry/target  java -jar mlquadrat.compilers.registry-2.0.0-SNAPSHOT-jar-with-dependencies.jar -c auto -s ../../..,ML2,SOFTWARE
Run the following commands in the Linux terminal:  ```bash  cd ML2/compilers/registry/target  java -jar mlquadrat.compilers.registry-2.0.0-SNAPSHOT-jar-with-dependencies.jar -c auto -s ../../..,java,SOFTWARE
/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml -o ../../../../..,ML2,SOFTWARE
"/Generated_ML2_Demo_PingPong  ```  The -c option specifies the model-to-code transformation (a.k.a. the code generator) or ""**c**ompiler"" that shall be deployed.",ML2,SOFTWARE
"Hence, it can be easily built/installed using the Apache Maven with one command (mvn clean install).",Apache Maven,SOFTWARE
"Also, the output of Maven will include an executable JAR file with all the dependencies as a bundle.",Maven,SOFTWARE
"Further, we show how the textual and the tree-based model editors can be used in the Eclipse IDE.",Eclipse IDE,SOFTWARE
"Install the latest version of [Anaconda for Python 3.x](https://docs.anaconda.com/anaconda/install/linux/), e.g., as follows (use ```sudo su``` to run the chmod command as root and then ```exit``` in order to get back to the non-root user):    ```bash  wget https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh  sudo su  chmod u+x .",Anaconda,SOFTWARE
"Install the latest version of [Anaconda for Python 3.x](https://docs.anaconda.com/anaconda/install/linux/), e.g., as follows (use ```sudo su``` to run the chmod command as root and then ```exit``` in order to get back to the non-root user):    ```bash  wget https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh  sudo su  chmod u+x .",anaconda,SOFTWARE
"Install the latest version of [Anaconda for Python 3.x](https://docs.anaconda.com/anaconda/install/linux/), e.g., as follows (use ```sudo su``` to run the chmod command as root and then ```exit``` in order to get back to the non-root user):    ```bash  wget https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh  sudo su  chmod u+x .",linux,SOFTWARE
"Install the latest version of [Anaconda for Python 3.x](https://docs.anaconda.com/anaconda/install/linux/), e.g., as follows (use ```sudo su``` to run the chmod command as root and then ```exit``` in order to get back to the non-root user):    ```bash  wget https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh  sudo su  chmod u+x .",sudo,SOFTWARE
"Install the latest version of [Anaconda for Python 3.x](https://docs.anaconda.com/anaconda/install/linux/), e.g., as follows (use ```sudo su``` to run the chmod command as root and then ```exit``` in order to get back to the non-root user):    ```bash  wget https://repo.anaconda.com/archive/Anaconda3-2021.05-Linux-x86_64.sh  sudo su  chmod u+x .",anaconda,SOFTWARE
/Anaconda3-2021.05-Linux-x86_64.sh  exit  .,Anaconda3,SOFTWARE
/Anaconda3-2021.05-Linux-x86_64.sh  exit  .,Linux,SOFTWARE
"/Anaconda3-2021.05-Linux-x86_64.sh  ```    Perhaps you need to add conda to the PATH environmental variable, if it is not there (you may check that with ```echo $PATH```).",Anaconda3,SOFTWARE
"/Anaconda3-2021.05-Linux-x86_64.sh  ```    Perhaps you need to add conda to the PATH environmental variable, if it is not there (you may check that with ```echo $PATH```).",Linux,SOFTWARE
"Assuming, you have downloaded and installed Anaconda in /home/user:    ```bash   export PATH=$PATH:/home/user/anaconda3/condabin/  ```  To make it permanent, use a text editor, such as vim, append that to the .bashrc file (replace user with your username), and run source as follows:  ```bash   vim /home/user/.bashrc  source /home/user/.bashrc  ```    Then, please follow the steps below, in order to create a conda virtual environment and install the latest versions of the required Python libraries.",anaconda3,SOFTWARE
"Assuming, you have downloaded and installed Anaconda in /home/user:    ```bash   export PATH=$PATH:/home/user/anaconda3/condabin/  ```  To make it permanent, use a text editor, such as vim, append that to the .bashrc file (replace user with your username), and run source as follows:  ```bash   vim /home/user/.bashrc  source /home/user/.bashrc  ```    Then, please follow the steps below, in order to create a conda virtual environment and install the latest versions of the required Python libraries.",vim,SOFTWARE
"Assuming, you have downloaded and installed Anaconda in /home/user:    ```bash   export PATH=$PATH:/home/user/anaconda3/condabin/  ```  To make it permanent, use a text editor, such as vim, append that to the .bashrc file (replace user with your username), and run source as follows:  ```bash   vim /home/user/.bashrc  source /home/user/.bashrc  ```    Then, please follow the steps below, in order to create a conda virtual environment and install the latest versions of the required Python libraries.",vim,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,conda,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,ml2,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,conda,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,ml2,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,conda,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,numpy,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,pandas,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,matplotlib,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,scikit-learn,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,scikit-learn-intelex,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,tensorflow-gpu,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,gensim,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,keras,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,pytorch,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,conda,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,conda,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,conda,SOFTWARE
```bash  conda create --name ml2 python=3.8  conda activate ml2  conda install nb_conda jupyter numpy pandas matplotlib scikit-learn scikit-learn-intelex tensorflow-gpu gensim keras pytorch  conda install jupyter_contrib_nbextensions -c conda-forge  conda deactivate  conda activate ml2  ```    II.,ml2,SOFTWARE
Donwload the latest version of the [Eclipse IDE](https://www.eclipse.org/downloads) and install the latest version of the [Eclipse Modeling Tools](https://projects.eclipse.org/projects/modeling) through the wizard.,Eclipse IDE,SOFTWARE
Install the [Xtext ANTLR plugin from this update site](https://download.itemis.de/updates/releases/2.1.1/) in the Eclipse IDE.    ### How to install and execute the generated code?,Xtext ANTLR,SOFTWARE
Install the [Xtext ANTLR plugin from this update site](https://download.itemis.de/updates/releases/2.1.1/) in the Eclipse IDE.    ### How to install and execute the generated code?,Eclipse IDE,SOFTWARE
"The code that we generated for the sample model instance through the [quick (15 mins) tutorial](#user-doc-quick) above, namely [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml), can be built easily using the Apache Maven (the generated code resides in the /home/user/Generated_ML2_Demo_PingPong directory):    ```bash  cd /home/user/Generated_ML2_Demo_PingPong  cd python_java/  mvn clean install  ```  As mentioned, if you want to see a more detailed output to debug, use the option -X:    ```bash  mvn clean install -X  ```    Moreover, as stated before, if you want to use Maven in the offline mode, e.g., in the case that your machine is behind a firewall that prohibits the Internet access, you should use the option -o, but, remember that you would need to first copy the .m2 directory, which includes the Maven cache from another computer behind the firewall (on which you have already cached ¬¥the required dependencies) to this computer.",ML2,SOFTWARE
"The code that we generated for the sample model instance through the [quick (15 mins) tutorial](#user-doc-quick) above, namely [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml), can be built easily using the Apache Maven (the generated code resides in the /home/user/Generated_ML2_Demo_PingPong directory):    ```bash  cd /home/user/Generated_ML2_Demo_PingPong  cd python_java/  mvn clean install  ```  As mentioned, if you want to see a more detailed output to debug, use the option -X:    ```bash  mvn clean install -X  ```    Moreover, as stated before, if you want to use Maven in the offline mode, e.g., in the case that your machine is behind a firewall that prohibits the Internet access, you should use the option -o, but, remember that you would need to first copy the .m2 directory, which includes the Maven cache from another computer behind the firewall (on which you have already cached ¬¥the required dependencies) to this computer.",ML-Quadrat,SOFTWARE
"The code that we generated for the sample model instance through the [quick (15 mins) tutorial](#user-doc-quick) above, namely [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml), can be built easily using the Apache Maven (the generated code resides in the /home/user/Generated_ML2_Demo_PingPong directory):    ```bash  cd /home/user/Generated_ML2_Demo_PingPong  cd python_java/  mvn clean install  ```  As mentioned, if you want to see a more detailed output to debug, use the option -X:    ```bash  mvn clean install -X  ```    Moreover, as stated before, if you want to use Maven in the offline mode, e.g., in the case that your machine is behind a firewall that prohibits the Internet access, you should use the option -o, but, remember that you would need to first copy the .m2 directory, which includes the Maven cache from another computer behind the firewall (on which you have already cached ¬¥the required dependencies) to this computer.",ML2,SOFTWARE
"The code that we generated for the sample model instance through the [quick (15 mins) tutorial](#user-doc-quick) above, namely [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml), can be built easily using the Apache Maven (the generated code resides in the /home/user/Generated_ML2_Demo_PingPong directory):    ```bash  cd /home/user/Generated_ML2_Demo_PingPong  cd python_java/  mvn clean install  ```  As mentioned, if you want to see a more detailed output to debug, use the option -X:    ```bash  mvn clean install -X  ```    Moreover, as stated before, if you want to use Maven in the offline mode, e.g., in the case that your machine is behind a firewall that prohibits the Internet access, you should use the option -o, but, remember that you would need to first copy the .m2 directory, which includes the Maven cache from another computer behind the firewall (on which you have already cached ¬¥the required dependencies) to this computer.",Apache Maven,SOFTWARE
"The code that we generated for the sample model instance through the [quick (15 mins) tutorial](#user-doc-quick) above, namely [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml), can be built easily using the Apache Maven (the generated code resides in the /home/user/Generated_ML2_Demo_PingPong directory):    ```bash  cd /home/user/Generated_ML2_Demo_PingPong  cd python_java/  mvn clean install  ```  As mentioned, if you want to see a more detailed output to debug, use the option -X:    ```bash  mvn clean install -X  ```    Moreover, as stated before, if you want to use Maven in the offline mode, e.g., in the case that your machine is behind a firewall that prohibits the Internet access, you should use the option -o, but, remember that you would need to first copy the .m2 directory, which includes the Maven cache from another computer behind the firewall (on which you have already cached ¬¥the required dependencies) to this computer.",ML2,SOFTWARE
"The code that we generated for the sample model instance through the [quick (15 mins) tutorial](#user-doc-quick) above, namely [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml), can be built easily using the Apache Maven (the generated code resides in the /home/user/Generated_ML2_Demo_PingPong directory):    ```bash  cd /home/user/Generated_ML2_Demo_PingPong  cd python_java/  mvn clean install  ```  As mentioned, if you want to see a more detailed output to debug, use the option -X:    ```bash  mvn clean install -X  ```    Moreover, as stated before, if you want to use Maven in the offline mode, e.g., in the case that your machine is behind a firewall that prohibits the Internet access, you should use the option -o, but, remember that you would need to first copy the .m2 directory, which includes the Maven cache from another computer behind the firewall (on which you have already cached ¬¥the required dependencies) to this computer.",ML2,SOFTWARE
"The code that we generated for the sample model instance through the [quick (15 mins) tutorial](#user-doc-quick) above, namely [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml), can be built easily using the Apache Maven (the generated code resides in the /home/user/Generated_ML2_Demo_PingPong directory):    ```bash  cd /home/user/Generated_ML2_Demo_PingPong  cd python_java/  mvn clean install  ```  As mentioned, if you want to see a more detailed output to debug, use the option -X:    ```bash  mvn clean install -X  ```    Moreover, as stated before, if you want to use Maven in the offline mode, e.g., in the case that your machine is behind a firewall that prohibits the Internet access, you should use the option -o, but, remember that you would need to first copy the .m2 directory, which includes the Maven cache from another computer behind the firewall (on which you have already cached ¬¥the required dependencies) to this computer.",Maven,SOFTWARE
"The code that we generated for the sample model instance through the [quick (15 mins) tutorial](#user-doc-quick) above, namely [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml), can be built easily using the Apache Maven (the generated code resides in the /home/user/Generated_ML2_Demo_PingPong directory):    ```bash  cd /home/user/Generated_ML2_Demo_PingPong  cd python_java/  mvn clean install  ```  As mentioned, if you want to see a more detailed output to debug, use the option -X:    ```bash  mvn clean install -X  ```    Moreover, as stated before, if you want to use Maven in the offline mode, e.g., in the case that your machine is behind a firewall that prohibits the Internet access, you should use the option -o, but, remember that you would need to first copy the .m2 directory, which includes the Maven cache from another computer behind the firewall (on which you have already cached ¬¥the required dependencies) to this computer.",Maven,SOFTWARE
"For now, we copy a dummy sample dataset that is available at: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData    For the current example, we should choose the sample dataset [ip_dataset.csv](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv).",ML-Quadrat,SOFTWARE
"For now, we copy a dummy sample dataset that is available at: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData    For the current example, we should choose the sample dataset [ip_dataset.csv](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv).",ML2,SOFTWARE
"For now, we copy a dummy sample dataset that is available at: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData    For the current example, we should choose the sample dataset [ip_dataset.csv](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv).",ML2,SOFTWARE
"For now, we copy a dummy sample dataset that is available at: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData    For the current example, we should choose the sample dataset [ip_dataset.csv](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv).",ML-Quadrat,SOFTWARE
"For now, we copy a dummy sample dataset that is available at: https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData    For the current example, we should choose the sample dataset [ip_dataset.csv](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv).",ML2,SOFTWARE
"We should copy this Comma-Sparated Values (CSV) file into the correct path, in this case at target/data (the data sub-directory must be created):    ```bash  cd target/  mkdir data  cp /home/user/ML-Quadrat/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv data/  ```  Now, in order to run the generated application / IoT service:    ```bash  java -jar SmartPingPongCfg-1.0.0-jar-with-dependencies.jar  ```  After running the service, you will see the output in the terminal.",ML2,SOFTWARE
"We should copy this Comma-Sparated Values (CSV) file into the correct path, in this case at target/data (the data sub-directory must be created):    ```bash  cd target/  mkdir data  cp /home/user/ML-Quadrat/ML2/org.thingml.samples/src/main/thingml/ML2_Demos_SampleData/ip_dataset.csv data/  ```  Now, in order to run the generated application / IoT service:    ```bash  java -jar SmartPingPongCfg-1.0.0-jar-with-dependencies.jar  ```  After running the service, you will see the output in the terminal.",java,SOFTWARE
"In principle, it is possible to use any text editor, e.g., vi/vim, nano or Notepad to create a model instance as long as you follow the syntax of the Domain-Specific Modeling Language (DSML) of ML2 and save the text file with the **.thingml** extension.",vim,SOFTWARE
"In principle, it is possible to use any text editor, e.g., vi/vim, nano or Notepad to create a model instance as long as you follow the syntax of the Domain-Specific Modeling Language (DSML) of ML2 and save the text file with the **.thingml** extension.",nano,SOFTWARE
"In principle, it is possible to use any text editor, e.g., vi/vim, nano or Notepad to create a model instance as long as you follow the syntax of the Domain-Specific Modeling Language (DSML) of ML2 and save the text file with the **.thingml** extension.",Notepad,SOFTWARE
"In principle, it is possible to use any text editor, e.g., vi/vim, nano or Notepad to create a model instance as long as you follow the syntax of the Domain-Specific Modeling Language (DSML) of ML2 and save the text file with the **.thingml** extension.",ML2,SOFTWARE
"There exist a textual model editor in the Eclipse IDE, which offers syntax highlighting, auto-completion and warnings/errors/hints at the design-time.",Eclipse IDE,SOFTWARE
"However, we first need to set up the workspace in the Eclipse IDE.    #### How to set up the workspace in the Eclipse IDE (Eclipse Modeling Tools)?",Eclipse IDE,SOFTWARE
"However, we first need to set up the workspace in the Eclipse IDE.    #### How to set up the workspace in the Eclipse IDE (Eclipse Modeling Tools)?",Eclipse IDE,SOFTWARE
"If that is not the case, sometimes cleaning the workspace (Project -> Clean... -> Clean all projects), updating the Maven projects (right click on a project -> Maven -> Update Project... -> Select All, you may also check the option **Force Update of Snapshots/Releases**) or restarting the Eclipse IDE (Eclipse Modeling Tools) might help.",Maven,SOFTWARE
"If that is not the case, sometimes cleaning the workspace (Project -> Clean... -> Clean all projects), updating the Maven projects (right click on a project -> Maven -> Update Project... -> Select All, you may also check the option **Force Update of Snapshots/Releases**) or restarting the Eclipse IDE (Eclipse Modeling Tools) might help.",Maven,SOFTWARE
"If that is not the case, sometimes cleaning the workspace (Project -> Clean... -> Clean all projects), updating the Maven projects (right click on a project -> Maven -> Update Project... -> Select All, you may also check the option **Force Update of Snapshots/Releases**) or restarting the Eclipse IDE (Eclipse Modeling Tools) might help.",Eclipse IDE,SOFTWARE
"Once you import the projects into the workspace in the Eclipse IDE, you will get notified about a number of missing extensions, such as the Maven support in Eclipse (m2e) and the Eclipse Enterprise Java and Web Developers Tools, which can be installed through the Eclipse Marketplace.",Eclipse IDE,SOFTWARE
"Once you import the projects into the workspace in the Eclipse IDE, you will get notified about a number of missing extensions, such as the Maven support in Eclipse (m2e) and the Eclipse Enterprise Java and Web Developers Tools, which can be installed through the Eclipse Marketplace.",Maven,SOFTWARE
"Once you import the projects into the workspace in the Eclipse IDE, you will get notified about a number of missing extensions, such as the Maven support in Eclipse (m2e) and the Eclipse Enterprise Java and Web Developers Tools, which can be installed through the Eclipse Marketplace.",Eclipse,SOFTWARE
"Once you import the projects into the workspace in the Eclipse IDE, you will get notified about a number of missing extensions, such as the Maven support in Eclipse (m2e) and the Eclipse Enterprise Java and Web Developers Tools, which can be installed through the Eclipse Marketplace.",Eclipse,SOFTWARE
"Once you import the projects into the workspace in the Eclipse IDE, you will get notified about a number of missing extensions, such as the Maven support in Eclipse (m2e) and the Eclipse Enterprise Java and Web Developers Tools, which can be installed through the Eclipse Marketplace.",Eclipse,SOFTWARE
You will need to restart the Eclipse IDE after each installation.,Eclipse IDE,SOFTWARE
"Finall, please go to Help -> Install New Software and install the following software through the default update site (e.g., http://download.eclipse.org/releases/2021-03): Under the Modeling category, select and install the MWE2 Language SDK, MWE2 Runtime SDK and Xtext Complete SDK.      #### Running the GenerateThingML.mwe2 workflow in the Eclipse IDE  One of the projects in the workspace, called thingml.ide might still have errors.",MWE2 Language SDK,SOFTWARE
"Finall, please go to Help -> Install New Software and install the following software through the default update site (e.g., http://download.eclipse.org/releases/2021-03): Under the Modeling category, select and install the MWE2 Language SDK, MWE2 Runtime SDK and Xtext Complete SDK.      #### Running the GenerateThingML.mwe2 workflow in the Eclipse IDE  One of the projects in the workspace, called thingml.ide might still have errors.",MWE2 Runtime SDK,SOFTWARE
"Finall, please go to Help -> Install New Software and install the following software through the default update site (e.g., http://download.eclipse.org/releases/2021-03): Under the Modeling category, select and install the MWE2 Language SDK, MWE2 Runtime SDK and Xtext Complete SDK.      #### Running the GenerateThingML.mwe2 workflow in the Eclipse IDE  One of the projects in the workspace, called thingml.ide might still have errors.",Xtext Complete SDK,SOFTWARE
"Finall, please go to Help -> Install New Software and install the following software through the default update site (e.g., http://download.eclipse.org/releases/2021-03): Under the Modeling category, select and install the MWE2 Language SDK, MWE2 Runtime SDK and Xtext Complete SDK.      #### Running the GenerateThingML.mwe2 workflow in the Eclipse IDE  One of the projects in the workspace, called thingml.ide might still have errors.",GenerateThingML.mwe2,SOFTWARE
"Finall, please go to Help -> Install New Software and install the following software through the default update site (e.g., http://download.eclipse.org/releases/2021-03): Under the Modeling category, select and install the MWE2 Language SDK, MWE2 Runtime SDK and Xtext Complete SDK.      #### Running the GenerateThingML.mwe2 workflow in the Eclipse IDE  One of the projects in the workspace, called thingml.ide might still have errors.",Eclipse IDE,SOFTWARE
"Please run the [GenerateThingML.mwe2](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/language/thingml/src/org/thingml/xtext/GenerateThingML.mwe2) workflow, which resides in the thingml project in the workspace (under src -> org.thingml.xtext) from within the Eclipse IDE by right-clicking on it and choosing Run as -> MWE2 Workflow from the context menu.",GenerateThingML.mwe2,SOFTWARE
"Please run the [GenerateThingML.mwe2](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/language/thingml/src/org/thingml/xtext/GenerateThingML.mwe2) workflow, which resides in the thingml project in the workspace (under src -> org.thingml.xtext) from within the Eclipse IDE by right-clicking on it and choosing Run as -> MWE2 Workflow from the context menu.",GenerateThingML.mwe2,SOFTWARE
"Please run the [GenerateThingML.mwe2](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/language/thingml/src/org/thingml/xtext/GenerateThingML.mwe2) workflow, which resides in the thingml project in the workspace (under src -> org.thingml.xtext) from within the Eclipse IDE by right-clicking on it and choosing Run as -> MWE2 Workflow from the context menu.",Eclipse IDE,SOFTWARE
"This shall deploy the [Xtext ANTLR plugin](https://download.itemis.de/updates/releases/2.1.1/) that we installed before, and fix the issues.",Xtext ANTLR,SOFTWARE
"This way, the customized textual model editor will work properly (see below).    #### Running a nested Eclipse instance to use the model editors  In order to use the textual or the EMF tree-based model editors, you must run a new/nested instance of the Eclipse IDE inside of the Eclipse IDE (the above-mentioned workspace).",Eclipse IDE,SOFTWARE
"This way, the customized textual model editor will work properly (see below).    #### Running a nested Eclipse instance to use the model editors  In order to use the textual or the EMF tree-based model editors, you must run a new/nested instance of the Eclipse IDE inside of the Eclipse IDE (the above-mentioned workspace).",Eclipse IDE,SOFTWARE
"To this aim, click on one of the projects in the workspace, let's say, e.g., thingml or thingml.ide, and select ""Run As"" and then ""Eclipse Application"" from the context menu.    #### Using the Textual (Xtext-based) Model Editor  Create a new project in the new/nested Eclipse instance, say Demo_ML2 and then create a new file with the **.thingml** extension, such as Test.thingml.",Eclipse,SOFTWARE
"By default, the customized, textual Xtext-based model editor will open for the files with the extension .thingml, i.e., for the ML2 model instances.",ML2,SOFTWARE
"Moreover, you will see warnings, errors and hints in the editor and also in the properties view/tab of the Eclipse IDE if applicable.    #### Using the Graphical EMF Tree-based Model Editor  In the new/nested Eclipse instance, if you right-click on a model instance, e.g., Test.thingml and select Open With -> **ThingML Model Editor**, the graphical EMF tree-based model editor of ML2 will open.",Eclipse IDE,SOFTWARE
"Moreover, you will see warnings, errors and hints in the editor and also in the properties view/tab of the Eclipse IDE if applicable.    #### Using the Graphical EMF Tree-based Model Editor  In the new/nested Eclipse instance, if you right-click on a model instance, e.g., Test.thingml and select Open With -> **ThingML Model Editor**, the graphical EMF tree-based model editor of ML2 will open.",Eclipse,SOFTWARE
"Moreover, you will see warnings, errors and hints in the editor and also in the properties view/tab of the Eclipse IDE if applicable.    #### Using the Graphical EMF Tree-based Model Editor  In the new/nested Eclipse instance, if you right-click on a model instance, e.g., Test.thingml and select Open With -> **ThingML Model Editor**, the graphical EMF tree-based model editor of ML2 will open.",ML2,SOFTWARE
"In contrast, if you right-click on a model instance and select Open With -> **ThingML Editor**, not **ThingML Model Editor**, the **textual** Xtext-based model editor of ML2 will open.    #### Which Model Editor to Choose?",ML2,SOFTWARE
"[Back to top](#toc)    ### Full documentation of the DSML  As mentioned, ML2 is based on the [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project) projects.",ML2,SOFTWARE
The **abstract syntax** of the DSML of ML2 is implemented through the [Xtext grammar](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/language/thingml/src/org/thingml/xtext/ThingML.xtext).,Xtext,SOFTWARE
The **abstract syntax** of the DSML of ML2 is implemented through the [Xtext grammar](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/language/thingml/src/org/thingml/xtext/ThingML.xtext).,xtext,SOFTWARE
The Ecore meta-model is generated automatically out of the Xtext grammar.,Xtext,SOFTWARE
"Moreover, the **concrete syntax** of the DSML is realized in the textual (Xtext-based) and graphical (EMF tree-based) forms, as explained above.",Xtext,SOFTWARE
"Last but not least, the **semantics** are implemented both via the Xtext framework (in Java and Xtend) on the modeling langauge layer and also through the model-to-code transformations (i.e., code generators, also known as ""compilers"").",Xtext,SOFTWARE
"A typical software model instance that conforms to the meta-model/grammar of the DSML of ML2 consists of 4 main sections and various subsections in Section 3, i.e., ""Things"":    #### Section 1.",ML2,SOFTWARE
Imports  One may import other model instances that conform to the meta-model/grammar of the DSML of ML2 here.,ML2,SOFTWARE
The provided examples [ML2_Demo_NIALM_PSM_Java.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_NIALM_PSM_Java.thingml) and [ML2_Demo_NIALM_PIM.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_NIALM_PIM.thingml) illustrate this.,ML2,SOFTWARE
The provided examples [ML2_Demo_NIALM_PSM_Java.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_NIALM_PSM_Java.thingml) and [ML2_Demo_NIALM_PIM.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_NIALM_PIM.thingml) illustrate this.,ML2,SOFTWARE
The provided examples [ML2_Demo_NIALM_PSM_Java.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_NIALM_PSM_Java.thingml) and [ML2_Demo_NIALM_PIM.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_NIALM_PIM.thingml) illustrate this.,ML2,SOFTWARE
The provided examples [ML2_Demo_NIALM_PSM_Java.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_NIALM_PSM_Java.thingml) and [ML2_Demo_NIALM_PIM.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_NIALM_PIM.thingml) illustrate this.,ML2,SOFTWARE
"For instance, the provided example [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) demonstrates how the String, Boolean<1> and Int32<4> datatypes are supposed to be mapped to the ""platform""-specific datatypes, such as String <-> char * for the C code generation, but String <-> String for the Java and the Javascript code generation.",ML2,SOFTWARE
"For instance, the provided example [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) demonstrates how the String, Boolean<1> and Int32<4> datatypes are supposed to be mapped to the ""platform""-specific datatypes, such as String <-> char * for the C code generation, but String <-> String for the Java and the Javascript code generation.",ML-Quadrat,SOFTWARE
"Things  Things are somehow analogues to the ""Classes"" in the Object-Oriented Programming (OOP) paradigm.    #####  For each thing, say Thing_A:  ###### Subsection 3.1: Messages, Ports and Properties  The keywords of this subsection of the model instance are highlighted in **orange** in the textual model editor of ML2.",ML2,SOFTWARE
"Further, each thing can have local variables, called **properties**.    ###### Subsection 3.2: Data Analytics (and Machine Learning)  This is the main innovation of ML2 compared to [ThingML](https://github.com/TelluIoT/ThingML) / [HEADS](https://github.com/HEADS-project).",ML2,SOFTWARE
The keywords of this subsection of the model instance are highlighted in **blue** in the textual model editor of ML2.,ML2,SOFTWARE
Let us consider two of the provided sample model instances [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) and [ML2_Demo_PingPong_Blackbox.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong_Blackbox.thingml).,ML2,SOFTWARE
Let us consider two of the provided sample model instances [ML2_Demo_PingPong.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong.thingml) and [ML2_Demo_PingPong_Blackbox.thingml](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/org.thingml.samples/src/main/thingml/ML2_Demo_PingPong_Blackbox.thingml).,ML2,SOFTWARE
"This is the data_analytics subsection of thing PingPongDataAnalytics in the former sample (namely ML2_Demo_PingPong.thingml):    ```   data_analytics da1    @dalib ""scikit-learn"" {   //@dalib ""keras-tensorflow"" {   //{    labels ON    features client_ip_address,client_code,prediction    prediction_results prediction    dataset ""data/ip_dataset.csv""    automl OFF    sequential TRUE    timestamps OFF     preprocess_feature_scaler StandardScaler    model_algorithm nn_multilayer_perceptron my_nn_mlp(activation relu, optimizer adam, loss SparseCategoricalCrossentropy)    training_results ""data/training.txt""     }  ```    In addition, this is the data_analytics subsection of thing PingPongDataAnalytics in the latter case (namely ML2_Demo_PingPong_Blackbox.thingml):    ```   data_analytics da1    @dalib ""scikit-learn"" {    labels ON    features client_ip_address,client_code,prediction    prediction_results prediction    blackbox_ml true     blackbox_ml_model ""pre_trained/pre_trained_ml_model.pickle""    blackbox_import_algorithm ""from sklearn.neural_network import MLPClassifier""    //blackbox_label_encoder ""pre_trained/pre_trained_label_encoder.pickle""   }  ```    Below, we explain them line-by-line:    (i) **data_analytics:** The **data_analytics** keyword must be followed by the name of this data_analytics component/block/workflow/pipeline since each thing may possess even more than one component/block/workflow/pipeline.",scikit-learn,SOFTWARE
"This is the data_analytics subsection of thing PingPongDataAnalytics in the former sample (namely ML2_Demo_PingPong.thingml):    ```   data_analytics da1    @dalib ""scikit-learn"" {   //@dalib ""keras-tensorflow"" {   //{    labels ON    features client_ip_address,client_code,prediction    prediction_results prediction    dataset ""data/ip_dataset.csv""    automl OFF    sequential TRUE    timestamps OFF     preprocess_feature_scaler StandardScaler    model_algorithm nn_multilayer_perceptron my_nn_mlp(activation relu, optimizer adam, loss SparseCategoricalCrossentropy)    training_results ""data/training.txt""     }  ```    In addition, this is the data_analytics subsection of thing PingPongDataAnalytics in the latter case (namely ML2_Demo_PingPong_Blackbox.thingml):    ```   data_analytics da1    @dalib ""scikit-learn"" {    labels ON    features client_ip_address,client_code,prediction    prediction_results prediction    blackbox_ml true     blackbox_ml_model ""pre_trained/pre_trained_ml_model.pickle""    blackbox_import_algorithm ""from sklearn.neural_network import MLPClassifier""    //blackbox_label_encoder ""pre_trained/pre_trained_label_encoder.pickle""   }  ```    Below, we explain them line-by-line:    (i) **data_analytics:** The **data_analytics** keyword must be followed by the name of this data_analytics component/block/workflow/pipeline since each thing may possess even more than one component/block/workflow/pipeline.",keras,SOFTWARE
"This is the data_analytics subsection of thing PingPongDataAnalytics in the former sample (namely ML2_Demo_PingPong.thingml):    ```   data_analytics da1    @dalib ""scikit-learn"" {   //@dalib ""keras-tensorflow"" {   //{    labels ON    features client_ip_address,client_code,prediction    prediction_results prediction    dataset ""data/ip_dataset.csv""    automl OFF    sequential TRUE    timestamps OFF     preprocess_feature_scaler StandardScaler    model_algorithm nn_multilayer_perceptron my_nn_mlp(activation relu, optimizer adam, loss SparseCategoricalCrossentropy)    training_results ""data/training.txt""     }  ```    In addition, this is the data_analytics subsection of thing PingPongDataAnalytics in the latter case (namely ML2_Demo_PingPong_Blackbox.thingml):    ```   data_analytics da1    @dalib ""scikit-learn"" {    labels ON    features client_ip_address,client_code,prediction    prediction_results prediction    blackbox_ml true     blackbox_ml_model ""pre_trained/pre_trained_ml_model.pickle""    blackbox_import_algorithm ""from sklearn.neural_network import MLPClassifier""    //blackbox_label_encoder ""pre_trained/pre_trained_label_encoder.pickle""   }  ```    Below, we explain them line-by-line:    (i) **data_analytics:** The **data_analytics** keyword must be followed by the name of this data_analytics component/block/workflow/pipeline since each thing may possess even more than one component/block/workflow/pipeline.",tensorflow,SOFTWARE
"This is the data_analytics subsection of thing PingPongDataAnalytics in the former sample (namely ML2_Demo_PingPong.thingml):    ```   data_analytics da1    @dalib ""scikit-learn"" {   //@dalib ""keras-tensorflow"" {   //{    labels ON    features client_ip_address,client_code,prediction    prediction_results prediction    dataset ""data/ip_dataset.csv""    automl OFF    sequential TRUE    timestamps OFF     preprocess_feature_scaler StandardScaler    model_algorithm nn_multilayer_perceptron my_nn_mlp(activation relu, optimizer adam, loss SparseCategoricalCrossentropy)    training_results ""data/training.txt""     }  ```    In addition, this is the data_analytics subsection of thing PingPongDataAnalytics in the latter case (namely ML2_Demo_PingPong_Blackbox.thingml):    ```   data_analytics da1    @dalib ""scikit-learn"" {    labels ON    features client_ip_address,client_code,prediction    prediction_results prediction    blackbox_ml true     blackbox_ml_model ""pre_trained/pre_trained_ml_model.pickle""    blackbox_import_algorithm ""from sklearn.neural_network import MLPClassifier""    //blackbox_label_encoder ""pre_trained/pre_trained_label_encoder.pickle""   }  ```    Below, we explain them line-by-line:    (i) **data_analytics:** The **data_analytics** keyword must be followed by the name of this data_analytics component/block/workflow/pipeline since each thing may possess even more than one component/block/workflow/pipeline.",scikit-learn,SOFTWARE
"In the shown example, [the Scikit-Learn library](https://scikit-learn.org/stable/) has been chosen.",Scikit-Learn,SOFTWARE
"In the shown example, [the Scikit-Learn library](https://scikit-learn.org/stable/) has been chosen.",scikit-learn,SOFTWARE
"**Note:** Currently, the @dalib annotation supports the following choices for the DAML library/framework out-of-the-box: [""scikit-learn"" for the Scikit-Learn (Sklearn) library/framework]((https://scikit-learn.org/stable/)) and [""keras-tensorflow"" for the Kears library/framework with the TensorFlow Backend](https://keras.io).",Scikit-Learn,SOFTWARE
"**Note:** Currently, the @dalib annotation supports the following choices for the DAML library/framework out-of-the-box: [""scikit-learn"" for the Scikit-Learn (Sklearn) library/framework]((https://scikit-learn.org/stable/)) and [""keras-tensorflow"" for the Kears library/framework with the TensorFlow Backend](https://keras.io).",scikit-learn,SOFTWARE
"**Note:** Currently, the @dalib annotation supports the following choices for the DAML library/framework out-of-the-box: [""scikit-learn"" for the Scikit-Learn (Sklearn) library/framework]((https://scikit-learn.org/stable/)) and [""keras-tensorflow"" for the Kears library/framework with the TensorFlow Backend](https://keras.io).",keras,SOFTWARE
"**Note:** Currently, the @dalib annotation supports the following choices for the DAML library/framework out-of-the-box: [""scikit-learn"" for the Scikit-Learn (Sklearn) library/framework]((https://scikit-learn.org/stable/)) and [""keras-tensorflow"" for the Kears library/framework with the TensorFlow Backend](https://keras.io).",tensorflow,SOFTWARE
"**Note:** Currently, the @dalib annotation supports the following choices for the DAML library/framework out-of-the-box: [""scikit-learn"" for the Scikit-Learn (Sklearn) library/framework]((https://scikit-learn.org/stable/)) and [""keras-tensorflow"" for the Kears library/framework with the TensorFlow Backend](https://keras.io).",Kears,SOFTWARE
"**Note:** Currently, the @dalib annotation supports the following choices for the DAML library/framework out-of-the-box: [""scikit-learn"" for the Scikit-Learn (Sklearn) library/framework]((https://scikit-learn.org/stable/)) and [""keras-tensorflow"" for the Kears library/framework with the TensorFlow Backend](https://keras.io).",TensorFlow,SOFTWARE
"**Note:** Currently, the @dalib annotation supports the following choices for the DAML library/framework out-of-the-box: [""scikit-learn"" for the Scikit-Learn (Sklearn) library/framework]((https://scikit-learn.org/stable/)) and [""keras-tensorflow"" for the Kears library/framework with the TensorFlow Backend](https://keras.io).",keras,SOFTWARE
"Supporting more DAML libraries/frameworks, such as [PyTorch](https://pytorch.org) (Python) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) (Java) is currently in progress.",PyTorch,SOFTWARE
"Supporting more DAML libraries/frameworks, such as [PyTorch](https://pytorch.org) (Python) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) (Java) is currently in progress.",pytorch,SOFTWARE
"Supporting more DAML libraries/frameworks, such as [PyTorch](https://pytorch.org) (Python) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) (Java) is currently in progress.",WEKA,SOFTWARE
"Supporting more DAML libraries/frameworks, such as [PyTorch](https://pytorch.org) (Python) and [WEKA](https://www.cs.waikato.ac.nz/~ml/weka/) (Java) is currently in progress.",weka,SOFTWARE
"(iv) **//:** Similar to Java, in order to comment out a line (i.e., disable or inactivate it) or write any comment, a double slash can be added to the beginning of the line, e.g., see ""//@dalib ""keras-tensorflow"" {"" above.",keras,SOFTWARE
"(iv) **//:** Similar to Java, in order to comment out a line (i.e., disable or inactivate it) or write any comment, a double slash can be added to the beginning of the line, e.g., see ""//@dalib ""keras-tensorflow"" {"" above.",tensorflow,SOFTWARE
"Logistic Regression for linear classification, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)  2.",Scikit-Learn,SOFTWARE
"Logistic Regression for linear classification, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)  2.",scikit-learn,SOFTWARE
"Linear Regression, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)  3.",Scikit-Learn,SOFTWARE
"Linear Regression, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)  3.",scikit-learn,SOFTWARE
"Gaussian Naive Bayes, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)  4.",Scikit-Learn,SOFTWARE
"Gaussian Naive Bayes, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)  4.",scikit-learn,SOFTWARE
"Multinomial Naive Bayes, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)  5.",Scikit-Learn,SOFTWARE
"Multinomial Naive Bayes, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)  5.",scikit-learn,SOFTWARE
"Complement Naive Bayes, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html)  6.",Scikit-Learn,SOFTWARE
"Complement Naive Bayes, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html)  6.",scikit-learn,SOFTWARE
"Bernoulli Naive Bayes, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)  7.",Scikit-Learn,SOFTWARE
"Bernoulli Naive Bayes, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)  7.",scikit-learn,SOFTWARE
"Categorical Naive Bayes, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html)  8.",Scikit-Learn,SOFTWARE
"Categorical Naive Bayes, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html)  8.",scikit-learn,SOFTWARE
"Decision Tree Regressor and Decision Tree Classifier, see the [Scikit-Learn API doc (regression)](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) / [Scikit-Learn API doc (classification)](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)  9.",Scikit-Learn,SOFTWARE
"Decision Tree Regressor and Decision Tree Classifier, see the [Scikit-Learn API doc (regression)](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) / [Scikit-Learn API doc (classification)](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)  9.",scikit-learn,SOFTWARE
"Decision Tree Regressor and Decision Tree Classifier, see the [Scikit-Learn API doc (regression)](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) / [Scikit-Learn API doc (classification)](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)  9.",Scikit-Learn,SOFTWARE
"Decision Tree Regressor and Decision Tree Classifier, see the [Scikit-Learn API doc (regression)](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) / [Scikit-Learn API doc (classification)](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)  9.",scikit-learn,SOFTWARE
"The Random Forest Regressor and Random Forest Classifier ensemble methods, see the [Scikit-Learn API doc (regression)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) / [Scikit-Learn API doc (classification)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)  10.",Scikit-Learn,SOFTWARE
"The Random Forest Regressor and Random Forest Classifier ensemble methods, see the [Scikit-Learn API doc (regression)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) / [Scikit-Learn API doc (classification)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)  10.",scikit-learn,SOFTWARE
"The Random Forest Regressor and Random Forest Classifier ensemble methods, see the [Scikit-Learn API doc (regression)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) / [Scikit-Learn API doc (classification)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)  10.",Scikit-Learn,SOFTWARE
"The Random Forest Regressor and Random Forest Classifier ensemble methods, see the [Scikit-Learn API doc (regression)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) / [Scikit-Learn API doc (classification)](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)  10.",scikit-learn,SOFTWARE
"The Multi-Layer Perceptron (MLP) Artificial Neural Networks (ANN) for classification and regression, see the [Keras API doc (default)](https://keras.io/guides/sequential_model/) and the [Scikit-Learn API doc (regression)](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) / [Scikit-Learn API doc (classification)](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).",Keras,SOFTWARE
"The Multi-Layer Perceptron (MLP) Artificial Neural Networks (ANN) for classification and regression, see the [Keras API doc (default)](https://keras.io/guides/sequential_model/) and the [Scikit-Learn API doc (regression)](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) / [Scikit-Learn API doc (classification)](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).",keras,SOFTWARE
"The Multi-Layer Perceptron (MLP) Artificial Neural Networks (ANN) for classification and regression, see the [Keras API doc (default)](https://keras.io/guides/sequential_model/) and the [Scikit-Learn API doc (regression)](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) / [Scikit-Learn API doc (classification)](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).",Scikit-Learn,SOFTWARE
"The Multi-Layer Perceptron (MLP) Artificial Neural Networks (ANN) for classification and regression, see the [Keras API doc (default)](https://keras.io/guides/sequential_model/) and the [Scikit-Learn API doc (regression)](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) / [Scikit-Learn API doc (classification)](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).",scikit-learn,SOFTWARE
"The Multi-Layer Perceptron (MLP) Artificial Neural Networks (ANN) for classification and regression, see the [Keras API doc (default)](https://keras.io/guides/sequential_model/) and the [Scikit-Learn API doc (regression)](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) / [Scikit-Learn API doc (classification)](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).",Scikit-Learn,SOFTWARE
"The Multi-Layer Perceptron (MLP) Artificial Neural Networks (ANN) for classification and regression, see the [Keras API doc (default)](https://keras.io/guides/sequential_model/) and the [Scikit-Learn API doc (regression)](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) / [Scikit-Learn API doc (classification)](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).",scikit-learn,SOFTWARE
"K-Means, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)  2.",Scikit-Learn,SOFTWARE
"K-Means, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)  2.",scikit-learn,SOFTWARE
"Mini-Batch K-Means, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html)  3.",Scikit-Learn,SOFTWARE
"Mini-Batch K-Means, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html)  3.",scikit-learn,SOFTWARE
"DB-SCAN, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)  4.",Scikit-Learn,SOFTWARE
"DB-SCAN, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)  4.",scikit-learn,SOFTWARE
"Spectral Clustering, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html)  5.",Scikit-Learn,SOFTWARE
"Spectral Clustering, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html)  5.",scikit-learn,SOFTWARE
"Gaussian Mixture Model, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)    **For semi-supervised ML:**  1.",Scikit-Learn,SOFTWARE
"Gaussian Mixture Model, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)    **For semi-supervised ML:**  1.",scikit-learn,SOFTWARE
"Self-Training, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.SelfTrainingClassifier.html)  2.",Scikit-Learn,SOFTWARE
"Self-Training, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.SelfTrainingClassifier.html)  2.",scikit-learn,SOFTWARE
"Label Propagation, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html)  3.",Scikit-Learn,SOFTWARE
"Label Propagation, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html)  3.",scikit-learn,SOFTWARE
"Label Spreading, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelSpreading.html)    Here is an example for the DAML model_algorithm (i.e., the DAML method):    model_algorithm k_means my_k_means(n_clusters 2, random_state 10)    **Note:** Obviously, not every ML method (model_algorithm) is implemented in every ML library/framework.",Scikit-Learn,SOFTWARE
"Label Spreading, see the [Scikit-Learn API doc](https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelSpreading.html)    Here is an example for the DAML model_algorithm (i.e., the DAML method):    model_algorithm k_means my_k_means(n_clusters 2, random_state 10)    **Note:** Obviously, not every ML method (model_algorithm) is implemented in every ML library/framework.",scikit-learn,SOFTWARE
"Hence, you should either avoid mentioning the annotation @dalib, so that the default choice for the ML model_algorithm is set (see the default option for the ML library/framework marked as ""(default)"", e.g., for the Multi-Layer Perceptron (MLP) above), or you should set the annotation @dalib to the auto mode: @dalib ""auto"", or you should specify the correct choice, e.g., in the case of the Multi-Layer Perceptron (MLP) the annotation can be set to either @dalib ""scikit-learn"" or @dalib ""keras-tensorflow"".",scikit-learn,SOFTWARE
"Hence, you should either avoid mentioning the annotation @dalib, so that the default choice for the ML model_algorithm is set (see the default option for the ML library/framework marked as ""(default)"", e.g., for the Multi-Layer Perceptron (MLP) above), or you should set the annotation @dalib to the auto mode: @dalib ""auto"", or you should specify the correct choice, e.g., in the case of the Multi-Layer Perceptron (MLP) the annotation can be set to either @dalib ""scikit-learn"" or @dalib ""keras-tensorflow"".",keras,SOFTWARE
"Hence, you should either avoid mentioning the annotation @dalib, so that the default choice for the ML model_algorithm is set (see the default option for the ML library/framework marked as ""(default)"", e.g., for the Multi-Layer Perceptron (MLP) above), or you should set the annotation @dalib to the auto mode: @dalib ""auto"", or you should specify the correct choice, e.g., in the case of the Multi-Layer Perceptron (MLP) the annotation can be set to either @dalib ""scikit-learn"" or @dalib ""keras-tensorflow"".",tensorflow,SOFTWARE
"For instance, the weights of the Keras ML model parameters are stored in the Hierarchical Data Format (HDF) version 5 (HDF5) with the .h5 file name extension.",Keras,SOFTWARE
"If you want to completely restart an IoT service and remove all its stored data and configurations, you must delete all the files in the pickles/objects directory, which resides in the **src** directory (not in the **target** directory) in the Maven root.",Maven,SOFTWARE
"Note that running ""mvn clean"" will not affect the contents of the src directory of Maven, but only the target directory.",Maven,SOFTWARE
"See ""from sklearn.neural_network import MLPClassifier"" in the example above.",sklearn,SOFTWARE
"However, in the blackbox-ML mode, if there exist categorical labels that had been transformed to one-hot-encoding via the Label Encoder in Python (Scikit-Learn), the path to the serialized Label Encoder object must be provided here.",Scikit-Learn,SOFTWARE
"(xx): **}:** Finally, please do not forget to close the braces (curly brackets) as shown above.    ###### Subsection 3.3: Statechart  The keywords of this subsection of the model instances are highlighted in **red** in the textual model editor of ML2.",ML2,SOFTWARE
"However, e.g., in the case of the Keras library/framework, in addition to the Pickles, the weights of the ML model parameters are saved in HDF5 (.h5).",Keras,SOFTWARE
Configuration  The keywords of this subsection of the model instances are highlighted in **green** in the textual model editor of ML2.,ML2,SOFTWARE
"As long as a model instance is valid (i.e., conforms to the meta-model/grammar of the DSML of ML2), and is complete, we can use a model-to-code transformation (code generator/""compiler"") of ML2 to gnerate the entire source code of the target (smart) IoT service out of it in a fully automated manner, provided that it has a configuration section at the end.",ML2,SOFTWARE
"As long as a model instance is valid (i.e., conforms to the meta-model/grammar of the DSML of ML2), and is complete, we can use a model-to-code transformation (code generator/""compiler"") of ML2 to gnerate the entire source code of the target (smart) IoT service out of it in a fully automated manner, provided that it has a configuration section at the end.",ML2,SOFTWARE
"However, enabling more code generators to support DAML out-of-the-box, such as the [pure Java (""java"") code generator](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/compilers/java) is currently in progress.",java,SOFTWARE
"For example, if you have already set @dalib ""scikit-learn"" or @dalib ""keras-tensorflow"", then you must choose the python_java code generator, not the pure Java code generator, as Scikit-learn/Keras are Python libraries.",scikit-learn,SOFTWARE
"For example, if you have already set @dalib ""scikit-learn"" or @dalib ""keras-tensorflow"", then you must choose the python_java code generator, not the pure Java code generator, as Scikit-learn/Keras are Python libraries.",keras,SOFTWARE
"For example, if you have already set @dalib ""scikit-learn"" or @dalib ""keras-tensorflow"", then you must choose the python_java code generator, not the pure Java code generator, as Scikit-learn/Keras are Python libraries.",tensorflow,SOFTWARE
"For example, if you have already set @dalib ""scikit-learn"" or @dalib ""keras-tensorflow"", then you must choose the python_java code generator, not the pure Java code generator, as Scikit-learn/Keras are Python libraries.",Scikit-learn,SOFTWARE
"For example, if you have already set @dalib ""scikit-learn"" or @dalib ""keras-tensorflow"", then you must choose the python_java code generator, not the pure Java code generator, as Scikit-learn/Keras are Python libraries.",Keras,SOFTWARE
"Similarly, if you have already set @dalib ""weka"", then you must choose the pure Java code generator here.",weka,SOFTWARE
"Maven Artifacts  You can find the Maven artifacts of ML2 at https://oss.sonatype.org (e.g., search for the groupid: io.github.arminmoin).",Maven,SOFTWARE
"Maven Artifacts  You can find the Maven artifacts of ML2 at https://oss.sonatype.org (e.g., search for the groupid: io.github.arminmoin).",Maven,SOFTWARE
"Basically, there exist 4 main extensio points: (i) The abstract syntax of the modeling langauge, i.e., the [Xtext grammar](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/language/thingml/src/org/thingml/xtext/ThingML.xtext) (which also implicitly means the Ecore meta-model since the latter is generated automatically out of the former as stated before).",Xtext,SOFTWARE
"Basically, there exist 4 main extensio points: (i) The abstract syntax of the modeling langauge, i.e., the [Xtext grammar](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/language/thingml/src/org/thingml/xtext/ThingML.xtext) (which also implicitly means the Ecore meta-model since the latter is generated automatically out of the former as stated before).",xtext,SOFTWARE
"(iii) The semantics realized through the Xtext framework, e.g., in the Java/Xtend classes at [ML2/language/thingml/src/org/thingml](https://github.com/arminmoin/ML-Quadrat/tree/master/ML2/language/thingml/src/org/thingml).",Xtext,SOFTWARE
"Below, we briefly explain each of them.    ### Contributing to the Grammar/Meta-model of the DSML    The Xtext grammar is the core of the DSML of ML2.",Xtext,SOFTWARE
"If you are not already familiar with the Xtext framework for DSML/DSL/programming language creation, please read the documentation here: https://www.eclipse.org/Xtext/documentation/index.html    **Note:** In some cases, the enforced scoping rules might prevent you from making the desired modifications working.",Xtext,SOFTWARE
"If you are not already familiar with the Xtext framework for DSML/DSL/programming language creation, please read the documentation here: https://www.eclipse.org/Xtext/documentation/index.html    **Note:** In some cases, the enforced scoping rules might prevent you from making the desired modifications working.",Xtext,SOFTWARE
"After any modifications, please build the entire project again using Maven in the terminal as follows:    ```bash  cd ML-Quadrat  mvn clean install -X  cd ML2/language  mvn clean install -X  ```  The -X option is optional and enables the debugging mode, thus resulting in a more detailed output.",Maven,SOFTWARE
"Moreover, you need to re-generate the graphical EMF tree-based model editor in the Eclipse IDE by following these steps:    1.",Eclipse IDE,SOFTWARE
"Last but not least, as mentioned before, you must run the [GenerateThingML.mwe2](https://github.com/arminmoin/ML-Quadrat/blob/master/ML2/language/thingml/src/org/thingml/xtext/GenerateThingML.mwe2) workflow, which resides in the thingml project in the workspace (under src -> org.thingml.xtext) from within the Eclipse IDE by right-clicking on it and choosing Run as -> MWE2 Workflow from the context menu.",Eclipse IDE,SOFTWARE
This shall deploy the [Xtext ANTLR plugin](https://download.itemis.de/updates/releases/2.1.1/) that we installed before.,Xtext ANTLR,SOFTWARE
"After any modifications, please build the corresponding projects again using Maven in the terminal as follows (e.g., for the python_java case):    ```bash  cd ML-Quadrat/ML2/compilers/python_java  mvn clean install -X  cd ML-Quadrat/ML2/compilers/registry  mvn clean install -X  ```  The -X option is optional and enables the debugging mode, thus resulting in a more detailed output.",Maven,SOFTWARE
"OpenSTL offers a modular and extensible framework, excelling in user-friendliness, organization, and comprehensiveness.",OpenSTL,SOFTWARE
We support PyTorch Lightning implementation [OpenSTL-Lightning](https://github.com/chengtan9907/OpenSTL/tree/OpenSTL-Lightning) (recommended) and naive PyTorch version [OpenSTL](https://github.com/chengtan9907/OpenSTL/tree/OpenSTL).,PyTorch Lightning,SOFTWARE
We support PyTorch Lightning implementation [OpenSTL-Lightning](https://github.com/chengtan9907/OpenSTL/tree/OpenSTL-Lightning) (recommended) and naive PyTorch version [OpenSTL](https://github.com/chengtan9907/OpenSTL/tree/OpenSTL).,PyTorch,SOFTWARE
</details>  ## News and Updates  [2023-12-15] [OpenSTL-Lightning](https://github.com/chengtan9907/OpenSTL/tree/OpenSTL-Lightning) (`OpenSTL` v1.0.0) is released,OpenSTL-Lightning,SOFTWARE
</details>  ## News and Updates  [2023-12-15] [OpenSTL-Lightning](https://github.com/chengtan9907/OpenSTL/tree/OpenSTL-Lightning) (`OpenSTL` v1.0.0) is released,OpenSTL` v1.0.0,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",OpenSTL` v0.3.0,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",conda,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",shell,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",git,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",conda,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",conda,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",argparse,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",dask,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",decord,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",fvcore,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",hickle,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",lpips,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",matplotlib,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",netcdf4,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",numpy,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",opencv-python,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",packaging,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",pandas,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",scikit-image,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",scikit-learn,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",torch,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",timm,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",tqdm,SOFTWARE
"[2023-06-19] `OpenSTL` v0.3.0 is released and will be enhanced in [#25](https://github.com/chengtan9907/OpenSTL/issues/25).  ## Installation  This project has provided an environment setting file of conda, users can easily reproduce the environment by the following commands: ```shell git clone https://github.com/chengtan9907/OpenSTL cd OpenSTL conda env create -f environment.yml conda activate OpenSTL python setup.py develop ```  <details close> <summary>Dependencies</summary>  * argparse * dask * decord * fvcore * hickle * lpips * matplotlib * netcdf4 * numpy * opencv-python * packaging * pandas * python<=3.10.8 * scikit-image * scikit-learn * torch * timm * tqdm * xarray==0.19.0 </details>  Please refer to [install.md](docs/en/install.md) for more detailed instructions.  ## Getting Started  Please see [get_started.md](docs/en/get_started.md) for the basic usage.",xarray,SOFTWARE
"```shell bash tools/prepare_data/download_mmnist.sh python tools/train.py -d mmnist --lr 1e-3 -c configs/mmnist/simvp/SimVP_gSTA.py --ex_name mmnist_simvp_gsta ```  ## Tutorial on using Custom Data  For the convenience of users, we provide a tutorial on how to train, evaluate, and visualize with OpenSTL on custom data.",python,SOFTWARE
"# Soft-QMIX  This repo is heavily based on the [pymarl](https://github.com/benellis3/pymarl2)  ## Installation  please follow the installation guide in the original repo [pymarl](https://github.com/benellis3/pymarl2)  ## Run  To run the code, you can use the following command:  ```bash bash .",pymarl,SOFTWARE
"# Soft-QMIX  This repo is heavily based on the [pymarl](https://github.com/benellis3/pymarl2)  ## Installation  please follow the installation guide in the original repo [pymarl](https://github.com/benellis3/pymarl2)  ## Run  To run the code, you can use the following command:  ```bash bash .",pymarl2,SOFTWARE
"# Soft-QMIX  This repo is heavily based on the [pymarl](https://github.com/benellis3/pymarl2)  ## Installation  please follow the installation guide in the original repo [pymarl](https://github.com/benellis3/pymarl2)  ## Run  To run the code, you can use the following command:  ```bash bash .",pymarl,SOFTWARE
"# Soft-QMIX  This repo is heavily based on the [pymarl](https://github.com/benellis3/pymarl2)  ## Installation  please follow the installation guide in the original repo [pymarl](https://github.com/benellis3/pymarl2)  ## Run  To run the code, you can use the following command:  ```bash bash .",pymarl2,SOFTWARE
"# Soft-QMIX  This repo is heavily based on the [pymarl](https://github.com/benellis3/pymarl2)  ## Installation  please follow the installation guide in the original repo [pymarl](https://github.com/benellis3/pymarl2)  ## Run  To run the code, you can use the following command:  ```bash bash .",bash,SOFTWARE
"style=flat)](https://arxiv.org/abs/2407.07564)  ### Requirements - python 3.12 - torch 2.2.2  - difw 0.0.29 - scikit-learn 1.5.1  `difw` can be fragile, make sure that it compiles successfully.",python 3.12,SOFTWARE
"style=flat)](https://arxiv.org/abs/2407.07564)  ### Requirements - python 3.12 - torch 2.2.2  - difw 0.0.29 - scikit-learn 1.5.1  `difw` can be fragile, make sure that it compiles successfully.",torch 2.2.2,SOFTWARE
"style=flat)](https://arxiv.org/abs/2407.07564)  ### Requirements - python 3.12 - torch 2.2.2  - difw 0.0.29 - scikit-learn 1.5.1  `difw` can be fragile, make sure that it compiles successfully.",difw 0.0.29,SOFTWARE
"style=flat)](https://arxiv.org/abs/2407.07564)  ### Requirements - python 3.12 - torch 2.2.2  - difw 0.0.29 - scikit-learn 1.5.1  `difw` can be fragile, make sure that it compiles successfully.",scikit-learn 1.5.1,SOFTWARE
"style=flat)](https://arxiv.org/abs/2407.07564)  ### Requirements - python 3.12 - torch 2.2.2  - difw 0.0.29 - scikit-learn 1.5.1  `difw` can be fragile, make sure that it compiles successfully.",difw,SOFTWARE
We've successfully compiled it with cuda version 12.2.,cuda version 12.2,SOFTWARE
"Note that you might have to update c++ compiler and/or paths, e.g.: ```sh conda install cxx-compiler -c conda-forge export CPATH=/usr/local/cuda-12/targets/x86_64-linux/include:$CPATH export LD_LIBRARY_PATH=/usr/local/cuda-12/targets/x86_64-linux/lib:$LD_LIBRARY_PATH export PATH=/usr/local/cuda-12/bin:$PATH ```  ### How to use You can import the activation layer and use it within your network.",conda,SOFTWARE
"Note that you might have to update c++ compiler and/or paths, e.g.: ```sh conda install cxx-compiler -c conda-forge export CPATH=/usr/local/cuda-12/targets/x86_64-linux/include:$CPATH export LD_LIBRARY_PATH=/usr/local/cuda-12/targets/x86_64-linux/lib:$LD_LIBRARY_PATH export PATH=/usr/local/cuda-12/bin:$PATH ```  ### How to use You can import the activation layer and use it within your network.",linux,SOFTWARE
"Note that you might have to update c++ compiler and/or paths, e.g.: ```sh conda install cxx-compiler -c conda-forge export CPATH=/usr/local/cuda-12/targets/x86_64-linux/include:$CPATH export LD_LIBRARY_PATH=/usr/local/cuda-12/targets/x86_64-linux/lib:$LD_LIBRARY_PATH export PATH=/usr/local/cuda-12/bin:$PATH ```  ### How to use You can import the activation layer and use it within your network.",cuda,SOFTWARE
"Note that you might have to update c++ compiler and/or paths, e.g.: ```sh conda install cxx-compiler -c conda-forge export CPATH=/usr/local/cuda-12/targets/x86_64-linux/include:$CPATH export LD_LIBRARY_PATH=/usr/local/cuda-12/targets/x86_64-linux/lib:$LD_LIBRARY_PATH export PATH=/usr/local/cuda-12/bin:$PATH ```  ### How to use You can import the activation layer and use it within your network.",linux,SOFTWARE
"Note that you might have to update c++ compiler and/or paths, e.g.: ```sh conda install cxx-compiler -c conda-forge export CPATH=/usr/local/cuda-12/targets/x86_64-linux/include:$CPATH export LD_LIBRARY_PATH=/usr/local/cuda-12/targets/x86_64-linux/lib:$LD_LIBRARY_PATH export PATH=/usr/local/cuda-12/bin:$PATH ```  ### How to use You can import the activation layer and use it within your network.",cuda,SOFTWARE
"```python import torch from ditac import DiTAC  act = DiTAC() x = torch.randn(8,32,20,20).to('cuda')  act(x) ```  You can recreate Fig. 1 using the sample code: ```sh python run_regression.py ```  **Other experiments' code will be uploaded soon**  ## License This project is released under the MIT license.",python,SOFTWARE
"```python import torch from ditac import DiTAC  act = DiTAC() x = torch.randn(8,32,20,20).to('cuda')  act(x) ```  You can recreate Fig. 1 using the sample code: ```sh python run_regression.py ```  **Other experiments' code will be uploaded soon**  ## License This project is released under the MIT license.",torch,SOFTWARE
"```python import torch from ditac import DiTAC  act = DiTAC() x = torch.randn(8,32,20,20).to('cuda')  act(x) ```  You can recreate Fig. 1 using the sample code: ```sh python run_regression.py ```  **Other experiments' code will be uploaded soon**  ## License This project is released under the MIT license.",ditac,SOFTWARE
"```python import torch from ditac import DiTAC  act = DiTAC() x = torch.randn(8,32,20,20).to('cuda')  act(x) ```  You can recreate Fig. 1 using the sample code: ```sh python run_regression.py ```  **Other experiments' code will be uploaded soon**  ## License This project is released under the MIT license.",DiTAC,SOFTWARE
"```python import torch from ditac import DiTAC  act = DiTAC() x = torch.randn(8,32,20,20).to('cuda')  act(x) ```  You can recreate Fig. 1 using the sample code: ```sh python run_regression.py ```  **Other experiments' code will be uploaded soon**  ## License This project is released under the MIT license.",torch,SOFTWARE
"```python import torch from ditac import DiTAC  act = DiTAC() x = torch.randn(8,32,20,20).to('cuda')  act(x) ```  You can recreate Fig. 1 using the sample code: ```sh python run_regression.py ```  **Other experiments' code will be uploaded soon**  ## License This project is released under the MIT license.",cuda,SOFTWARE
"```python import torch from ditac import DiTAC  act = DiTAC() x = torch.randn(8,32,20,20).to('cuda')  act(x) ```  You can recreate Fig. 1 using the sample code: ```sh python run_regression.py ```  **Other experiments' code will be uploaded soon**  ## License This project is released under the MIT license.",python,SOFTWARE
**WARNING:** Please note that the repository contains [Git LFS](https://git-lfs.github.com/) files.,Git LFS,SOFTWARE
**WARNING:** Please note that the repository contains [Git LFS](https://git-lfs.github.com/) files.,git-lfs,SOFTWARE
/NUBes-guias-de-anotacion.pdf) (in Spanish). * NUBes and IULA+ are distributed in [BRAT standoff format](https://brat.nlplab.org/standoff.html).    ### Size and Composition  * NUBes is divided into 10 samples of approximately 3K sentences each. * The first sample ([SAMPLE-001](.,BRAT,SOFTWARE
/NUBes-guias-de-anotacion.pdf) (in Spanish). * NUBes and IULA+ are distributed in [BRAT standoff format](https://brat.nlplab.org/standoff.html).    ### Size and Composition  * NUBes is divided into 10 samples of approximately 3K sentences each. * The first sample ([SAMPLE-001](.,brat,SOFTWARE
**WARNING:** Please note that this dataset is stored with [Git LFS](https://git-lfs.github.com/).,Git LFS,SOFTWARE
**WARNING:** Please note that this dataset is stored with [Git LFS](https://git-lfs.github.com/).,git-lfs,SOFTWARE
/LREC2020/ablation.py)  | Python version | Dependencies                         | | -------------- | ------------------------------------ | | \>= Python3.5  | [pandas](https://pandas.pydata.org/) |  This script generates the files necessary to perform the ablation study described in the paper.,Python3.5,SOFTWARE
/LREC2020/ablation.py)  | Python version | Dependencies                         | | -------------- | ------------------------------------ | | \>= Python3.5  | [pandas](https://pandas.pydata.org/) |  This script generates the files necessary to perform the ablation study described in the paper.,pandas,SOFTWARE
"/LREC2020/eval.py)  | Python version | Dependencies                                                                                | | -------------- | ------------------------------------------------------------------------------------------- | | \>= Python3.5  | [sklearn](https://scikit-learn.org/stable/index.html), [pandas](https://pandas.pydata.org/) |  This is the script we used to obtain the results reported in the article.",Python3.5,SOFTWARE
"/LREC2020/eval.py)  | Python version | Dependencies                                                                                | | -------------- | ------------------------------------------------------------------------------------------- | | \>= Python3.5  | [sklearn](https://scikit-learn.org/stable/index.html), [pandas](https://pandas.pydata.org/) |  This is the script we used to obtain the results reported in the article.",sklearn,SOFTWARE
"/LREC2020/eval.py)  | Python version | Dependencies                                                                                | | -------------- | ------------------------------------------------------------------------------------------- | | \>= Python3.5  | [sklearn](https://scikit-learn.org/stable/index.html), [pandas](https://pandas.pydata.org/) |  This is the script we used to obtain the results reported in the article.",scikit-learn,SOFTWARE
"/LREC2020/eval.py)  | Python version | Dependencies                                                                                | | -------------- | ------------------------------------------------------------------------------------------- | | \>= Python3.5  | [sklearn](https://scikit-learn.org/stable/index.html), [pandas](https://pandas.pydata.org/) |  This is the script we used to obtain the results reported in the article.",pandas,SOFTWARE
"/LREC2020/eval.py)  | Python version | Dependencies                                                                                | | -------------- | ------------------------------------------------------------------------------------------- | | \>= Python3.5  | [sklearn](https://scikit-learn.org/stable/index.html), [pandas](https://pandas.pydata.org/) |  This is the script we used to obtain the results reported in the article.",pandas,SOFTWARE
"To diversify the instructions, we used ChatGPT to generate varied versions.",ChatGPT,SOFTWARE
"GPT4V) to ask edited images                                                       #         questions(from jsons in `EditData`), get the                                                       #         raw `VLM_judgement` outputs, saved in `EvalData`      ‚îÇ   ‚îú‚îÄ‚îÄ high_level_eval_stage2_final_judge.py    # stage2: using LLM(e.g.",GPT4V,SOFTWARE
"GPT4-turbo) with designed template                                                       #         to get the more stable `final_judgement`      ‚îÇ   ‚îú‚îÄ‚îÄ low_level_eval.py                        ## evaluation for low-level dimensions, e.g.",GPT4-turbo,SOFTWARE
"GPT4V, GPT4-turbo, CLIP, SSIM      ‚îÇ   ‚îú‚îÄ‚îÄ sample_rank_gen.py                       ## generate script for `EditRank_ori` and `EditRank`      ‚îÇ   ‚îú‚îÄ‚îÄ summary.json                             ## generated by `summary.py`      ‚îÇ   ‚îú‚îÄ‚îÄ summary_ori.json                         ## generated by `summary.py`      ‚îÇ   ‚îú‚îÄ‚îÄ summary.py                               ## generate script for `summary.json` and `summary.json`,                                                       #  describe metric scores for every models in every dimensions      ‚îÇ   ‚îú‚îÄ‚îÄ summary_model_type_avg_score.json        ## generated by `summary_model_type_avg_score.py`      ‚îÇ   ‚îî‚îÄ‚îÄ summary_model_type_avg_score.py          ## generate script for `summary_model_type_avg_score.json`,                                                       # describe metric scores for every editing models                                                       # in every dimensions      ‚îî‚îÄ‚îÄ readme.md    ```    # ü§î How to evaluate with my own editing model    [Check how to evaluate with my own editing model](.",GPT4V,SOFTWARE
"GPT4V, GPT4-turbo, CLIP, SSIM      ‚îÇ   ‚îú‚îÄ‚îÄ sample_rank_gen.py                       ## generate script for `EditRank_ori` and `EditRank`      ‚îÇ   ‚îú‚îÄ‚îÄ summary.json                             ## generated by `summary.py`      ‚îÇ   ‚îú‚îÄ‚îÄ summary_ori.json                         ## generated by `summary.py`      ‚îÇ   ‚îú‚îÄ‚îÄ summary.py                               ## generate script for `summary.json` and `summary.json`,                                                       #  describe metric scores for every models in every dimensions      ‚îÇ   ‚îú‚îÄ‚îÄ summary_model_type_avg_score.json        ## generated by `summary_model_type_avg_score.py`      ‚îÇ   ‚îî‚îÄ‚îÄ summary_model_type_avg_score.py          ## generate script for `summary_model_type_avg_score.json`,                                                       # describe metric scores for every editing models                                                       # in every dimensions      ‚îî‚îÄ‚îÄ readme.md    ```    # ü§î How to evaluate with my own editing model    [Check how to evaluate with my own editing model](.",GPT4-turbo,SOFTWARE
"GPT4V, GPT4-turbo, CLIP, SSIM      ‚îÇ   ‚îú‚îÄ‚îÄ sample_rank_gen.py                       ## generate script for `EditRank_ori` and `EditRank`      ‚îÇ   ‚îú‚îÄ‚îÄ summary.json                             ## generated by `summary.py`      ‚îÇ   ‚îú‚îÄ‚îÄ summary_ori.json                         ## generated by `summary.py`      ‚îÇ   ‚îú‚îÄ‚îÄ summary.py                               ## generate script for `summary.json` and `summary.json`,                                                       #  describe metric scores for every models in every dimensions      ‚îÇ   ‚îú‚îÄ‚îÄ summary_model_type_avg_score.json        ## generated by `summary_model_type_avg_score.py`      ‚îÇ   ‚îî‚îÄ‚îÄ summary_model_type_avg_score.py          ## generate script for `summary_model_type_avg_score.json`,                                                       # describe metric scores for every editing models                                                       # in every dimensions      ‚îî‚îÄ‚îÄ readme.md    ```    # ü§î How to evaluate with my own editing model    [Check how to evaluate with my own editing model](.",CLIP,SOFTWARE
"(https://github.com/slideflow/slideflow/assets/48372806/7f43d8cb-dc80-427d-84c4-3e5a35fa1472)  </div>  **Slideflow is a deep learning library for digital pathology, offering a user-friendly interface for model development.**    Designed for both medical researchers and AI enthusiasts, the goal of Slideflow is to provide an accessible, easy-to-use interface for developing state-of-the-art pathology models.",Slideflow,SOFTWARE
"(https://github.com/slideflow/slideflow/assets/48372806/7f43d8cb-dc80-427d-84c4-3e5a35fa1472)  </div>  **Slideflow is a deep learning library for digital pathology, offering a user-friendly interface for model development.**    Designed for both medical researchers and AI enthusiasts, the goal of Slideflow is to provide an accessible, easy-to-use interface for developing state-of-the-art pathology models.",Slideflow,SOFTWARE
"Slideflow has been built with the future in mind, offering a scalable platform for digital biomarker development that bridges the gap between ever-evolving, sophisticated methods and the needs of a clinical researcher.",Slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",Slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",Heatmaps,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",heatmaps,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",mosaic maps,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",mosaic-maps,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",saliency maps,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"For developers, Slideflow provides multiple endpoints for integration with other packages and external training paradigms, allowing you to leverage highly optimized, pathology-specific processes with the latest ML methodologies.    ## üöÄ  Features - Easy-to-use, highly customizable training pipelines - Robust **[slide processing](https://slideflow.dev/slide_processing) and [stain normalization](https://slideflow.dev/norm)** toolkit - Support for training with **[weakly-supervised](https://slideflow.dev/training) or [strongly-supervised](https://slideflow.dev/tile_labels)** labels - Built-in, state-of-the-art **[foundation models](https://slideflow.dev/features)** - **[Multiple-instance learning (MIL)](https://slideflow.dev/mil)** - **[Self-supervised learning (SSL)](https://slideflow.dev/ssl)** - **[Generative adversarial networks (GANs)](https://slideflow.dev/training)** - **Explainability tools**: [Heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [saliency maps](https://slideflow.dev/saliency/), [synthetic histology](https://slideflow.dev/stylegan) - Robust **[layer activation analysis](https://slideflow.dev/posthoc)** tools - **[Uncertainty quantification](https://slideflow.dev/uq)** - **[Interactive user interface](https://slideflow.dev/studio)** for model deployment - ... and more!",slideflow,SOFTWARE
"Full documentation with example tutorials can be found at [slideflow.dev](https://www.slideflow.dev/).  ## Requirements - Python >= 3.7 (<3.10 if using [cuCIM](https://docs.rapids.ai/api/cucim/stable/)) - [PyTorch](https://pytorch.org/) >= 1.9 _or_ [Tensorflow](https://www.tensorflow.org/) 2.5-2.11  ### Optional - [Libvips](https://libvips.github.io/libvips/) >= 8.9 (alternative slide reader, adds support for *.scn, *.mrxs, *.ndpi, *.vms, and *.vmu files). - Linear solver (for preserved-site cross-validation)   - [CPLEX](https://www.ibm.com/docs/en/icos/12.10.0?",slideflow,SOFTWARE
"Full documentation with example tutorials can be found at [slideflow.dev](https://www.slideflow.dev/).  ## Requirements - Python >= 3.7 (<3.10 if using [cuCIM](https://docs.rapids.ai/api/cucim/stable/)) - [PyTorch](https://pytorch.org/) >= 1.9 _or_ [Tensorflow](https://www.tensorflow.org/) 2.5-2.11  ### Optional - [Libvips](https://libvips.github.io/libvips/) >= 8.9 (alternative slide reader, adds support for *.scn, *.mrxs, *.ndpi, *.vms, and *.vmu files). - Linear solver (for preserved-site cross-validation)   - [CPLEX](https://www.ibm.com/docs/en/icos/12.10.0?",slideflow,SOFTWARE
"Full documentation with example tutorials can be found at [slideflow.dev](https://www.slideflow.dev/).  ## Requirements - Python >= 3.7 (<3.10 if using [cuCIM](https://docs.rapids.ai/api/cucim/stable/)) - [PyTorch](https://pytorch.org/) >= 1.9 _or_ [Tensorflow](https://www.tensorflow.org/) 2.5-2.11  ### Optional - [Libvips](https://libvips.github.io/libvips/) >= 8.9 (alternative slide reader, adds support for *.scn, *.mrxs, *.ndpi, *.vms, and *.vmu files). - Linear solver (for preserved-site cross-validation)   - [CPLEX](https://www.ibm.com/docs/en/icos/12.10.0?",cuCIM,SOFTWARE
"Full documentation with example tutorials can be found at [slideflow.dev](https://www.slideflow.dev/).  ## Requirements - Python >= 3.7 (<3.10 if using [cuCIM](https://docs.rapids.ai/api/cucim/stable/)) - [PyTorch](https://pytorch.org/) >= 1.9 _or_ [Tensorflow](https://www.tensorflow.org/) 2.5-2.11  ### Optional - [Libvips](https://libvips.github.io/libvips/) >= 8.9 (alternative slide reader, adds support for *.scn, *.mrxs, *.ndpi, *.vms, and *.vmu files). - Linear solver (for preserved-site cross-validation)   - [CPLEX](https://www.ibm.com/docs/en/icos/12.10.0?",cucim,SOFTWARE
"Full documentation with example tutorials can be found at [slideflow.dev](https://www.slideflow.dev/).  ## Requirements - Python >= 3.7 (<3.10 if using [cuCIM](https://docs.rapids.ai/api/cucim/stable/)) - [PyTorch](https://pytorch.org/) >= 1.9 _or_ [Tensorflow](https://www.tensorflow.org/) 2.5-2.11  ### Optional - [Libvips](https://libvips.github.io/libvips/) >= 8.9 (alternative slide reader, adds support for *.scn, *.mrxs, *.ndpi, *.vms, and *.vmu files). - Linear solver (for preserved-site cross-validation)   - [CPLEX](https://www.ibm.com/docs/en/icos/12.10.0?",PyTorch,SOFTWARE
"Full documentation with example tutorials can be found at [slideflow.dev](https://www.slideflow.dev/).  ## Requirements - Python >= 3.7 (<3.10 if using [cuCIM](https://docs.rapids.ai/api/cucim/stable/)) - [PyTorch](https://pytorch.org/) >= 1.9 _or_ [Tensorflow](https://www.tensorflow.org/) 2.5-2.11  ### Optional - [Libvips](https://libvips.github.io/libvips/) >= 8.9 (alternative slide reader, adds support for *.scn, *.mrxs, *.ndpi, *.vms, and *.vmu files). - Linear solver (for preserved-site cross-validation)   - [CPLEX](https://www.ibm.com/docs/en/icos/12.10.0?",pytorch,SOFTWARE
"Full documentation with example tutorials can be found at [slideflow.dev](https://www.slideflow.dev/).  ## Requirements - Python >= 3.7 (<3.10 if using [cuCIM](https://docs.rapids.ai/api/cucim/stable/)) - [PyTorch](https://pytorch.org/) >= 1.9 _or_ [Tensorflow](https://www.tensorflow.org/) 2.5-2.11  ### Optional - [Libvips](https://libvips.github.io/libvips/) >= 8.9 (alternative slide reader, adds support for *.scn, *.mrxs, *.ndpi, *.vms, and *.vmu files). - Linear solver (for preserved-site cross-validation)   - [CPLEX](https://www.ibm.com/docs/en/icos/12.10.0?",Tensorflow,SOFTWARE
"Full documentation with example tutorials can be found at [slideflow.dev](https://www.slideflow.dev/).  ## Requirements - Python >= 3.7 (<3.10 if using [cuCIM](https://docs.rapids.ai/api/cucim/stable/)) - [PyTorch](https://pytorch.org/) >= 1.9 _or_ [Tensorflow](https://www.tensorflow.org/) 2.5-2.11  ### Optional - [Libvips](https://libvips.github.io/libvips/) >= 8.9 (alternative slide reader, adds support for *.scn, *.mrxs, *.ndpi, *.vms, and *.vmu files). - Linear solver (for preserved-site cross-validation)   - [CPLEX](https://www.ibm.com/docs/en/icos/12.10.0?",tensorflow,SOFTWARE
"Full documentation with example tutorials can be found at [slideflow.dev](https://www.slideflow.dev/).  ## Requirements - Python >= 3.7 (<3.10 if using [cuCIM](https://docs.rapids.ai/api/cucim/stable/)) - [PyTorch](https://pytorch.org/) >= 1.9 _or_ [Tensorflow](https://www.tensorflow.org/) 2.5-2.11  ### Optional - [Libvips](https://libvips.github.io/libvips/) >= 8.9 (alternative slide reader, adds support for *.scn, *.mrxs, *.ndpi, *.vms, and *.vmu files). - Linear solver (for preserved-site cross-validation)   - [CPLEX](https://www.ibm.com/docs/en/icos/12.10.0?",Libvips,SOFTWARE
"Full documentation with example tutorials can be found at [slideflow.dev](https://www.slideflow.dev/).  ## Requirements - Python >= 3.7 (<3.10 if using [cuCIM](https://docs.rapids.ai/api/cucim/stable/)) - [PyTorch](https://pytorch.org/) >= 1.9 _or_ [Tensorflow](https://www.tensorflow.org/) 2.5-2.11  ### Optional - [Libvips](https://libvips.github.io/libvips/) >= 8.9 (alternative slide reader, adds support for *.scn, *.mrxs, *.ndpi, *.vms, and *.vmu files). - Linear solver (for preserved-site cross-validation)   - [CPLEX](https://www.ibm.com/docs/en/icos/12.10.0?",CPLEX,SOFTWARE
"topic=cplex-setting-up-python-api)   - _or_ [Pyomo](http://www.pyomo.org/installation) with [Bonmin](https://anaconda.org/conda-forge/coinbonmin) solver   ## üì•  Installation Slideflow can be installed with PyPI, as a Docker container, or run from source.  ### Method 1: Install via pip  ``` pip3 install --upgrade setuptools pip wheel pip3 install slideflow[cucim] cupy-cuda11x ```  The `cupy` package name depends on the installed CUDA version; [see here](https://docs.cupy.dev/en/stable/install.html#installing-cupy) for installation instructions.",Pyomo,SOFTWARE
"topic=cplex-setting-up-python-api)   - _or_ [Pyomo](http://www.pyomo.org/installation) with [Bonmin](https://anaconda.org/conda-forge/coinbonmin) solver   ## üì•  Installation Slideflow can be installed with PyPI, as a Docker container, or run from source.  ### Method 1: Install via pip  ``` pip3 install --upgrade setuptools pip wheel pip3 install slideflow[cucim] cupy-cuda11x ```  The `cupy` package name depends on the installed CUDA version; [see here](https://docs.cupy.dev/en/stable/install.html#installing-cupy) for installation instructions.",Bonmin,SOFTWARE
"topic=cplex-setting-up-python-api)   - _or_ [Pyomo](http://www.pyomo.org/installation) with [Bonmin](https://anaconda.org/conda-forge/coinbonmin) solver   ## üì•  Installation Slideflow can be installed with PyPI, as a Docker container, or run from source.  ### Method 1: Install via pip  ``` pip3 install --upgrade setuptools pip wheel pip3 install slideflow[cucim] cupy-cuda11x ```  The `cupy` package name depends on the installed CUDA version; [see here](https://docs.cupy.dev/en/stable/install.html#installing-cupy) for installation instructions.",PyPI,SOFTWARE
"topic=cplex-setting-up-python-api)   - _or_ [Pyomo](http://www.pyomo.org/installation) with [Bonmin](https://anaconda.org/conda-forge/coinbonmin) solver   ## üì•  Installation Slideflow can be installed with PyPI, as a Docker container, or run from source.  ### Method 1: Install via pip  ``` pip3 install --upgrade setuptools pip wheel pip3 install slideflow[cucim] cupy-cuda11x ```  The `cupy` package name depends on the installed CUDA version; [see here](https://docs.cupy.dev/en/stable/install.html#installing-cupy) for installation instructions.",pip,SOFTWARE
"topic=cplex-setting-up-python-api)   - _or_ [Pyomo](http://www.pyomo.org/installation) with [Bonmin](https://anaconda.org/conda-forge/coinbonmin) solver   ## üì•  Installation Slideflow can be installed with PyPI, as a Docker container, or run from source.  ### Method 1: Install via pip  ``` pip3 install --upgrade setuptools pip wheel pip3 install slideflow[cucim] cupy-cuda11x ```  The `cupy` package name depends on the installed CUDA version; [see here](https://docs.cupy.dev/en/stable/install.html#installing-cupy) for installation instructions.",pip,SOFTWARE
"topic=cplex-setting-up-python-api)   - _or_ [Pyomo](http://www.pyomo.org/installation) with [Bonmin](https://anaconda.org/conda-forge/coinbonmin) solver   ## üì•  Installation Slideflow can be installed with PyPI, as a Docker container, or run from source.  ### Method 1: Install via pip  ``` pip3 install --upgrade setuptools pip wheel pip3 install slideflow[cucim] cupy-cuda11x ```  The `cupy` package name depends on the installed CUDA version; [see here](https://docs.cupy.dev/en/stable/install.html#installing-cupy) for installation instructions.",slideflow,SOFTWARE
"topic=cplex-setting-up-python-api)   - _or_ [Pyomo](http://www.pyomo.org/installation) with [Bonmin](https://anaconda.org/conda-forge/coinbonmin) solver   ## üì•  Installation Slideflow can be installed with PyPI, as a Docker container, or run from source.  ### Method 1: Install via pip  ``` pip3 install --upgrade setuptools pip wheel pip3 install slideflow[cucim] cupy-cuda11x ```  The `cupy` package name depends on the installed CUDA version; [see here](https://docs.cupy.dev/en/stable/install.html#installing-cupy) for installation instructions.",cucim,SOFTWARE
"topic=cplex-setting-up-python-api)   - _or_ [Pyomo](http://www.pyomo.org/installation) with [Bonmin](https://anaconda.org/conda-forge/coinbonmin) solver   ## üì•  Installation Slideflow can be installed with PyPI, as a Docker container, or run from source.  ### Method 1: Install via pip  ``` pip3 install --upgrade setuptools pip wheel pip3 install slideflow[cucim] cupy-cuda11x ```  The `cupy` package name depends on the installed CUDA version; [see here](https://docs.cupy.dev/en/stable/install.html#installing-cupy) for installation instructions.",cupy,SOFTWARE
"topic=cplex-setting-up-python-api)   - _or_ [Pyomo](http://www.pyomo.org/installation) with [Bonmin](https://anaconda.org/conda-forge/coinbonmin) solver   ## üì•  Installation Slideflow can be installed with PyPI, as a Docker container, or run from source.  ### Method 1: Install via pip  ``` pip3 install --upgrade setuptools pip wheel pip3 install slideflow[cucim] cupy-cuda11x ```  The `cupy` package name depends on the installed CUDA version; [see here](https://docs.cupy.dev/en/stable/install.html#installing-cupy) for installation instructions.",CUDA,SOFTWARE
"topic=cplex-setting-up-python-api)   - _or_ [Pyomo](http://www.pyomo.org/installation) with [Bonmin](https://anaconda.org/conda-forge/coinbonmin) solver   ## üì•  Installation Slideflow can be installed with PyPI, as a Docker container, or run from source.  ### Method 1: Install via pip  ``` pip3 install --upgrade setuptools pip wheel pip3 install slideflow[cucim] cupy-cuda11x ```  The `cupy` package name depends on the installed CUDA version; [see here](https://docs.cupy.dev/en/stable/install.html#installing-cupy) for installation instructions.",cupy,SOFTWARE
"topic=cplex-setting-up-python-api)   - _or_ [Pyomo](http://www.pyomo.org/installation) with [Bonmin](https://anaconda.org/conda-forge/coinbonmin) solver   ## üì•  Installation Slideflow can be installed with PyPI, as a Docker container, or run from source.  ### Method 1: Install via pip  ``` pip3 install --upgrade setuptools pip wheel pip3 install slideflow[cucim] cupy-cuda11x ```  The `cupy` package name depends on the installed CUDA version; [see here](https://docs.cupy.dev/en/stable/install.html#installing-cupy) for installation instructions.",cupy,SOFTWARE
"`cupy` is not required if using Libvips.  ### Method 2: Docker image  Alternatively, pre-configured [docker images](https://hub.docker.com/repository/docker/jamesdolezal/slideflow) are available with OpenSlide/Libvips and the latest version of either Tensorflow and PyTorch.",cupy,SOFTWARE
"`cupy` is not required if using Libvips.  ### Method 2: Docker image  Alternatively, pre-configured [docker images](https://hub.docker.com/repository/docker/jamesdolezal/slideflow) are available with OpenSlide/Libvips and the latest version of either Tensorflow and PyTorch.",Docker,SOFTWARE
"`cupy` is not required if using Libvips.  ### Method 2: Docker image  Alternatively, pre-configured [docker images](https://hub.docker.com/repository/docker/jamesdolezal/slideflow) are available with OpenSlide/Libvips and the latest version of either Tensorflow and PyTorch.",docker,SOFTWARE
"`cupy` is not required if using Libvips.  ### Method 2: Docker image  Alternatively, pre-configured [docker images](https://hub.docker.com/repository/docker/jamesdolezal/slideflow) are available with OpenSlide/Libvips and the latest version of either Tensorflow and PyTorch.",docker,SOFTWARE
"`cupy` is not required if using Libvips.  ### Method 2: Docker image  Alternatively, pre-configured [docker images](https://hub.docker.com/repository/docker/jamesdolezal/slideflow) are available with OpenSlide/Libvips and the latest version of either Tensorflow and PyTorch.",slideflow,SOFTWARE
"`cupy` is not required if using Libvips.  ### Method 2: Docker image  Alternatively, pre-configured [docker images](https://hub.docker.com/repository/docker/jamesdolezal/slideflow) are available with OpenSlide/Libvips and the latest version of either Tensorflow and PyTorch.",Tensorflow,SOFTWARE
"`cupy` is not required if using Libvips.  ### Method 2: Docker image  Alternatively, pre-configured [docker images](https://hub.docker.com/repository/docker/jamesdolezal/slideflow) are available with OpenSlide/Libvips and the latest version of either Tensorflow and PyTorch.",PyTorch,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",Tensorflow,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",docker,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",slideflow,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",docker,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",slideflow,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",PyTorch,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",docker,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",slideflow,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",torch,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",docker,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",slideflow,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",torch,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",slideflow,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",slideflow,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",pip,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",slideflow,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",cupy,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",pip,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",slideflow,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",StyleGAN3,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",stylegan3,SOFTWARE
"To install with the Tensorflow backend:  ``` docker pull jamesdolezal/slideflow:latest-tf docker run -it --gpus all jamesdolezal/slideflow:latest-tf ```  To install with the PyTorch backend:  ``` docker pull jamesdolezal/slideflow:latest-torch docker run -it --shm-size=2g --gpus all jamesdolezal/slideflow:latest-torch ```  ### Method 3: From source  To run from source, clone this repository, install the conda development environment, and build a wheel:  ``` git clone https://github.com/slideflow/slideflow conda env create -f slideflow/environment.yml conda activate slideflow pip install -e slideflow/ cupy-cuda11x ```  ### Non-Commercial Add-ons  To add additional tools and pretrained models available under a non-commercial license, install `slideflow-gpl` and `slideflow-noncommercial`:  ``` pip install slideflow-gpl slideflow-noncommercial ```  This will provide integrated access to 6 additional pretrained foundation models ([UNI](https://www.nature.com/articles/s41591-024-02857-3), [HistoSSL](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2.full.pdf), [GigaPath](https://aka.ms/gigapath), [PLIP](https://www.nature.com/articles/s41591-023-02504-3), [RetCCL](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730), and [CTransPath](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)), the MIL architecture [CLAM](https://www.nature.com/articles/s41551-020-00682-w), the UQ algorithm [BISCUIT](https://www.nature.com/articles/s41467-022-34025-x), and the GAN framework [StyleGAN3](https://nvlabs-fi-cdn.nvidia.com/stylegan3/stylegan3-paper.pdf).  ## ‚öôÔ∏è Configuration  ### Deep learning (PyTorch vs.",PyTorch,SOFTWARE
"Tensorflow)  Slideflow supports both PyTorch and Tensorflow, defaulting to PyTorch if both are available.",Tensorflow,SOFTWARE
"Tensorflow)  Slideflow supports both PyTorch and Tensorflow, defaulting to PyTorch if both are available.",PyTorch,SOFTWARE
"Tensorflow)  Slideflow supports both PyTorch and Tensorflow, defaulting to PyTorch if both are available.",Tensorflow,SOFTWARE
"Tensorflow)  Slideflow supports both PyTorch and Tensorflow, defaulting to PyTorch if both are available.",PyTorch,SOFTWARE
For example:  ``` export SF_BACKEND=tensorflow ```  ### Slide reading (cuCIM vs.,tensorflow,SOFTWARE
For example:  ``` export SF_BACKEND=tensorflow ```  ### Slide reading (cuCIM vs.,cuCIM,SOFTWARE
"Libvips)  By default, Slideflow reads whole-slide images using [cuCIM](https://docs.rapids.ai/api/cucim/stable/).",Libvips,SOFTWARE
"Libvips)  By default, Slideflow reads whole-slide images using [cuCIM](https://docs.rapids.ai/api/cucim/stable/).",Slideflow,SOFTWARE
"Libvips)  By default, Slideflow reads whole-slide images using [cuCIM](https://docs.rapids.ai/api/cucim/stable/).",cuCIM,SOFTWARE
"Slideflow also includes a [Libvips](https://libvips.github.io/libvips/) backend, which adds support for *.scn, *.mrxs, *.ndpi, *.vms, and *.vmu files.",Libvips,SOFTWARE
"Slideflow also includes a [Libvips](https://libvips.github.io/libvips/) backend, which adds support for *.scn, *.mrxs, *.ndpi, *.vms, and *.vmu files.",libvips,SOFTWARE
"The fastest way to get started is to use one of our preconfigured projects, which will automatically download slides from the Genomic Data Commons:  ```python import slideflow as sf  P = sf.create_project(     root='/project/destination',     cfg=sf.project.LungAdenoSquam(),     download=True ) ```  After the slides have been downloaded and verified, you can skip to [Extract tiles from slides](#extract-tiles-from-slides).",slideflow,SOFTWARE
"Alternatively, to create a new custom project, supply the location of patient-level annotations (CSV), slides, and a destination for TFRecords to be saved:  ```python import slideflow as sf P = sf.create_project(   '/project/path',   annotations=""/patient/annotations.csv"",   slides=""/slides/directory"",   tfrecords=""/tfrecords/directory"" ) ```  Ensure that the annotations file has a `slide` column for each annotation entry with the filename (without extension) of the corresponding slide.  ### Extract tiles from slides  Next, whole-slide images are segmented into smaller image tiles and saved in `*.tfrecords` format.",slideflow,SOFTWARE
"[Extract tiles](https://slideflow.dev/slide_processing) from slides at a given magnification (width in microns size) and resolution (width in pixels) using `sf.Project.extract_tiles()`:  ```python P.extract_tiles(   tile_px=299,  # Tile size, in pixels   tile_um=302   # Tile size, in microns ) ```  If slides are on a network drive or a spinning HDD, tile extraction can be accelerated by buffering slides to a SSD or ramdisk:  ```python P.extract_tiles(   ...,   buffer=""/mnt/ramdisk"" ) ```  ### Training models  Once tiles are extracted, models can be [trained](https://slideflow.dev/training).",slideflow,SOFTWARE
"Start by configuring a set of [hyperparameters](https://slideflow.dev/model#modelparams):  ```python params = sf.ModelParams(   tile_px=299,   tile_um=302,   batch_size=32,   model='xception',   learning_rate=0.0001,   ... ) ```  Models can then be trained using these parameters.",slideflow,SOFTWARE
"Models can be trained to categorical, multi-categorical, continuous, or time-series outcomes, and the training process is [highly configurable](https://slideflow.dev/training).",slideflow,SOFTWARE
"For example, to train models in cross-validation to predict the outcome `'category1'` as stored in the project annotations file:  ```python P.train(   'category1',   params=params,   save_predictions=True,   multi_gpu=True ) ```  ### Evaluation, heatmaps, mosaic maps, and more  Slideflow includes a host of additional tools, including model [evaluation and prediction](https://slideflow.dev/evaluation), [heatmaps](https://slideflow.dev/evaluation#heatmaps), analysis of [layer activations](https://slideflow.dev/posthoc), [mosaic maps](https://slideflow.dev/posthoc#mosaic-maps), and more.",Slideflow,SOFTWARE
"For example, to train models in cross-validation to predict the outcome `'category1'` as stored in the project annotations file:  ```python P.train(   'category1',   params=params,   save_predictions=True,   multi_gpu=True ) ```  ### Evaluation, heatmaps, mosaic maps, and more  Slideflow includes a host of additional tools, including model [evaluation and prediction](https://slideflow.dev/evaluation), [heatmaps](https://slideflow.dev/evaluation#heatmaps), analysis of [layer activations](https://slideflow.dev/posthoc), [mosaic maps](https://slideflow.dev/posthoc#mosaic-maps), and more.",slideflow,SOFTWARE
"For example, to train models in cross-validation to predict the outcome `'category1'` as stored in the project annotations file:  ```python P.train(   'category1',   params=params,   save_predictions=True,   multi_gpu=True ) ```  ### Evaluation, heatmaps, mosaic maps, and more  Slideflow includes a host of additional tools, including model [evaluation and prediction](https://slideflow.dev/evaluation), [heatmaps](https://slideflow.dev/evaluation#heatmaps), analysis of [layer activations](https://slideflow.dev/posthoc), [mosaic maps](https://slideflow.dev/posthoc#mosaic-maps), and more.",slideflow,SOFTWARE
"For example, to train models in cross-validation to predict the outcome `'category1'` as stored in the project annotations file:  ```python P.train(   'category1',   params=params,   save_predictions=True,   multi_gpu=True ) ```  ### Evaluation, heatmaps, mosaic maps, and more  Slideflow includes a host of additional tools, including model [evaluation and prediction](https://slideflow.dev/evaluation), [heatmaps](https://slideflow.dev/evaluation#heatmaps), analysis of [layer activations](https://slideflow.dev/posthoc), [mosaic maps](https://slideflow.dev/posthoc#mosaic-maps), and more.",slideflow,SOFTWARE
"For example, to train models in cross-validation to predict the outcome `'category1'` as stored in the project annotations file:  ```python P.train(   'category1',   params=params,   save_predictions=True,   multi_gpu=True ) ```  ### Evaluation, heatmaps, mosaic maps, and more  Slideflow includes a host of additional tools, including model [evaluation and prediction](https://slideflow.dev/evaluation), [heatmaps](https://slideflow.dev/evaluation#heatmaps), analysis of [layer activations](https://slideflow.dev/posthoc), [mosaic maps](https://slideflow.dev/posthoc#mosaic-maps), and more.",slideflow,SOFTWARE
"See our [full documentation](https://slideflow.dev) for more details and tutorials.  ## üìö  Publications  Slideflow has been used by:  - [Dolezal et al](https://www.nature.com/articles/s41379-020-00724-3), _Modern Pathology_, 2020 - [Rosenberg et al](https://ascopubs.org/doi/10.1200/JCO.2020.38.15_suppl.e23529), _Journal of Clinical Oncology_ [abstract], 2020 - [Howard et al](https://www.nature.com/articles/s41467-021-24698-1), _Nature Communications_, 2021 - [Dolezal et al](https://www.nature.com/articles/s41467-022-34025-x) _Nature Communications_, 2022 - [Storozuk et al](https://www.nature.com/articles/s41379-022-01039-1.pdf), _Modern Pathology_ [abstract], 2022 - [Partin et al](https://doi.org/10.3389/fmed.2023.1058919) _Front Med_, 2022 - [Dolezal et al](https://ascopubs.org/doi/abs/10.1200/JCO.2022.40.16_suppl.8549) _Journal of Clinical Oncology_ [abstract], 2022 - [Dolezal et al](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9792820/) _Mediastinum_ [abstract], 2022 - [Howard et al](https://www.nature.com/articles/s41523-023-00530-5) _npj Breast Cancer_, 2023 - [Dolezal et al](https://www.nature.com/articles/s41698-023-00399-4) _npj Precision Oncology_, 2023 - [Hieromnimon et al](https://doi.org/10.1101/2023.03.22.533810) [bioRxiv], 2023 - [Carrillo-Perez et al](https://doi.org/10.1186/s40644-023-00586-3) _Cancer Imaging_, 2023  ## üîì  License This code is made available under the Apache-2.0 license.  ## üîó  Reference If you find our work useful for your research, or if you use parts of this code, please consider citing as follows:  Dolezal, J.M., Kochanny, S., Dyer, E. et al.",slideflow,SOFTWARE
"See our [full documentation](https://slideflow.dev) for more details and tutorials.  ## üìö  Publications  Slideflow has been used by:  - [Dolezal et al](https://www.nature.com/articles/s41379-020-00724-3), _Modern Pathology_, 2020 - [Rosenberg et al](https://ascopubs.org/doi/10.1200/JCO.2020.38.15_suppl.e23529), _Journal of Clinical Oncology_ [abstract], 2020 - [Howard et al](https://www.nature.com/articles/s41467-021-24698-1), _Nature Communications_, 2021 - [Dolezal et al](https://www.nature.com/articles/s41467-022-34025-x) _Nature Communications_, 2022 - [Storozuk et al](https://www.nature.com/articles/s41379-022-01039-1.pdf), _Modern Pathology_ [abstract], 2022 - [Partin et al](https://doi.org/10.3389/fmed.2023.1058919) _Front Med_, 2022 - [Dolezal et al](https://ascopubs.org/doi/abs/10.1200/JCO.2022.40.16_suppl.8549) _Journal of Clinical Oncology_ [abstract], 2022 - [Dolezal et al](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9792820/) _Mediastinum_ [abstract], 2022 - [Howard et al](https://www.nature.com/articles/s41523-023-00530-5) _npj Breast Cancer_, 2023 - [Dolezal et al](https://www.nature.com/articles/s41698-023-00399-4) _npj Precision Oncology_, 2023 - [Hieromnimon et al](https://doi.org/10.1101/2023.03.22.533810) [bioRxiv], 2023 - [Carrillo-Perez et al](https://doi.org/10.1186/s40644-023-00586-3) _Cancer Imaging_, 2023  ## üîì  License This code is made available under the Apache-2.0 license.  ## üîó  Reference If you find our work useful for your research, or if you use parts of this code, please consider citing as follows:  Dolezal, J.M., Kochanny, S., Dyer, E. et al.",Slideflow,SOFTWARE
[PyPI version](https://badge.fury.io/py/prdc.svg)](https://badge.fury.io/py/prdc) [!,PyPI,SOFTWARE
[PyPI download month](https://img.shields.io/pypi/dm/prdc.svg)](https://pypi.python.org/pypi/prdc/) [!,PyPI,SOFTWARE
[PyPI download month](https://img.shields.io/pypi/dm/prdc.svg)](https://pypi.python.org/pypi/prdc/) [!,pypi,SOFTWARE
[PyPI download month](https://img.shields.io/pypi/dm/prdc.svg)](https://pypi.python.org/pypi/prdc/) [!,pypi,SOFTWARE
[PyPI download month](https://img.shields.io/pypi/dm/prdc.svg)](https://pypi.python.org/pypi/prdc/) [!,pypi,SOFTWARE
"[PyPI license](https://img.shields.io/pypi/l/prdc.svg)](https://pypi.python.org/pypi/prdc/)  ## Reliable Fidelity and Diversity Metrics for Generative Models (ICML 2020)  [Paper: Reliable Fidelity and Diversity Metrics for Generative Models](https://arxiv.org/abs/2002.09797)  Muhammad Ferjad Naeem <sup>1,3*</sup>, Seong Joon Oh<sup>2*</sup>, Yunjey Choi<sup>1</sup>,  Youngjung Uh<sup>1</sup>, Jaejun Yoo<sup>1,4</sup>    <sub>**Work done at Clova AI Research**</sub>  <sub>\* Equal contribution</sub> <sup>1</sup> <sub>Clova AI Research, NAVER Corp.",pypi,SOFTWARE
"[PyPI license](https://img.shields.io/pypi/l/prdc.svg)](https://pypi.python.org/pypi/prdc/)  ## Reliable Fidelity and Diversity Metrics for Generative Models (ICML 2020)  [Paper: Reliable Fidelity and Diversity Metrics for Generative Models](https://arxiv.org/abs/2002.09797)  Muhammad Ferjad Naeem <sup>1,3*</sup>, Seong Joon Oh<sup>2*</sup>, Yunjey Choi<sup>1</sup>,  Youngjung Uh<sup>1</sup>, Jaejun Yoo<sup>1,4</sup>    <sub>**Work done at Clova AI Research**</sub>  <sub>\* Equal contribution</sub> <sup>1</sup> <sub>Clova AI Research, NAVER Corp.",pypi,SOFTWARE
"[PyPI license](https://img.shields.io/pypi/l/prdc.svg)](https://pypi.python.org/pypi/prdc/)  ## Reliable Fidelity and Diversity Metrics for Generative Models (ICML 2020)  [Paper: Reliable Fidelity and Diversity Metrics for Generative Models](https://arxiv.org/abs/2002.09797)  Muhammad Ferjad Naeem <sup>1,3*</sup>, Seong Joon Oh<sup>2*</sup>, Yunjey Choi<sup>1</sup>,  Youngjung Uh<sup>1</sup>, Jaejun Yoo<sup>1,4</sup>    <sub>**Work done at Clova AI Research**</sub>  <sub>\* Equal contribution</sub> <sup>1</sup> <sub>Clova AI Research, NAVER Corp.",pypi,SOFTWARE
"Usage  ### Installation  ```bash pip3 install prdc ```  ### Example  Test 10000 real and fake samples form the standard normal distribution N(0,I) in 1000-dimensional Euclidean space.",pip3,SOFTWARE
"Usage  ### Installation  ```bash pip3 install prdc ```  ### Example  Test 10000 real and fake samples form the standard normal distribution N(0,I) in 1000-dimensional Euclidean space.",prdc,SOFTWARE
"```python import numpy as np from prdc import compute_prdc   num_real_samples = num_fake_samples = 10000 feature_dim = 1000 nearest_k = 5 real_features = np.random.normal(loc=0.0, scale=1.0,                                  size=[num_real_samples, feature_dim])  fake_features = np.random.normal(loc=0.0, scale=1.0,                                  size=[num_fake_samples, feature_dim])  metrics = compute_prdc(real_features=real_features,                        fake_features=fake_features,                        nearest_k=nearest_k)  print(metrics) ``` Above test code will result in the following estimates (may fluctuate due to randomness).",numpy,SOFTWARE
"```python import numpy as np from prdc import compute_prdc   num_real_samples = num_fake_samples = 10000 feature_dim = 1000 nearest_k = 5 real_features = np.random.normal(loc=0.0, scale=1.0,                                  size=[num_real_samples, feature_dim])  fake_features = np.random.normal(loc=0.0, scale=1.0,                                  size=[num_fake_samples, feature_dim])  metrics = compute_prdc(real_features=real_features,                        fake_features=fake_features,                        nearest_k=nearest_k)  print(metrics) ``` Above test code will result in the following estimates (may fluctuate due to randomness).",prdc,SOFTWARE
"/tox.ini)** locally and on **[github action](.github/workflows/ci.yml)**:  | Env | versions | | --- | --- | | os  | `ubuntu-18.04`, `ubuntu-20.04` | | python | `3.7`, `3.8`, `3.9`, `3.10` | | pytorch | `1.8.1`, `1.9.1`, `1.10.2`, `1.11.0`, `1.12.1` , `1.13.1` , `2.0.1` , `2.1.0` |  ## Star History  [!",python,SOFTWARE
"/tox.ini)** locally and on **[github action](.github/workflows/ci.yml)**:  | Env | versions | | --- | --- | | os  | `ubuntu-18.04`, `ubuntu-20.04` | | python | `3.7`, `3.8`, `3.9`, `3.10` | | pytorch | `1.8.1`, `1.9.1`, `1.10.2`, `1.11.0`, `1.12.1` , `1.13.1` , `2.0.1` , `2.1.0` |  ## Star History  [!",pytorch,SOFTWARE
The following shows a simple example and you can find more details in our [documentation](https://s3prl.github.io/s3prl/).  1.,s3prl,SOFTWARE
The following shows a simple example and you can find more details in our [documentation](https://s3prl.github.io/s3prl/).  1.,s3prl,SOFTWARE
Install the S3PRL package:  ```sh pip install s3prl ```  2.,S3PRL,SOFTWARE
Install the S3PRL package:  ```sh pip install s3prl ```  2.,pip,SOFTWARE
Install the S3PRL package:  ```sh pip install s3prl ```  2.,s3prl,SOFTWARE
**Python** >= 3.6 2.,Python,SOFTWARE
Install **sox** on your OS 3.,sox,SOFTWARE
"Install s3prl: [Read doc](https://s3prl.github.io/s3prl/tutorial/installation.html#) or `pip install -e "".",s3prl,SOFTWARE
"Install s3prl: [Read doc](https://s3prl.github.io/s3prl/tutorial/installation.html#) or `pip install -e "".",s3prl,SOFTWARE
"Install s3prl: [Read doc](https://s3prl.github.io/s3prl/tutorial/installation.html#) or `pip install -e "".",s3prl,SOFTWARE
"Install s3prl: [Read doc](https://s3prl.github.io/s3prl/tutorial/installation.html#) or `pip install -e "".",pip,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",Pytorch,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",pytorch,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",Pytorch,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",audio,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",Pytorch,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",Kaldi,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",kaldi,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",Kaldi-ASR,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",Transformers,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",transformers,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",PyTorch-Kald,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",pytorch-kaldi,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",fairseq,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",fairseq,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",CPC,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",CPC_audio,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",APC,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",Autoregressive-Predictive-Coding,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",VQ-APC,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",VQ-APC),SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",NPC,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",NPC,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",Pytorch,SOFTWARE
"E.g., `upstream/pase/README.md`=  ## Reference Repositories  * [Pytorch](https://github.com/pytorch/pytorch), Pytorch. * [Audio](https://github.com/pytorch/audio), Pytorch. * [Kaldi](https://github.com/kaldi-asr/kaldi), Kaldi-ASR. * [Transformers](https://github.com/huggingface/transformers), Hugging Face. * [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi), Mirco Ravanelli. * [fairseq](https://github.com/pytorch/fairseq), Facebook AI Research. * [CPC](https://github.com/facebookresearch/CPC_audio), Facebook AI Research. * [APC](https://github.com/iamyuanchung/Autoregressive-Predictive-Coding), Yu-An Chung. * [VQ-APC](https://github.com/s3prl/VQ-APC), Yu-An Chung. * [NPC](https://github.com/Alexander-H-Liu/NPC), Alexander-H-Liu. * [End-to-end-ASR-Pytorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch), Alexander-H-Liu * [Mockingjay](https://github.com/andi611/Mockingjay-Speech-Representation), Andy T.",Pytorch,SOFTWARE
"Liu. * [ESPnet](https://github.com/espnet/espnet), Shinji Watanabe * [speech-representations](https://github.com/awslabs/speech-representations), aws lab * [PASE](https://github.com/santi-pdp/pase), Santiago Pascual and Mirco Ravanelli * [LibriMix](https://github.com/JorisCos/LibriMix), Joris Cosentino and Manuel Pariente  ## License  The majority of S3PRL Toolkit is licensed under the Apache License version 2.0, however all the files authored by Facebook, Inc.",ESPnet,SOFTWARE
"Liu. * [ESPnet](https://github.com/espnet/espnet), Shinji Watanabe * [speech-representations](https://github.com/awslabs/speech-representations), aws lab * [PASE](https://github.com/santi-pdp/pase), Santiago Pascual and Mirco Ravanelli * [LibriMix](https://github.com/JorisCos/LibriMix), Joris Cosentino and Manuel Pariente  ## License  The majority of S3PRL Toolkit is licensed under the Apache License version 2.0, however all the files authored by Facebook, Inc.",espnet,SOFTWARE
"Liu. * [ESPnet](https://github.com/espnet/espnet), Shinji Watanabe * [speech-representations](https://github.com/awslabs/speech-representations), aws lab * [PASE](https://github.com/santi-pdp/pase), Santiago Pascual and Mirco Ravanelli * [LibriMix](https://github.com/JorisCos/LibriMix), Joris Cosentino and Manuel Pariente  ## License  The majority of S3PRL Toolkit is licensed under the Apache License version 2.0, however all the files authored by Facebook, Inc.",PASE,SOFTWARE
"Liu. * [ESPnet](https://github.com/espnet/espnet), Shinji Watanabe * [speech-representations](https://github.com/awslabs/speech-representations), aws lab * [PASE](https://github.com/santi-pdp/pase), Santiago Pascual and Mirco Ravanelli * [LibriMix](https://github.com/JorisCos/LibriMix), Joris Cosentino and Manuel Pariente  ## License  The majority of S3PRL Toolkit is licensed under the Apache License version 2.0, however all the files authored by Facebook, Inc.",pase,SOFTWARE
"Liu. * [ESPnet](https://github.com/espnet/espnet), Shinji Watanabe * [speech-representations](https://github.com/awslabs/speech-representations), aws lab * [PASE](https://github.com/santi-pdp/pase), Santiago Pascual and Mirco Ravanelli * [LibriMix](https://github.com/JorisCos/LibriMix), Joris Cosentino and Manuel Pariente  ## License  The majority of S3PRL Toolkit is licensed under the Apache License version 2.0, however all the files authored by Facebook, Inc.",LibriMix,SOFTWARE
"Liu. * [ESPnet](https://github.com/espnet/espnet), Shinji Watanabe * [speech-representations](https://github.com/awslabs/speech-representations), aws lab * [PASE](https://github.com/santi-pdp/pase), Santiago Pascual and Mirco Ravanelli * [LibriMix](https://github.com/JorisCos/LibriMix), Joris Cosentino and Manuel Pariente  ## License  The majority of S3PRL Toolkit is licensed under the Apache License version 2.0, however all the files authored by Facebook, Inc.",LibriMix,SOFTWARE
"Liu. * [ESPnet](https://github.com/espnet/espnet), Shinji Watanabe * [speech-representations](https://github.com/awslabs/speech-representations), aws lab * [PASE](https://github.com/santi-pdp/pase), Santiago Pascual and Mirco Ravanelli * [LibriMix](https://github.com/JorisCos/LibriMix), Joris Cosentino and Manuel Pariente  ## License  The majority of S3PRL Toolkit is licensed under the Apache License version 2.0, however all the files authored by Facebook, Inc.",S3PRL,SOFTWARE
Julia version 1.7.1. 2.,Julia version 1.7.1.,SOFTWARE
A text editor such as [VS Code](https://code.visualstudio.com/) or [Atom](https://atom.io/). 4.,VS Code,SOFTWARE
A text editor such as [VS Code](https://code.visualstudio.com/) or [Atom](https://atom.io/). 4.,Atom,SOFTWARE
A text editor such as [VS Code](https://code.visualstudio.com/) or [Atom](https://atom.io/). 4.,atom,SOFTWARE
[Git](https://git-scm.com/) 3.,Git,SOFTWARE
The matching engine used is a fork from the official release of version [v1.1.0](https://github.com/dharmeshsing/CoinTossX/tree/v1.1.0) of [CoinTossX](https://github.com/dharmeshsing/CoinTossX).,CoinTossX,SOFTWARE
The matching engine used is a fork from the official release of version [v1.1.0](https://github.com/dharmeshsing/CoinTossX/tree/v1.1.0) of [CoinTossX](https://github.com/dharmeshsing/CoinTossX).,CoinTossX,SOFTWARE
The matching engine used is a fork from the official release of version [v1.1.0](https://github.com/dharmeshsing/CoinTossX/tree/v1.1.0) of [CoinTossX](https://github.com/dharmeshsing/CoinTossX).,CoinTossX,SOFTWARE
The fork can be found [here](https://github.com/IvanJericevich/CoinTossX).,CoinTossX,SOFTWARE
"<Need to update to include the wait loop removal.>  ## Usage   To clone this repository:  ```console git clone https://github.com/matthewdicks98/MDTG-MALABM.git ```  Julia packages can be installed from the Julia REPL using:  ```console using Pkg Pkg.add("" "") ```  ### Branch structure  Given that there are multiple stages to the project it was decided that each stage will be published on its own branch.",Julia,SOFTWARE
"<Need to update to include the wait loop removal.>  ## Usage   To clone this repository:  ```console git clone https://github.com/matthewdicks98/MDTG-MALABM.git ```  Julia packages can be installed from the Julia REPL using:  ```console using Pkg Pkg.add("" "") ```  ### Branch structure  Given that there are multiple stages to the project it was decided that each stage will be published on its own branch.",Julia,SOFTWARE
"**`beNNch` tackles this challenge by implementing a unified, modular workflow for configuring, executing, and analyzing such benchmarks.**   The software framework builds around the [JUBE Benchmarking Environment](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html), installs simulation software, provides an interface to benchmark models, automates data and metadata annotation, and accounts for storage and presentation of results.",JUBE,SOFTWARE
"**`beNNch` tackles this challenge by implementing a unified, modular workflow for configuring, executing, and analyzing such benchmarks.**   The software framework builds around the [JUBE Benchmarking Environment](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html), installs simulation software, provides an interface to benchmark models, automates data and metadata annotation, and accounts for storage and presentation of results.",JUBE,SOFTWARE
/benchmarks/)  | JUBE benchmark scripts for select neuroscientific models | | [config](.,JUBE,SOFTWARE
/helpers/)     | JUBE helper functions and parameter sets | | [models](.,JUBE,SOFTWARE
"If set-up problems persist after following both approaches, don't hesitate to get into contact via the [Community discussion](https://github.com/INM-6/beNNch#community-discussion).   ### Initialization  - Download git submodules:  ```bash git submodule init ```   + _optional: if you want to change the url of any of the submodules (requires `git v2.25.0`):_     `git submodule set-url -- <submodule> <new_url>`  ```bash git submodule update --remote ```  - Install benchplot as Python module:  ```bash pip install -e plot --user ```    ### Software dependencies  - [git annex](https://git-annex.branchable.com)   + can e.g. be installed via  ```bash wget 'http://downloads.kitenet.net/git-annex/linux/current/git-annex-standalone-amd64.tar.gz' tar -xzf git-annex-standalone-amd64.tar.gz export PATH=$PATH:<install_path>/git-annex.linux ``` - [JUBE](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)   _Note that  JUBE version 2.4.2 or later is necessary_   - [Builder](https://github.com/INM-6/Builder)   + see Builder documentation for installation guide  - Python 3.X  ### Models  For the following network models, there is currently a NEST implementation in the *models* submodule and corresponding JUBE benchmark script in the `benchmarks/` folder:  - **Multi-Area Model**    - `multi-area-model_2` for usage with NEST 2",git v2.25.0,SOFTWARE
"If set-up problems persist after following both approaches, don't hesitate to get into contact via the [Community discussion](https://github.com/INM-6/beNNch#community-discussion).   ### Initialization  - Download git submodules:  ```bash git submodule init ```   + _optional: if you want to change the url of any of the submodules (requires `git v2.25.0`):_     `git submodule set-url -- <submodule> <new_url>`  ```bash git submodule update --remote ```  - Install benchplot as Python module:  ```bash pip install -e plot --user ```    ### Software dependencies  - [git annex](https://git-annex.branchable.com)   + can e.g. be installed via  ```bash wget 'http://downloads.kitenet.net/git-annex/linux/current/git-annex-standalone-amd64.tar.gz' tar -xzf git-annex-standalone-amd64.tar.gz export PATH=$PATH:<install_path>/git-annex.linux ``` - [JUBE](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)   _Note that  JUBE version 2.4.2 or later is necessary_   - [Builder](https://github.com/INM-6/Builder)   + see Builder documentation for installation guide  - Python 3.X  ### Models  For the following network models, there is currently a NEST implementation in the *models* submodule and corresponding JUBE benchmark script in the `benchmarks/` folder:  - **Multi-Area Model**    - `multi-area-model_2` for usage with NEST 2",git,SOFTWARE
"If set-up problems persist after following both approaches, don't hesitate to get into contact via the [Community discussion](https://github.com/INM-6/beNNch#community-discussion).   ### Initialization  - Download git submodules:  ```bash git submodule init ```   + _optional: if you want to change the url of any of the submodules (requires `git v2.25.0`):_     `git submodule set-url -- <submodule> <new_url>`  ```bash git submodule update --remote ```  - Install benchplot as Python module:  ```bash pip install -e plot --user ```    ### Software dependencies  - [git annex](https://git-annex.branchable.com)   + can e.g. be installed via  ```bash wget 'http://downloads.kitenet.net/git-annex/linux/current/git-annex-standalone-amd64.tar.gz' tar -xzf git-annex-standalone-amd64.tar.gz export PATH=$PATH:<install_path>/git-annex.linux ``` - [JUBE](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)   _Note that  JUBE version 2.4.2 or later is necessary_   - [Builder](https://github.com/INM-6/Builder)   + see Builder documentation for installation guide  - Python 3.X  ### Models  For the following network models, there is currently a NEST implementation in the *models* submodule and corresponding JUBE benchmark script in the `benchmarks/` folder:  - **Multi-Area Model**    - `multi-area-model_2` for usage with NEST 2",pip,SOFTWARE
"If set-up problems persist after following both approaches, don't hesitate to get into contact via the [Community discussion](https://github.com/INM-6/beNNch#community-discussion).   ### Initialization  - Download git submodules:  ```bash git submodule init ```   + _optional: if you want to change the url of any of the submodules (requires `git v2.25.0`):_     `git submodule set-url -- <submodule> <new_url>`  ```bash git submodule update --remote ```  - Install benchplot as Python module:  ```bash pip install -e plot --user ```    ### Software dependencies  - [git annex](https://git-annex.branchable.com)   + can e.g. be installed via  ```bash wget 'http://downloads.kitenet.net/git-annex/linux/current/git-annex-standalone-amd64.tar.gz' tar -xzf git-annex-standalone-amd64.tar.gz export PATH=$PATH:<install_path>/git-annex.linux ``` - [JUBE](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)   _Note that  JUBE version 2.4.2 or later is necessary_   - [Builder](https://github.com/INM-6/Builder)   + see Builder documentation for installation guide  - Python 3.X  ### Models  For the following network models, there is currently a NEST implementation in the *models* submodule and corresponding JUBE benchmark script in the `benchmarks/` folder:  - **Multi-Area Model**    - `multi-area-model_2` for usage with NEST 2",JUBE,SOFTWARE
"If set-up problems persist after following both approaches, don't hesitate to get into contact via the [Community discussion](https://github.com/INM-6/beNNch#community-discussion).   ### Initialization  - Download git submodules:  ```bash git submodule init ```   + _optional: if you want to change the url of any of the submodules (requires `git v2.25.0`):_     `git submodule set-url -- <submodule> <new_url>`  ```bash git submodule update --remote ```  - Install benchplot as Python module:  ```bash pip install -e plot --user ```    ### Software dependencies  - [git annex](https://git-annex.branchable.com)   + can e.g. be installed via  ```bash wget 'http://downloads.kitenet.net/git-annex/linux/current/git-annex-standalone-amd64.tar.gz' tar -xzf git-annex-standalone-amd64.tar.gz export PATH=$PATH:<install_path>/git-annex.linux ``` - [JUBE](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)   _Note that  JUBE version 2.4.2 or later is necessary_   - [Builder](https://github.com/INM-6/Builder)   + see Builder documentation for installation guide  - Python 3.X  ### Models  For the following network models, there is currently a NEST implementation in the *models* submodule and corresponding JUBE benchmark script in the `benchmarks/` folder:  - **Multi-Area Model**    - `multi-area-model_2` for usage with NEST 2",JUBE,SOFTWARE
"If set-up problems persist after following both approaches, don't hesitate to get into contact via the [Community discussion](https://github.com/INM-6/beNNch#community-discussion).   ### Initialization  - Download git submodules:  ```bash git submodule init ```   + _optional: if you want to change the url of any of the submodules (requires `git v2.25.0`):_     `git submodule set-url -- <submodule> <new_url>`  ```bash git submodule update --remote ```  - Install benchplot as Python module:  ```bash pip install -e plot --user ```    ### Software dependencies  - [git annex](https://git-annex.branchable.com)   + can e.g. be installed via  ```bash wget 'http://downloads.kitenet.net/git-annex/linux/current/git-annex-standalone-amd64.tar.gz' tar -xzf git-annex-standalone-amd64.tar.gz export PATH=$PATH:<install_path>/git-annex.linux ``` - [JUBE](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)   _Note that  JUBE version 2.4.2 or later is necessary_   - [Builder](https://github.com/INM-6/Builder)   + see Builder documentation for installation guide  - Python 3.X  ### Models  For the following network models, there is currently a NEST implementation in the *models* submodule and corresponding JUBE benchmark script in the `benchmarks/` folder:  - **Multi-Area Model**    - `multi-area-model_2` for usage with NEST 2",JUBE,SOFTWARE
"If set-up problems persist after following both approaches, don't hesitate to get into contact via the [Community discussion](https://github.com/INM-6/beNNch#community-discussion).   ### Initialization  - Download git submodules:  ```bash git submodule init ```   + _optional: if you want to change the url of any of the submodules (requires `git v2.25.0`):_     `git submodule set-url -- <submodule> <new_url>`  ```bash git submodule update --remote ```  - Install benchplot as Python module:  ```bash pip install -e plot --user ```    ### Software dependencies  - [git annex](https://git-annex.branchable.com)   + can e.g. be installed via  ```bash wget 'http://downloads.kitenet.net/git-annex/linux/current/git-annex-standalone-amd64.tar.gz' tar -xzf git-annex-standalone-amd64.tar.gz export PATH=$PATH:<install_path>/git-annex.linux ``` - [JUBE](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)   _Note that  JUBE version 2.4.2 or later is necessary_   - [Builder](https://github.com/INM-6/Builder)   + see Builder documentation for installation guide  - Python 3.X  ### Models  For the following network models, there is currently a NEST implementation in the *models* submodule and corresponding JUBE benchmark script in the `benchmarks/` folder:  - **Multi-Area Model**    - `multi-area-model_2` for usage with NEST 2",Builder,SOFTWARE
"If set-up problems persist after following both approaches, don't hesitate to get into contact via the [Community discussion](https://github.com/INM-6/beNNch#community-discussion).   ### Initialization  - Download git submodules:  ```bash git submodule init ```   + _optional: if you want to change the url of any of the submodules (requires `git v2.25.0`):_     `git submodule set-url -- <submodule> <new_url>`  ```bash git submodule update --remote ```  - Install benchplot as Python module:  ```bash pip install -e plot --user ```    ### Software dependencies  - [git annex](https://git-annex.branchable.com)   + can e.g. be installed via  ```bash wget 'http://downloads.kitenet.net/git-annex/linux/current/git-annex-standalone-amd64.tar.gz' tar -xzf git-annex-standalone-amd64.tar.gz export PATH=$PATH:<install_path>/git-annex.linux ``` - [JUBE](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)   _Note that  JUBE version 2.4.2 or later is necessary_   - [Builder](https://github.com/INM-6/Builder)   + see Builder documentation for installation guide  - Python 3.X  ### Models  For the following network models, there is currently a NEST implementation in the *models* submodule and corresponding JUBE benchmark script in the `benchmarks/` folder:  - **Multi-Area Model**    - `multi-area-model_2` for usage with NEST 2",Builder,SOFTWARE
"If set-up problems persist after following both approaches, don't hesitate to get into contact via the [Community discussion](https://github.com/INM-6/beNNch#community-discussion).   ### Initialization  - Download git submodules:  ```bash git submodule init ```   + _optional: if you want to change the url of any of the submodules (requires `git v2.25.0`):_     `git submodule set-url -- <submodule> <new_url>`  ```bash git submodule update --remote ```  - Install benchplot as Python module:  ```bash pip install -e plot --user ```    ### Software dependencies  - [git annex](https://git-annex.branchable.com)   + can e.g. be installed via  ```bash wget 'http://downloads.kitenet.net/git-annex/linux/current/git-annex-standalone-amd64.tar.gz' tar -xzf git-annex-standalone-amd64.tar.gz export PATH=$PATH:<install_path>/git-annex.linux ``` - [JUBE](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)   _Note that  JUBE version 2.4.2 or later is necessary_   - [Builder](https://github.com/INM-6/Builder)   + see Builder documentation for installation guide  - Python 3.X  ### Models  For the following network models, there is currently a NEST implementation in the *models* submodule and corresponding JUBE benchmark script in the `benchmarks/` folder:  - **Multi-Area Model**    - `multi-area-model_2` for usage with NEST 2",Builder,SOFTWARE
"If set-up problems persist after following both approaches, don't hesitate to get into contact via the [Community discussion](https://github.com/INM-6/beNNch#community-discussion).   ### Initialization  - Download git submodules:  ```bash git submodule init ```   + _optional: if you want to change the url of any of the submodules (requires `git v2.25.0`):_     `git submodule set-url -- <submodule> <new_url>`  ```bash git submodule update --remote ```  - Install benchplot as Python module:  ```bash pip install -e plot --user ```    ### Software dependencies  - [git annex](https://git-annex.branchable.com)   + can e.g. be installed via  ```bash wget 'http://downloads.kitenet.net/git-annex/linux/current/git-annex-standalone-amd64.tar.gz' tar -xzf git-annex-standalone-amd64.tar.gz export PATH=$PATH:<install_path>/git-annex.linux ``` - [JUBE](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)   _Note that  JUBE version 2.4.2 or later is necessary_   - [Builder](https://github.com/INM-6/Builder)   + see Builder documentation for installation guide  - Python 3.X  ### Models  For the following network models, there is currently a NEST implementation in the *models* submodule and corresponding JUBE benchmark script in the `benchmarks/` folder:  - **Multi-Area Model**    - `multi-area-model_2` for usage with NEST 2",JUBE,SOFTWARE
"For example: `software = nest-simulator`, `version = 3.0`, `variant = gcc9.3`.",nest-simulato,SOFTWARE
"To install software for which a plan file does not yet exist (e.g. a new dependency or simulator), you need to configure Builder by adding a ```common``` file explicating the necessary steps of installation that is shared between all variants to  ```bash <path/to/Builder>/plans/<software>/common ```  Note that Builder already provides a `common` file for `nest-simulator`.",Builder,SOFTWARE
"To install software for which a plan file does not yet exist (e.g. a new dependency or simulator), you need to configure Builder by adding a ```common``` file explicating the necessary steps of installation that is shared between all variants to  ```bash <path/to/Builder>/plans/<software>/common ```  Note that Builder already provides a `common` file for `nest-simulator`.",Builder,SOFTWARE
"To install software for which a plan file does not yet exist (e.g. a new dependency or simulator), you need to configure Builder by adding a ```common``` file explicating the necessary steps of installation that is shared between all variants to  ```bash <path/to/Builder>/plans/<software>/common ```  Note that Builder already provides a `common` file for `nest-simulator`.",Builder,SOFTWARE
"If the `common` file for the simulator or software you wish to install already exists and you only want to add a new version or variant, add both a plan file and a module file template to  ```bash <path/to/Builder>/plans/<software>/<version>/<{variant, variant.module}> ```  In ```variant```, you state the source location of the software as well as the chosen dependencies.",Builder,SOFTWARE
See as an example the ```nest-simulator``` plan files that Builder ships with.,nest-simulator,SOFTWARE
See as an example the ```nest-simulator``` plan files that Builder ships with.,Builder,SOFTWARE
_Specific to NEST benchmarking: don't forget to include `-Dwith-detailed-timers=ON` in the `CMAKEFLAGS` if you want to have access to C++ level timers._   ### Run benchmarks  The JUBE benchmarking scripts can be found in `benchmarks/`.,JUBE,SOFTWARE
"To run a benchmark, execute:  ```bash jube run benchmarks/<model>.yaml ```  JUBE displays a table summarizing the submitted job(s) and the corresponding `job id`.  ### Analyze benchmarks  First, create a new instance of the analysis configuration with ```bash cp config/templates/analysis_config_template.yaml config/analysis_config.yaml ``` Here, fill in - whether the scaling benchmark runs across threads or nodes.",jube,SOFTWARE
"To run a benchmark, execute:  ```bash jube run benchmarks/<model>.yaml ```  JUBE displays a table summarizing the submitted job(s) and the corresponding `job id`.  ### Analyze benchmarks  First, create a new instance of the analysis configuration with ```bash cp config/templates/analysis_config_template.yaml config/analysis_config.yaml ``` Here, fill in - whether the scaling benchmark runs across threads or nodes.",JUBE,SOFTWARE
"`beNNch` provides defaults for plotting timers across `nodes` and `threads`, but alternatives can be readily implemented by adding to `analysis/plot_helpers.py`. - the path to the JUBE output (usually the same as the `outpath` of the `<benchmark>` in `benchmarks/<model>`)  To start the analysis, execute ```bash cd results ``` - _optional: initialize for the first time_   + `git pull origin main`   + `git checkout main`   + `git annex init`   + `git annex sync` ```bash python ..",JUBE,SOFTWARE
"`beNNch` provides defaults for plotting timers across `nodes` and `threads`, but alternatives can be readily implemented by adding to `analysis/plot_helpers.py`. - the path to the JUBE output (usually the same as the `outpath` of the `<benchmark>` in `benchmarks/<model>`)  To start the analysis, execute ```bash cd results ``` - _optional: initialize for the first time_   + `git pull origin main`   + `git checkout main`   + `git annex init`   + `git annex sync` ```bash python ..",git,SOFTWARE
"`beNNch` provides defaults for plotting timers across `nodes` and `threads`, but alternatives can be readily implemented by adding to `analysis/plot_helpers.py`. - the path to the JUBE output (usually the same as the `outpath` of the `<benchmark>` in `benchmarks/<model>`)  To start the analysis, execute ```bash cd results ``` - _optional: initialize for the first time_   + `git pull origin main`   + `git checkout main`   + `git annex init`   + `git annex sync` ```bash python ..",git,SOFTWARE
"`beNNch` provides defaults for plotting timers across `nodes` and `threads`, but alternatives can be readily implemented by adding to `analysis/plot_helpers.py`. - the path to the JUBE output (usually the same as the `outpath` of the `<benchmark>` in `benchmarks/<model>`)  To start the analysis, execute ```bash cd results ``` - _optional: initialize for the first time_   + `git pull origin main`   + `git checkout main`   + `git annex init`   + `git annex sync` ```bash python ..",git,SOFTWARE
"`beNNch` provides defaults for plotting timers across `nodes` and `threads`, but alternatives can be readily implemented by adding to `analysis/plot_helpers.py`. - the path to the JUBE output (usually the same as the `outpath` of the `<benchmark>` in `benchmarks/<model>`)  To start the analysis, execute ```bash cd results ``` - _optional: initialize for the first time_   + `git pull origin main`   + `git checkout main`   + `git annex init`   + `git annex sync` ```bash python ..",git,SOFTWARE
"`beNNch` provides defaults for plotting timers across `nodes` and `threads`, but alternatives can be readily implemented by adding to `analysis/plot_helpers.py`. - the path to the JUBE output (usually the same as the `outpath` of the `<benchmark>` in `benchmarks/<model>`)  To start the analysis, execute ```bash cd results ``` - _optional: initialize for the first time_   + `git pull origin main`   + `git checkout main`   + `git annex init`   + `git annex sync` ```bash python ..",python,SOFTWARE
"For sharing, upload the results to the central repository via ```bash git annex sync ```  ### Get remote benchmark results  ```bash cd results ``` - _optional: add a new remote_    + `git add remote <name> <location>`, e.g.",git,SOFTWARE
"For sharing, upload the results to the central repository via ```bash git annex sync ```  ### Get remote benchmark results  ```bash cd results ``` - _optional: add a new remote_    + `git add remote <name> <location>`, e.g.",git,SOFTWARE
"To ""go back"" a view, execute ```bash git annex vpop ``` After choosing which benchmarks to display via filtering above and ordering them via `<differing_metadata>`, you can create a flip book of all plots with ```bash python ..",git,SOFTWARE
"To ""go back"" a view, execute ```bash git annex vpop ``` After choosing which benchmarks to display via filtering above and ordering them via `<differing_metadata>`, you can create a flip book of all plots with ```bash python ..",python,SOFTWARE
"`<scaling_type>` defines the style of plotting, c.f. section on [Analyze Benchmarks](#analyze-benchmarks).  ### Known issues - error `jinja2.exceptions.TemplateNotFound: index.html.j2`   + [issue](https://github.com/jupyter/nbconvert/issues/1394) with a recent version of `nbconvert`, try to install version `5.6.1` instead (e.g.",jupyter,SOFTWARE
"`<scaling_type>` defines the style of plotting, c.f. section on [Analyze Benchmarks](#analyze-benchmarks).  ### Known issues - error `jinja2.exceptions.TemplateNotFound: index.html.j2`   + [issue](https://github.com/jupyter/nbconvert/issues/1394) with a recent version of `nbconvert`, try to install version `5.6.1` instead (e.g.",nbconvert,SOFTWARE
`pip install nbconvert==5.6.1 --user`)  ### Community discussion  We organize a discussion with a focus on both community usage and development synchronization.,pip,SOFTWARE
`pip install nbconvert==5.6.1 --user`)  ### Community discussion  We organize a discussion with a focus on both community usage and development synchronization.,nbconvert==5.6.1,SOFTWARE
subject=[beNNch]%20Community%20Discussion%20Meeting).  ___ ## Developer guide  ### Add a new model  `benchmarks/template.yaml` provides a template for a JUBE benchmarking script and can be used as a starting point for adding a new model.,JUBE,SOFTWARE
"In particular, this concerns how JUBE feeds the configuration parameters to the network and how JUBE reads the performance measurement output.  #### Input  A new model needs to be able to receive input from JUBE for setting parameters.",JUBE,SOFTWARE
"In particular, this concerns how JUBE feeds the configuration parameters to the network and how JUBE reads the performance measurement output.  #### Input  A new model needs to be able to receive input from JUBE for setting parameters.",JUBE,SOFTWARE
"In particular, this concerns how JUBE feeds the configuration parameters to the network and how JUBE reads the performance measurement output.  #### Input  A new model needs to be able to receive input from JUBE for setting parameters.",JUBE,SOFTWARE
"# SGDLibrary : Stochastic Optimization Algorithm Library in MATLAB/Octave ----------  Authors: [Hiroyuki Kasai](http://kasai.comm.waseda.ac.jp/kasai/)  Last page update: November 20, 2020  Latest library version: 1.0.20 (see Release notes for more info)  <br />  Announcement ---------- We are very welcome to your contribution.",SGDLibrary,SOFTWARE
"# SGDLibrary : Stochastic Optimization Algorithm Library in MATLAB/Octave ----------  Authors: [Hiroyuki Kasai](http://kasai.comm.waseda.ac.jp/kasai/)  Last page update: November 20, 2020  Latest library version: 1.0.20 (see Release notes for more info)  <br />  Announcement ---------- We are very welcome to your contribution.",MATLAB,SOFTWARE
"# SGDLibrary : Stochastic Optimization Algorithm Library in MATLAB/Octave ----------  Authors: [Hiroyuki Kasai](http://kasai.comm.waseda.ac.jp/kasai/)  Last page update: November 20, 2020  Latest library version: 1.0.20 (see Release notes for more info)  <br />  Announcement ---------- We are very welcome to your contribution.",Octave,SOFTWARE
"Please tell us  - Stochastic optimization solvers written by MATLAB, and  - your comments and suggestions.",MATLAB,SOFTWARE
<br />  Introduction ---------- The SGDLibrary is a **pure-MATLAB** library or toolbox of a collection of **stochastic optimization algorithms**.,MATLAB,SOFTWARE
The SGDLibrary is also operable on [GNU Octave](https://www.gnu.org/software/octave/) (Free software compatible with many MATLAB scripts).,GNU Octave,SOFTWARE
The SGDLibrary is also operable on [GNU Octave](https://www.gnu.org/software/octave/) (Free software compatible with many MATLAB scripts).,MATLAB,SOFTWARE
Note that this SGDLibrary internally contains the [GDLibrary](https://github.com/hiroyuki-kasai/GDLibrary).,SGDLibrary,SOFTWARE
Note that this SGDLibrary internally contains the [GDLibrary](https://github.com/hiroyuki-kasai/GDLibrary).,GDLibrary,SOFTWARE
<br />  Document ---------- The document of SGDLibrary can be obtained from below;  - H.,SGDLibrary,SOFTWARE
"<br />  ## <a name=""supp_solver""> List of the algorithms available in SGDLibrary </a>   - **SGD variants** (stochastic gradient descent)     - Vanila SGD         - H.",SGDLibrary,SOFTWARE
```Matlab %% First run the setup script run_me_first;  ```  <br />  Simplest usage example: 4 steps!,Matlab,SOFTWARE
"```Matlab %% Execute the demonstration script demo;  ```  The ""**demo.m**"" file contains below.",Matlab,SOFTWARE
"```Matlab %% generate synthetic data         % set number of dimensions d = 3; % set number of samples     n = 300; % generate data data = logistic_regression_data_generator(n, d);   %% define problem definitions problem = logistic_regression(data.x_train, data.y_train, data.x_test, data.y_test);    %% perform algorithms SGD and SVRG  options.w_init = data.w_init; options.step_init = 0.01;        [w_sgd, info_sgd] = sgd(problem, options);   [w_svrg, info_svrg] = svrg(problem, options);   %% display cost/optimality gap vs number of gradient evaluations display_graph('grad_calc_count','cost', {'SGD', 'SVRG'}, {w_sgd, w_svrg}, {info_sgd, info_svrg});  ```  <br /> Let take a closer look at the code above bit by bit.",Matlab,SOFTWARE
"```Matlab     d = 3; n = 300; data = logistic_regression_data_generator(n, d); ```  **Step 2: Define problem**  The problem to be solved should be defined properly from the [supported problems](#supp_pro).",Matlab,SOFTWARE
"```Matlab problem = logistic_regression(data.x_train, data.y_train, data.x_test, data.y_test);  ```  **Step 3: Perform solver**  Now, you can perform optimization solvers, i.e., SGD and SVRG, calling [solver functions](#supp_solver), i.e., `sgd()` function and `svrg()` function after setting some optimization options.",Matlab,SOFTWARE
"```Matlab options.w_init = data.w_init; options.step_init = 0.01;   [w_sgd, info_sgd] = sgd(problem, options);   [w_svrg, info_svrg] = svrg(problem, options); ``` They return the final solutions of `w` and the statistics information that include the histories of epoch numbers, cost values, norms of gradient, the number of gradient evaluations and so on.",Matlab,SOFTWARE
"```Matlab display_graph('grad_calc_count','cost', {'SGD', 'SVRG'}, {w_sgd, w_svrg}, {info_sgd, info_svrg}); ```  That's it!",Matlab,SOFTWARE
```Matlab %% calculate optimal solution for optimality gap w_opt = problem.calc_solution(1000); options.f_opt = problem.cost(w_opt); ```  This case uses the full gradient descent solve `gd()` to obtain an optimal solution under max iteration 1000 with very precise tolerant stopping condition.,Matlab,SOFTWARE
"```Matlab display_graph('grad_calc_count','optimality_gap', {'SGD', 'SVRG'}, {w_sgd, w_svrg}, {info_sgd, info_svrg});     ```  - **Demonstration of ""classification accuracy""**  Additionally, in this case of logistic regression, the results of classification accuracy are calculated using the corresponding prediction function `prediction()` and `accuracy` of the problem definition function `logistic_regression()`.",Matlab,SOFTWARE
"Furthermore, the classification accuracies are illustrated by `display_classification_result()` function that is written in ""**demo.m**"" like below; ```Matlab %% calculate classification accuracy % for SGD % predict y_pred_sgd = problem.prediction(w_sgd); % calculate accuracy accuracy_sgd = problem.accuracy(y_pred_sgd);  fprintf('Classificaiton accuracy: %s: %.4f\n', 'SGD', accuracy_sgd); % convert from {1,-1} to {1,2} y_pred_sgd(y_pred_sgd==-1) = 2; y_pred_sgd(y_pred_sgd==1) = 1;   % for SVRG % predict     y_pred_svrg = problem.prediction(w_svrg); % calculate accuracy accuracy_svrg = problem.accuracy(y_pred_svrg);  fprintf('Classificaiton accuracy: %s: %.4f\n', 'SVRG', accuracy_svrg); % convert from {1,-1} to {1,2} y_pred_svrg(y_pred_svrg==-1) = 2; y_pred_svrg(y_pred_svrg==1) = 1;   %% display classification results  % convert from {1,-1} to {1,2} data.y_train(data.y_train==-1) = 2; data.y_train(data.y_train==1) = 1; data.y_test(data.y_test==-1) = 2; data.y_test(data.y_test==1) = 1;   % display results display_classification_result(problem, {'SGD', 'SVRG'}, {w_sgd, w_svrg}, {y_pred_sgd, y_pred_svrg}, {accuracy_sgd, accuracy_svrg}, data.x_train, data.y_train, data.x_test, data.y_test);      ```      Output results:  <img src=""http://www.kasailab.com/public/github/SGDLibrary/images/log_reg_results.png"" width=""900""> <br /><br />   - **Demonstration of ""convergence animation""**  You need specify additional options before executing solvers.",Matlab,SOFTWARE
"```Matlab %% set options for convergence animation options.max_epoch = 100;     options.store_w = true;  ```    Then, `draw_convergence_animation()` draws a convergence animation.",Matlab,SOFTWARE
"```Matlab %% display convergence animation draw_convergence_animation(problem, {'SGD', 'SVRG'}, {info_sgd.w, info_svrg.w}, options.max_epoch);     ```   <br />  Example results of other problems ----------------------------  - Linear regression problem    <img src=""http://www.kasailab.com/public/github/SGDLibrary/images/linear_reg_results.png"" width=""900"">  - Softmax classifier problem  <img src=""http://www.kasailab.com/public/github/SGDLibrary/images/soft_class_results.png"" width=""900"">  - Linear SVM problem  <img src=""http://www.kasailab.com/public/github/SGDLibrary/images/linear_svm_results.png"" width=""900""> <br /><br />   <br />  Convergence behavior animation example (Linear regression problem) ----------------------------  **""test_convergence_animation_demo.m""** provides you an animation of convergence behaviors of algorithms.",Matlab,SOFTWARE
These MATLAB codes are ported with original authors' big helps. - Third party files are included,MATLAB,SOFTWARE
"- Softmax classification problem does not support ""Hessian-vector product"" type algorithms, i.e., SQN, SVRG-SQN and SVRG-LBFGS. - This SGDLibrary internally contains the [GDLibrary](https://github.com/hiroyuki-kasai/GDLibrary).",GDLibrary,SOFTWARE
"<br />  Problems or questions --------------------- If you have any problems or questions, please contact the author: [Hiroyuki Kasai](http://kasai.comm.waseda.ac.jp/kasai/) (email: hiroyuki **dot** kasai **at** waseda **dot** jp)  <br />  Release Notes -------------- * Version 1.0.20 (Nov. 10, 2020)     - Buf fixed, and some files are added. * Version 1.0.19 (Oct. 27, 2020)     - Buf fixed, and some files are added. * Version 1.0.17 (Apr. 17, 2018)     - Sub-sampled CR (including ARC) and Sub-sampled TR are nely added. * Version 1.0.16 (Apr. 01, 2018)     - GNU Octave is supported",GNU Octave,SOFTWARE
"* Version 1.0.5 (Jan. 12, 2017)     - Add some functions and modify items. * Version 1.0.4 (Nov. 04, 2016)     - Integrate GDLibrary with SGDLibrary. * Version 1.0.3 (Nov. 04, 2016)     - Modify many items",GDLibrary,SOFTWARE
"* Version 1.0.5 (Jan. 12, 2017)     - Add some functions and modify items. * Version 1.0.4 (Nov. 04, 2016)     - Integrate GDLibrary with SGDLibrary. * Version 1.0.3 (Nov. 04, 2016)     - Modify many items",SGDLibrary,SOFTWARE
"We use unique image ids in both datasets to represent the UI and they can be used to retrieve the original UI data in [Rico](https://interactionmining.org/rico).  ## Data format  Both data are saved as TFRecords, which makes it easy to be loaded in Tensorflow.  1.",Tensorflow,SOFTWARE
"The repo contains:  - The [35K data](#data-release) used for evaluating the LLM. - The code for [generating the data](#data-generation-process). - The code for [evaluating the model](#evaluation). - The code for [analyzing the model](#analysis).  ## Overview  HaluEval includes 5,000 general user queries with ChatGPT responses and  30,000 task-specific examples from three tasks, i.e., question answering, knowledge-grounded dialogue, and text summarization.",ChatGPT,SOFTWARE
"To further screen user queries where LLMs are most likely to produce hallucinations, we use ChatGPT to sample three responses  for each query and finally retain the queries with low-similarity responses for human labeling.",ChatGPT,SOFTWARE
"First, based on existing task datasets (e.g., HotpotQA) as seed data, we design task-specific instructions for ChatGPT to generate hallucinated samples in two methods, i.e., one-pass and conversational.",ChatGPT,SOFTWARE
"Second, to select the most plausible and difficult hallucinated sample for LLMs evaluation, we elaborate the filtering instruction enhanced  by ground-truth examples and leverage ChatGPT for sample selection.",ChatGPT,SOFTWARE
/data/general_data.json): 5K human-annotated samples for ChatGPT responses to general user queries from [Alpaca](https://github.com/tatsu-lab/stanford_alpaca).,ChatGPT,SOFTWARE
"For each sample dictionary, the fields `user_query`, `chatgpt_response`, and `hallucination_label` refer to the posed user query, ChatGPT response, and hallucination label (Yes/No) annotated by humans.",chatgpt,SOFTWARE
"For each sample dictionary, the fields `user_query`, `chatgpt_response`, and `hallucination_label` refer to the posed user query, ChatGPT response, and hallucination label (Yes/No) annotated by humans.",ChatGPT,SOFTWARE
"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy",ChatGPT,SOFTWARE
"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy",wget,SOFTWARE
"Based on these data, you can evaluate the ability of LLMs to recognize hallucinations and analyze what type of contents/topics LLMs tend to hallucinate (or fail to recognize the contained hallucination).   ## Data Generation Process  We executed the data generation pipeline via ChatGPT according to the following steps:  - First, we download the training sets of HotpotQA, OpenDialKG, and CNN/Daily Mail.  ``` cd generation wget http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json wget https://raw.githubusercontent.com/facebookresearch/opendialkg/main/data/opendialkg.csv wget https://huggingface.co/datasets/ccdv/cnn_dailymail/blob/main/cnn_stories.tgz ```  - Second, we sample 10K samples and generate their hallucinated counterparts by setting the task and sampling strategy",wget,SOFTWARE
# Diffusion-Explainer  Diffusion Explainer is an interactive visualization tool designed to help anyone learn how Stable Diffusion transforms text prompts into images.,Diffusion-Explainer,SOFTWARE
# Diffusion-Explainer  Diffusion Explainer is an interactive visualization tool designed to help anyone learn how Stable Diffusion transforms text prompts into images.,Diffusion Explainer,SOFTWARE
Try Diffusion Explainer at https://poloclub.github.io/diffusion-explainer and watch a demo video on YouTube https://youtu.be/Zg4gxdIWDds!,Diffusion Explainer,SOFTWARE
Try Diffusion Explainer at https://poloclub.github.io/diffusion-explainer and watch a demo video on YouTube https://youtu.be/Zg4gxdIWDds!,diffusion-explainer,SOFTWARE
Try Diffusion Explainer at https://poloclub.github.io/diffusion-explainer and watch a demo video on YouTube https://youtu.be/Zg4gxdIWDds!,YouTube,SOFTWARE
"Short paper, IEEE VIS 2024.  ## How to run locally ``` git clone https://github.com/poloclub/diffusion-explainer.git cd diffusion-explainer python -m http.server 8000 ```  Then, on your web browser, access http://localhost:8000.",git,SOFTWARE
"Short paper, IEEE VIS 2024.  ## How to run locally ``` git clone https://github.com/poloclub/diffusion-explainer.git cd diffusion-explainer python -m http.server 8000 ```  Then, on your web browser, access http://localhost:8000.",diffusion-explainer,SOFTWARE
"Short paper, IEEE VIS 2024.  ## How to run locally ``` git clone https://github.com/poloclub/diffusion-explainer.git cd diffusion-explainer python -m http.server 8000 ```  Then, on your web browser, access http://localhost:8000.",python,SOFTWARE
"You can replace 8000 with other port numbers you want to use.  ## Credits Led by [Seongmin Lee](http://www.seongmin.xyz), Diffusion Explainer is created by Machine Learning and Human-computer Interaction researchers at Georgia Tech and IBM Research.",Diffusion Explainer,SOFTWARE
"The team includes [Seongmin Lee](http://www.seongmin.xyz), [Benjamin Hoover](https://bhoov.com), [Hendrik Strobelt](http://hendrik.strobelt.com), [Jay Wang](https://zijie.wang), [ShengYun (Anthony) Peng](https://shengyun-peng.github.io), [Austin Wright](https://www.austinpwright.com), [Kevin Li](https://www.linkedin.com/in/kevinyli/), [Haekyu Park](https://haekyu.github.io/), [Alex Yang](https://alexanderyang.me/), and [Polo Chau](http://www.cc.gatech.edu/~dchau/).  ## Citation  ```bibTeX @article{lee2024diffusion,   title = {{D}iffusion {E}xplainer: {V}isual {E}xplanation for {T}ext-to-image {S}table {D}iffusion},   shorttitle = {Diffusion Explainer},   author = {Lee, Seongmin and Hoover, Benjamin and Strobelt, Hendrik and Wang, Zijie J and Peng, ShengYun and Wright, Austin and Li, Kevin and Park, Haekyu and Yang, Haoyang and Chau, Duen Horng},   journal={IEEE VIS},   year={2024} } ```  ## License The software is available under the [MIT License](https://github.com/poloclub/diffusion-explainer/blob/main/LICENSE).  ## Contact If you have any questions, feel free to [open an issue](https://github.com/poloclub/diffusion-explainer/issues/new/choose) or contact [Seongmin Lee](http://www.seongmin.xyz/).",diffusion-explainer,SOFTWARE
"The team includes [Seongmin Lee](http://www.seongmin.xyz), [Benjamin Hoover](https://bhoov.com), [Hendrik Strobelt](http://hendrik.strobelt.com), [Jay Wang](https://zijie.wang), [ShengYun (Anthony) Peng](https://shengyun-peng.github.io), [Austin Wright](https://www.austinpwright.com), [Kevin Li](https://www.linkedin.com/in/kevinyli/), [Haekyu Park](https://haekyu.github.io/), [Alex Yang](https://alexanderyang.me/), and [Polo Chau](http://www.cc.gatech.edu/~dchau/).  ## Citation  ```bibTeX @article{lee2024diffusion,   title = {{D}iffusion {E}xplainer: {V}isual {E}xplanation for {T}ext-to-image {S}table {D}iffusion},   shorttitle = {Diffusion Explainer},   author = {Lee, Seongmin and Hoover, Benjamin and Strobelt, Hendrik and Wang, Zijie J and Peng, ShengYun and Wright, Austin and Li, Kevin and Park, Haekyu and Yang, Haoyang and Chau, Duen Horng},   journal={IEEE VIS},   year={2024} } ```  ## License The software is available under the [MIT License](https://github.com/poloclub/diffusion-explainer/blob/main/LICENSE).  ## Contact If you have any questions, feel free to [open an issue](https://github.com/poloclub/diffusion-explainer/issues/new/choose) or contact [Seongmin Lee](http://www.seongmin.xyz/).",diffusion-explainer,SOFTWARE
This repository hosts **epromela** ---a tool for building models of smart contracts that can be validated by the Spin model checker.,epromela,SOFTWARE
This repository hosts **epromela** ---a tool for building models of smart contracts that can be validated by the Spin model checker.,Spin,SOFTWARE
Details of the contraval tool are  explained next,contraval,SOFTWARE
"Contraval (smart contract validation)  ### Validate your smart contracts before deployment A particularity of smart contracts deployed on--blockchain  is that because of their descentralisation and openess,  they are hard to amend after deployment.",Contraval,SOFTWARE
Epromela is a language for writing models that can be model checked by the Spin model checker.,Spin,SOFTWARE
We use the promela tool for   * *model checking:* we verify the logical correctness of the contract at design time and  * *generation of execution sequences* we generate excution sequences     (test cases) with the tool for testing for conformance the     actual implementation.    ### Model checking   Spin can verify the logical consistency of the model  epromela of the smart contract against correctness   properties written   in LTL (Linear Temporal Logics) formulae.,Spin,SOFTWARE
<br />  ### Generation of execution sequences (test cases) for conformance testing  Spin can be instructed to generate all the execution  sequences encoded in the epromela model of the  smart contract.,Spin,SOFTWARE
"To reduce the number of states of the model, the BEG abstracts  away the communication between the contracting parties and generates the business events by itself as shown in the figure.",BEG,SOFTWARE
"/figures/executionModelBilateralOperationsWithoutFailuresBEG.png""     width=""400"" title=""Execution model of bilateral operations                        without failures, BEG",BEG,SOFTWARE
"<br/> In this execution model, the BEG abstracts  away the communication between the contracting parties and plays the role of the output synchronizer as shown in the figure.",BEG,SOFTWARE
"/figures/executionModelOfBilateralOperationsWithFailuresBEG.png"" width=""550"" title=""Execution model of contractual                    operations with failures, BEG",BEG,SOFTWARE
"""> </p>  The event generated by the BEG are sent to a set of business rules for evaluation for contract compliance",BEG,SOFTWARE
The reader is expected to have some basic knowledge of  [SPIN and Promela](http://spinroot.com/spin/whatispin.html) and a local computer to dowload and deploy them,SPIN,SOFTWARE
We have implemented a Contract Compliant Checker---a tool for  assisting designers in this task.,Contract Compliant Checker,SOFTWARE
Github has a 100MB file size limit.,Github,SOFTWARE
"*  ## Our contributions  Our contributions are:  (1) a reusable GP-based accuracy probabilistic extrapolator (APEx-GP) that can match existing curve-fitting approaches in terms of error while providing additional uncertainty estimates, and   (2) a careful assessment of our proposed probabilistic extrapolations compared to ground truth on larger datasets across six medical classification tasks involving both 2D and 3D images across diverse modalities (x-ray, ultrasound, and CT) with various sample sizes.  ## Using our method  To use our Gaussian process to extrapolate classifier accuracy to larger datasets see `notebooks/demo.ipynb`.  ### Initializing our Gaussian process  ```python likelihood = gpytorch.likelihoods.GaussianLikelihood() # Note: If you want to use the Gaussian process with an arctan mean function use models.GPArctan() instead. model = models.GPPowerLaw(X_train, y_train, likelihood, epsilon_min=0.05, with_priors=True) ```  ### Extrapolating classifier accuracy  ```python with torch.no_grad(): predictions = likelihood(model(X_test)) loc = predictions.mean.numpy() scale = predictions.stddev.numpy() # Note: If you want to forecast with 20%-80% change lower and upper percentile. lower, upper = priors.truncated_normal_uncertainty(a=0.0, b=1.0, loc=loc, scale=scale, lower_percentile=0.025, upper_percentile=0.975)  ```  ## Citation  ```bibtex @inproceedings{harvey2023probabilistic,   author={Harvey, Ethan and Chen, Wansu and Kent, David M. and Hughes, Michael C.},   title={A Probabilistic Method to Predict Classifier Accuracy on Larger Datasets given Small Pilot Data},   booktitle={Machine Learning for Health (ML4H)},   year={2023} } ```  ## Reproducing results  To reproduce model performance at varying dataset sizes 1) download datasets (see `encode_images/README.md` and `label_images/README.md` for more details) and 2) fit classifiers to each dataset (see `src/finetune_2D.py` and `src/finetune_3D.py`).",gpytorch,SOFTWARE
"*  ## Our contributions  Our contributions are:  (1) a reusable GP-based accuracy probabilistic extrapolator (APEx-GP) that can match existing curve-fitting approaches in terms of error while providing additional uncertainty estimates, and   (2) a careful assessment of our proposed probabilistic extrapolations compared to ground truth on larger datasets across six medical classification tasks involving both 2D and 3D images across diverse modalities (x-ray, ultrasound, and CT) with various sample sizes.  ## Using our method  To use our Gaussian process to extrapolate classifier accuracy to larger datasets see `notebooks/demo.ipynb`.  ### Initializing our Gaussian process  ```python likelihood = gpytorch.likelihoods.GaussianLikelihood() # Note: If you want to use the Gaussian process with an arctan mean function use models.GPArctan() instead. model = models.GPPowerLaw(X_train, y_train, likelihood, epsilon_min=0.05, with_priors=True) ```  ### Extrapolating classifier accuracy  ```python with torch.no_grad(): predictions = likelihood(model(X_test)) loc = predictions.mean.numpy() scale = predictions.stddev.numpy() # Note: If you want to forecast with 20%-80% change lower and upper percentile. lower, upper = priors.truncated_normal_uncertainty(a=0.0, b=1.0, loc=loc, scale=scale, lower_percentile=0.025, upper_percentile=0.975)  ```  ## Citation  ```bibtex @inproceedings{harvey2023probabilistic,   author={Harvey, Ethan and Chen, Wansu and Kent, David M. and Hughes, Michael C.},   title={A Probabilistic Method to Predict Classifier Accuracy on Larger Datasets given Small Pilot Data},   booktitle={Machine Learning for Health (ML4H)},   year={2023} } ```  ## Reproducing results  To reproduce model performance at varying dataset sizes 1) download datasets (see `encode_images/README.md` and `label_images/README.md` for more details) and 2) fit classifiers to each dataset (see `src/finetune_2D.py` and `src/finetune_3D.py`).",torch,SOFTWARE
# ü™Ñ  Prompt-OIRL: Learning to Prompt LLMs with Expert Knowledge (Known Magic Words üßô )  ### üíª  Implementation and üìí  tutorial for ICLR 2024 paper   !,Prompt-OIRL,SOFTWARE
id=N6o0ZtPzTg)   ## üî•  News - (2024.2) (Internal Code-Reviewing) Code with GPT3.5 and TigerBot to be released. - (2024.1) Prompt-OIRL has been accepted by ICLR'2024.,GPT3.5,SOFTWARE
id=N6o0ZtPzTg)   ## üî•  News - (2024.2) (Internal Code-Reviewing) Code with GPT3.5 and TigerBot to be released. - (2024.1) Prompt-OIRL has been accepted by ICLR'2024.,TigerBot,SOFTWARE
id=N6o0ZtPzTg)   ## üî•  News - (2024.2) (Internal Code-Reviewing) Code with GPT3.5 and TigerBot to be released. - (2024.1) Prompt-OIRL has been accepted by ICLR'2024.,Prompt-OIRL,SOFTWARE
- (2024.12) Prompt-OIRL has been presented at the NeurIPS conference.,Prompt-OIRL,SOFTWARE
"- (2023.10) Code with LLaMA2 has been released. - (2023.10) Prompt-OIRL has been featured in a positioning [paper](https://arxiv.org/pdf/2310.06147.pdf) as an example of **inverse alignment**. - (2023.9) Prompt-OIRL has been selected as an **oral presentation** at the ENLSP workshop at NeurIPS'2023.  ## üìñ  Abstract  > In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization.",LLaMA2,SOFTWARE
"- (2023.10) Code with LLaMA2 has been released. - (2023.10) Prompt-OIRL has been featured in a positioning [paper](https://arxiv.org/pdf/2310.06147.pdf) as an example of **inverse alignment**. - (2023.9) Prompt-OIRL has been selected as an **oral presentation** at the ENLSP workshop at NeurIPS'2023.  ## üìñ  Abstract  > In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization.",Prompt-OIRL,SOFTWARE
"- (2023.10) Code with LLaMA2 has been released. - (2023.10) Prompt-OIRL has been featured in a positioning [paper](https://arxiv.org/pdf/2310.06147.pdf) as an example of **inverse alignment**. - (2023.9) Prompt-OIRL has been selected as an **oral presentation** at the ENLSP workshop at NeurIPS'2023.  ## üìñ  Abstract  > In this study, we aim to enhance the arithmetic reasoning ability of Large Language Models (LLMs) through zero-shot prompt optimization.",Prompt-OIRL,SOFTWARE
"To address this, we introduce Prompt-OIRL, which harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data.",Prompt-OIRL,SOFTWARE
"With Prompt-OIRL, the query-dependent prompt optimization objective is achieved by first learning an offline reward model.",Prompt-OIRL,SOFTWARE
Prompt-OIRL optimizes prompt during inference time on a **query-dependent** level effectively and cost-efficiently.,Prompt-OIRL,SOFTWARE
"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ‚öôÔ∏è Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1.",GPT4,SOFTWARE
"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ‚öôÔ∏è Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1.",LLaMA2,SOFTWARE
"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ‚öôÔ∏è Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1.",LLaMA-2,SOFTWARE
"(original chat logs with GPT4 for those motivating examples can be found at [Left](https://chat.openai.com/share/0f2d11b1-322a-4c47-a877-ad6fbace8179), [Right](https://chat.openai.com/share/15870a47-93c7-4b98-96c8-af0516c0c999))  ## ‚öôÔ∏è Reproduction  ### Preliminaries  To reproduce our results (e.g., using LLaMA2)  1. get the [license to use LLaMA-2](https://ai.meta.com/llama/).  2. get access to the datasets: [SVAMP](https://github.com/arkilpatel/SVAMP), [GSM8K](https://huggingface.co/datasets/gsm8k), [MAWPS](https://github.com/sroy9/mawps)  ### Create a Virtual Env 1.",llama,SOFTWARE
Clone the repository ``` git clone git@github.com:holarissun/Prompt-OIRL.git ``` 2.,git,SOFTWARE
Clone the repository ``` git clone git@github.com:holarissun/Prompt-OIRL.git ``` 2.,git,SOFTWARE
"Create a new virtual environment with Python 3.10, e.g., ``` conda create --name prompt-oirl python==3.10 conda activate prompt-oirl cd Prompt-OIRL ``` 3.",conda,SOFTWARE
"Create a new virtual environment with Python 3.10, e.g., ``` conda create --name prompt-oirl python==3.10 conda activate prompt-oirl cd Prompt-OIRL ``` 3.",conda,SOFTWARE
"Create a new virtual environment with Python 3.10, e.g., ``` conda create --name prompt-oirl python==3.10 conda activate prompt-oirl cd Prompt-OIRL ``` 3.",Prompt-OIRL,SOFTWARE
Install the requirements ``` pip install -r requirements.txt ```  ### Reproduce the Main Results  #### Step 1.,pip,SOFTWARE
"To avoid repeating such a computationally expensive (when running LLMs on local machines) or costly (when calling the commercial APIs like GPT3.5 or TigerBot) process, we have **released all the interactive logs with those LLMs collected in our experiments.**.",GPT3.5,SOFTWARE
"To avoid repeating such a computationally expensive (when running LLMs on local machines) or costly (when calling the commercial APIs like GPT3.5 or TigerBot) process, we have **released all the interactive logs with those LLMs collected in our experiments.**.",TigerBot,SOFTWARE
"Therefore, if you are just looking to explore the method and don't need to re-create everything from scratch, we recommend that you skip this step,   If you would like to reproduce the offline dataset with the llama2 model, you need to follow these steps:   ```  git clone git@github.com:facebookresearch/llama.git  ``` and then move ```Prompt-OIRL/llama_exps/llama_step1_gen_offline.py``` to the ```llama``` folder  then run the following command   ``` torchrun --nproc_per_node 1 llama_step1_gen_offline.py \     --ckpt_dir llama-2-7b-chat/ \     --tokenizer_path tokenizer.model \     --max_seq_len 512 --max_batch_size 8 --prompt_idx 0 --dataset_eval gsm8k  ```  #### Step 2.",llama2,SOFTWARE
"Therefore, if you are just looking to explore the method and don't need to re-create everything from scratch, we recommend that you skip this step,   If you would like to reproduce the offline dataset with the llama2 model, you need to follow these steps:   ```  git clone git@github.com:facebookresearch/llama.git  ``` and then move ```Prompt-OIRL/llama_exps/llama_step1_gen_offline.py``` to the ```llama``` folder  then run the following command   ``` torchrun --nproc_per_node 1 llama_step1_gen_offline.py \     --ckpt_dir llama-2-7b-chat/ \     --tokenizer_path tokenizer.model \     --max_seq_len 512 --max_batch_size 8 --prompt_idx 0 --dataset_eval gsm8k  ```  #### Step 2.",git,SOFTWARE
"Therefore, if you are just looking to explore the method and don't need to re-create everything from scratch, we recommend that you skip this step,   If you would like to reproduce the offline dataset with the llama2 model, you need to follow these steps:   ```  git clone git@github.com:facebookresearch/llama.git  ``` and then move ```Prompt-OIRL/llama_exps/llama_step1_gen_offline.py``` to the ```llama``` folder  then run the following command   ``` torchrun --nproc_per_node 1 llama_step1_gen_offline.py \     --ckpt_dir llama-2-7b-chat/ \     --tokenizer_path tokenizer.model \     --max_seq_len 512 --max_batch_size 8 --prompt_idx 0 --dataset_eval gsm8k  ```  #### Step 2.",torchrun,SOFTWARE
"This step will take a few seconds to finish, it will do some file renaming and training-test split and save corresponding files to a new folder ```LMllama2```   ```  python3 llama_step2_reorg_data.py  ```   #### Step 3.",LMllama2,SOFTWARE
"This step will take a few seconds to finish, it will do some file renaming and training-test split and save corresponding files to a new folder ```LMllama2```   ```  python3 llama_step2_reorg_data.py  ```   #### Step 3.",python3,SOFTWARE
Note: please make sure that you select the relevant task by updating the code (the line marked with `NOTE`) in this and the following 2 steps.  ```  python3 llama_step3_data_processing.py  ```  #### Step 4.,python3,SOFTWARE
"In general, training an XGBoost reward model will take a bit longer time, and using a LightGBM reward model can be faster.  ```  python3 llama_step4_offline_evaluation.py  ``` - Note: you will need to download a missing embedding file from [this link](https://drive.google.com/file/d/1ER50FoLInO1pTr50dDjjZMBGPX-pXVA1/view?",python3,SOFTWARE
"(oversized for Github, ~ 230Mb)   #### Step 5.",Github,SOFTWARE
Results under different settings will be all saved to ```.csv``` files  ```  python3 llama_step5_offline_optimization.py  ```    ## üöÄ  A Related Discussion on RLHF: Prompt-OIRL addresses the prompting problems in LLMs using an RLAIF approach.,python3,SOFTWARE
Results under different settings will be all saved to ```.csv``` files  ```  python3 llama_step5_offline_optimization.py  ```    ## üöÄ  A Related Discussion on RLHF: Prompt-OIRL addresses the prompting problems in LLMs using an RLAIF approach.,Prompt-OIRL,SOFTWARE
"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",mlbench,SOFTWARE
"</p>    <li><b>MLBENCH Datasets</b></li>  <p align=""justify""> A number of datasets in <a href=""https://www.rdocumentation.org/packages/mlbench/versions/2.1-1"">MLBENCH library</a> of R such as <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_XOR_Dataset"">XOR</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Spiral_Dataset"">Spiral</a>, <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Circle_Dataset"">Circle</a>, and <a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R/Examples/2D_Smiley_Dataset"">Smiley</a> [<a href=""https://github.com/ISorokos/SafeML/tree/master/Implementation_in_R"">R implementation</a>].",MLBENCH,SOFTWARE
"</li>   <li><a href = ""https://www.cs.toronto.edu/~kriz/cifar.html""><b>CIFAR 10/100 Datasets</b></a> [<a href = ""https://github.com/ISorokos/SafeML/blob/master/Implementation_in_MATLAB/CIFAR_10_Dataset/README.md"">MATLAB implementation</a>]</li>   <li>More datasets will be tested.",MATLAB,SOFTWARE
"Thus, we try to control the version of some of the major packages (`torch`, `transformers`, etc.) and provide two sets of requirements",torch,SOFTWARE
"Thus, we try to control the version of some of the major packages (`torch`, `transformers`, etc.) and provide two sets of requirements",transformers,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",meta-llama/Meta-Llama-3-8B-Instruct,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",mistralai/Mistral-7B-Instruct-v0.2,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",lmsys/longchat-7b-v1.5-32k,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",Mamba,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",Mamba-Chat,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",havenhq/mamba-chat,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",RecurrantGemma,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",recurrentgemma,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",RWKV-5-World,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",FlexGen,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",StreamingLLM,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",InfLLM,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",H2O,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",LLMLingua2,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",KIVI,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",LLM baselines,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",FlexGen,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",StreamingLLM,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",InfLLM,SOFTWARE
"* [`requirements/tf_4.40.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.40.txt) supports the three transformers-based LLM baseline (`meta-llama/Meta-Llama-3-8B-Instruct`/`mistralai/Mistral-7B-Instruct-v0.2`/`lmsys/longchat-7b-v1.5-32k`), [Mamba](https://arxiv.org/abs/2312.00752), [Mamba-Chat](https://huggingface.co/havenhq/mamba-chat), [RecurrantGemma](https://storage.googleapis.com/deepmind-media/gemma/recurrentgemma-report.pdf), [RWKV-5-World](https://pypi.org/project/rwkv/), [FlexGen](https://arxiv.org/abs/2303.06865), [StreamingLLM](https://arxiv.org/abs/2309.17453), [InfLLM](https://arxiv.org/abs/2402.04617), [H2O](https://arxiv.org/abs/2306.14048), and [LLMLingua2](https://arxiv.org/abs/2403.12968). * [`requirements/tf_4.36.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.36.txt) supports [KIVI](https://arxiv.org/abs/2402.02750) (it does also run LLM baselines, FlexGen, StreamingLLM, and InfLLM; but we opt to conduct such experiments in the above environment for maximum possible consistency). * [`requirements/tf_4.45.txt`](https://github.com/henryzhongsc/longctx_bench/blob/main/requirements/tf_4.45.txt) supports [Mamba 2](https://arxiv.org/abs/2405.21060).",Mamba 2,SOFTWARE
"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",KIVI,SOFTWARE
"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",pip,SOFTWARE
"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",pip,SOFTWARE
"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",flash-attn==2.6.3,SOFTWARE
"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",pip,SOFTWARE
"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",LongBench,SOFTWARE
"As an example, suppose one wants to try out KIVI under our codebase; one should:  ``` cd longctx_bench pip install -r requirements/tf_4.36.txt pip install flash-attn==2.6.3 cd pipeline/kivi/quant & pip install -e . ```  ---  ## Dataset and Access Preparation  Currently, our benchmark features 15 datasets (NarrativeQA, Qasper, MultiFieldQA, HotpotQA, 2WikiMQA, Musique, GovReport, QMSum, MultiNews, TREC, TriviaQA, SAMSum, PassageRetrieval, LCC, RepoBench-P) from [**LongBench**](https://github.com/THUDM/LongBench) and a [**Needle-in-a-Haystack/PaulGraham Passkey**](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) test with [Paul Graham essays](https://paulgraham.com/articles.html) as background and [passkey retrieval](https://arxiv.org/abs/2305.16300) as needle, following the spirit of [Arize-ai](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)'s NIAH implementation utilized in Google DeepMind's [Gemini 1.5 report](https://arxiv.org/pdf/2403.05530).",Gemini 1.5,SOFTWARE
"We supply the following script to process LongBench:  ``` python scripts/dataset_prep/download_longbench.py ```  Our paper features some models that are gated (e.g., `meta-llama/Meta-Llama-3-8B-Instruct`).",python,SOFTWARE
"We supply the following script to process LongBench:  ``` python scripts/dataset_prep/download_longbench.py ```  Our paper features some models that are gated (e.g., `meta-llama/Meta-Llama-3-8B-Instruct`).",meta-llama/Meta-Llama-3-8B-Instruct,SOFTWARE
"You may consider setting `git update-index --skip-worktree config/access_tokens.py` so that Git will no longer track this file to avoid your locally stored token accidentally get synced to upstream.  --- ## Experiment Reproduction  We supply all scripts in the [`scripts`](https://github.com/henryzhongsc/longctx_bench/blob/main/scripts) folder, with a folder structure that clearly indicate which script is for which experiment.",git,SOFTWARE
"You may consider setting `git update-index --skip-worktree config/access_tokens.py` so that Git will no longer track this file to avoid your locally stored token accidentally get synced to upstream.  --- ## Experiment Reproduction  We supply all scripts in the [`scripts`](https://github.com/henryzhongsc/longctx_bench/blob/main/scripts) folder, with a folder structure that clearly indicate which script is for which experiment.",Git,SOFTWARE
"For viewer's convenience of comparison and potential table/plot construction needs, we share all experiment logs, filtered, and raw results at [Google Drive link](https://drive.google.com/drive/folders/1PBB5bkR88QbaA1-dBTwP5xKLq8iEBcBb?",Google Drive,SOFTWARE
"Here is an example for InfLLM on PaulGraham Passkey; the results can be found under `<output_folder_dir>`.  ``` compress_ratio=$1 # 2x, 4x, 6x, 8x output_dir_root=$2  task=""paulgraham_passkey"" dataset=""20480words_10x10x3_7digits"" model=""mistral-7b-instruct-v0.2_infinity_model_len"" method=""infllm""  python pipeline/inf_stream_llm/main.py \ --exp_desc ${task}_${dataset}_${model}_${method}_${compress_ratio} \ # this argument is purely cosmetic.",InfLLM,SOFTWARE
"Here is an example for InfLLM on PaulGraham Passkey; the results can be found under `<output_folder_dir>`.  ``` compress_ratio=$1 # 2x, 4x, 6x, 8x output_dir_root=$2  task=""paulgraham_passkey"" dataset=""20480words_10x10x3_7digits"" model=""mistral-7b-instruct-v0.2_infinity_model_len"" method=""infllm""  python pipeline/inf_stream_llm/main.py \ --exp_desc ${task}_${dataset}_${model}_${method}_${compress_ratio} \ # this argument is purely cosmetic.",mistral-7b-instruct-v0.2_infinity_model_len,SOFTWARE
"Here is an example for InfLLM on PaulGraham Passkey; the results can be found under `<output_folder_dir>`.  ``` compress_ratio=$1 # 2x, 4x, 6x, 8x output_dir_root=$2  task=""paulgraham_passkey"" dataset=""20480words_10x10x3_7digits"" model=""mistral-7b-instruct-v0.2_infinity_model_len"" method=""infllm""  python pipeline/inf_stream_llm/main.py \ --exp_desc ${task}_${dataset}_${model}_${method}_${compress_ratio} \ # this argument is purely cosmetic.",infllm,SOFTWARE
"Here is an example for InfLLM on PaulGraham Passkey; the results can be found under `<output_folder_dir>`.  ``` compress_ratio=$1 # 2x, 4x, 6x, 8x output_dir_root=$2  task=""paulgraham_passkey"" dataset=""20480words_10x10x3_7digits"" model=""mistral-7b-instruct-v0.2_infinity_model_len"" method=""infllm""  python pipeline/inf_stream_llm/main.py \ --exp_desc ${task}_${dataset}_${model}_${method}_${compress_ratio} \ # this argument is purely cosmetic.",python,SOFTWARE
Baseline learners implemented in JAX and PyTorch.,JAX,SOFTWARE
Baseline learners implemented in JAX and PyTorch.,PyTorch,SOFTWARE
"The JAX learners are     identical to the learners used for the figures in the [paper], the PyTorch     learners are provided for example purposes.",JAX,SOFTWARE
"The JAX learners are     identical to the learners used for the figures in the [paper], the PyTorch     learners are provided for example purposes.",PyTorch,SOFTWARE
-   Our datasets use the Tensorflow(-datasets) API.,Tensorflow,SOFTWARE
"Our JAX learners use     TensorFlow and JAX, and our PyTorch Learners use PyTorch.",JAX,SOFTWARE
"Our JAX learners use     TensorFlow and JAX, and our PyTorch Learners use PyTorch.",TensorFlow,SOFTWARE
"Our JAX learners use     TensorFlow and JAX, and our PyTorch Learners use PyTorch.",JAX,SOFTWARE
"Our JAX learners use     TensorFlow and JAX, and our PyTorch Learners use PyTorch.",PyTorch,SOFTWARE
"Our JAX learners use     TensorFlow and JAX, and our PyTorch Learners use PyTorch.",PyTorch,SOFTWARE
"Each (datasets,     jax learners, and pytorch learners) have their own `requirements.txt` that     you are welcome to install with `pip` and a Python version above 3.8",jax,SOFTWARE
"Each (datasets,     jax learners, and pytorch learners) have their own `requirements.txt` that     you are welcome to install with `pip` and a Python version above 3.8",pytorch,SOFTWARE
"Each (datasets,     jax learners, and pytorch learners) have their own `requirements.txt` that     you are welcome to install with `pip` and a Python version above 3.8",pip,SOFTWARE
see [here](https://docs.docker.com/get-docker/) for installing Docker,docker,SOFTWARE
see [here](https://docs.docker.com/get-docker/) for installing Docker,Docker,SOFTWARE
-   Some datasets are downloaded from Kaggle.,Kaggle,SOFTWARE
"If you're a dataset owner and wish to update any part of it (description, citation, etc.), or do not want your dataset URL to be included in this library, please get in touch through a GitHub issue.",GitHub,SOFTWARE
"-b                | Build docker before running         -e                | Develop mode where code is mounted         -h                | Help message ```  If running for the first time, pass the option `-b` alongside other commands to build the **docker** (`nevis-data`).",docker,SOFTWARE
"-b                | Build docker before running         -e                | Develop mode where code is mounted         -h                | Help message ```  If running for the first time, pass the option `-b` alongside other commands to build the **docker** (`nevis-data`).",docker,SOFTWARE
The develop mode is useful if you need to change the codebase (e.g. for adding a new dataset) and need to debug quickly without having to re-building the docker everytime (you still need to build the docker in develop mode!,docker,SOFTWARE
Some datasets are downloaded from Kaggle.,Kaggle,SOFTWARE
See on the [Kaggle website](https://www.kaggle.com/docs/api) how to configure your credentials and place them in the folder `~/.kaggle`.  ### 1.3.,Kaggle,SOFTWARE
See on the [Kaggle website](https://www.kaggle.com/docs/api) how to configure your credentials and place them in the folder `~/.kaggle`.  ### 1.3.,kaggle,SOFTWARE
See on the [Kaggle website](https://www.kaggle.com/docs/api) how to configure your credentials and place them in the folder `~/.kaggle`.  ### 1.3.,kaggle,SOFTWARE
Please check the [instructions](https://www.tensorflow.org/datasets/catalog/imagenet2012).,tensorflow,SOFTWARE
"/launch_local.sh <X> example ```  With `<X>` being the framework to use (`jax` or `torch`), second argument is the config to use.",jax,SOFTWARE
"/launch_local.sh <X> example ```  With `<X>` being the framework to use (`jax` or `torch`), second argument is the config to use.",torch,SOFTWARE
You can specify a different path by overriding the environment variable `NEVIS_DEFAULT_OUTPUT_DIR`.  ## Metrics visualization with TensorBoard  The TensorBoard events file will be saved to `~/nevis/tensorboard`.,TensorBoard,SOFTWARE
You can specify a different path by overriding the environment variable `NEVIS_DEFAULT_OUTPUT_DIR`.  ## Metrics visualization with TensorBoard  The TensorBoard events file will be saved to `~/nevis/tensorboard`.,TensorBoard,SOFTWARE
You can specify a different path by overriding the environment variable `NEVIS_DEFAULT_OUTPUT_DIR`.  ## Metrics visualization with TensorBoard  The TensorBoard events file will be saved to `~/nevis/tensorboard`.,tensorboard,SOFTWARE
The tensorboard can be launched with the following command.,tensorboard,SOFTWARE
"`tensorboard --lodir=~/nevis/tensorboard`  You will need to have `tensorboard` installed outside the docker using  ```bash pip install tensorboard ```  Regarding the different groups of plots on tensorboard dashboard: - `benchmark_metrics` contains metrics from prediction events across the stream, where the x-axis is the index (0-based) of the most training event. - `train_event_<i>` contains training and validation metrics on the training index with index `i`.  ## 3.",tensorboard,SOFTWARE
"`tensorboard --lodir=~/nevis/tensorboard`  You will need to have `tensorboard` installed outside the docker using  ```bash pip install tensorboard ```  Regarding the different groups of plots on tensorboard dashboard: - `benchmark_metrics` contains metrics from prediction events across the stream, where the x-axis is the index (0-based) of the most training event. - `train_event_<i>` contains training and validation metrics on the training index with index `i`.  ## 3.",tensorboard,SOFTWARE
"`tensorboard --lodir=~/nevis/tensorboard`  You will need to have `tensorboard` installed outside the docker using  ```bash pip install tensorboard ```  Regarding the different groups of plots on tensorboard dashboard: - `benchmark_metrics` contains metrics from prediction events across the stream, where the x-axis is the index (0-based) of the most training event. - `train_event_<i>` contains training and validation metrics on the training index with index `i`.  ## 3.",tensorboard,SOFTWARE
"`tensorboard --lodir=~/nevis/tensorboard`  You will need to have `tensorboard` installed outside the docker using  ```bash pip install tensorboard ```  Regarding the different groups of plots on tensorboard dashboard: - `benchmark_metrics` contains metrics from prediction events across the stream, where the x-axis is the index (0-based) of the most training event. - `train_event_<i>` contains training and validation metrics on the training index with index `i`.  ## 3.",docker,SOFTWARE
"`tensorboard --lodir=~/nevis/tensorboard`  You will need to have `tensorboard` installed outside the docker using  ```bash pip install tensorboard ```  Regarding the different groups of plots on tensorboard dashboard: - `benchmark_metrics` contains metrics from prediction events across the stream, where the x-axis is the index (0-based) of the most training event. - `train_event_<i>` contains training and validation metrics on the training index with index `i`.  ## 3.",tensorboard,SOFTWARE
"`tensorboard --lodir=~/nevis/tensorboard`  You will need to have `tensorboard` installed outside the docker using  ```bash pip install tensorboard ```  Regarding the different groups of plots on tensorboard dashboard: - `benchmark_metrics` contains metrics from prediction events across the stream, where the x-axis is the index (0-based) of the most training event. - `train_event_<i>` contains training and validation metrics on the training index with index `i`.  ## 3.",tensorboard,SOFTWARE
/launch_local.sh jax X.py`.,jax,SOFTWARE
"There are two directories containing baseline model implementations, one for jax (`experiments_jax`), and one for pytorch (`experiments_torch`).",jax,SOFTWARE
"There are two directories containing baseline model implementations, one for jax (`experiments_jax`), and one for pytorch (`experiments_torch`).",jax,SOFTWARE
"There are two directories containing baseline model implementations, one for jax (`experiments_jax`), and one for pytorch (`experiments_torch`).",pytorch,SOFTWARE
"There are two directories containing baseline model implementations, one for jax (`experiments_jax`), and one for pytorch (`experiments_torch`).",torch,SOFTWARE
"In each, `launch.py` is the Docker entrypoint, `experiment.py` is the module where all the execution happens, `configs/` provides the hyperparameters for each learner, `learners/` implements the learners (note: in some cases, there are different configs for the same learner), `metrics/` implements the metrics used in NEVIS'22, `environment/` provides the logger and checkpointer, and `training/` provides learner-agnostic utilities such as the heads, the backbone, but also a flops counter for example",Docker,SOFTWARE
"In each, `launch.py` is the Docker entrypoint, `experiment.py` is the module where all the execution happens, `configs/` provides the hyperparameters for each learner, `learners/` implements the learners (note: in some cases, there are different configs for the same learner), `metrics/` implements the metrics used in NEVIS'22, `environment/` provides the logger and checkpointer, and `training/` provides learner-agnostic utilities such as the heads, the backbone, but also a flops counter for example",NEVIS'22,SOFTWARE
"# Contact  If you wish to contact us, please raise a GitHub issue.",GitHub,SOFTWARE
"If you are using the NEVIS'22 benchmark, please cite the following paper,  ```bibtex @article{bornschein2022nevis,   author={Bornschein, J\""org and Galashov, Alexandre and Hemsley, Ross and Rannen-Triki, Amal and Chen, Yutian and Chaudhry, Arslan and He, Xu Owen and Douillard, Arthur and Caccia, Massimo and Feng, Qixuang and Shen, Jiajun and Rebuffi, Sylvestre-Alvise and Stacpoole, Kitty and de las Casas, Diego and Hawkins, Will and Lazaridou, Angeliki and Teh, Yee Whye and Rusu, Andrei A. and Pascanu, Razvan and Ranzato, Marc'Aurelio},   title={Nevis\'22: A Stream of 100 Tasks Sampled from 30 Years of Computer Vision Research},   journal={CoRR},   volume={abs/2211.11747},   year={2022},   url={https://arxiv.org/abs/2211.11747},   eprinttype={arXiv} } ```  [paper]: https://arxiv.org/abs/2211.11747 [blog post]: https://www.deepmind.com/blog/benchmarking-the-next-generation-of-never-ending-learners [tfds]: https://www.tensorflow.org/datasets/api_docs/python/tfds",NEVIS'22,SOFTWARE
"If you are using the NEVIS'22 benchmark, please cite the following paper,  ```bibtex @article{bornschein2022nevis,   author={Bornschein, J\""org and Galashov, Alexandre and Hemsley, Ross and Rannen-Triki, Amal and Chen, Yutian and Chaudhry, Arslan and He, Xu Owen and Douillard, Arthur and Caccia, Massimo and Feng, Qixuang and Shen, Jiajun and Rebuffi, Sylvestre-Alvise and Stacpoole, Kitty and de las Casas, Diego and Hawkins, Will and Lazaridou, Angeliki and Teh, Yee Whye and Rusu, Andrei A. and Pascanu, Razvan and Ranzato, Marc'Aurelio},   title={Nevis\'22: A Stream of 100 Tasks Sampled from 30 Years of Computer Vision Research},   journal={CoRR},   volume={abs/2211.11747},   year={2022},   url={https://arxiv.org/abs/2211.11747},   eprinttype={arXiv} } ```  [paper]: https://arxiv.org/abs/2211.11747 [blog post]: https://www.deepmind.com/blog/benchmarking-the-next-generation-of-never-ending-learners [tfds]: https://www.tensorflow.org/datasets/api_docs/python/tfds",tensorflow,SOFTWARE
"# sql4ml  Sql4ml is a Haskell project that takes as input SQL code which defines the objective/cost/loss function of a supervised machine learning model (ML), as well as a set of parameters, and generates TensorFlow (Python API) code that trains this model.",TensorFlow,SOFTWARE
(https://arxiv.org/abs/1907.12415).  ## Requirements  The software has been tested on macOS High Sierra and Ubuntu 16.04.,macOS High Sierra,SOFTWARE
(https://arxiv.org/abs/1907.12415).  ## Requirements  The software has been tested on macOS High Sierra and Ubuntu 16.04.,Ubuntu 16.04,SOFTWARE
To compile it you need to install the Haskell Platform (https://www.haskell.org/platform/).,Haskell Platform,SOFTWARE
To compile it you need to install the Haskell Platform (https://www.haskell.org/platform/).,haskell,SOFTWARE
You can load the module via ghci.,ghci,SOFTWARE
File main.hs containts two examples on how to translate SQL to TensorFlow code end-to-end.,TensorFlow,SOFTWARE
"To compile main.hs, run in a terminal:      ghc -o main main.hs  Sql4ml uses the MySQL database (https://www.mysql.com/) for storing data.",MySQL,SOFTWARE
"To compile main.hs, run in a terminal:      ghc -o main main.hs  Sql4ml uses the MySQL database (https://www.mysql.com/) for storing data.",mysql,SOFTWARE
"To run the generated TensorFlow code, you need to install MySQL.",TensorFlow,SOFTWARE
"To run the generated TensorFlow code, you need to install MySQL.",MySQL,SOFTWARE
To install it follow the instructions in https://www.mysql.com/.,mysql,SOFTWARE
"After installing MySQL, you can run the SQL scripts in the directory /db_setups to create two toy databases, one based on the Boston Housing dataset (https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) and the other based on the Iris dataset (https://archive.ics.uci.edu/ml/datasets/iris), which you can find in the directory /data.  ## Use  In file main.hs you can find two examples on how to call function endToEndTranslate, which takes the following input parameters:  * A list of SQL queries defining the objective function of a supervised ML model.",MySQL,SOFTWARE
"Currently sql4ml supports the translation of SQL create view queries of the following form,      CREATE VIEW $(name) AS     SELECT $(columns), $(numericExpr)     FROM $(tables)     WHERE $(joinElement)     GROUP BY $(groupingElement)  and generate an equivalent TensorFlow expression as below:      $(name) = $(translateNumericExpr(numericExpr))  For examples, check SQL files in the directory /working_examples.",sql4ml,SOFTWARE
"Currently sql4ml supports the translation of SQL create view queries of the following form,      CREATE VIEW $(name) AS     SELECT $(columns), $(numericExpr)     FROM $(tables)     WHERE $(joinElement)     GROUP BY $(groupingElement)  and generate an equivalent TensorFlow expression as below:      $(name) = $(translateNumericExpr(numericExpr))  For examples, check SQL files in the directory /working_examples.",TensorFlow,SOFTWARE
/main linear //To generate TensorFlow code for the Linear Regression model in working_examples/linear_regression.sql     .,TensorFlow,SOFTWARE
/main logistic //To generate TensorFlow code for the Logistic Regression model in working_examples/logistic_regression.sql  The files with the generated TensorFlow/Python code can be executed like any other TensorFlow program.,TensorFlow,SOFTWARE
/main logistic //To generate TensorFlow code for the Logistic Regression model in working_examples/logistic_regression.sql  The files with the generated TensorFlow/Python code can be executed like any other TensorFlow program.,TensorFlow,SOFTWARE
/main logistic //To generate TensorFlow code for the Logistic Regression model in working_examples/logistic_regression.sql  The files with the generated TensorFlow/Python code can be executed like any other TensorFlow program.,TensorFlow,SOFTWARE
"You can also try to translate individual SQL queries by loading the sql4ml_translator module in ghci and type:      translateToTensorFlowCommand (L.pack ""CREATE VIEW squaredErrors AS SELECT POW(errors.errorValue, 2) AS squaredErrorValue, errors.observationID AS observationID FROM errors;"") [""features""] [""weights""] [[""f1"", ""f2""]]  where      translateToTensorFlowCommand :: L.Text -> [String] -> [String] -> [[String]] -> String     translateToTensorFlowCommand sql_statement feature_tables variable_tables feature_names  and * sql_statement: a SQL create view query in type Text * feature_tables: a list of names of the tables storing the features of the model. * variable_tables: a list of names of the tables storing the weights of the model. * feature_names: a list of lists, each of which has the actual names of features stored in each table.",TensorFlow,SOFTWARE
"You can also try to translate individual SQL queries by loading the sql4ml_translator module in ghci and type:      translateToTensorFlowCommand (L.pack ""CREATE VIEW squaredErrors AS SELECT POW(errors.errorValue, 2) AS squaredErrorValue, errors.observationID AS observationID FROM errors;"") [""features""] [""weights""] [[""f1"", ""f2""]]  where      translateToTensorFlowCommand :: L.Text -> [String] -> [String] -> [[String]] -> String     translateToTensorFlowCommand sql_statement feature_tables variable_tables feature_names  and * sql_statement: a SQL create view query in type Text * feature_tables: a list of names of the tables storing the features of the model. * variable_tables: a list of names of the tables storing the weights of the model. * feature_names: a list of lists, each of which has the actual names of features stored in each table.",TensorFlow,SOFTWARE
"You can also try to translate individual SQL queries by loading the sql4ml_translator module in ghci and type:      translateToTensorFlowCommand (L.pack ""CREATE VIEW squaredErrors AS SELECT POW(errors.errorValue, 2) AS squaredErrorValue, errors.observationID AS observationID FROM errors;"") [""features""] [""weights""] [[""f1"", ""f2""]]  where      translateToTensorFlowCommand :: L.Text -> [String] -> [String] -> [[String]] -> String     translateToTensorFlowCommand sql_statement feature_tables variable_tables feature_names  and * sql_statement: a SQL create view query in type Text * feature_tables: a list of names of the tables storing the features of the model. * variable_tables: a list of names of the tables storing the weights of the model. * feature_names: a list of lists, each of which has the actual names of features stored in each table.",TensorFlow,SOFTWARE
"# knowledge-aware-med-classification Contains the codebase for our paper **Knowledge-Aware Neural Networks for Medical Forum Question Classification** that is accepted for publication at the 30th ACM International Conference on Information and Knowledge Management (CIKM 2021) [arXiv](https://arxiv.org/abs/2109.13141), [DOI](https://dl.acm.org/doi/10.1145/3459637.3482128)  The codebase is based on huggingface transformers [Github codebase](https://github.com/huggingface/transformers).",transformers,SOFTWARE
"# knowledge-aware-med-classification Contains the codebase for our paper **Knowledge-Aware Neural Networks for Medical Forum Question Classification** that is accepted for publication at the 30th ACM International Conference on Information and Knowledge Management (CIKM 2021) [arXiv](https://arxiv.org/abs/2109.13141), [DOI](https://dl.acm.org/doi/10.1145/3459637.3482128)  The codebase is based on huggingface transformers [Github codebase](https://github.com/huggingface/transformers).",Github,SOFTWARE
"# knowledge-aware-med-classification Contains the codebase for our paper **Knowledge-Aware Neural Networks for Medical Forum Question Classification** that is accepted for publication at the 30th ACM International Conference on Information and Knowledge Management (CIKM 2021) [arXiv](https://arxiv.org/abs/2109.13141), [DOI](https://dl.acm.org/doi/10.1145/3459637.3482128)  The codebase is based on huggingface transformers [Github codebase](https://github.com/huggingface/transformers).",transformers,SOFTWARE
The LCF-BERT and LCF-associated codes is based on ABSA-Pytorch [Github codebase](https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/lcf_bert.py)  !,ABSA-Pytorch,SOFTWARE
The LCF-BERT and LCF-associated codes is based on ABSA-Pytorch [Github codebase](https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/lcf_bert.py)  !,Github,SOFTWARE
The LCF-BERT and LCF-associated codes is based on ABSA-Pytorch [Github codebase](https://github.com/songyouwei/ABSA-PyTorch/blob/master/models/lcf_bert.py)  !,ABSA-PyTorch,SOFTWARE
"[Proposed Knowledge-aware BERT model](medbert-ichi.png)  ### Running Bert-plus models  The code files in the form of self-contained Jupyter notebooks is available at: **src/ProposedKnowledgeAwareModel/** Please go through the notebook, which produces at the end a 'run_ichi.py'.   ``` python .",Jupyter,SOFTWARE
"[Proposed Knowledge-aware BERT model](medbert-ichi.png)  ### Running Bert-plus models  The code files in the form of self-contained Jupyter notebooks is available at: **src/ProposedKnowledgeAwareModel/** Please go through the notebook, which produces at the end a 'run_ichi.py'.   ``` python .",python,SOFTWARE
"/tmp/ichi_bert_base_new --overwrite_output_dir ```  ### Experiments and Results The BERT and MedBERT models were trained and evaluated on two datasets: CADEC (multi-label) and ICHI (multi-class) (Datasets provided in the data directory).   ### Annotated CADEC, a multi-label medical forum question classificaton dataset We annotate CADEC as a multi-label multi-class dataset, for the task of ""Medical Forum Question Classification"".",BERT,SOFTWARE
"/tmp/ichi_bert_base_new --overwrite_output_dir ```  ### Experiments and Results The BERT and MedBERT models were trained and evaluated on two datasets: CADEC (multi-label) and ICHI (multi-class) (Datasets provided in the data directory).   ### Annotated CADEC, a multi-label medical forum question classificaton dataset We annotate CADEC as a multi-label multi-class dataset, for the task of ""Medical Forum Question Classification"".",MedBERT,SOFTWARE
"We also have an additional column, containing the extracted medical concepts using QuickUMLS tool.",QuickUMLS,SOFTWARE
"The annotated files can be found at **CADEC-Annotations/**  ### ICHI, a multi-class medical forum question classification dataset We obtain the ICHI data from rakshajalan/ECIR-2018 [Github codebase](https://github.com/rakshajalan/ECIR-2018/tree/master/ECIR-18_medical_question_classification/Dataset), and maintain the same train-test data split.  ## Citing MedBERT If you use the labeled CADEC dataset or use MedBERT model, please cite the paper:   ``` @inproceedings{10.1145/3459637.3482128, author = {Roy, Soumyadeep and Chakraborty, Sudip and Mandal, Aishik and Balde, Gunjan and Sharma, Prakhar and Natarajan, Anandhavelu and Khosla, Megha and Sural, Shamik and Ganguly, Niloy}, title = {Knowledge-Aware Neural Networks for Medical Forum Question Classification}, year = {2021}, isbn = {9781450384469}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3459637.3482128}, doi = {10.1145/3459637.3482128}, abstract = {Online medical forums have become a predominant platform for answering health-related information needs of consumers.",MedBERT,SOFTWARE
"The annotated files can be found at **CADEC-Annotations/**  ### ICHI, a multi-class medical forum question classification dataset We obtain the ICHI data from rakshajalan/ECIR-2018 [Github codebase](https://github.com/rakshajalan/ECIR-2018/tree/master/ECIR-18_medical_question_classification/Dataset), and maintain the same train-test data split.  ## Citing MedBERT If you use the labeled CADEC dataset or use MedBERT model, please cite the paper:   ``` @inproceedings{10.1145/3459637.3482128, author = {Roy, Soumyadeep and Chakraborty, Sudip and Mandal, Aishik and Balde, Gunjan and Sharma, Prakhar and Natarajan, Anandhavelu and Khosla, Megha and Sural, Shamik and Ganguly, Niloy}, title = {Knowledge-Aware Neural Networks for Medical Forum Question Classification}, year = {2021}, isbn = {9781450384469}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3459637.3482128}, doi = {10.1145/3459637.3482128}, abstract = {Online medical forums have become a predominant platform for answering health-related information needs of consumers.",MedBERT,SOFTWARE
"Here, we develop a novel medical knowledge-aware BERT-based model (MedBERT) that explicitly gives more weightage to medical concept-bearing words, and utilize domain-specific side information obtained from a popular medical knowledge base.",MedBERT,SOFTWARE
"MedBERT achieves state-of-the-art performance on two benchmark datasets and performs very well in low resource settings.}, booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management}, pages = {3398‚Äì3402}, numpages = {5}, keywords = {online health communities, clinical text classification}, location = {Virtual Event, Queensland, Australia}, series = {CIKM '21} } ```",MedBERT,SOFTWARE
# OpenAUC: Towards AUC-Oriented Open-Set Recognition This is a Pytorch implementation of our paper: [OpenAUC: Towards AUC-Oriented Open-Set Recognition](https://arxiv.org/abs/2210.13458).,Pytorch,SOFTWARE
We provide the bash script in the folder `bash_scripts` for the experiments on the *CUB* dataset.,bash,SOFTWARE
This repository contains a [colab](https://github.com/google/haloquest/blob/main/HaloQuest_Colab.ipynb) that shows how to load the HaloQuest data and how to use the Auto-Eval system.,colab,SOFTWARE
This repository contains a [colab](https://github.com/google/haloquest/blob/main/HaloQuest_Colab.ipynb) that shows how to load the HaloQuest data and how to use the Auto-Eval system.,Colab,SOFTWARE
This repository contains a [colab](https://github.com/google/haloquest/blob/main/HaloQuest_Colab.ipynb) that shows how to load the HaloQuest data and how to use the Auto-Eval system.,Auto-Eval,SOFTWARE
"The dataset is useful for both evaluation and fine-tuning purposes, aiming to advance multimodal reasoning.  ## Dataset Details  ### Data Collection HaloQuest includes a mix of real images from the Open Images dataset and synthetic images generated using Midjourney.",Midjourney,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",Gemini 1.0 Pro,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",Gemini 1.5 Pro,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",GPT-4,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",BEiT-3,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",InstructBLIP,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",InstructBLIP,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",BLIP2,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",MiniGPT4,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",MiniGPT4,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",Open-flamingo,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",LLaVA,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",BLIP2,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",mPLUG-Owl1,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",mPLUG-Owl2,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",OFA,SOFTWARE
"|                | Real Images | Synthetic Images | False Premise Questions | Visually Challenging Questions | Insufficient Context Questions | Total Entries | |----------------|:-----------:|:----------------:|:-----------------------:|:-----------------------------:|:-----------------------------:|:-------------:| | Training Set   | 2985        | 4155             | 2698                    | 2973                          | 1469                          | 7140          | | Evaluation Set | 217         | 391              | 304                     | 183                           | 121                           | 608           | | Total          | 3202        | 4546             | 3002                    | 3156                          | 1590                          | 7748          |   ## Leaderboard  (Gemini 1.0 Pro was used for Auto-Eval) <table>   <thead>     <tr>       <th rowspan=""2"">Model (#Param)</th>       <th rowspan=""2"">Rank</th>       <th colspan=""2"">Overall</th>       <th colspan=""2"">Generated</th>       <th colspan=""2"">Real</th>       <th colspan=""2"">False Premise</th>       <th colspan=""2"">Visually Challenging</th>       <th colspan=""2"">Insufficient Context</th>     </tr>     <tr>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>       <th>Human Eval</th>       <th>Auto-Eval</th>     </tr>   </thead><tbody><tr><td>Gemini 1.5 Pro (May 2024)</td><td>1</td><td>76.1</td><td>77.9</td><td>74.7</td><td>78.3</td><td>78.7</td><td>77.2</td><td>80.4</td><td>83.7</td><td>57.3</td><td>56.3</td><td>91</td><td>92.5</td></tr><tr><td>GPT-4o (May 2024)</td><td>2</td><td>68.1</td><td>63.2</td><td>68.8</td><td>63.8</td><td>66.9</td><td>62.2</td><td>68.5</td><td>65.2</td><td>58.3</td><td>55.2</td><td>80.6</td><td>68.7</td></tr><tr><td>GPT-4 (May 2024)</td><td>3</td><td>62.9</td><td>61.2</td><td>64.3</td><td>61.1</td><td>60.6</td><td>61.4</td><td>64.7</td><td>63</td><td>46.9</td><td>44.8</td><td>80.6</td><td>79.1</td></tr><tr><td>BEiT-3 (0.7B) (Mar 2024)</td><td>4</td><td>35.9</td><td>40</td><td>41.2</td><td>44.3</td><td>26.3</td><td>32.3</td><td>24.1</td><td>28.4</td><td>36.6</td><td>36.1</td><td>9.1</td><td>10.7</td></tr><tr><td>InstructBLIP (12B) (Mar 2024)</td><td>5</td><td>25.5</td><td>28.5</td><td>28.4</td><td>31.5</td><td>20.3</td><td>23</td><td>28.4</td><td>32</td><td>33.3</td><td>33.9</td><td>6.6</td><td>11.6</td></tr><tr><td>InstructBLIP (8B) (Mar 2024)</td><td>6</td><td>25</td><td>27.3</td><td>28.4</td><td>29.7</td><td>18.9</td><td>23</td><td>28.4</td><td>32</td><td>6.6</td><td>11.6</td><td>33.3</td><td>33.9</td></tr><tr><td>BLIP2 (12B) (Mar 2024)</td><td>7</td><td>21.1</td><td>22.5</td><td>24.8</td><td>26.1</td><td>14.29</td><td>16.1</td><td>16.8</td><td>19.5</td><td>35.5</td><td>32.8</td><td>9.9</td><td>14.9</td></tr><tr><td>MiniGPT4 (13B) (Mar 2024)</td><td>8</td><td>18.7</td><td>25.2</td><td>18.2</td><td>24</td><td>18.9</td><td>27.2</td><td>16.2</td><td>21.5</td><td>10.4</td><td>13.7</td><td>36.4</td><td>51.2</td></tr><tr><td>MiniGPT4 (7B) (Mar 2024)</td><td>9</td><td>18.6</td><td>19.1</td><td>18.1</td><td>19.4</td><td>18</td><td>18.4</td><td>13.2</td><td>13.2</td><td>26.5</td><td>27.3</td><td>15.7</td><td>16.5</td></tr><tr><td>Open-flamingo (9B) (Mar 2024)</td><td>10</td><td>13.8</td><td>15</td><td>16.1</td><td>17.1</td><td>9.7</td><td>11.1</td><td>13.2</td><td>13.9</td><td>19.1</td><td>21.3</td><td>7.4</td><td>8.3</td></tr><tr><td>LLaVA (13B) (Mar 2024)</td><td>11</td><td>10.9</td><td>10.9</td><td>12.3</td><td>12.8</td><td>8.2</td><td>7.4</td><td>2.3</td><td>1.7</td><td>30.6</td><td>31.2</td><td>2.5</td><td>3.3</td></tr><tr><td>BLIP2 (8B) (Mar 2024)</td><td>12</td><td>10.9</td><td>11.8</td><td>11.5</td><td>11.8</td><td>9.7</td><td>12</td><td>5</td><td>4.6</td><td>26.8</td><td>26.8</td><td>1.7</td><td>6.6</td></tr><tr><td>mPLUG-Owl1 (7B) (Mar 2024)</td><td>13</td><td>9.7</td><td>8.7</td><td>11.3</td><td>10.2</td><td>6.9</td><td>6</td><td>1</td><td>0.3</td><td>29</td><td>26.8</td><td>2.5</td><td>2.5</td></tr><tr><td>mPLUG-Owl2 (7B) (Mar 2024)</td><td>14</td><td>9.2</td><td>10.4</td><td>11</td><td>11.3</td><td>6</td><td>8.8</td><td>0.8</td><td>3.3</td><td>28.4</td><td>27.9</td><td>0.8</td><td>3.3</td></tr><tr><td>OFA (1B) (Mar 2024)</td><td>15</td><td>8.7</td><td>10.2</td><td>9.7</td><td>11.3</td><td>6.9</td><td>8.3</td><td>5</td><td>6.3</td><td>19.7</td><td>20.2</td><td>1.7</td><td>5</td></tr><tr><td>Open-flamingo (3B) (Mar 2024)</td><td>16</td><td>6.9</td><td>8.2</td><td>7.4</td><td>8.7</td><td>6</td><td>7.4</td><td>0.7</td><td>1.3</td><td>19.1</td><td>21.3</td><td>4.1</td><td>5.8</td></tr></tbody> </table>  ## Contributions  [Zhecan Wang](https://www.zhecanwang.com/)\*, [Garrett Bingham](https://garrettbingham.com/)\*, [Adams Wei Yu](https://adamsyu.github.io/), [Quoc V.",Open-flamingo,SOFTWARE
You may obtain a copy of the CC-BY license at: https://creativecommons.org/licenses/by/4.0/legalcode  Image URLs are from the [Open Images Dataset v7](https://storage.googleapis.com/openimages/web/factsfigures_v7.html#publications) and [Midjourney Showcase](https://www.midjourney.com/showcase).,Midjourney,SOFTWARE
# [Visual Relation Grounding in Videos](https://arxiv.org/pdf/2007.08814.pdf)  This is the pytorch implementation of our work at ECCV2020 (Spotlight).  !,pytorch,SOFTWARE
"[](https://github.com/doc-doc/vRGV/blob/master/model.png) ## Notes Fix issue on unstable result [2021/10/07].  ## Environment  Anaconda 3, python 3.6.5, pytorch 0.4.1 (Higher version is OK once feature is ready) and cuda >= 9.0.",Anaconda 3,SOFTWARE
"[](https://github.com/doc-doc/vRGV/blob/master/model.png) ## Notes Fix issue on unstable result [2021/10/07].  ## Environment  Anaconda 3, python 3.6.5, pytorch 0.4.1 (Higher version is OK once feature is ready) and cuda >= 9.0.",pytorch 0.4.1,SOFTWARE
"[](https://github.com/doc-doc/vRGV/blob/master/model.png) ## Notes Fix issue on unstable result [2021/10/07].  ## Environment  Anaconda 3, python 3.6.5, pytorch 0.4.1 (Higher version is OK once feature is ready) and cuda >= 9.0.",cuda >= 9.0,SOFTWARE
"For others libs, please refer to the file requirements.txt.  ## Install Please create an env for this project using anaconda3 (should install [anaconda](https://docs.anaconda.com/anaconda/install/linux/) first) ``` >conda create -n envname python=3.6.5 # Create >conda activate envname # Enter >pip install -r requirements.txt # Install the provided libs >sh vRGV/lib/make.sh # Set the environment for detection, make sure you have nvcc ``` ## Data Preparation Please download the data [here](https://drive.google.com/file/d/1qNJ3jBPPoi0BPkvLqooS66czvCxsib1M/view?",anaconda3,SOFTWARE
"For others libs, please refer to the file requirements.txt.  ## Install Please create an env for this project using anaconda3 (should install [anaconda](https://docs.anaconda.com/anaconda/install/linux/) first) ``` >conda create -n envname python=3.6.5 # Create >conda activate envname # Enter >pip install -r requirements.txt # Install the provided libs >sh vRGV/lib/make.sh # Set the environment for detection, make sure you have nvcc ``` ## Data Preparation Please download the data [here](https://drive.google.com/file/d/1qNJ3jBPPoi0BPkvLqooS66czvCxsib1M/view?",anaconda,SOFTWARE
"For others libs, please refer to the file requirements.txt.  ## Install Please create an env for this project using anaconda3 (should install [anaconda](https://docs.anaconda.com/anaconda/install/linux/) first) ``` >conda create -n envname python=3.6.5 # Create >conda activate envname # Enter >pip install -r requirements.txt # Install the provided libs >sh vRGV/lib/make.sh # Set the environment for detection, make sure you have nvcc ``` ## Data Preparation Please download the data [here](https://drive.google.com/file/d/1qNJ3jBPPoi0BPkvLqooS66czvCxsib1M/view?",anaconda,SOFTWARE
"For others libs, please refer to the file requirements.txt.  ## Install Please create an env for this project using anaconda3 (should install [anaconda](https://docs.anaconda.com/anaconda/install/linux/) first) ``` >conda create -n envname python=3.6.5 # Create >conda activate envname # Enter >pip install -r requirements.txt # Install the provided libs >sh vRGV/lib/make.sh # Set the environment for detection, make sure you have nvcc ``` ## Data Preparation Please download the data [here](https://drive.google.com/file/d/1qNJ3jBPPoi0BPkvLqooS66czvCxsib1M/view?",anaconda,SOFTWARE
"For others libs, please refer to the file requirements.txt.  ## Install Please create an env for this project using anaconda3 (should install [anaconda](https://docs.anaconda.com/anaconda/install/linux/) first) ``` >conda create -n envname python=3.6.5 # Create >conda activate envname # Enter >pip install -r requirements.txt # Install the provided libs >sh vRGV/lib/make.sh # Set the environment for detection, make sure you have nvcc ``` ## Data Preparation Please download the data [here](https://drive.google.com/file/d/1qNJ3jBPPoi0BPkvLqooS66czvCxsib1M/view?",conda,SOFTWARE
"For others libs, please refer to the file requirements.txt.  ## Install Please create an env for this project using anaconda3 (should install [anaconda](https://docs.anaconda.com/anaconda/install/linux/) first) ``` >conda create -n envname python=3.6.5 # Create >conda activate envname # Enter >pip install -r requirements.txt # Install the provided libs >sh vRGV/lib/make.sh # Set the environment for detection, make sure you have nvcc ``` ## Data Preparation Please download the data [here](https://drive.google.com/file/d/1qNJ3jBPPoi0BPkvLqooS66czvCxsib1M/view?",conda,SOFTWARE
"For others libs, please refer to the file requirements.txt.  ## Install Please create an env for this project using anaconda3 (should install [anaconda](https://docs.anaconda.com/anaconda/install/linux/) first) ``` >conda create -n envname python=3.6.5 # Create >conda activate envname # Enter >pip install -r requirements.txt # Install the provided libs >sh vRGV/lib/make.sh # Set the environment for detection, make sure you have nvcc ``` ## Data Preparation Please download the data [here](https://drive.google.com/file/d/1qNJ3jBPPoi0BPkvLqooS66czvCxsib1M/view?",pip,SOFTWARE
/detection.sh 0 val #(or train) ``` Sample video features: ``` cd tools python sample_video_feature.py ``` Test.,python,SOFTWARE
/ground.sh 0 val # Output the relation-aware spatio-temporal attention python generate_track_link.py # Generate relation-aware trajectories with Viterbi algorithm. python eval_ground.py # Evaluate the performance ``` You will get accuracy Acc_R: 24.58%.,python,SOFTWARE
"[last commit](https://img.shields.io/github/last-commit/TheoBourdais/ComputationalHypergraphDiscovery.svg)](https://github.com/TheoBourdais/ComputationalHypergraphDiscovery/commits/main)    This is the source code for the paper [""Codiscovering graphical structure and functional relationships within data: A Gaussian Process framework for connecting the dots""][paper_url].",ComputationalHypergraphDiscovery,SOFTWARE
"[last commit](https://img.shields.io/github/last-commit/TheoBourdais/ComputationalHypergraphDiscovery.svg)](https://github.com/TheoBourdais/ComputationalHypergraphDiscovery/commits/main)    This is the source code for the paper [""Codiscovering graphical structure and functional relationships within data: A Gaussian Process framework for connecting the dots""][paper_url].",ComputationalHypergraphDiscovery,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",matplotlib,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",NumPy,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",scipy,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",JAX,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",networkx,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",pip,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",PyPI,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",ComputationalHypergraphDiscovery,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",pip,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",pip,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",ComputationalHypergraphDiscovery,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",pip,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",git,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",ComputationalHypergraphDiscovery,SOFTWARE
"Please see the [companion blog post][blog_url] for a gentle introduction to the method and the code.   ## Installation   The code is written in Python 3 and requires the following packages: - matplotlib - NumPy - scipy - JAX - networkx  ### Using pip  The code is available on [PyPI](https://pypi.org/project/ComputationalHypergraphDiscovery/) and can be installed using pip:  ```bash pip install ComputationalHypergraphDiscovery ```  ### From source  After cloning this repo, you install the required packages using pip: ```bash pip install -r requirements.txt ```  and add the repo to your PYTHONPATH: ```bash git clone https://github.com/TheoBourdais/ComputationalHypergraphDiscovery.git export PYTHONPATH=$PYTHONPATH:/path/to/ComputationalHypergraphDiscovery/src ```  ## Quick start  Graph discovery takes very little time.",ComputationalHypergraphDiscovery,SOFTWARE
"The graph is a `networkx` object, which can be easily plotted using `.plot_graph()`.",networkx,SOFTWARE
">You can find the Sachs dataset in the repo, at this [link](https://github.com/TheoBourdais/ComputationalHypergraphDiscovery/blob/main/examples/SachsData.csv).",ComputationalHypergraphDiscovery,SOFTWARE
```python import ComputationalHypergraphDiscovery as CHD import pandas as pd df=pd.read_csv('https://github.com/TheoBourdais/ComputationalHypergraphDiscovery/blob/8c3fed6dfe58a01cb73e469579ff7703b0681f7e/examples/Sachs/SachsData.csv?,ComputationalHypergraphDiscovery,SOFTWARE
```python import ComputationalHypergraphDiscovery as CHD import pandas as pd df=pd.read_csv('https://github.com/TheoBourdais/ComputationalHypergraphDiscovery/blob/8c3fed6dfe58a01cb73e469579ff7703b0681f7e/examples/Sachs/SachsData.csv?,pandas,SOFTWARE
```python import ComputationalHypergraphDiscovery as CHD import pandas as pd df=pd.read_csv('https://github.com/TheoBourdais/ComputationalHypergraphDiscovery/blob/8c3fed6dfe58a01cb73e469579ff7703b0681f7e/examples/Sachs/SachsData.csv?,ComputationalHypergraphDiscovery,SOFTWARE
"Here is an example:   ```python import ComputationalHypergraphDiscovery as CHD import numpy as np X=np.random.rand(10,100) node_names=[f'node_{i}' for i in range(10)] graph_discovery = CHD.GraphDiscovery(X,node_names) ```  > **Note**: This shows how to create a `GraphDiscovery` object from a numpy array.",ComputationalHypergraphDiscovery,SOFTWARE
"Here is an example:   ```python import ComputationalHypergraphDiscovery as CHD import numpy as np X=np.random.rand(10,100) node_names=[f'node_{i}' for i in range(10)] graph_discovery = CHD.GraphDiscovery(X,node_names) ```  > **Note**: This shows how to create a `GraphDiscovery` object from a numpy array.",numpy,SOFTWARE
"You can use the code below to load it:  If you have a Pandas dataframe, you can use the `from_dataframe` method (see the method's docstring for more details):  ```python import ComputationalHypergraphDiscovery as CHD import pandas as pd df=pd.read_csv('https://github.com/TheoBourdais/ComputationalHypergraphDiscovery/blob/8c3fed6dfe58a01cb73e469579ff7703b0681f7e/examples/Sachs/SachsData.csv?",Pandas,SOFTWARE
"You can use the code below to load it:  If you have a Pandas dataframe, you can use the `from_dataframe` method (see the method's docstring for more details):  ```python import ComputationalHypergraphDiscovery as CHD import pandas as pd df=pd.read_csv('https://github.com/TheoBourdais/ComputationalHypergraphDiscovery/blob/8c3fed6dfe58a01cb73e469579ff7703b0681f7e/examples/Sachs/SachsData.csv?",ComputationalHypergraphDiscovery,SOFTWARE
"You can use the code below to load it:  If you have a Pandas dataframe, you can use the `from_dataframe` method (see the method's docstring for more details):  ```python import ComputationalHypergraphDiscovery as CHD import pandas as pd df=pd.read_csv('https://github.com/TheoBourdais/ComputationalHypergraphDiscovery/blob/8c3fed6dfe58a01cb73e469579ff7703b0681f7e/examples/Sachs/SachsData.csv?",pandas,SOFTWARE
"You can use the code below to load it:  If you have a Pandas dataframe, you can use the `from_dataframe` method (see the method's docstring for more details):  ```python import ComputationalHypergraphDiscovery as CHD import pandas as pd df=pd.read_csv('https://github.com/TheoBourdais/ComputationalHypergraphDiscovery/blob/8c3fed6dfe58a01cb73e469579ff7703b0681f7e/examples/Sachs/SachsData.csv?",ComputationalHypergraphDiscovery,SOFTWARE
"It is a `networkx` object, and you can use any of the methods provided by `networkx`.",networkx,SOFTWARE
"The interface is designed to ressemble the scikit-learn API, and you can use any kernel from scikit-learn",scikit-learn,SOFTWARE
"The interface is designed to ressemble the scikit-learn API, and you can use any kernel from scikit-learn",scikit-learn,SOFTWARE
"To manipulate kernels, import the modes module: ```python import ComputationalHypergraphDiscovery.Modes as Modes ```   The `ModeKernel` interface defines our kernel modes.",ComputationalHypergraphDiscovery,SOFTWARE
```python import ComputationalHypergraphDiscovery.decision as decision ```  There are two leading indicators that we use to make decisions (See the [paper][paper_url] or the [blog post][blog_url] for more details): - **Signal-to-noise ratio**: The signal-to-noise ratio is a measure of how much the signal is stronger than the noise.,ComputationalHypergraphDiscovery,SOFTWARE
"For example, with the Sachs dataset, we can define the following clusters: ```python clusters=[     ['$PKC$','$P38$','$Jnk$'],     ['$Erk$','$Akt$','$PKA$'],     ['$Raf$','$Mek$'],     ['$Plcg$','$PIP2$','$PIP3$'] ] graph_discovery = CHD.GraphDiscovery.from_dataframe(df,clusters=clusters) ```  #### After creating a `GraphDiscovery` object, for the second run  If you already have a `GraphDiscovery` object named `graph_discovery` on which you have run the algorithm, you can choose the clusters based on the results and apply them to get a new `GraphDiscovery` object: ```python graph_discovery2=graph_discovery.prepare_new_graph_with_clusters(clusters) ```  ### Using clusters Once the clusters have been defined, you can use the `GraphDiscovery` object as usual.",python,SOFTWARE
"```python import numpy as np import ComputationalHypergraphDiscovery as CHD from ComputationalHypergraphDiscovery.Modes import *  # Load the data data = np.loadtxt('data/Sachs.txt', delimiter='\t') # Normalize the data data = (data - np.mean(data, axis=0)) / np.std(data, axis=0) node_names=np.array(['$Raf$','$Mek$','$Plcg$','$PIP2$','$PIP3$','$Erk$','$Akt$','$PKA$','$PKC$','$P38$','$Jnk$'])  # Define the kernel kernel = [0.1*LinearMode(), 0.01*QuadraticMode()] # Set up CHD graph_discovery = CHD.GraphDiscovery(data,node_names,kernel) # Perform CHD graph_discovery.fit() graph_discovery.plot_graph()  # Refine the graph with clusters clusters=[     ['$PKC$','$P38$','$Jnk$'],     ['$Erk$','$Akt$','$PKA$'],     ['$Raf$','$Mek$'],     ['$Plcg$','$PIP2$','$PIP3$'] ] graph_discovery2=graph_discovery.prepare_new_graph_with_clusters(clusters) graph_discovery2.fit() graph_discovery2.plot_graph() ``` This code recovers the following graph:  <img style=""width:100%;"" alt=""Resulting graph Sachs example"" src=""_images/sachs_cluster_plot.png""></a>  ### Possible edges  As a practitioner, you may have some knowledge about the data that you wish to use to inform the graph discovery process.",numpy,SOFTWARE
"```python import numpy as np import ComputationalHypergraphDiscovery as CHD from ComputationalHypergraphDiscovery.Modes import *  # Load the data data = np.loadtxt('data/Sachs.txt', delimiter='\t') # Normalize the data data = (data - np.mean(data, axis=0)) / np.std(data, axis=0) node_names=np.array(['$Raf$','$Mek$','$Plcg$','$PIP2$','$PIP3$','$Erk$','$Akt$','$PKA$','$PKC$','$P38$','$Jnk$'])  # Define the kernel kernel = [0.1*LinearMode(), 0.01*QuadraticMode()] # Set up CHD graph_discovery = CHD.GraphDiscovery(data,node_names,kernel) # Perform CHD graph_discovery.fit() graph_discovery.plot_graph()  # Refine the graph with clusters clusters=[     ['$PKC$','$P38$','$Jnk$'],     ['$Erk$','$Akt$','$PKA$'],     ['$Raf$','$Mek$'],     ['$Plcg$','$PIP2$','$PIP3$'] ] graph_discovery2=graph_discovery.prepare_new_graph_with_clusters(clusters) graph_discovery2.fit() graph_discovery2.plot_graph() ``` This code recovers the following graph:  <img style=""width:100%;"" alt=""Resulting graph Sachs example"" src=""_images/sachs_cluster_plot.png""></a>  ### Possible edges  As a practitioner, you may have some knowledge about the data that you wish to use to inform the graph discovery process.",ComputationalHypergraphDiscovery,SOFTWARE
"```python import numpy as np import ComputationalHypergraphDiscovery as CHD from ComputationalHypergraphDiscovery.Modes import *  # Load the data data = np.loadtxt('data/Sachs.txt', delimiter='\t') # Normalize the data data = (data - np.mean(data, axis=0)) / np.std(data, axis=0) node_names=np.array(['$Raf$','$Mek$','$Plcg$','$PIP2$','$PIP3$','$Erk$','$Akt$','$PKA$','$PKC$','$P38$','$Jnk$'])  # Define the kernel kernel = [0.1*LinearMode(), 0.01*QuadraticMode()] # Set up CHD graph_discovery = CHD.GraphDiscovery(data,node_names,kernel) # Perform CHD graph_discovery.fit() graph_discovery.plot_graph()  # Refine the graph with clusters clusters=[     ['$PKC$','$P38$','$Jnk$'],     ['$Erk$','$Akt$','$PKA$'],     ['$Raf$','$Mek$'],     ['$Plcg$','$PIP2$','$PIP3$'] ] graph_discovery2=graph_discovery.prepare_new_graph_with_clusters(clusters) graph_discovery2.fit() graph_discovery2.plot_graph() ``` This code recovers the following graph:  <img style=""width:100%;"" alt=""Resulting graph Sachs example"" src=""_images/sachs_cluster_plot.png""></a>  ### Possible edges  As a practitioner, you may have some knowledge about the data that you wish to use to inform the graph discovery process.",ComputationalHypergraphDiscovery,SOFTWARE
"We tested our method on various LLMs such as Vicuna, Falcon, and Mistral, achieving an Attack Success Rate (ASR) over 80%.",Vicuna,SOFTWARE
"We tested our method on various LLMs such as Vicuna, Falcon, and Mistral, achieving an Attack Success Rate (ASR) over 80%.",Falcon,SOFTWARE
"We tested our method on various LLMs such as Vicuna, Falcon, and Mistral, achieving an Attack Success Rate (ASR) over 80%.",Mistral,SOFTWARE
"We also tested the model against Llama2-chat, the fine-tuned version of Llama2 designed to resist Jailbreak attacks, achieving good ASR with a suboptimal initial trigger seed.",Llama2-chat,SOFTWARE
"We also tested the model against Llama2-chat, the fine-tuned version of Llama2 designed to resist Jailbreak attacks, achieving good ASR with a suboptimal initial trigger seed.",Llama2,SOFTWARE
Clone the repository:      ```bash     git clone https://github.com/qroa/qroa.git     cd qroa     ```  2.,git,SOFTWARE
"Install the required packages:      ```bash     pip install -r requirements.txt     ```  ## üöÄ  Usage   ### ‚öîÔ∏è Running the Attack  To run the script, you need to provide the path to the input file containing the instructions.",pip,SOFTWARE
"Run the script from the command line by specifying the path to the instruction file and the authentication token:  ```bash python main.py data/instructions.csv [API_AUTH_TOKEN] ```  Replace `instructions.csv` with the path to your text file containing the instructions, and `API_AUTH_TOKEN` with the actual authentication token.  ### üß†  Supported Models  You can test the following models with QROA:  - **Llama2-chat** (`llama2_chat_hf`) - **Llama2** (`llama2_hf`) - **Vicuna** (`vicuna_hf`) - **Mistral** (`mistral_hf`) - **Falcon** (`falcon_hf`) - **OpenAI GPT** (`openai-0613`) - **Mistral Next** (`mistral`)  Simply change the `model` parameter in the `main` function to the desired model.  ### üß™  Demo and Testing Model Generation - **Notebook Demo:** Run `demo.ipynb` to see a demonstration of the process. - **Notebook Analysis Experiement:** Run `analysis.ipynb` to analyse results and calculate metrics value (ASR). - **Testing Model:** Generation: Execute `generate.py` to test the generation process on custom instructions and triggers.",python,SOFTWARE
"Run the script from the command line by specifying the path to the instruction file and the authentication token:  ```bash python main.py data/instructions.csv [API_AUTH_TOKEN] ```  Replace `instructions.csv` with the path to your text file containing the instructions, and `API_AUTH_TOKEN` with the actual authentication token.  ### üß†  Supported Models  You can test the following models with QROA:  - **Llama2-chat** (`llama2_chat_hf`) - **Llama2** (`llama2_hf`) - **Vicuna** (`vicuna_hf`) - **Mistral** (`mistral_hf`) - **Falcon** (`falcon_hf`) - **OpenAI GPT** (`openai-0613`) - **Mistral Next** (`mistral`)  Simply change the `model` parameter in the `main` function to the desired model.  ### üß™  Demo and Testing Model Generation - **Notebook Demo:** Run `demo.ipynb` to see a demonstration of the process. - **Notebook Analysis Experiement:** Run `analysis.ipynb` to analyse results and calculate metrics value (ASR). - **Testing Model:** Generation: Execute `generate.py` to test the generation process on custom instructions and triggers.",Llama2-chat,SOFTWARE
"Run the script from the command line by specifying the path to the instruction file and the authentication token:  ```bash python main.py data/instructions.csv [API_AUTH_TOKEN] ```  Replace `instructions.csv` with the path to your text file containing the instructions, and `API_AUTH_TOKEN` with the actual authentication token.  ### üß†  Supported Models  You can test the following models with QROA:  - **Llama2-chat** (`llama2_chat_hf`) - **Llama2** (`llama2_hf`) - **Vicuna** (`vicuna_hf`) - **Mistral** (`mistral_hf`) - **Falcon** (`falcon_hf`) - **OpenAI GPT** (`openai-0613`) - **Mistral Next** (`mistral`)  Simply change the `model` parameter in the `main` function to the desired model.  ### üß™  Demo and Testing Model Generation - **Notebook Demo:** Run `demo.ipynb` to see a demonstration of the process. - **Notebook Analysis Experiement:** Run `analysis.ipynb` to analyse results and calculate metrics value (ASR). - **Testing Model:** Generation: Execute `generate.py` to test the generation process on custom instructions and triggers.",Llama2,SOFTWARE
"Run the script from the command line by specifying the path to the instruction file and the authentication token:  ```bash python main.py data/instructions.csv [API_AUTH_TOKEN] ```  Replace `instructions.csv` with the path to your text file containing the instructions, and `API_AUTH_TOKEN` with the actual authentication token.  ### üß†  Supported Models  You can test the following models with QROA:  - **Llama2-chat** (`llama2_chat_hf`) - **Llama2** (`llama2_hf`) - **Vicuna** (`vicuna_hf`) - **Mistral** (`mistral_hf`) - **Falcon** (`falcon_hf`) - **OpenAI GPT** (`openai-0613`) - **Mistral Next** (`mistral`)  Simply change the `model` parameter in the `main` function to the desired model.  ### üß™  Demo and Testing Model Generation - **Notebook Demo:** Run `demo.ipynb` to see a demonstration of the process. - **Notebook Analysis Experiement:** Run `analysis.ipynb` to analyse results and calculate metrics value (ASR). - **Testing Model:** Generation: Execute `generate.py` to test the generation process on custom instructions and triggers.",Vicuna,SOFTWARE
"Run the script from the command line by specifying the path to the instruction file and the authentication token:  ```bash python main.py data/instructions.csv [API_AUTH_TOKEN] ```  Replace `instructions.csv` with the path to your text file containing the instructions, and `API_AUTH_TOKEN` with the actual authentication token.  ### üß†  Supported Models  You can test the following models with QROA:  - **Llama2-chat** (`llama2_chat_hf`) - **Llama2** (`llama2_hf`) - **Vicuna** (`vicuna_hf`) - **Mistral** (`mistral_hf`) - **Falcon** (`falcon_hf`) - **OpenAI GPT** (`openai-0613`) - **Mistral Next** (`mistral`)  Simply change the `model` parameter in the `main` function to the desired model.  ### üß™  Demo and Testing Model Generation - **Notebook Demo:** Run `demo.ipynb` to see a demonstration of the process. - **Notebook Analysis Experiement:** Run `analysis.ipynb` to analyse results and calculate metrics value (ASR). - **Testing Model:** Generation: Execute `generate.py` to test the generation process on custom instructions and triggers.",Mistral,SOFTWARE
"Run the script from the command line by specifying the path to the instruction file and the authentication token:  ```bash python main.py data/instructions.csv [API_AUTH_TOKEN] ```  Replace `instructions.csv` with the path to your text file containing the instructions, and `API_AUTH_TOKEN` with the actual authentication token.  ### üß†  Supported Models  You can test the following models with QROA:  - **Llama2-chat** (`llama2_chat_hf`) - **Llama2** (`llama2_hf`) - **Vicuna** (`vicuna_hf`) - **Mistral** (`mistral_hf`) - **Falcon** (`falcon_hf`) - **OpenAI GPT** (`openai-0613`) - **Mistral Next** (`mistral`)  Simply change the `model` parameter in the `main` function to the desired model.  ### üß™  Demo and Testing Model Generation - **Notebook Demo:** Run `demo.ipynb` to see a demonstration of the process. - **Notebook Analysis Experiement:** Run `analysis.ipynb` to analyse results and calculate metrics value (ASR). - **Testing Model:** Generation: Execute `generate.py` to test the generation process on custom instructions and triggers.",Falcon,SOFTWARE
"Run the script from the command line by specifying the path to the instruction file and the authentication token:  ```bash python main.py data/instructions.csv [API_AUTH_TOKEN] ```  Replace `instructions.csv` with the path to your text file containing the instructions, and `API_AUTH_TOKEN` with the actual authentication token.  ### üß†  Supported Models  You can test the following models with QROA:  - **Llama2-chat** (`llama2_chat_hf`) - **Llama2** (`llama2_hf`) - **Vicuna** (`vicuna_hf`) - **Mistral** (`mistral_hf`) - **Falcon** (`falcon_hf`) - **OpenAI GPT** (`openai-0613`) - **Mistral Next** (`mistral`)  Simply change the `model` parameter in the `main` function to the desired model.  ### üß™  Demo and Testing Model Generation - **Notebook Demo:** Run `demo.ipynb` to see a demonstration of the process. - **Notebook Analysis Experiement:** Run `analysis.ipynb` to analyse results and calculate metrics value (ASR). - **Testing Model:** Generation: Execute `generate.py` to test the generation process on custom instructions and triggers.",OpenAI GPT,SOFTWARE
"This script can be run from the command line as follows:  ```bash python generate.py -auth_token [API_AUTH_TOKEN] -instruction [THE INSTRUCTION HERE] -suffix [THE SUFFIX HERE] ```  Where:   - **auth_token:** Authentication token required for accessing the model. - **instruction:** The specific instruction you want the model to follow. - **suffix:** The adversarial trigger that, when appended to the instruction, causes the LLM to obey the instruction.  ### üìÅ  Output Files The following output files are generated during the execution of the script:  Generated and validated triggers are saved in JSON format:  - **Generated Triggers:** `.",python,SOFTWARE
"Each parameter plays a role in the setup and execution of the process:  | Parameter              | Description | |------------------------|-------------| | `model`                | Specifies the Large Language Model (LLM) to be used for the attack, such as 'vicuna_hf', 'falcon_hf', etc.| | `apply_defense_methods`| A boolean parameter that determines whether defense methods are activated to protect the model during the JailBreak process.| | `auth_token`           | Authentication token required for accessing the model.",vicuna_hf,SOFTWARE
"Each parameter plays a role in the setup and execution of the process:  | Parameter              | Description | |------------------------|-------------| | `model`                | Specifies the Large Language Model (LLM) to be used for the attack, such as 'vicuna_hf', 'falcon_hf', etc.| | `apply_defense_methods`| A boolean parameter that determines whether defense methods are activated to protect the model during the JailBreak process.| | `auth_token`           | Authentication token required for accessing the model.",falcon_hf,SOFTWARE
"# Streaming Video Model  > **Streaming Video Model** <br> > Yucheng Zhao, Chong Luo, Chuanxin Tang, Dongdong Chen, Noel Codella, Zheng-Jun Zha <br> > *CVPR 2023* <br>  [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Streaming_Video_Model_CVPR_2023_paper.html)]  [[arXiv](https://arxiv.org/abs/2303.17228)]   ## Description     **Streaming video model** is a general video model, which is applicable to general video understanding tasks.",Streaming video model,SOFTWARE
"We build an instance of streaming video model, namely the streaming video Transformer (S-ViT).S-ViT first produces frame-level features with a memory-enabled temporally-aware spatial encoder to serve the frame-based video tasks.",streaming video Transformer,SOFTWARE
"We build an instance of streaming video model, namely the streaming video Transformer (S-ViT).S-ViT first produces frame-level features with a memory-enabled temporally-aware spatial encoder to serve the frame-based video tasks.",S-ViT,SOFTWARE
"We build an instance of streaming video model, namely the streaming video Transformer (S-ViT).S-ViT first produces frame-level features with a memory-enabled temporally-aware spatial encoder to serve the frame-based video tasks.",S-ViT,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",conda,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",conda,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",conda,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",pytorch==1.12.0,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",torchvision==0.13.0,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",cudatoolkit=11.3,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",pytorch,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",pip,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",git,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",pip,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",mmcv-full==1.7.0,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",pip,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",mmdet==2.26.0,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",pip,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",pip,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",pip,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",einops,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",pip,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",future,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",tensorboard,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",pip,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",fvcore,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",pip,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",click,SOFTWARE
"The efficiency and efficacy of S-ViT is demonstrated by the state-of-the-art accuracy in the sequence-based action recognition task and the competitive advantage over conventional architecture in the frame-based MOT task.   ## Usage ### Installation  Clone the repo and install requirements:  ```shell conda create -n svm python=3.7 -y conda activate svm conda install pytorch==1.12.0 torchvision==0.13.0 cudatoolkit=11.3 -c pytorch pip install git+https://github.com/JonathonLuiten/TrackEval.git pip install mmcv-full==1.7.0 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html pip install mmdet==2.26.0 pip install -r requirements/build.txt pip install --user -v -e . pip install einops pip install future tensorboard pip install -U fvcore pip install click imageio[ffmpeg] path ```  ### Dataset preparation  Download [MOT17](https://motchallenge.net/data/MOT17/), [crowdhuman](https://www.crowdhuman.org/), and [MOTSynth](https://motchallenge.net/) datasets and put them under the data directory.",imageio[ffmpeg],SOFTWARE
"The data directory is structured as follows:  ``` data |-- crowdhuman ‚îÇ   ‚îú‚îÄ‚îÄ annotation_train.odgt ‚îÇ   ‚îú‚îÄ‚îÄ annotation_val.odgt ‚îÇ   ‚îú‚îÄ‚îÄ train ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Images ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CrowdHuman_train01.zip ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CrowdHuman_train02.zip ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CrowdHuman_train03.zip ‚îÇ   ‚îú‚îÄ‚îÄ val ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Images ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CrowdHuman_val.zip |-- MOT17 ‚îÇ   ‚îú‚îÄ‚îÄ train ‚îÇ   ‚îú‚îÄ‚îÄ test |-- MOTSynth |   ‚îú‚îÄ‚îÄ videos ‚îÇ   ‚îú‚îÄ‚îÄ annotations ```  Then, we need to convert the all dataset to COCO format.",COCO,SOFTWARE
We provide scripts to do this:  ```shell # crowdhuman python .,python,SOFTWARE
/data/crowdhuman/annotations  # MOT17 python .,python,SOFTWARE
/data/MOT17/annotations --split-train --convert-det  # MOTSynth python .,python,SOFTWARE
/data/MOTSynth/train/ python .,python,SOFTWARE
You can download them from [here](https://github.com/openai/CLIP/tree/main) and put them under the `pretrain` directory.  ### Training and Evaluation  Training on single node ```shell bash .,bash,SOFTWARE
/pretrain/ViT-B-16.pt ``` Evaluation on MOT17 half validation set ```shell bash .,bash,SOFTWARE
[3d-printed_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/aa4898ff-5481-4d89-98ba-656302ac70b1)                                                                                                                 |  Find all the 3D printable files [on the OWL repository](#3d-printing) or download them from  [Printables](https://www.printables.com/model/875853-raspberry-pi-rugged-imaging-enclosure).  **13-04-2024** - OpenWeedLocator now supports Raspberry Pi 5 and picamera2!,OWL,SOFTWARE
[3d-printed_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/aa4898ff-5481-4d89-98ba-656302ac70b1)                                                                                                                 |  Find all the 3D printable files [on the OWL repository](#3d-printing) or download them from  [Printables](https://www.printables.com/model/875853-raspberry-pi-rugged-imaging-enclosure).  **13-04-2024** - OpenWeedLocator now supports Raspberry Pi 5 and picamera2!,raspberry-pi,SOFTWARE
[3d-printed_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/aa4898ff-5481-4d89-98ba-656302ac70b1)                                                                                                                 |  Find all the 3D printable files [on the OWL repository](#3d-printing) or download them from  [Printables](https://www.printables.com/model/875853-raspberry-pi-rugged-imaging-enclosure).  **13-04-2024** - OpenWeedLocator now supports Raspberry Pi 5 and picamera2!,Raspberry Pi 5,SOFTWARE
[3d-printed_crop](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/aa4898ff-5481-4d89-98ba-656302ac70b1)                                                                                                                 |  Find all the 3D printable files [on the OWL repository](#3d-printing) or download them from  [Printables](https://www.printables.com/model/875853-raspberry-pi-rugged-imaging-enclosure).  **13-04-2024** - OpenWeedLocator now supports Raspberry Pi 5 and picamera2!,picamera2,SOFTWARE
"Improvements include: * support for both picamera and picamera2 * implementation of a `config/config.ini` approach to setting detection parameters * cleaner, more consistent code  **10-04-2024** - v2.1 of the [OWL driver board released](https://github.com/geezacoleman/owl-driver-board). * simplifies assembly * more robust and improved performance  ### OWLs in Action  |                                                                OWL on a vehicle                                                                 |                                            OWL on the AgroIntelli Robotti                                             | OWL on the Agerris Digital Farmhand | OWL on a bicycle | |:-----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------:|-------------------------------------|------------------| | !",picamera,SOFTWARE
"Improvements include: * support for both picamera and picamera2 * implementation of a `config/config.ini` approach to setting detection parameters * cleaner, more consistent code  **10-04-2024** - v2.1 of the [OWL driver board released](https://github.com/geezacoleman/owl-driver-board). * simplifies assembly * more robust and improved performance  ### OWLs in Action  |                                                                OWL on a vehicle                                                                 |                                            OWL on the AgroIntelli Robotti                                             | OWL on the Agerris Digital Farmhand | OWL on a bicycle | |:-----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------:|-------------------------------------|------------------| | !",picamera2,SOFTWARE
"Improvements include: * support for both picamera and picamera2 * implementation of a `config/config.ini` approach to setting detection parameters * cleaner, more consistent code  **10-04-2024** - v2.1 of the [OWL driver board released](https://github.com/geezacoleman/owl-driver-board). * simplifies assembly * more robust and improved performance  ### OWLs in Action  |                                                                OWL on a vehicle                                                                 |                                            OWL on the AgroIntelli Robotti                                             | OWL on the Agerris Digital Farmhand | OWL on a bicycle | |:-----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------:|-------------------------------------|------------------| | !",OWL,SOFTWARE
"Improvements include: * support for both picamera and picamera2 * implementation of a `config/config.ini` approach to setting detection parameters * cleaner, more consistent code  **10-04-2024** - v2.1 of the [OWL driver board released](https://github.com/geezacoleman/owl-driver-board). * simplifies assembly * more robust and improved performance  ### OWLs in Action  |                                                                OWL on a vehicle                                                                 |                                            OWL on the AgroIntelli Robotti                                             | OWL on the Agerris Digital Farmhand | OWL on a bicycle | |:-----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------:|-------------------------------------|------------------| | !",OWL,SOFTWARE
"Improvements include: * support for both picamera and picamera2 * implementation of a `config/config.ini` approach to setting detection parameters * cleaner, more consistent code  **10-04-2024** - v2.1 of the [OWL driver board released](https://github.com/geezacoleman/owl-driver-board). * simplifies assembly * more robust and improved performance  ### OWLs in Action  |                                                                OWL on a vehicle                                                                 |                                            OWL on the AgroIntelli Robotti                                             | OWL on the Agerris Digital Farmhand | OWL on a bicycle | |:-----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------:|-------------------------------------|------------------| | !",OWL,SOFTWARE
"Improvements include: * support for both picamera and picamera2 * implementation of a `config/config.ini` approach to setting detection parameters * cleaner, more consistent code  **10-04-2024** - v2.1 of the [OWL driver board released](https://github.com/geezacoleman/owl-driver-board). * simplifies assembly * more robust and improved performance  ### OWLs in Action  |                                                                OWL on a vehicle                                                                 |                                            OWL on the AgroIntelli Robotti                                             | OWL on the Agerris Digital Farmhand | OWL on a bicycle | |:-----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------:|-------------------------------------|------------------| | !",OWL,SOFTWARE
"Improvements include: * support for both picamera and picamera2 * implementation of a `config/config.ini` approach to setting detection parameters * cleaner, more consistent code  **10-04-2024** - v2.1 of the [OWL driver board released](https://github.com/geezacoleman/owl-driver-board). * simplifies assembly * more robust and improved performance  ### OWLs in Action  |                                                                OWL on a vehicle                                                                 |                                            OWL on the AgroIntelli Robotti                                             | OWL on the Agerris Digital Farmhand | OWL on a bicycle | |:-----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------:|-------------------------------------|------------------| | !",OWL,SOFTWARE
"Improvements include: * support for both picamera and picamera2 * implementation of a `config/config.ini` approach to setting detection parameters * cleaner, more consistent code  **10-04-2024** - v2.1 of the [OWL driver board released](https://github.com/geezacoleman/owl-driver-board). * simplifies assembly * more robust and improved performance  ### OWLs in Action  |                                                                OWL on a vehicle                                                                 |                                            OWL on the AgroIntelli Robotti                                             | OWL on the Agerris Digital Farmhand | OWL on a bicycle | |:-----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------:|-------------------------------------|------------------| | !",OWL,SOFTWARE
"[bike_owl_cropped](https://github.com/geezacoleman/OpenWeedLocator/assets/51358498/17ad4ead-429e-4384-9e74-b050a536897f)        |  ### Official Publications  #### OpenWeedLocator (OWL): An open-source, low-cost device for fallow weed detection  This is the original OWL publication, released in [Scientific Reports (open access)](https://www.nature.com/articles/s41598-021-03858-9).",OWL,SOFTWARE
"[DOI](https://zenodo.org/badge/399194159.svg)](https://zenodo.org/badge/latestdoi/399194159)  # Overview  * [OWL Use Cases](#owl-use-cases) * [Community Development](#community-development-and-contribution) * [Hardware Requirements](#hardware-requirements)     - [Hardware Assembly](#hardware-assembly)     - [Single Board Computer (SBC) Options](#sbc-options) * [Software Installation](#software)     - [Quick Method](#quick-method)     - [Detailed Method](#detailed-method)     - [Changing Detection Settings](#changing-detection-settings)     - [Green-on-Green (almost) :eyes::dart::seedling:](#green-on-green)     - [Installing on non-Raspberry Pi Computers](#non-raspberry-pi-installation) * [Controller](#connecting-a-controller) * [3D Printing](#3d-printing) * [Updating OWL](#updating-owl)     - [Version History](#version-history) * [Troubleshooting](#troubleshooting) * [Citing OWL](#citing-owl) * [Acknowledgements](#acknowledgements) * [References](#references)  ### Manuals  If you prefer a hardcopy version of these instructions, you can view and download the PDF using one of the links below.",OWL,SOFTWARE
"[DOI](https://zenodo.org/badge/399194159.svg)](https://zenodo.org/badge/latestdoi/399194159)  # Overview  * [OWL Use Cases](#owl-use-cases) * [Community Development](#community-development-and-contribution) * [Hardware Requirements](#hardware-requirements)     - [Hardware Assembly](#hardware-assembly)     - [Single Board Computer (SBC) Options](#sbc-options) * [Software Installation](#software)     - [Quick Method](#quick-method)     - [Detailed Method](#detailed-method)     - [Changing Detection Settings](#changing-detection-settings)     - [Green-on-Green (almost) :eyes::dart::seedling:](#green-on-green)     - [Installing on non-Raspberry Pi Computers](#non-raspberry-pi-installation) * [Controller](#connecting-a-controller) * [3D Printing](#3d-printing) * [Updating OWL](#updating-owl)     - [Version History](#version-history) * [Troubleshooting](#troubleshooting) * [Citing OWL](#citing-owl) * [Acknowledgements](#acknowledgements) * [References](#references)  ### Manuals  If you prefer a hardcopy version of these instructions, you can view and download the PDF using one of the links below.",owl,SOFTWARE
"[20220714-150328__frame_420_n_4](https://user-images.githubusercontent.com/51358498/178903171-af7f8f1b-9caf-435e-96d0-4f01e76c53fb.png) | | Deactivated (DEFAULT)  | None |  |  ## Community development and contribution  As more OWLs are built and fallow weed control systems developed, we would love to share the end results here.",OWL,SOFTWARE
"It provides a more production friendly, durable and water/chemical resisant option over 3D printed plastic.  #### OWL Driver Board The [Official OWL driver board](https://github.com/geezacoleman/owl-driver-board) combines the relay control board, power supply and wiring.",OWL,SOFTWARE
"It provides a more production friendly, durable and water/chemical resisant option over 3D printed plastic.  #### OWL Driver Board The [Official OWL driver board](https://github.com/geezacoleman/owl-driver-board) combines the relay control board, power supply and wiring.",OWL,SOFTWARE
"The parts list is substantially reduced:  | **Component**                                                        | **Quantity**      | **Link**                                                                                                                                                                                                                                                                                                                                                                                                          | |----------------------------------------------------------------------|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | **Enclosure**                                                        |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | **OFFICIAL OWL ENCLOSURE** - aluminium                               | 1                 | TBD                                                                                                                                                                                                                                                                                                                                                                                                               | | Extrusion - 3D printed                                               | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Main%20Body.stl)                                                                                                                                                                                                                                                                                                   | | Front plate                                                          | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Frontplate.stl)                                                                                                                                                                                                                                                                                                    | | Tray                                                                 | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/picamera2/3D%20Models/Compact%20OWL/Tray.stl)                                                                                                                                                                                                                                                                                                     | | Back plate - Amphenol, Adafruit RJ45                                 | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Receptacle%20and%20Ethernet.stl)                                                                                                                                                                                                                                                                   | | Back plate - Amphenol only                                           | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Receptacle%20Only.stl)                                                                                                                                                                                                                                                                             | | Back plate - 16 mm cable gland                                       | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Backplate%20-%20Gland.stl)                                                                                                                                                                                                                                                                                         | | Lens mount                                                           | 1                 | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/main/3D%20Models/Compact%20OWL/Lens%20Mount.stl)                                                                                                                                                                                                                                                                                                  | | Camera mount                                                         | 1*                | [STL File](https://github.com/geezacoleman/OpenWeedLocator/blob/picamera2/3D%20Models/Compact%20OWL/Camera%20Mount.stl)                                                                                                                                                                                                                                                                                           | | **Computing**                                                        |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | Raspberry Pi 5 4GB (or Pi 4 or 3B+)                                  | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-5-model-b-4gb.html)                                                                                                                                                                                                                                                                                                                                           | | *Green-on-Green ONLY - Google Coral USB Accelerator                  | 1                 | [Link](https://coral.ai/products/accelerator)                                                                                                                                                                                                                                                                                                                                                                     | | 64GB SD Card (min. 16 GB)                                            | 1                 | [Link](https://core-electronics.com.au/extreme-sd-microsd-memory-card-64gb-class-10-adapter-included.html)                                                                                                                                                                                                                                                                                                        | | **Camera** (choose one)                                              |                   |                                                                                                                                                                                                                                                                                                                                                                                                                   | | RECOMMENDED: Raspberry Pi Global Shutter Camera                      | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-global-shutter-camera.html)                                                                                                                                                                                                                                                                                                                                   | | CCTV 6mm Wide Angle Lens                                             | 1 (GS or HQ only) | [Link](https://core-electronics.com.au/raspberry-pi-6mm-wide-angle-lens.html)                                                                                                                                                                                                                                                                                                                                     | | SUPPORTED: Raspberry Pi 12MP HQ Camera                               | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-hq-camera.html)                                                                                                                                                                                                                                                                                                                                               | | SUPPORTED: Raspberry Pi Camera Module 3                              | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-3.html)                                                                                                                                                                                                                                                                                                                                                | | SUPPORTED: Raspberry Pi V2 Camera (NOT RECOMMENDED)                  | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-board-v2-8-megapixels-38552.html)                                                                                                                                                                                                                                                                                                                      | | ‚ö†Ô∏èNOTE‚ö†Ô∏è If you use the RPi 5, make sure you have the right camera cable | 1                 | [Link](https://core-electronics.com.au/raspberry-pi-camera-fpc-adapter-cable-200mm.html)                                                                                                                                                                                                                                                                                                                          | | **Power Management** * items only needed in place of OWL driver board | 1                 |                                                                                                                                                                                                                                                                                                                                                                                                                   | | **OFFICIAL OWL DRIVER BOARD** (incl. power mgmt, relay control)      | 1                 | TBD                                                                                                                                                                                                                                                                                                                                                                                                               | | * 5V 5A Step Down Voltage Regulator                                  | 1                 | [Link](https://core-electronics.com.au/pololu-5v-5a-step-down-voltage-regulator-d24v50f5.html)                                                                                                                                                                                                                                                                                                                    | | * 4 Channel, Relay Control Board HAT                                 | 1                 | [Link](https://core-electronics.com.au/pirelay-v2-relay-board-for-raspberry-pi-1.html?",picamera2,SOFTWARE
"The LEDs, fuse and Bulgin connector are all mounted on the rear of the OWL unit, rather than where they are located in the diagram.",OWL,SOFTWARE
>‚ö†Ô∏è**NOTE**‚ö†Ô∏è 08/05/2024 - OWL transitioned from `picamera` to `picamera2` support.,picamera,SOFTWARE
>‚ö†Ô∏è**NOTE**‚ö†Ô∏è 08/05/2024 - OWL transitioned from `picamera` to `picamera2` support.,picamera2,SOFTWARE
The v1.0.0 disk image below (Buster) does not support `picamera2` and will not work on the Raspberry Pi 5 nor with the recent camera releases.,picamera2,SOFTWARE
We strongly recommend using the most up to date version of Raspbian with the latest OWL software.,OWL,SOFTWARE
"Once you are in the `owl` environment, enter these commands on each new line:  ``` (owl) owl@raspberrypi:~ $ cd ~ (owl) owl@raspberrypi:~ $ mv owl owl-old      # this renames the old 'owl' folder to 'owl-old' (owl) owl@raspberrypi:~ $ git clone https://github.com/geezacoleman/OpenWeedLocator        # download the new software (owl) owl@raspberrypi:~ $ mv OpenWeedLocator owl      # rename the download to 'owl' (owl) owl@raspberrypi:~ $ cd ~/owl (owl) owl@raspberrypi:~/owl $ pip install -r requirements.txt                # installs the necessary software into the (owl) environment  (owl) owl@raspberrypi:~/owl $ chmod a+x owl.py                  # changes owl.py to be executable (owl) owl@raspberrypi:~/owl $ chmod a+x owl_boot.sh                     # changes owl_boot.sh to be executable ```  Once this is complete your software will be up to date and you can move on to focusing the camera.  ### Step 5 - focusing the camera  The final step in the process is to make sure the camera is correctly focused for the mounting height.",pip,SOFTWARE
Simply run:   ``` (owl) owl@raspberrypi:~ $ cd ~/owl (owl) owl@raspberrypi:~/owl $ bash focus_owl.sh   ```  This will automate all the steps below.,bash,SOFTWARE
"```commandline echo ""# virtualenv and virtualenvwraper"" >> ~/.bashrc ``` ```commandline echo ""export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python"" >> ~/.bashrc ``` Finally, reload the `.bashrc` file.",virtualenv,SOFTWARE
"```commandline echo ""# virtualenv and virtualenvwraper"" >> ~/.bashrc ``` ```commandline echo ""export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python"" >> ~/.bashrc ``` Finally, reload the `.bashrc` file.",virtualenvwraper,SOFTWARE
"```commandline source ~/.bashrc ``` Once that is complete, you can install `virtualenv` and `virtualenvwrapper` and add a few more lines to the same `bashrc` file.",virtualenv,SOFTWARE
"```commandline source ~/.bashrc ``` Once that is complete, you can install `virtualenv` and `virtualenvwrapper` and add a few more lines to the same `bashrc` file.",virtualenvwrapper,SOFTWARE
"```commandline sudo apt-get install python3-virtualenv ``` ```commandline sudo apt-get install python3-virtualenvwrapper ``` Following the installation of these packages, add the following lines to the `.bashrc` file.",python3-virtualenv,SOFTWARE
"```commandline sudo apt-get install python3-virtualenv ``` ```commandline sudo apt-get install python3-virtualenvwrapper ``` Following the installation of these packages, add the following lines to the `.bashrc` file.",python3-virtualenvwrapper,SOFTWARE
"Importantly, we need to inherit the site-packages (i.e. everything currently on the Pi) because they contain `picamera2` pre-installed.",picamera2,SOFTWARE
"The most import is OpenCV, which we'll do first before downloading the OWL repository and installing the remainder from the `requirements.txt` file.",OpenCV,SOFTWARE
```commandline pip3 install opencv-contrib-python ``` This should have successfully installed OpenCV into the `owl` virtual environment.,pip3,SOFTWARE
```commandline pip3 install opencv-contrib-python ``` This should have successfully installed OpenCV into the `owl` virtual environment.,opencv-contrib-python,SOFTWARE
```commandline pip3 install opencv-contrib-python ``` This should have successfully installed OpenCV into the `owl` virtual environment.,OpenCV,SOFTWARE
You can double check by quickly starting a Python session at the command line: ```commandline python ``` This will open up an interactive Python session (indicated by >>>) from which you should type:  ```commandline >>> import cv2 >>> import picamera2 >>> exit() ``` This should then exit you from the Python session.,picamera2,SOFTWARE
"The command line should look like this:  ```commandline (owl) owl@raspberrypi:~ $ ```  If both of these complete without error, then you've successfully set up the virtual environment and installed OpenCV.  ### Step 3 - Downloading the 'owl' repository  Now you should have:  * A virtual environment called 'owl' * A working version of OpenCV installed into that environment * a Terminal window open with the 'owl' environment activated.",OpenCV,SOFTWARE
"The command line should look like this:  ```commandline (owl) owl@raspberrypi:~ $ ```  If both of these complete without error, then you've successfully set up the virtual environment and installed OpenCV.  ### Step 3 - Downloading the 'owl' repository  Now you should have:  * A virtual environment called 'owl' * A working version of OpenCV installed into that environment * a Terminal window open with the 'owl' environment activated.",OpenCV,SOFTWARE
"These include:  * OpenCV (should already be in 'owl' virtual environment from Step 1) * numpy * imutils * gpiozero * pandas * RPi.GPIO * tqdm * blessed (for command line visualisation) * threading, multiprocessing, collections, queue, time, os (though these are included as standard Python modules).",OpenCV,SOFTWARE
"To install all the requirements.txt, change into the owl directory:  ```commandline cd ~/owl ```  then run:  ```commandline pip install -r requirements.txt ``` Now to double-check this has worked, we can open up another Python session and try importing the packages.",pip,SOFTWARE
"To install all the requirements.txt, change into the owl directory:  ```commandline cd ~/owl ```  then run:  ```commandline pip install -r requirements.txt ``` Now to double-check this has worked, we can open up another Python session and try importing the packages.",Python,SOFTWARE
Begin by typing:  ```commandline python ``` Python should start up an interactive session; type each of these in and make sure you don't get any errors.,python,SOFTWARE
Begin by typing:  ```commandline python ``` Python should start up an interactive session; type each of these in and make sure you don't get any errors.,Python,SOFTWARE
"__version__) >>> exit() ``` If any errors appear, you'll need to go back and check that the modules above have (1) been installed into the owl virtual environment, (2) that Python was started in the owl environment, and/or (3) they all installed correctly.",Python,SOFTWARE
"Once that is complete, exit Python and continue with the installation process.  ### Step 5 - starting OWL on boot  Now that these dependencies have been installed into the owl virtual environment, it's time to make sure the software  runs on startup.",OWL,SOFTWARE
"/owl.py > $HOME_DIR/owl/logs/owl_$LOG_DATE.log 2>&1 & ```  In the file, the first two commands launch our `owl` virtual environment, then we change directory `cd` into the owl  folder and run the python program.",python,SOFTWARE
This is where you'll need to reconnect the camera and all the GPIO pins/power in the OWL unit if they have been disconnected.,OWL,SOFTWARE
"If the relays start clicking (the Official OWL  HAT uses transistors and will not click - look for the lights) and lights come on, congratulations, you've successfully  set the OWL up!",OWL,SOFTWARE
Useful if using the OWL for dedicated image data collection,OWL,SOFTWARE
| |        **Relays**         |                 Integer/GPIO Boardpin                  |                                                                                    Maps a relay number to a boardpin on the GPIO header                                                                                     |    </details>  ## Connecting a Controller  <details> <summary>Adding simple controllers to manage four OWLS or fewer</summary> <br>  The software and designs for the first generation GPIO-based controllers (between 1 and 4 OWL units) are now available.,OWL,SOFTWARE
"[YOLOv8](https://github.com/ultralytics/ultralytics) and [YOLOv5](https://github.com/ultralytics/yolov5) currently provide the most user friendly methods of training, optimisation and exporting as `.tflite` files for use with the Google Coral.",YOLOv8,SOFTWARE
"[YOLOv8](https://github.com/ultralytics/ultralytics) and [YOLOv5](https://github.com/ultralytics/yolov5) currently provide the most user friendly methods of training, optimisation and exporting as `.tflite` files for use with the Google Coral.",YOLOv5,SOFTWARE
"</details>  ## Non-Raspberry Pi Installation  <details> <summary>Installing OWL software on a non-Raspberry Pi system</summary> <br> Using OWL software on your laptop/desktop or other non-Raspberry Pi system is a great way to test, develop and learn more about how it works.",OWL,SOFTWARE
"</details>  ## Non-Raspberry Pi Installation  <details> <summary>Installing OWL software on a non-Raspberry Pi system</summary> <br> Using OWL software on your laptop/desktop or other non-Raspberry Pi system is a great way to test, develop and learn more about how it works.",OWL,SOFTWARE
"This method has been successfully tested on PyCharm with Anaconda environments.  ``` > git clone https://github.com/geezacoleman/OpenWeedLocator > cd OpenWeedLocator ```  For the next part, make sure you are in the virtual environment you will be working from.",PyCharm,SOFTWARE
"This method has been successfully tested on PyCharm with Anaconda environments.  ``` > git clone https://github.com/geezacoleman/OpenWeedLocator > cd OpenWeedLocator ```  For the next part, make sure you are in the virtual environment you will be working from.",Anaconda,SOFTWARE
"Assuming the virtual environment is working and is activated, run through these next couple of steps:  ``` > pip install -r non_rpi_requirements.txt     # this will install all the necessary packages, without including the Raspberry Pi specific ones. ```  It may take a minute or two for those to complete installing.",pip,SOFTWARE
But once they are done you are free to run the `owl.py` software.  ``` > python owl.py --show-display ```  From there you can change the command line flags (as described above) or play around with the settings to see how it works.,python,SOFTWARE
All 3D model files are availabe on to edit and download on [TinkerCAD](https://www.tinkercad.com/things/id1FMJrWtJp-compact-owl) or [Printables](https://www.printables.com/model/875853-raspberry-pi-rugged-imaging-enclosure).,TinkerCAD,SOFTWARE
</details>  # Updating OWL  <details> <summary>Updating OWL software</summary> <br>  We and others will be continually contributing to and improving OWL as we become aware of issues or opportunities to increase detection performance.,OWL,SOFTWARE
</details>  # Updating OWL  <details> <summary>Updating OWL software</summary> <br>  We and others will be continually contributing to and improving OWL as we become aware of issues or opportunities to increase detection performance.,OWL,SOFTWARE
Run owl_setup.sh                        | | **`DependencyError`**      | ‚Äî                                     | - Missing Python packages<br>- Wrong virtual environment                                      | 1.,Python,SOFTWARE
Run pip install for package<br>3.,pip,SOFTWARE
We recommend updating to the latest software by following the procedure detailed in the [Updating OWL](#updating-owl) section above,OWL,SOFTWARE
**Blog Posts** [How to run a Raspberry Pi script at startup](https://www.makeuseof.com/how-to-run-a-raspberry-pi-program-script-at-startup/)  [How to Run a Script at Boot on Raspberry Pi (with cron)](https://www.tomshardware.com/how-to/run-script-at-boot-raspberry-pi)  [Install OpenCV 4 on Raspberry Pi 4 and Raspbian Buster](https://www.pyimagesearch.com/2019/09/16/install-opencv-4-on-raspberry-pi-4-and-raspbian-buster/)  [How to solder](https://www.makerspaces.com/how-to-solder/)  </details>  # Repository Stats  ### Star History  [!,OpenCV 4,SOFTWARE
**Blog Posts** [How to run a Raspberry Pi script at startup](https://www.makeuseof.com/how-to-run-a-raspberry-pi-program-script-at-startup/)  [How to Run a Script at Boot on Raspberry Pi (with cron)](https://www.tomshardware.com/how-to/run-script-at-boot-raspberry-pi)  [Install OpenCV 4 on Raspberry Pi 4 and Raspbian Buster](https://www.pyimagesearch.com/2019/09/16/install-opencv-4-on-raspberry-pi-4-and-raspbian-buster/)  [How to solder](https://www.makerspaces.com/how-to-solder/)  </details>  # Repository Stats  ### Star History  [!,opencv-4,SOFTWARE
"Mixture Proportion Estimation and PU Learning: A Modern Approach. arxiv preprint  arXiv:2111.00980.  ``` @inproceedings{garg2021PUlearning,     title={Mixture Proportion Estimation and {PU} Learning: A Modern Approach},     author={Garg, Saurabh and Wu, Yifan and Smola, Alex and Balakrishnan, Sivaraman and Lipton, Zachary},     year={2021},     booktitle={Advances in Neural Information Processing Systems (NeurIPS)}  } ```  ## Requirements  The code is written in Python and uses [PyTorch](https://pytorch.org/).",PyTorch,SOFTWARE
"Mixture Proportion Estimation and PU Learning: A Modern Approach. arxiv preprint  arXiv:2111.00980.  ``` @inproceedings{garg2021PUlearning,     title={Mixture Proportion Estimation and {PU} Learning: A Modern Approach},     author={Garg, Saurabh and Wu, Yifan and Smola, Alex and Balakrishnan, Sivaraman and Lipton, Zachary},     year={2021},     booktitle={Advances in Neural Information Processing Systems (NeurIPS)}  } ```  ## Requirements  The code is written in Python and uses [PyTorch](https://pytorch.org/).",pytorch,SOFTWARE
"# Thermometer: Towards Universal Calibration for Large Language Models #### This repository contains official implementation of the paper [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819).  ## Requirements ```bash pip install torch==2.2.1 transformers==4.28.1 evaluate==0.4.1 tqdm pandas ``` ## File Structure #### *src* folder includes all the source code for the experiments. - ### *configs* folder:      includes all the information of training configurations and model hyper-parameter. - ### *data* folder:     includes the dataloader to load and process the datasets. - ### other code files:   - *process_mrqa.py* pre-process the raw data in free-form QA datasets MRQA;    - *extract_features.py* aims to extract labels, features, and logits from pretrained LLMs;   - *train_thermometer.py* and *eval_thermometer.py* contain the main function to train Thermometer,   and the functions to evaluate calibration performance of trained Thermometer, respectively.   ## Usage #### We provide the scripts to help reproduce the results of our paper. - ### Step-1: extract the features and logits from pre-trained LLMs,     ```     exract.sh     ``` - ### Step-2: train Thermometer model,     ```     train.sh     ``` - ### Step-3: evaluate calibration of trained Thermometer model,     ```     eval.sh     ``` - ### Free-form QA task requires an additional step to pre-process the raw data, i.e., append the LLM's response to the prompts,     ```     mrqa.sh     ``` - ### Choose different types of LLMs,     ```     --model_type decoder_only --model_name Llama-2-7b-chat-hf     --model_type encoder_decoder --model_name flan-t5-xl     ```  ## Citation #### If you find this repository helpful for your research, please consider citing our paper,  ``` @InProceedings{pmlr-v235-shen24c,   title = {Thermometer: Towards Universal Calibration for Large Language Models},   author =  {Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W. and Ghosh, Soumya},   booktitle = {Proceedings of the 41st International Conference on Machine Learning},   pages = {44687--44711},   year =  {2024},   editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},   volume =  {235},   series =  {Proceedings of Machine Learning Research},   month = {21--27 Jul},   publisher = {PMLR} } ```",pip,SOFTWARE
"# Thermometer: Towards Universal Calibration for Large Language Models #### This repository contains official implementation of the paper [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819).  ## Requirements ```bash pip install torch==2.2.1 transformers==4.28.1 evaluate==0.4.1 tqdm pandas ``` ## File Structure #### *src* folder includes all the source code for the experiments. - ### *configs* folder:      includes all the information of training configurations and model hyper-parameter. - ### *data* folder:     includes the dataloader to load and process the datasets. - ### other code files:   - *process_mrqa.py* pre-process the raw data in free-form QA datasets MRQA;    - *extract_features.py* aims to extract labels, features, and logits from pretrained LLMs;   - *train_thermometer.py* and *eval_thermometer.py* contain the main function to train Thermometer,   and the functions to evaluate calibration performance of trained Thermometer, respectively.   ## Usage #### We provide the scripts to help reproduce the results of our paper. - ### Step-1: extract the features and logits from pre-trained LLMs,     ```     exract.sh     ``` - ### Step-2: train Thermometer model,     ```     train.sh     ``` - ### Step-3: evaluate calibration of trained Thermometer model,     ```     eval.sh     ``` - ### Free-form QA task requires an additional step to pre-process the raw data, i.e., append the LLM's response to the prompts,     ```     mrqa.sh     ``` - ### Choose different types of LLMs,     ```     --model_type decoder_only --model_name Llama-2-7b-chat-hf     --model_type encoder_decoder --model_name flan-t5-xl     ```  ## Citation #### If you find this repository helpful for your research, please consider citing our paper,  ``` @InProceedings{pmlr-v235-shen24c,   title = {Thermometer: Towards Universal Calibration for Large Language Models},   author =  {Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W. and Ghosh, Soumya},   booktitle = {Proceedings of the 41st International Conference on Machine Learning},   pages = {44687--44711},   year =  {2024},   editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},   volume =  {235},   series =  {Proceedings of Machine Learning Research},   month = {21--27 Jul},   publisher = {PMLR} } ```",torch==2.2.1,SOFTWARE
"# Thermometer: Towards Universal Calibration for Large Language Models #### This repository contains official implementation of the paper [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819).  ## Requirements ```bash pip install torch==2.2.1 transformers==4.28.1 evaluate==0.4.1 tqdm pandas ``` ## File Structure #### *src* folder includes all the source code for the experiments. - ### *configs* folder:      includes all the information of training configurations and model hyper-parameter. - ### *data* folder:     includes the dataloader to load and process the datasets. - ### other code files:   - *process_mrqa.py* pre-process the raw data in free-form QA datasets MRQA;    - *extract_features.py* aims to extract labels, features, and logits from pretrained LLMs;   - *train_thermometer.py* and *eval_thermometer.py* contain the main function to train Thermometer,   and the functions to evaluate calibration performance of trained Thermometer, respectively.   ## Usage #### We provide the scripts to help reproduce the results of our paper. - ### Step-1: extract the features and logits from pre-trained LLMs,     ```     exract.sh     ``` - ### Step-2: train Thermometer model,     ```     train.sh     ``` - ### Step-3: evaluate calibration of trained Thermometer model,     ```     eval.sh     ``` - ### Free-form QA task requires an additional step to pre-process the raw data, i.e., append the LLM's response to the prompts,     ```     mrqa.sh     ``` - ### Choose different types of LLMs,     ```     --model_type decoder_only --model_name Llama-2-7b-chat-hf     --model_type encoder_decoder --model_name flan-t5-xl     ```  ## Citation #### If you find this repository helpful for your research, please consider citing our paper,  ``` @InProceedings{pmlr-v235-shen24c,   title = {Thermometer: Towards Universal Calibration for Large Language Models},   author =  {Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W. and Ghosh, Soumya},   booktitle = {Proceedings of the 41st International Conference on Machine Learning},   pages = {44687--44711},   year =  {2024},   editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},   volume =  {235},   series =  {Proceedings of Machine Learning Research},   month = {21--27 Jul},   publisher = {PMLR} } ```",transformers==4.28.1,SOFTWARE
"# Thermometer: Towards Universal Calibration for Large Language Models #### This repository contains official implementation of the paper [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819).  ## Requirements ```bash pip install torch==2.2.1 transformers==4.28.1 evaluate==0.4.1 tqdm pandas ``` ## File Structure #### *src* folder includes all the source code for the experiments. - ### *configs* folder:      includes all the information of training configurations and model hyper-parameter. - ### *data* folder:     includes the dataloader to load and process the datasets. - ### other code files:   - *process_mrqa.py* pre-process the raw data in free-form QA datasets MRQA;    - *extract_features.py* aims to extract labels, features, and logits from pretrained LLMs;   - *train_thermometer.py* and *eval_thermometer.py* contain the main function to train Thermometer,   and the functions to evaluate calibration performance of trained Thermometer, respectively.   ## Usage #### We provide the scripts to help reproduce the results of our paper. - ### Step-1: extract the features and logits from pre-trained LLMs,     ```     exract.sh     ``` - ### Step-2: train Thermometer model,     ```     train.sh     ``` - ### Step-3: evaluate calibration of trained Thermometer model,     ```     eval.sh     ``` - ### Free-form QA task requires an additional step to pre-process the raw data, i.e., append the LLM's response to the prompts,     ```     mrqa.sh     ``` - ### Choose different types of LLMs,     ```     --model_type decoder_only --model_name Llama-2-7b-chat-hf     --model_type encoder_decoder --model_name flan-t5-xl     ```  ## Citation #### If you find this repository helpful for your research, please consider citing our paper,  ``` @InProceedings{pmlr-v235-shen24c,   title = {Thermometer: Towards Universal Calibration for Large Language Models},   author =  {Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W. and Ghosh, Soumya},   booktitle = {Proceedings of the 41st International Conference on Machine Learning},   pages = {44687--44711},   year =  {2024},   editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},   volume =  {235},   series =  {Proceedings of Machine Learning Research},   month = {21--27 Jul},   publisher = {PMLR} } ```",evaluate==0.4.1,SOFTWARE
"# Thermometer: Towards Universal Calibration for Large Language Models #### This repository contains official implementation of the paper [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819).  ## Requirements ```bash pip install torch==2.2.1 transformers==4.28.1 evaluate==0.4.1 tqdm pandas ``` ## File Structure #### *src* folder includes all the source code for the experiments. - ### *configs* folder:      includes all the information of training configurations and model hyper-parameter. - ### *data* folder:     includes the dataloader to load and process the datasets. - ### other code files:   - *process_mrqa.py* pre-process the raw data in free-form QA datasets MRQA;    - *extract_features.py* aims to extract labels, features, and logits from pretrained LLMs;   - *train_thermometer.py* and *eval_thermometer.py* contain the main function to train Thermometer,   and the functions to evaluate calibration performance of trained Thermometer, respectively.   ## Usage #### We provide the scripts to help reproduce the results of our paper. - ### Step-1: extract the features and logits from pre-trained LLMs,     ```     exract.sh     ``` - ### Step-2: train Thermometer model,     ```     train.sh     ``` - ### Step-3: evaluate calibration of trained Thermometer model,     ```     eval.sh     ``` - ### Free-form QA task requires an additional step to pre-process the raw data, i.e., append the LLM's response to the prompts,     ```     mrqa.sh     ``` - ### Choose different types of LLMs,     ```     --model_type decoder_only --model_name Llama-2-7b-chat-hf     --model_type encoder_decoder --model_name flan-t5-xl     ```  ## Citation #### If you find this repository helpful for your research, please consider citing our paper,  ``` @InProceedings{pmlr-v235-shen24c,   title = {Thermometer: Towards Universal Calibration for Large Language Models},   author =  {Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W. and Ghosh, Soumya},   booktitle = {Proceedings of the 41st International Conference on Machine Learning},   pages = {44687--44711},   year =  {2024},   editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},   volume =  {235},   series =  {Proceedings of Machine Learning Research},   month = {21--27 Jul},   publisher = {PMLR} } ```",tqdm,SOFTWARE
"# Thermometer: Towards Universal Calibration for Large Language Models #### This repository contains official implementation of the paper [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819).  ## Requirements ```bash pip install torch==2.2.1 transformers==4.28.1 evaluate==0.4.1 tqdm pandas ``` ## File Structure #### *src* folder includes all the source code for the experiments. - ### *configs* folder:      includes all the information of training configurations and model hyper-parameter. - ### *data* folder:     includes the dataloader to load and process the datasets. - ### other code files:   - *process_mrqa.py* pre-process the raw data in free-form QA datasets MRQA;    - *extract_features.py* aims to extract labels, features, and logits from pretrained LLMs;   - *train_thermometer.py* and *eval_thermometer.py* contain the main function to train Thermometer,   and the functions to evaluate calibration performance of trained Thermometer, respectively.   ## Usage #### We provide the scripts to help reproduce the results of our paper. - ### Step-1: extract the features and logits from pre-trained LLMs,     ```     exract.sh     ``` - ### Step-2: train Thermometer model,     ```     train.sh     ``` - ### Step-3: evaluate calibration of trained Thermometer model,     ```     eval.sh     ``` - ### Free-form QA task requires an additional step to pre-process the raw data, i.e., append the LLM's response to the prompts,     ```     mrqa.sh     ``` - ### Choose different types of LLMs,     ```     --model_type decoder_only --model_name Llama-2-7b-chat-hf     --model_type encoder_decoder --model_name flan-t5-xl     ```  ## Citation #### If you find this repository helpful for your research, please consider citing our paper,  ``` @InProceedings{pmlr-v235-shen24c,   title = {Thermometer: Towards Universal Calibration for Large Language Models},   author =  {Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W. and Ghosh, Soumya},   booktitle = {Proceedings of the 41st International Conference on Machine Learning},   pages = {44687--44711},   year =  {2024},   editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},   volume =  {235},   series =  {Proceedings of Machine Learning Research},   month = {21--27 Jul},   publisher = {PMLR} } ```",pandas,SOFTWARE
"# Thermometer: Towards Universal Calibration for Large Language Models #### This repository contains official implementation of the paper [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819).  ## Requirements ```bash pip install torch==2.2.1 transformers==4.28.1 evaluate==0.4.1 tqdm pandas ``` ## File Structure #### *src* folder includes all the source code for the experiments. - ### *configs* folder:      includes all the information of training configurations and model hyper-parameter. - ### *data* folder:     includes the dataloader to load and process the datasets. - ### other code files:   - *process_mrqa.py* pre-process the raw data in free-form QA datasets MRQA;    - *extract_features.py* aims to extract labels, features, and logits from pretrained LLMs;   - *train_thermometer.py* and *eval_thermometer.py* contain the main function to train Thermometer,   and the functions to evaluate calibration performance of trained Thermometer, respectively.   ## Usage #### We provide the scripts to help reproduce the results of our paper. - ### Step-1: extract the features and logits from pre-trained LLMs,     ```     exract.sh     ``` - ### Step-2: train Thermometer model,     ```     train.sh     ``` - ### Step-3: evaluate calibration of trained Thermometer model,     ```     eval.sh     ``` - ### Free-form QA task requires an additional step to pre-process the raw data, i.e., append the LLM's response to the prompts,     ```     mrqa.sh     ``` - ### Choose different types of LLMs,     ```     --model_type decoder_only --model_name Llama-2-7b-chat-hf     --model_type encoder_decoder --model_name flan-t5-xl     ```  ## Citation #### If you find this repository helpful for your research, please consider citing our paper,  ``` @InProceedings{pmlr-v235-shen24c,   title = {Thermometer: Towards Universal Calibration for Large Language Models},   author =  {Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W. and Ghosh, Soumya},   booktitle = {Proceedings of the 41st International Conference on Machine Learning},   pages = {44687--44711},   year =  {2024},   editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},   volume =  {235},   series =  {Proceedings of Machine Learning Research},   month = {21--27 Jul},   publisher = {PMLR} } ```",Thermometer,SOFTWARE
"# Thermometer: Towards Universal Calibration for Large Language Models #### This repository contains official implementation of the paper [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819).  ## Requirements ```bash pip install torch==2.2.1 transformers==4.28.1 evaluate==0.4.1 tqdm pandas ``` ## File Structure #### *src* folder includes all the source code for the experiments. - ### *configs* folder:      includes all the information of training configurations and model hyper-parameter. - ### *data* folder:     includes the dataloader to load and process the datasets. - ### other code files:   - *process_mrqa.py* pre-process the raw data in free-form QA datasets MRQA;    - *extract_features.py* aims to extract labels, features, and logits from pretrained LLMs;   - *train_thermometer.py* and *eval_thermometer.py* contain the main function to train Thermometer,   and the functions to evaluate calibration performance of trained Thermometer, respectively.   ## Usage #### We provide the scripts to help reproduce the results of our paper. - ### Step-1: extract the features and logits from pre-trained LLMs,     ```     exract.sh     ``` - ### Step-2: train Thermometer model,     ```     train.sh     ``` - ### Step-3: evaluate calibration of trained Thermometer model,     ```     eval.sh     ``` - ### Free-form QA task requires an additional step to pre-process the raw data, i.e., append the LLM's response to the prompts,     ```     mrqa.sh     ``` - ### Choose different types of LLMs,     ```     --model_type decoder_only --model_name Llama-2-7b-chat-hf     --model_type encoder_decoder --model_name flan-t5-xl     ```  ## Citation #### If you find this repository helpful for your research, please consider citing our paper,  ``` @InProceedings{pmlr-v235-shen24c,   title = {Thermometer: Towards Universal Calibration for Large Language Models},   author =  {Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W. and Ghosh, Soumya},   booktitle = {Proceedings of the 41st International Conference on Machine Learning},   pages = {44687--44711},   year =  {2024},   editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},   volume =  {235},   series =  {Proceedings of Machine Learning Research},   month = {21--27 Jul},   publisher = {PMLR} } ```",Thermometer,SOFTWARE
"# Thermometer: Towards Universal Calibration for Large Language Models #### This repository contains official implementation of the paper [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819).  ## Requirements ```bash pip install torch==2.2.1 transformers==4.28.1 evaluate==0.4.1 tqdm pandas ``` ## File Structure #### *src* folder includes all the source code for the experiments. - ### *configs* folder:      includes all the information of training configurations and model hyper-parameter. - ### *data* folder:     includes the dataloader to load and process the datasets. - ### other code files:   - *process_mrqa.py* pre-process the raw data in free-form QA datasets MRQA;    - *extract_features.py* aims to extract labels, features, and logits from pretrained LLMs;   - *train_thermometer.py* and *eval_thermometer.py* contain the main function to train Thermometer,   and the functions to evaluate calibration performance of trained Thermometer, respectively.   ## Usage #### We provide the scripts to help reproduce the results of our paper. - ### Step-1: extract the features and logits from pre-trained LLMs,     ```     exract.sh     ``` - ### Step-2: train Thermometer model,     ```     train.sh     ``` - ### Step-3: evaluate calibration of trained Thermometer model,     ```     eval.sh     ``` - ### Free-form QA task requires an additional step to pre-process the raw data, i.e., append the LLM's response to the prompts,     ```     mrqa.sh     ``` - ### Choose different types of LLMs,     ```     --model_type decoder_only --model_name Llama-2-7b-chat-hf     --model_type encoder_decoder --model_name flan-t5-xl     ```  ## Citation #### If you find this repository helpful for your research, please consider citing our paper,  ``` @InProceedings{pmlr-v235-shen24c,   title = {Thermometer: Towards Universal Calibration for Large Language Models},   author =  {Shen, Maohao and Das, Subhro and Greenewald, Kristjan and Sattigeri, Prasanna and Wornell, Gregory W. and Ghosh, Soumya},   booktitle = {Proceedings of the 41st International Conference on Machine Learning},   pages = {44687--44711},   year =  {2024},   editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},   volume =  {235},   series =  {Proceedings of Machine Learning Research},   month = {21--27 Jul},   publisher = {PMLR} } ```",Thermometer,SOFTWARE
The coarse-to-fine annotations enable multi-perspective analysis regarding PAV-SOD modeling.,PAV-SOD,SOFTWARE
"With extensive experimental results, we gain several findings about PAV-SOD challenges and insights towards PAV-SOD model interpretability.",PAV-SOD ,SOFTWARE
"According to the features of defined salient objects within each of the sequences, multiple attributes, e.g., ‚Äúmultiple objects‚Äù (MO), ‚Äúcompeting sounds‚Äù (CS), ‚Äúgeometrical distortion‚Äù (GD), ‚Äúmotion blur‚Äù (MB), ‚Äúocclusions‚Äù (OC) and ‚Äúlow resolution‚Äù (LR) are further annotated to enable detailed analysis for PAV-SOD modeling.",PAV-SOD,SOFTWARE
The pre-trained models can be downloaded at [Google Drive](https://drive.google.com/file/d/1gNWmgmlBfJqCYE5phDuHTMFIou1TAmXs/view?,Google Drive,SOFTWARE
usp=sharing).  ------  # Dataset Downloads  The whole object-/instance-level ground truth with default split can be downloaded from [Google Drive](https://drive.google.com/file/d/1Whp_ftuXza8-vkjNtICdxdRebcmzcrFi/view?,Google Drive,SOFTWARE
The videos (with ambisonics) with default split can be downloaded from [Google Drive](https://drive.google.com/file/d/13FEv1yAyMmK4GkiZ2Mce6gJxQuME7vG3/view?,Google Drive,SOFTWARE
The videos (with mono sound) can be downloaded from [Google Drive](https://drive.google.com/file/d/1klJnHSiUM7Ow2LkdaLe-O6CEQ9qmdo2F/view?,Google Drive,SOFTWARE
usp=sharing)  The audio files (.wav) can be downloaded from [Google Drive](https://drive.google.com/file/d/1-jqDArcm8vBhyku3Xb8HLopG1XmpAS13/view?,Google Drive,SOFTWARE
The head movement and eye fixation data can be downloaded from [Google Drive](https://drive.google.com/drive/folders/1EpWc7GVcGFAn5VigV3c2-ZtIZElfXPX1?,Google Drive,SOFTWARE
**[CVPR'22]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2022/papers/Quan_Which_Images_To_Label_for_Few-Shot_Medical_Landmark_Detection_CVPR_2022_paper.pdf)**  Towards Fewer Annotations: Active Learning via Region Impurity and Prediction Uncertainty for Domain Adaptive Semantic Segmentation   **[CVPR'22]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_Towards_Fewer_Annotations_Active_Learning_via_Region_Impurity_and_Prediction_CVPR_2022_paper.pdf)** **[[Code]](https://github.com/BIT-DA/RIPU)**  Learning Distinctive Margin Toward Active Domain Adaptation   **[CVPR'22]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2022/papers/Xie_Learning_Distinctive_Margin_Toward_Active_Domain_Adaptation_CVPR_2022_paper.pdf)** **[[Code]](https://github.com/TencentYoutuResearch/ActiveLearning-SDM)**  BoostMIS: Boosting Medical Image Semi-Supervised Learning With Adaptive Pseudo Labeling and Informative Active Annotation   **[CVPR'22]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_BoostMIS_Boosting_Medical_Image_Semi-Supervised_Learning_With_Adaptive_Pseudo_Labeling_CVPR_2022_paper.pdf)** **[[Code]](https://github.com/wannature/BoostMIS)**  One-Bit Active Query With Contrastive Pairs   **[CVPR'22]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_One-Bit_Active_Query_With_Contrastive_Pairs_CVPR_2022_paper.pdf)**  Revisiting Superpixels for Active Learning in Semantic Segmentation With Realistic Annotation Costs   **[CVPR'21]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2021/papers/Cai_Revisiting_Superpixels_for_Active_Learning_in_Semantic_Segmentation_With_Realistic_CVPR_2021_paper.pdf)** **[[Code]](https://github.com/cailile/Revisiting-Superpixels-for-Active-Learning)**  Sequential Graph Convolutional Network for Active Learning   **[CVPR'21]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2021/papers/Caramalau_Sequential_Graph_Convolutional_Network_for_Active_Learning_CVPR_2021_paper.pdf)** **[[Code]](https://github.com/razvancaramalau/Sequential-GCN-for-Active-Learning)**  VaB-AL: Incorporating Class Imbalance and Difficulty With Variational Bayes for Active Learning   **[CVPR'21]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2021/papers/Choi_VaB-AL_Incorporating_Class_Imbalance_and_Difficulty_With_Variational_Bayes_for_CVPR_2021_paper.pdf)** **[[Code]](https://github.com/jongwon20000/vabal)**  Transferable Query Selection for Active Domain Adaptation   **[CVPR'21]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2021/papers/Fu_Transferable_Query_Selection_for_Active_Domain_Adaptation_CVPR_2021_paper.pdf)** **[[Code]](https://github.com/thuml/Transferable-Query-Selection)**  Exploring Data-Efficient 3D Scene Understanding With Contrastive Scene Contexts   **[CVPR'21]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2021/papers/Hou_Exploring_Data-Efficient_3D_Scene_Understanding_With_Contrastive_Scene_Contexts_CVPR_2021_paper.pdf)** **[[Code]](https://github.com/facebookresearch/ContrastiveSceneContexts)**  Task-Aware Variational Adversarial Active Learning   **[CVPR'21]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2021/papers/Kim_Task-Aware_Variational_Adversarial_Active_Learning_CVPR_2021_paper.pdf)** **[[Code]](https://github.com/cubeyoung/TA-VAAL)**  Multiple Instance Active Learning for Object Detection   **[CVPR'21]** **[[PDF]](https://openaccess.thecvf.com/content/CVPR2021/papers/Yuan_Multiple_Instance_Active_Learning_for_Object_Detection_CVPR_2021_paper.pdf)** **[[Code]](https://github.com/yuantn/MI-AOD)**  Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision   üïù  **[CVPR'20]** **[[PDF]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Gudovskiy_Deep_Active_Learning_for_Biased_Datasets_via_Fisher_Kernel_Self-Supervision_CVPR_2020_paper.pdf)** **[[Code]](https://github.com/gudovskiy/al-fk-self-supervision)**  State-Relabeling Adversarial Active Learning   **[CVPR'20]** **[[PDF]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_State-Relabeling_Adversarial_Active_Learning_CVPR_2020_paper.pdf)** **[[Code]](https://github.com/Beichen1996/SRAAL)**  ViewAL: Active Learning with Viewpoint Entropy for Semantic Segmentation   **[CVPR'20]** **[[PDF]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Siddiqui_ViewAL_Active_Learning_With_Viewpoint_Entropy_for_Semantic_Segmentation_CVPR_2020_paper.pdf)** **[[Code]](https://github.com/nihalsid/ViewAL)**  Learning Loss for Active Learning   **[CVPR'19]** **[[PDF]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Yoo_Learning_Loss_for_Active_Learning_CVPR_2019_paper.pdf)**  Reducing Uncertainty in Undersampled MRI Reconstruction with Active Acquisition   **[CVPR'19]** **[[PDF]](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Reducing_Uncertainty_in_Undersampled_MRI_Reconstruction_With_Active_Acquisition_CVPR_2019_paper.pdf)**  The Power of Ensembles for Active Learning in Image Classification   **[CVPR'18]** **[[PDF]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Beluch_The_Power_of_CVPR_2018_paper.pdf)**  Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation   **[CVPR'18]** **[[PDF]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_Quantization_of_Fully_CVPR_2018_paper.pdf)**  Fine-Tuning Convolutional Neural Networks for Biomedical Image Analysis: Actively and Incrementally   **[CVPR'17]** **[[PDF]](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Fine-Tuning_Convolutional_Neural_CVPR_2017_paper.pdf)** **[[Code]](https://github.com/MrGiovanni/Active-Learning)**   ### International Conference on Computer Vision (ICCV) HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling   üïù  **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_HAL3D_Hierarchical_Active_Learning_for_Fine-Grained_3D_Part_Labeling_ICCV_2023_paper.pdf)**  Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation   üïù  **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Hierarchical_Point-based_Active_Learning_for_Semi-supervised_Point_Cloud_Semantic_Segmentation_ICCV_2023_paper.pdf)** **[[Code]](https://github.com/SmiletoE/HPAL)**  ALWOD: Active Learning for Weakly-Supervised Object Detection   üïù  **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_ALWOD_Active_Learning_for_Weakly-Supervised_Object_Detection_ICCV_2023_paper.pdf)** **[[Code]](https://github.com/seqam-lab/ALWOD)**  You Never Get a Second Chance To Make a Good First Impression: Seeding Active Learning for 3D Semantic Segmentation   üïù  **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Samet_You_Never_Get_a_Second_Chance_To_Make_a_Good_ICCV_2023_paper.pdf)** **[[Code]](https://github.com/nerminsamet/seedal)**  Heterogeneous Diversity Driven Active Learning for Multi-Object Tracking   üïù  **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Heterogeneous_Diversity_Driven_Active_Learning_for_Multi-Object_Tracking_ICCV_2023_paper.pdf)**  TiDAL: Learning Training Dynamics for Active Learning   üïù  **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Kye_TiDAL_Learning_Training_Dynamics_for_Active_Learning_ICCV_2023_paper.pdf)** **[[Code]](https://github.com/hyperconnect/TiDAL)**  Knowledge-Aware Federated Active Learning with Non-IID Data   üïù  **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_Knowledge-Aware_Federated_Active_Learning_with_Non-IID_Data_ICCV_2023_paper.pdf)** **[[Code]](https://github.com/ycao5602/KAFAL)**  Adaptive Superpixel for Active Learning in Semantic Segmentation   **[ICCV'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Adaptive_Superpixel_for_Active_Learning_in_Semantic_Segmentation_ICCV_2023_paper.pdf)** **[[Code]](https://github.com/ml-postech/adaptive-superpixel-for-active-learning-in-semantic-segmentation)**  Active Universal Domain Adaptation   üïù  **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Ma_Active_Universal_Domain_Adaptation_ICCV_2021_paper.pdf)**  Semi-Supervised Active Learning for Semi-Supervised Models: Exploit Adversarial Examples With Graph-Based Virtual Labels   üïù  **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Guo_Semi-Supervised_Active_Learning_for_Semi-Supervised_Models_Exploit_Adversarial_Examples_With_ICCV_2021_paper.pdf)**  Active Learning for Deep Object Detection via Probabilistic Modeling   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Choi_Active_Learning_for_Deep_Object_Detection_via_Probabilistic_Modeling_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/NVlabs/AL-MDN)**  Contrastive Coding for Active Learning Under Class Distribution Mismatch   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Du_Contrastive_Coding_for_Active_Learning_Under_Class_Distribution_Mismatch_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/RUC-DWBI-ML/CCAL)**  Semi-Supervised Active Learning With Temporal Output Discrepancy   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Semi-Supervised_Active_Learning_With_Temporal_Output_Discrepancy_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/siyuhuang/TOD)**  Influence Selection for Active Learning   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Influence_Selection_for_Active_Learning_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/dragonlzm/ISAL)**  Multi-Anchor Active Domain Adaptation for Semantic Segmentation   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Ning_Multi-Anchor_Active_Domain_Adaptation_for_Semantic_Segmentation_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/munanning/MADA)**  Active Learning for Lane Detection: A Knowledge Distillation Approach   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Peng_Active_Learning_for_Lane_Detection_A_Knowledge_Distillation_Approach_ICCV_2021_paper.pdf)**  Active Domain Adaptation via Clustering Uncertainty-Weighted Embeddings   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Prabhu_Active_Domain_Adaptation_via_Clustering_Uncertainty-Weighted_Embeddings_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/virajprabhu/CLUE)**  S3VAADA: Submodular Subset Selection for Virtual Adversarial Active Domain Adaptation   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Rangwani_S3VAADA_Submodular_Subset_Selection_for_Virtual_Adversarial_Active_Domain_Adaptation_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/val-iisc/s3vaada)**  LabOR: Labeling Only If Required for Domain Adaptive Semantic Segmentation   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Shin_LabOR_Labeling_Only_if_Required_for_Domain_Adaptive_Semantic_Segmentation_ICCV_2021_paper.pdf)**  ReDAL: Region-Based and Diversity-Aware Active Learning for Point Cloud Semantic Segmentation   **[ICCV'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_ReDAL_Region-Based_and_Diversity-Aware_Active_Learning_for_Point_Cloud_Semantic_ICCV_2021_paper.pdf)** **[[Code]](https://github.com/tsunghan-wu/ReDAL)**  Active Learning for Deep Detection Neural Networks   **[ICCV'19]** **[[PDF]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Aghdam_Active_Learning_for_Deep_Detection_Neural_Networks_ICCV_2019_paper.pdf)** **[[Code]](https://gitlab.com/haghdam/deep_active_learning)**  Deep Reinforcement Active Learning for Human-in-the-Loop Person Re-Identification   **[ICCV'19]** **[[PDF]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Deep_Reinforcement_Active_Learning_for_Human-in-the-Loop_Person_Re-Identification_ICCV_2019_paper.pdf)**  Variational Adversarial Active Learning   **[ICCV'19]** **[[PDF]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Sinha_Variational_Adversarial_Active_Learning_ICCV_2019_paper.pdf)** **[[Code]](https://github.com/sinhasam/vaal)**   ### ICCV Workshop Computational Evaluation of the Combination of Semi-Supervised and Active Learning for Histopathology Image Segmentation with Missing Annotations   **[ICCVW'23]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2023W/CVAMD/papers/Jimenez_Computational_Evaluation_of_the_Combination_of_Semi-Supervised_and_Active_Learning_ICCVW_2023_paper.pdf)**  Reducing Label Effort: Self-Supervised Meets Active Learning   **[ICCVW'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021W/ILDAV/papers/Bengar_Reducing_Label_Effort_Self-Supervised_Meets_Active_Learning_ICCVW_2021_paper.pdf)**  Joint semi-supervised and active learning for segmentation of gigapixel pathology images with cost-effective labeling   **[ICCVW'21]** **[[PDF]](https://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Lai_Joint_Semi-Supervised_and_Active_Learning_for_Segmentation_of_Gigapixel_Pathology_ICCVW_2021_paper.pdf)**   ### European Conference on Computer Vision (ECCV) Learn from the Learnt: Source-Free Active Domain Adaptation via Contrastive Sampling and Visual Persistence   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00073.pdf)**  Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02596.pdf)**  Bidirectional Uncertainty-Based Active Learning for Open-Set Annotation   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04026.pdf)**  Active Coarse-to-Fine Segmentation of Moveable Parts from Real Images   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04970.pdf)**  Efficient Active Domain Adaptation for Semantic Segmentation by Selecting Information-rich Superpixels   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05060.pdf)**  Two-Stage Active Learning for Efficient Temporal Action Segmentation   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06348.pdf)**  Active Generation for Image Classification   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06459.pdf)**  Dataset Quantization with Active Learning based Adaptive Sampling   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/07772.pdf)**  MetaAT: Active Testing for Label-Efficient Evaluation of Dense Recognition Tasks   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10062.pdf)**  Generalized Coverage for More Robust Low-Budget Active Learning   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11051.pdf)**  Exploring Active Learning in Meta-Learning: Enhancing Context Set Labeling   üïù  **[ECCV'24]** **[[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/12574.pdf)**  Optical Flow Training under Limited Label Budget via Active Learning   üïù  **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-20047-2_24)** **[[Code]](https://github.com/duke-vision/optical-flow-active-learning-release)**  ActiveNeRF: Learning where to See with Uncertainty Estimation   üïù  **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19827-4_14)** **[[Code]](https://github.com/LeapLabTHU/ActiveNeRF)**  Active Label Correction Using Robust Parameter Update and Entropy Propagation   üïù  **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19803-8_1)**  LiDAL: Inter-Frame Uncertainty Based Active Learning for 3D LiDAR Semantic Segmentation   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19812-0_15)** **[[Code]](https://github.com/hzykent/LiDAL)**  Combating Label Distribution Shift for Active Domain Adaptation   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19827-4_32)** **[[Code]](https://github.com/sehyun03/ADA-label-distribution-matching)**  Unsupervised Selective Labeling for More Effective Semi-Supervised Learning   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-20056-4_25)** **[[Code]](https://github.com/TonyLianLong/UnsupervisedSelectiveLabeling)**  D2ADA: Dynamic Density-Aware Active Domain Adaptation for Semantic Segmentation   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19818-2_26)** **[[Code]](https://github.com/tsunghan-wu/D2ADA)**  When Active Learning Meets Implicit Semantic Data Augmentation   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-20056-4_25)**  Talisman: Targeted Active Learning for Object Detection with Rare Classes and Slices Using Submodular Mutual Information   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19839-7_1)** **[[Code]](https://github.com/surajkothawade/talisman)**  PT4AL: Using Self-Supervised Pretext Tasks for Active Learning   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19809-0_34)** **[[Code]](https://github.com/johnsk95/PT4AL)**  Active learning strategies for weakly-supervised object detection   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-20056-4_13)** **[[Code]](https://github.com/huyvvo/BiB)**  Active Pointly-Supervised Instance Segmentation   **[ECCV'22]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-031-19815-1_35)** **[[Code]](https://github.com/chufengt/APIS)**  Contextual Diversity for Active Learning   **[ECCV'20]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-030-58517-4_9)** **[[Code]](https://github.com/sharat29ag/CDAL)**  Active Crowd Counting with Limited Supervision   üïù  **[ECCV'20]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-030-58565-5_34)**  Weight Decay Scheduling and Knowledge Distillation for Active Learning   üïù  **[ECCV'20]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-030-58574-7_26)**  Consistency-Based Semi-Supervised Active Learning: Towards Minimizing Labeling Cost   **[ECCV'20]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-030-58607-2_30)**  Two Stream Active Query Suggestion for Active Learning in Connectomics   **[ECCV'20]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-030-58523-5_7)**  Dual Adversarial Network for Deep Active Learning   **[ECCV'20]** **[[PDF]](https://link.springer.com/chapter/10.1007/978-3-030-58586-0_40)**  What do I Annotate Next?,github,SOFTWARE
"We collect attributed explanations for each question by eliciting prompts from [GPT-3.5](https://openai.com/blog/chatgpt), based on the given relevant passages.",GPT-3.5,SOFTWARE
[arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2207.10434) [[Paper]](https://openaccess.thecvf.com/content/ICCV2021/papers/Jin_DC-ShadowNet_Single-Image_Hard_and_Soft_Shadow_Removal_Using_Unsupervised_Domain-Classifier_ICCV_2021_paper.pdf)  [[Supplementary]](https://openaccess.thecvf.com/content/ICCV2021/supplemental/Jin_DC-ShadowNet_Single-Image_Hard_ICCV_2021_supplemental.pdf)  [[Poster]](poster_slides/DC-ShadowNet_poster.pdf) [[Slides]](poster_slides/DC-ShadowNet_slides.pdf)  [[Video]](https://www.bilibili.com/video/BV12e411H7bx/?,bilibili,SOFTWARE
p=dc-shadownet-single-image-hard-and-soft-1)  ## Prerequisites ``` git clone https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal.git cd DC-ShadowNet-Hard-and-Soft-Shadow-Removal/ conda create -n shadow python=3.7 conda activate shadow conda install pytorch=1.10.2 torchvision torchaudio cudatoolkit=11.3 -c pytorch python3 -m pip install -r requirements.txt ```  ## Datasets 1.,git,SOFTWARE
p=dc-shadownet-single-image-hard-and-soft-1)  ## Prerequisites ``` git clone https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal.git cd DC-ShadowNet-Hard-and-Soft-Shadow-Removal/ conda create -n shadow python=3.7 conda activate shadow conda install pytorch=1.10.2 torchvision torchaudio cudatoolkit=11.3 -c pytorch python3 -m pip install -r requirements.txt ```  ## Datasets 1.,conda,SOFTWARE
p=dc-shadownet-single-image-hard-and-soft-1)  ## Prerequisites ``` git clone https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal.git cd DC-ShadowNet-Hard-and-Soft-Shadow-Removal/ conda create -n shadow python=3.7 conda activate shadow conda install pytorch=1.10.2 torchvision torchaudio cudatoolkit=11.3 -c pytorch python3 -m pip install -r requirements.txt ```  ## Datasets 1.,conda,SOFTWARE
p=dc-shadownet-single-image-hard-and-soft-1)  ## Prerequisites ``` git clone https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal.git cd DC-ShadowNet-Hard-and-Soft-Shadow-Removal/ conda create -n shadow python=3.7 conda activate shadow conda install pytorch=1.10.2 torchvision torchaudio cudatoolkit=11.3 -c pytorch python3 -m pip install -r requirements.txt ```  ## Datasets 1.,conda,SOFTWARE
p=dc-shadownet-single-image-hard-and-soft-1)  ## Prerequisites ``` git clone https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal.git cd DC-ShadowNet-Hard-and-Soft-Shadow-Removal/ conda create -n shadow python=3.7 conda activate shadow conda install pytorch=1.10.2 torchvision torchaudio cudatoolkit=11.3 -c pytorch python3 -m pip install -r requirements.txt ```  ## Datasets 1.,pytorch=1.10.2,SOFTWARE
p=dc-shadownet-single-image-hard-and-soft-1)  ## Prerequisites ``` git clone https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal.git cd DC-ShadowNet-Hard-and-Soft-Shadow-Removal/ conda create -n shadow python=3.7 conda activate shadow conda install pytorch=1.10.2 torchvision torchaudio cudatoolkit=11.3 -c pytorch python3 -m pip install -r requirements.txt ```  ## Datasets 1.,torchvision,SOFTWARE
p=dc-shadownet-single-image-hard-and-soft-1)  ## Prerequisites ``` git clone https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal.git cd DC-ShadowNet-Hard-and-Soft-Shadow-Removal/ conda create -n shadow python=3.7 conda activate shadow conda install pytorch=1.10.2 torchvision torchaudio cudatoolkit=11.3 -c pytorch python3 -m pip install -r requirements.txt ```  ## Datasets 1.,torchaudio,SOFTWARE
p=dc-shadownet-single-image-hard-and-soft-1)  ## Prerequisites ``` git clone https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal.git cd DC-ShadowNet-Hard-and-Soft-Shadow-Removal/ conda create -n shadow python=3.7 conda activate shadow conda install pytorch=1.10.2 torchvision torchaudio cudatoolkit=11.3 -c pytorch python3 -m pip install -r requirements.txt ```  ## Datasets 1.,cudatoolkit=11.3,SOFTWARE
p=dc-shadownet-single-image-hard-and-soft-1)  ## Prerequisites ``` git clone https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal.git cd DC-ShadowNet-Hard-and-Soft-Shadow-Removal/ conda create -n shadow python=3.7 conda activate shadow conda install pytorch=1.10.2 torchvision torchaudio cudatoolkit=11.3 -c pytorch python3 -m pip install -r requirements.txt ```  ## Datasets 1.,pytorch,SOFTWARE
p=dc-shadownet-single-image-hard-and-soft-1)  ## Prerequisites ``` git clone https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal.git cd DC-ShadowNet-Hard-and-Soft-Shadow-Removal/ conda create -n shadow python=3.7 conda activate shadow conda install pytorch=1.10.2 torchvision torchaudio cudatoolkit=11.3 -c pytorch python3 -m pip install -r requirements.txt ```  ## Datasets 1.,python3,SOFTWARE
p=dc-shadownet-single-image-hard-and-soft-1)  ## Prerequisites ``` git clone https://github.com/jinyeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal.git cd DC-ShadowNet-Hard-and-Soft-Shadow-Removal/ conda create -n shadow python=3.7 conda activate shadow conda install pytorch=1.10.2 torchvision torchaudio cudatoolkit=11.3 -c pytorch python3 -m pip install -r requirements.txt ```  ## Datasets 1.,pip,SOFTWARE
"SRD [Train](https://drive.google.com/file/d/1W8vBRJYDG9imMgr9I2XaA13tlFIEHOjS/view)|[BaiduPan](https://pan.baidu.com/s/1mj3BoRQ), [Test](https://drive.google.com/file/d/1GTi4BmQ0SJ7diDMmf-b7x2VismmXtfTo/view).",BaiduPan,SOFTWARE
rlkey=eyfjn7dhd9pbz6rh247ylbt0c&st=01lh80r8&dl=0)|[[BaiduPan(code:t9c7)]](https://pan.baidu.com/s/1c_VsDVC92WnvI92v8cldsg?,BaiduPan,SOFTWARE
dl=0) | [[BaiduPan(code:gr59)]](https://pan.baidu.com/s/1EyYvjeu6AnJuY3wEuJS74A?,BaiduPan,SOFTWARE
pwd=gr59)  | Dataset  | Model Dropbox | Model BaiduPan | Model Put in Path| Results Dropbox | Results BaiduPan | | :----: | :-----------: | :----------: |:---------------: |  :----------: |:---------------: |  | SRD | [[Dropbox]](https://www.dropbox.com/scl/fi/icj273vu98w1l9zzwjxt7/SRD_params_0500000.pt?,BaiduPan,SOFTWARE
pwd=gr59)  | Dataset  | Model Dropbox | Model BaiduPan | Model Put in Path| Results Dropbox | Results BaiduPan | | :----: | :-----------: | :----------: |:---------------: |  :----------: |:---------------: |  | SRD | [[Dropbox]](https://www.dropbox.com/scl/fi/icj273vu98w1l9zzwjxt7/SRD_params_0500000.pt?,BaiduPan,SOFTWARE
rlkey=6jzx33gwat7t4fv30spw3c0za&dl=0) |[[BaiduPan(code:zhd2)]](https://pan.baidu.com/s/1CV1wQkSMR9OOw9ROAdY-pg?,BaiduPan,SOFTWARE
dl=0) | [[BaiduPan(code:28bv)]](https://pan.baidu.com/s/1T-KK7iWAyRzBgGg9WUAcAg?,BaiduPan,SOFTWARE
rlkey=e9iylpj6vcpxfcjcud72gw1id&dl=0) |[[BaiduPan(code:cfn9)]](https://pan.baidu.com/s/1wuAZ9ACx6w_2v2MbzrYY7Q?,BaiduPan,SOFTWARE
dl=0) | [[BaiduPan(code:3waf)]](https://pan.baidu.com/s/1BdBW2H3YAEJdPyuUYGiiuQ?,BaiduPan,SOFTWARE
rlkey=pdylqoxxx0krjza4a6uwzgd85&dl=0) |[[BaiduPan(code:b8o0)]](https://pan.baidu.com/s/1qtC0PtCqS5drYRi1-Ta2gw?,BaiduPan,SOFTWARE
dl=0) |[BaiduPan(code:hh4n)](https://pan.baidu.com/s/1mX-bjzSbzojWAhy54JvmTA?,BaiduPan,SOFTWARE
rlkey=f14177w0udgh07q669q8shj1a&dl=0) | [BaiduPan(code:e0a8)](https://pan.baidu.com/s/16MYozQ3QYT3bAhE-eTehXA?,BaiduPan,SOFTWARE
dl=0) | [[BaiduPan(code:u7ec)]](https://pan.baidu.com/s/1GINuAY3V39TodVdrH3mVlA?,BaiduPan,SOFTWARE
dl=0) | [[BaiduPan(code:bbns)]](https://pan.baidu.com/s/1yLxFKLH7QJr_f75ITUCRMQ?,BaiduPan,SOFTWARE
Download the pre-trained SRD model [[Dropbox]](https://www.dropbox.com/scl/fi/icj273vu98w1l9zzwjxt7/SRD_params_0500000.pt?,Dropbox,SOFTWARE
rlkey=6jzx33gwat7t4fv30spw3c0za&dl=0) | [[BaiduPan(code:zhd2)]](https://pan.baidu.com/s/1CV1wQkSMR9OOw9ROAdY-pg?,BaiduPan,SOFTWARE
"Put the test images in `test_input`, results in: `results/output/` <br> ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- test_input           ## Shadow |-- results     |-- output           ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test_single.py ``` <p align=""left"">     <img width=350"" src=""results/SRD/500000/inputA_outputB/IMG_6456.png"" > </p>  ## Dataset Test 1.",python,SOFTWARE
"For the SRD test dataset `/dataset/SRD/testA/`, results in: `results/SRD/500000(iteration)/outputB/` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- testA           ## Shadow     |-- AISTD       |-- testA           ## Shadow     |-- USR       |-- testA           ## Shadow |-- results     |-- SRD       |-- model           ## SRD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- AISTD       |-- model           ## AISTD_params_0500000.pt       |-- 500000/outputB/ ## Results     |-- ISTD       |-- model           ## ISTD_params_0600000.pt       |-- 600000/outputB/ ## Results     |-- USR       |-- model           ## USR_params_0600000.pt       |-- 600000/outputB/ ## Results ```  ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --use_original_name True --im_suf_A .jpg ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset AISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/AISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset ISTD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/ISTD --use_original_name True --im_suf_A .png ``` ``` CUDA_VISIBLE_DEVICES='0' python main_test.py --dataset USR --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/USR --use_original_name True --im_suf_A .jpg ```  <p align=""left"">   <img width=550"" src=""teaser/hard_shadow.PNG""> </p>  <p align=""left"">   <img width=550"" src=""teaser/soft_shadow.PNG""> </p>   ## Train ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- testA  ## Shadow        |-- testB  ## Shadow-free  ``` ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 ```  ## Train with Shadow-Free Chromaticity Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_ch_loss True ``` ``` ${DC-ShadowNet-Hard-and-Soft-Shadow-Removal} |-- dataset     |-- SRD       |-- trainA ## Shadow        |-- trainB ## Shadow-free        |-- trainC ## Shadow-Free Chromaticity Maps after Illumination Compensation       |-- testA  ## Shadow        |-- testB  ## Shadow-free       |-- testC  ## Shadow-Free Chromaticity Maps after Illumination Compensation ```  The `trainC` and `testC` folder are the results of `0_Shadow-Free_Chromaticity_matlab/physics_all.m`   | SRD trainC [[Dropbox]](https://www.dropbox.com/scl/fo/tjpau664y1hq6wgvgbfkb/AKWvDeEVIlUX1ZWCBBBgiZs?",python,SOFTWARE
rlkey=bsjbrz279oyzvngixqt0k795b&dl=0) [[BaiduPan(code:srdc)]](https://pan.baidu.com/s/16WBQQL4GkXllD-LkvYb6eA?,BaiduPan,SOFTWARE
rlkey=xop89aujiryt24x0ypd3xhh7j&st=ahx73wgi&dl=0) [[BaiduPan(code:srdc)]](https://pan.baidu.com/s/1T3huME5UQPfALzfEaAYGQw?,BaiduPan,SOFTWARE
rlkey=fyby4kexn1g009xhb5bkph32e&dl=0) [[BaiduPan(code:istd)]](https://pan.baidu.com/s/1aSPXH6DVpGGc_RfDP7mADQ?,BaiduPan,SOFTWARE
rlkey=30yg1v0orktptc92tvsettdrv&st=5xpq3fji&dl=0)[[BaiduPan(code:istd)]](https://pan.baidu.com/s/1TmrPgDHwEaP95cyEppfCjQ?,BaiduPan,SOFTWARE
rlkey=ls25f7eojdwmiasyebt9i42j2&dl=0) [[BaiduPan(code:usrc)]](https://pan.baidu.com/s/1mBZ0HrhOeFZCDW10Qtgmtw?,BaiduPan,SOFTWARE
rlkey=gx583mwymzb8setv7nhr6zgcv&st=33jh9c14&dl=0) [[BaiduPan(code:usrc)]](https://pan.baidu.com/s/1Rhmr3-ks4ANRUrJDPDW6eg?,BaiduPan,SOFTWARE
rlkey=0ghgo6m4fegm6gml0p611f28e&dl=0) [[BaiduPan(code:lrss)]](https://pan.baidu.com/s/1hjIheKDy3vGChoF0WlW-LQ?,BaiduPan,SOFTWARE
rlkey=kpqb5dxeneiroskj9wpfh6rob&st=v1mvl266&dl=0) [[BaiduPan(code:lrss)]](https://pan.baidu.com/s/1SRikZuabEgu43vqqlCIh9A?,BaiduPan,SOFTWARE
pwd=lrss) | |-----|-----|-----|-----|-----|-----|-----|-----|   Option 1 [MATLAB](./0_Shadow-Free_Chromaticity_matlab): [inputs](./0_Shadow-Free_Chromaticity_matlab/input/) and [results](./0_Shadow-Free_Chromaticity_matlab/sfchroma/) ``` 0_Shadow-Free_Chromaticity_matlab/physics_all.m ```  Option 2 [Python](.,MATLAB,SOFTWARE
pwd=lrss) | |-----|-----|-----|-----|-----|-----|-----|-----|   Option 1 [MATLAB](./0_Shadow-Free_Chromaticity_matlab): [inputs](./0_Shadow-Free_Chromaticity_matlab/input/) and [results](./0_Shadow-Free_Chromaticity_matlab/sfchroma/) ``` 0_Shadow-Free_Chromaticity_matlab/physics_all.m ```  Option 2 [Python](.,matlab,SOFTWARE
pwd=lrss) | |-----|-----|-----|-----|-----|-----|-----|-----|   Option 1 [MATLAB](./0_Shadow-Free_Chromaticity_matlab): [inputs](./0_Shadow-Free_Chromaticity_matlab/input/) and [results](./0_Shadow-Free_Chromaticity_matlab/sfchroma/) ``` 0_Shadow-Free_Chromaticity_matlab/physics_all.m ```  Option 2 [Python](.,matlab,SOFTWARE
pwd=lrss) | |-----|-----|-----|-----|-----|-----|-----|-----|   Option 1 [MATLAB](./0_Shadow-Free_Chromaticity_matlab): [inputs](./0_Shadow-Free_Chromaticity_matlab/input/) and [results](./0_Shadow-Free_Chromaticity_matlab/sfchroma/) ``` 0_Shadow-Free_Chromaticity_matlab/physics_all.m ```  Option 2 [Python](.,matlab,SOFTWARE
"/_Shadow-Free_Chromaticity_python): [inputs](./0_Shadow-Free_Chromaticity_python/input/) and [results](./0_Shadow-Free_Chromaticity_python/sfchroma/) ``` cd 0_Shadow-Free_Chromaticity_python python physics_all.py ```  <p align=""left"">   <img width=450"" src=""teaser/chromaticity.png""> </p>   ## Train with Shadow-Robust Feature Loss ``` CUDA_VISIBLE_DEVICES='0' python main_train.py --dataset SRD --datasetpath /home1/yeying/DC-ShadowNet-Hard-and-Soft-Shadow-Removal/dataset/SRD --iteration 1000000 --use_pecp_loss True ```  Get the following Figure 5 in the main paper, VGG feature visualization [results](.",python,SOFTWARE
"LRSS Soft Shadow Dataset Evaluation set the paths of the shadow removal result and the dataset in `evaluation/demo_lrss_release.m` and then run it. ``` demo_lrss_release.m ```  Get the following Table 3 in the main paper on the LRSS dataset (size: 256x256): | Method | Training | All |  |------------------|----------|----------| | **DC-ShadowNet** | Unpaired | **3.48** | | Input Image | N/A | 12.26 |   ## Acknowledgments Code is implemented based [U-GAT-IT](https://github.com/znxlwm/UGATIT-pytorch), we would like to thank them.",U-GAT-IT,SOFTWARE
"LRSS Soft Shadow Dataset Evaluation set the paths of the shadow removal result and the dataset in `evaluation/demo_lrss_release.m` and then run it. ``` demo_lrss_release.m ```  Get the following Table 3 in the main paper on the LRSS dataset (size: 256x256): | Method | Training | All |  |------------------|----------|----------| | **DC-ShadowNet** | Unpaired | **3.48** | | Input Image | N/A | 12.26 |   ## Acknowledgments Code is implemented based [U-GAT-IT](https://github.com/znxlwm/UGATIT-pytorch), we would like to thank them.",UGATIT,SOFTWARE
"LRSS Soft Shadow Dataset Evaluation set the paths of the shadow removal result and the dataset in `evaluation/demo_lrss_release.m` and then run it. ``` demo_lrss_release.m ```  Get the following Table 3 in the main paper on the LRSS dataset (size: 256x256): | Method | Training | All |  |------------------|----------|----------| | **DC-ShadowNet** | Unpaired | **3.48** | | Input Image | N/A | 12.26 |   ## Acknowledgments Code is implemented based [U-GAT-IT](https://github.com/znxlwm/UGATIT-pytorch), we would like to thank them.",pytorch,SOFTWARE
"<h1 align=""center"">PFHedge: Deep Hedging in PyTorch</h1>  [!",PFHedge,SOFTWARE
"<h1 align=""center"">PFHedge: Deep Hedging in PyTorch</h1>  [!",PyTorch,SOFTWARE
[pypi](https://img.shields.io/pypi/v/pfhedge.svg)](https://pypi.org/project/pfhedge) [!,pypi,SOFTWARE
[codecov](https://codecov.io/gh/pfnet-research/pfhedge/branch/main/graph/badge.svg?,codecov,SOFTWARE
[codecov](https://codecov.io/gh/pfnet-research/pfhedge/branch/main/graph/badge.svg?,pfhedge,SOFTWARE
token=GpXV1ldVCN)](https://codecov.io/gh/pfnet-research/pfhedge) [!,codecov,SOFTWARE
token=GpXV1ldVCN)](https://codecov.io/gh/pfnet-research/pfhedge) [!,pfhedge,SOFTWARE
[pytorch](https://img.shields.io/badge/torch-1.9.0--%20%7C%202.0.0---red)  **PFHedge** is a [PyTorch](https://pytorch.org/)-based framework for [Deep Hedging][deep-hedging-arxiv],pytorch,SOFTWARE
[pytorch](https://img.shields.io/badge/torch-1.9.0--%20%7C%202.0.0---red)  **PFHedge** is a [PyTorch](https://pytorch.org/)-based framework for [Deep Hedging][deep-hedging-arxiv],PFHedge,SOFTWARE
[pytorch](https://img.shields.io/badge/torch-1.9.0--%20%7C%202.0.0---red)  **PFHedge** is a [PyTorch](https://pytorch.org/)-based framework for [Deep Hedging][deep-hedging-arxiv],PyTorch,SOFTWARE
[pytorch](https://img.shields.io/badge/torch-1.9.0--%20%7C%202.0.0---red)  **PFHedge** is a [PyTorch](https://pytorch.org/)-based framework for [Deep Hedging][deep-hedging-arxiv],pytorch,SOFTWARE
* [PFHedge Documentation](https://pfnet-research.github.io/pfhedge/) * [Neural Network Architecture for Efficient Deep Hedging](https://tech.preferred.jp/en/blog/neural-network-architecture-for-efficient-deep-hedging/) ([Japanese version](https://tech.preferred.jp/ja/blog/deep-hedging/))  ## What is Deep Hedging?,PFHedge,SOFTWARE
PFHedge enables you to experience this revolutionary framework on your own.,PFHedge,SOFTWARE
"You can try, tweak, and delve into Deep Hedging algorithms using PyTorch.",PyTorch,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",PFHedge,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",PFHedge,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",PyTorch,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",pytorch,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",PFHedge,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",PyTorch,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",pytorch,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",pytorch,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",torch,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",pytorch,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",pytorch,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",torch,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",PFHedge,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",pytorch,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",torch,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",pfhedge,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",pfhedge,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",pip,SOFTWARE
"We hope PFHedge accelerates the research and development of Deep Hedging.  ## Features  ### Imperative Experiences  * PFHedge is designed to be intuitive and imperative to streamline your research on Deep Hedging. * You can quickly build a `Hedger` and then `fit` and `price` derivatives right away. * You can easily tweak your model, risk measure, derivative, optimizer, and other setups on the fly.  ### Seamless Integration with [PyTorch](https://pytorch.org/)  * PFHedge is built to be deeply integrated into [PyTorch](https://pytorch.org/). * Your Deep-Hedger can be built as a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and trained by any [`Optimizer`](https://pytorch.org/docs/stable/optim.html). * You can use GPUs to boost your hedging optimization (See below).  ### Effortless Extensions  * You can build new hedging models, derivatives, and features with little glue code. * You can build new hedging models by just subclassing [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). * You can quickly try out your own stochastic processes, derivatives, and input features.  ### Batteries Included  * PFHedge provides useful [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)s for Deep Hedging in [`pfhedge.nn`](https://pfnet-research.github.io/pfhedge/nn.html). * You can create [Black-Scholes' delta-hedging](https://en.wikipedia.org/wiki/Delta_neutral), [Whalley-Wilmott's strategy][whalley-wilmott], and so forth. * Common risk measures such as [an entropic risk measure](https://en.wikipedia.org/wiki/Entropic_risk_measure) and [an expected shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) are available.  ## Install  ```sh pip install pfhedge ```  ## How to Use  [!",pfhedge,SOFTWARE
"[Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)][example-readme-colab]  ### Prepare a Derivative to Hedge  Financial instruments are provided in [`pfhedge.instruments`](https://pfnet-research.github.io/pfhedge/instruments.html) and classified into two types:  * **`Primary` instruments**: A primary instrument is a basic financial instrument that is traded on a market, and therefore their prices are accessible as the market prices.",Colab,SOFTWARE
"[Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)][example-readme-colab]  ### Prepare a Derivative to Hedge  Financial instruments are provided in [`pfhedge.instruments`](https://pfnet-research.github.io/pfhedge/instruments.html) and classified into two types:  * **`Primary` instruments**: A primary instrument is a basic financial instrument that is traded on a market, and therefore their prices are accessible as the market prices.",colab,SOFTWARE
"[Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)][example-readme-colab]  ### Prepare a Derivative to Hedge  Financial instruments are provided in [`pfhedge.instruments`](https://pfnet-research.github.io/pfhedge/instruments.html) and classified into two types:  * **`Primary` instruments**: A primary instrument is a basic financial instrument that is traded on a market, and therefore their prices are accessible as the market prices.",pfhedge,SOFTWARE
"Examples include [`EuropeanOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.EuropeanOption.html), [`LookbackOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.LookbackOption.html), [`VarianceSwap`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.VarianceSwap.html), and so forth.",pfhedge,SOFTWARE
"Examples include [`EuropeanOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.EuropeanOption.html), [`LookbackOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.LookbackOption.html), [`VarianceSwap`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.VarianceSwap.html), and so forth.",pfhedge,SOFTWARE
"Examples include [`EuropeanOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.EuropeanOption.html), [`LookbackOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.LookbackOption.html), [`VarianceSwap`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.VarianceSwap.html), and so forth.",pfhedge,SOFTWARE
"We consider a [`BrownianStock`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.BrownianStock.html), which is a stock following the [geometric Brownian motion](https://en.wikipedia.org/wiki/Geometric_Brownian_motion), and a [`EuropeanOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.EuropeanOption.html) which is contingent on it.",pfhedge,SOFTWARE
"We consider a [`BrownianStock`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.BrownianStock.html), which is a stock following the [geometric Brownian motion](https://en.wikipedia.org/wiki/Geometric_Brownian_motion), and a [`EuropeanOption`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.instruments.EuropeanOption.html) which is contingent on it.",pfhedge,SOFTWARE
"```py from pfhedge.instruments import BrownianStock from pfhedge.instruments import EuropeanOption  stock = BrownianStock(cost=1e-4) derivative = EuropeanOption(stock)  derivative # EuropeanOption( #   strike=1., maturity=0.0800 #   (underlier): BrownianStock(sigma=0.2000, cost=1.0000e-04, dt=0.0040) # ) ```  ### Create Your Hedger  A [`Hedger`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.nn.Hedger.html) in Deep Hedging is basically characterized by three elements:  * **Inputs**: A hedger uses any market information as input features",pfhedge,SOFTWARE
"```py from pfhedge.instruments import BrownianStock from pfhedge.instruments import EuropeanOption  stock = BrownianStock(cost=1e-4) derivative = EuropeanOption(stock)  derivative # EuropeanOption( #   strike=1., maturity=0.0800 #   (underlier): BrownianStock(sigma=0.2000, cost=1.0000e-04, dt=0.0040) # ) ```  ### Create Your Hedger  A [`Hedger`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.nn.Hedger.html) in Deep Hedging is basically characterized by three elements:  * **Inputs**: A hedger uses any market information as input features",pfhedge,SOFTWARE
"```py from pfhedge.instruments import BrownianStock from pfhedge.instruments import EuropeanOption  stock = BrownianStock(cost=1e-4) derivative = EuropeanOption(stock)  derivative # EuropeanOption( #   strike=1., maturity=0.0800 #   (underlier): BrownianStock(sigma=0.2000, cost=1.0000e-04, dt=0.0040) # ) ```  ### Create Your Hedger  A [`Hedger`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.nn.Hedger.html) in Deep Hedging is basically characterized by three elements:  * **Inputs**: A hedger uses any market information as input features",pfhedge,SOFTWARE
- [`MultiLayerPerceptron`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.nn.MultiLayerPerceptron.html): [Multi-layer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron),pfhedge,SOFTWARE
- [`BlackScholes`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.nn.BlackScholes.html): [Black-Scholes](https://en.wikipedia.org/wiki/Delta_neutral)' delta-hedging strategy,pfhedge,SOFTWARE
- [`WhalleyWilmott`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.nn.WhalleyWilmott.html): [Whalley-Wilmott][whalley-wilmott]'s asymptotically optimal strategy for small costs,pfhedge,SOFTWARE
- Any PyTorch [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) which you build. * **Criterion**: A hedger wishes to minimize their [risk measure](https://en.wikipedia.org/wiki/Risk_measure),PyTorch,SOFTWARE
"- [`EntropicRiskMeasure`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.nn.EntropicRiskMeasure.html): [Entropic Risk Measure](https://en.wikipedia.org/wiki/Entropic_risk_measure), a risk measure derived from [exponential utility](https://en.wikipedia.org/wiki/Exponential_utility)",pfhedge,SOFTWARE
"- [`ExpectedShortFall`](https://pfnet-research.github.io/pfhedge/generated/pfhedge.nn.ExpectedShortfall.html): [Expected Shortfall](https://en.wikipedia.org/wiki/Expected_shortfall) or CVaR, a common measure to assess portfolio risk.",pfhedge,SOFTWARE
"```py from pfhedge.nn import Hedger from pfhedge.nn import MultiLayerPerceptron  model = MultiLayerPerceptron() hedger = Hedger(model, inputs=[""log_moneyness"", ""expiry_time"", ""volatility"", ""prev_hedge""]) ```  The `hedger` is also a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).",pfhedge,SOFTWARE
"```py from pfhedge.nn import Hedger from pfhedge.nn import MultiLayerPerceptron  model = MultiLayerPerceptron() hedger = Hedger(model, inputs=[""log_moneyness"", ""expiry_time"", ""volatility"", ""prev_hedge""]) ```  The `hedger` is also a [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).",pfhedge,SOFTWARE
"```py import torch import torch.nn.functional as fn from torch import Tensor from torch.nn import Module  from pfhedge.nn import BlackScholes from pfhedge.nn import Clamp from pfhedge.nn import Hedger from pfhedge.nn import MultiLayerPerceptron   class NoTransactionBandNet(Module):     def __init__(self, derivative):         super().",torch,SOFTWARE
"```py import torch import torch.nn.functional as fn from torch import Tensor from torch.nn import Module  from pfhedge.nn import BlackScholes from pfhedge.nn import Clamp from pfhedge.nn import Hedger from pfhedge.nn import MultiLayerPerceptron   class NoTransactionBandNet(Module):     def __init__(self, derivative):         super().",torch,SOFTWARE
"```py import torch import torch.nn.functional as fn from torch import Tensor from torch.nn import Module  from pfhedge.nn import BlackScholes from pfhedge.nn import Clamp from pfhedge.nn import Hedger from pfhedge.nn import MultiLayerPerceptron   class NoTransactionBandNet(Module):     def __init__(self, derivative):         super().",torch,SOFTWARE
"```py import torch import torch.nn.functional as fn from torch import Tensor from torch.nn import Module  from pfhedge.nn import BlackScholes from pfhedge.nn import Clamp from pfhedge.nn import Hedger from pfhedge.nn import MultiLayerPerceptron   class NoTransactionBandNet(Module):     def __init__(self, derivative):         super().",Tensor,SOFTWARE
"```py import torch import torch.nn.functional as fn from torch import Tensor from torch.nn import Module  from pfhedge.nn import BlackScholes from pfhedge.nn import Clamp from pfhedge.nn import Hedger from pfhedge.nn import MultiLayerPerceptron   class NoTransactionBandNet(Module):     def __init__(self, derivative):         super().",BlackScholes,SOFTWARE
"```py import torch import torch.nn.functional as fn from torch import Tensor from torch.nn import Module  from pfhedge.nn import BlackScholes from pfhedge.nn import Clamp from pfhedge.nn import Hedger from pfhedge.nn import MultiLayerPerceptron   class NoTransactionBandNet(Module):     def __init__(self, derivative):         super().",Clamp,SOFTWARE
"__init__()          self.delta = BlackScholes(derivative)         self.mlp = MultiLayerPerceptron(out_features=2)         self.clamp = Clamp()      def inputs(self):         return self.delta.inputs() + [""prev_hedge""]      def forward(self, input: Tensor) -> Tensor:         prev_hedge = input[..., [-1]]          delta = self.delta(input[..., :-1])         width = self.mlp(input[..., :-1])          min = delta - fn.leaky_relu(width[..., [0]])         max = delta + fn.leaky_relu(width[..., [1]])          return self.clamp(prev_hedge, min=min, max=max)   model = NoTransactionBandNet(derivative) hedger = Hedger(model, inputs=model.inputs()) ```  ### Autogreek  A module [`pfhedge.autogreek`](https://pfnet-research.github.io/pfhedge/autogreek.html) provides functions implementing automatic evaluation of greeks using automatic differentiation.",pfhedge,SOFTWARE
"```py import pfhedge.autogreek as autogreek from pfhedge.instruments import BrownianStock from pfhedge.instruments import EuropeanOption from pfhedge.nn import Hedger from pfhedge.nn import WhalleyWilmott  derivative = EuropeanOption(BrownianStock(cost=1e-4))  model = WhalleyWilmott(derivative) hedger = Hedger(model, inputs=model.inputs())  def pricer(spot):     return hedger.price(derivative, init_state=(spot,), enable_grad=True)  delta = autogreek.delta(pricer, spot=torch.tensor(1.0)) # tensor(0.5092) gamma = autogreek.gamma(pricer, spot=torch.tensor(1.0)) # tensor(0.0885) ```  ## Contribution  Any contributions to PFHedge are more than welcome!",Hedger,SOFTWARE
"```py import pfhedge.autogreek as autogreek from pfhedge.instruments import BrownianStock from pfhedge.instruments import EuropeanOption from pfhedge.nn import Hedger from pfhedge.nn import WhalleyWilmott  derivative = EuropeanOption(BrownianStock(cost=1e-4))  model = WhalleyWilmott(derivative) hedger = Hedger(model, inputs=model.inputs())  def pricer(spot):     return hedger.price(derivative, init_state=(spot,), enable_grad=True)  delta = autogreek.delta(pricer, spot=torch.tensor(1.0)) # tensor(0.5092) gamma = autogreek.gamma(pricer, spot=torch.tensor(1.0)) # tensor(0.0885) ```  ## Contribution  Any contributions to PFHedge are more than welcome!",PFHedge,SOFTWARE
**You can explore ITO in our [ITO Explorer](https://openbiolink.github.io/ITOExplorer/) dashboard or browse the ITO class hierarchy [online at BioPortal](https://bioportal.bioontology.org/ontologies/ITO/?,ITO Explorer,SOFTWARE
**You can explore ITO in our [ITO Explorer](https://openbiolink.github.io/ITOExplorer/) dashboard or browse the ITO class hierarchy [online at BioPortal](https://bioportal.bioontology.org/ontologies/ITO/?,ITOExplorer,SOFTWARE
**You can explore ITO in our [ITO Explorer](https://openbiolink.github.io/ITOExplorer/) dashboard or browse the ITO class hierarchy [online at BioPortal](https://bioportal.bioontology.org/ontologies/ITO/?,BioPortal,SOFTWARE
**You can explore ITO in our [ITO Explorer](https://openbiolink.github.io/ITOExplorer/) dashboard or browse the ITO class hierarchy [online at BioPortal](https://bioportal.bioontology.org/ontologies/ITO/?,bioportal,SOFTWARE
You can use the [Protege ontology editor](https://protege.stanford.edu/) to explore and edit the resource.,Protege ontology editor,SOFTWARE
You can use the [Protege ontology editor](https://protege.stanford.edu/) to explore and edit the resource.,protege,SOFTWARE
We recommend using the [Blazegraph](https://blazegraph.com/) triple store.,Blazegraph,SOFTWARE
We recommend using the [Blazegraph](https://blazegraph.com/) triple store.,blazegraph,SOFTWARE
Example: [Google Colab notebook demonstrating SPARQL queries with ITO](https://colab.research.google.com/drive/1g3gDgakBcmAfIi4opXX99KXB7yALK66S?,Google Colab,SOFTWARE
Example: [Google Colab notebook demonstrating SPARQL queries with ITO](https://colab.research.google.com/drive/1g3gDgakBcmAfIi4opXX99KXB7yALK66S?,colab,SOFTWARE
"## Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set ##  <p align=""center"">  <img src=""/images/example.gif""> </p>  ### **_\*\*\*07/20/2021: A [PyTorch implementation](https://github.com/sicxu/Deep3DFaceRecon_pytorch) which has much better performance and is much easier to use is available now.",PyTorch,SOFTWARE
"## Accurate 3D Face Reconstruction with Weakly-Supervised Learning: From Single Image to Image Set ##  <p align=""center"">  <img src=""/images/example.gif""> </p>  ### **_\*\*\*07/20/2021: A [PyTorch implementation](https://github.com/sicxu/Deep3DFaceRecon_pytorch) which has much better performance and is much easier to use is available now.",pytorch,SOFTWARE
This repo will not be maintained in future. \*\*\*_**   This is a tensorflow implementation of the following paper:  Y.,tensorflow,SOFTWARE
"However, we suggest running on Linux because the rendering process is only supported on Linux. - Python 3.6 (numpy, scipy, pillow, argparse). - Tensorflow 1.12. - [Basel Face Model 2009 (BFM09)](https://faces.dmi.unibas.ch/bfm/main.php?",Python 3.6,SOFTWARE
"However, we suggest running on Linux because the rendering process is only supported on Linux. - Python 3.6 (numpy, scipy, pillow, argparse). - Tensorflow 1.12. - [Basel Face Model 2009 (BFM09)](https://faces.dmi.unibas.ch/bfm/main.php?",numpy,SOFTWARE
"However, we suggest running on Linux because the rendering process is only supported on Linux. - Python 3.6 (numpy, scipy, pillow, argparse). - Tensorflow 1.12. - [Basel Face Model 2009 (BFM09)](https://faces.dmi.unibas.ch/bfm/main.php?",scipy,SOFTWARE
"However, we suggest running on Linux because the rendering process is only supported on Linux. - Python 3.6 (numpy, scipy, pillow, argparse). - Tensorflow 1.12. - [Basel Face Model 2009 (BFM09)](https://faces.dmi.unibas.ch/bfm/main.php?",pillow,SOFTWARE
"However, we suggest running on Linux because the rendering process is only supported on Linux. - Python 3.6 (numpy, scipy, pillow, argparse). - Tensorflow 1.12. - [Basel Face Model 2009 (BFM09)](https://faces.dmi.unibas.ch/bfm/main.php?",argparse,SOFTWARE
"However, we suggest running on Linux because the rendering process is only supported on Linux. - Python 3.6 (numpy, scipy, pillow, argparse). - Tensorflow 1.12. - [Basel Face Model 2009 (BFM09)](https://faces.dmi.unibas.ch/bfm/main.php?",Tensorflow 1.12,SOFTWARE
- [tf mesh renderer](https://github.com/google/tf_mesh_renderer/tree/ba27ea1798f6ee8d03ddbc52f42ab4241f9328bb).,tf mesh renderer,SOFTWARE
Clone the repository ``` git clone https://github.com/Microsoft/Deep3DFaceReconstruction --recursive cd Deep3DFaceReconstruction ```  #### 2.,git,SOFTWARE
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",anaconda,SOFTWARE
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",conda,SOFTWARE
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",conda,SOFTWARE
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",tensorflow-gpu==1.12.0,SOFTWARE
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",pip,SOFTWARE
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",pillow,SOFTWARE
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",argparse,SOFTWARE
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",tensorflow,SOFTWARE
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",pip,SOFTWARE
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",cuda,SOFTWARE
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",cuda-9.0),SOFTWARE
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",pip,SOFTWARE
"Set up the python environment If you use anaconda, run the following: ``` conda create -n deep3d python=3.6 source activate deep3d conda install tensorflow-gpu==1.12.0 scipy pip install pillow argparse ```  Alternatively, you can install tensorflow via pip install (In this way, you need to link /usr/local/cuda to cuda-9.0): ``` pip install tensorflow-gpu==1.12.0 ```  #### 3.",tensorflow-gpu==1.12.0,SOFTWARE
"Compile tf_mesh_renderer  If you install tensorflow using pip,  we provide a [pre-compiled binary file (rasterize_triangles_kernel.so)](https://drive.google.com/file/d/1VUtJPdg0UiJkKWxkACs8ZTf5L7Y4P9Wj/view?",tf_mesh_renderer,SOFTWARE
"Compile tf_mesh_renderer  If you install tensorflow using pip,  we provide a [pre-compiled binary file (rasterize_triangles_kernel.so)](https://drive.google.com/file/d/1VUtJPdg0UiJkKWxkACs8ZTf5L7Y4P9Wj/view?",tensorflow,SOFTWARE
"Compile tf_mesh_renderer  If you install tensorflow using pip,  we provide a [pre-compiled binary file (rasterize_triangles_kernel.so)](https://drive.google.com/file/d/1VUtJPdg0UiJkKWxkACs8ZTf5L7Y4P9Wj/view?",pip,SOFTWARE
"**Note that the pre-compiled file can only be run with tensorflow 1.12.**  If you install tensorflow using conda, you have to compile tf_mesh_renderer from sources.",tensorflow 1.12,SOFTWARE
"**Note that the pre-compiled file can only be run with tensorflow 1.12.**  If you install tensorflow using conda, you have to compile tf_mesh_renderer from sources.",tensorflow,SOFTWARE
"**Note that the pre-compiled file can only be run with tensorflow 1.12.**  If you install tensorflow using conda, you have to compile tf_mesh_renderer from sources.",conda,SOFTWARE
"**Note that the pre-compiled file can only be run with tensorflow 1.12.**  If you install tensorflow using conda, you have to compile tf_mesh_renderer from sources.",tf_mesh_renderer,SOFTWARE
Compile tf_mesh_renderer with Bazel.,tf_mesh_renderer,SOFTWARE
Compile tf_mesh_renderer with Bazel.,Bazel,SOFTWARE
"/mesh_renderer/kernels/BUILD before the compilation**: ``` cd tf_mesh_renderer git checkout ba27ea1798 git checkout master WORKSPACE bazel test ... cd .. ``` If the library is compiled correctly, there should be a file named ""rasterize_triangles_kernel.so"" in .",tf_mesh_renderer,SOFTWARE
"/mesh_renderer/kernels/BUILD before the compilation**: ``` cd tf_mesh_renderer git checkout ba27ea1798 git checkout master WORKSPACE bazel test ... cd .. ``` If the library is compiled correctly, there should be a file named ""rasterize_triangles_kernel.so"" in .",git,SOFTWARE
"/mesh_renderer/kernels/BUILD before the compilation**: ``` cd tf_mesh_renderer git checkout ba27ea1798 git checkout master WORKSPACE bazel test ... cd .. ``` If the library is compiled correctly, there should be a file named ""rasterize_triangles_kernel.so"" in .",git,SOFTWARE
"/mesh_renderer/kernels/BUILD before the compilation**: ``` cd tf_mesh_renderer git checkout ba27ea1798 git checkout master WORKSPACE bazel test ... cd .. ``` If the library is compiled correctly, there should be a file named ""rasterize_triangles_kernel.so"" in .",bazel,SOFTWARE
/tf_mesh_renderer/bazel-bin/mesh_renderer/kernels.,tf_mesh_renderer,SOFTWARE
Training process has been tested with this model to ensure similar results. - [Resnet50-v1](https://github.com/tensorflow/models/blob/master/research/slim/README.md) pre-trained on ImageNet from Tensorflow Slim.,tensorflow/,SOFTWARE
Training process has been tested with this model to ensure similar results. - [Resnet50-v1](https://github.com/tensorflow/models/blob/master/research/slim/README.md) pre-trained on ImageNet from Tensorflow Slim.,Tensorflow,SOFTWARE
"Download the [pre-trained weights](http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz) of Resnet_v1_50 provided by Tensorflow Slim, unzip it and put resnet_v1_50.ckpt in .",tensorflow,SOFTWARE
"Download the [pre-trained weights](http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz) of Resnet_v1_50 provided by Tensorflow Slim, unzip it and put resnet_v1_50.ckpt in .",Tensorflow,SOFTWARE
"Download the [pre-trained weights](http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz) of Resnet_v1_50 provided by Tensorflow Slim, unzip it and put resnet_v1_50.ckpt in .",unzip,SOFTWARE
We recommend using [dlib](http://dlib.net/) or [MTCNN](https://github.com/ipazc/mtcnn).,dlib,SOFTWARE
We recommend using [dlib](http://dlib.net/) or [MTCNN](https://github.com/ipazc/mtcnn).,dlib,SOFTWARE
We recommend using [dlib](http://dlib.net/) or [MTCNN](https://github.com/ipazc/mtcnn).,MTCNN,SOFTWARE
We recommend using [dlib](http://dlib.net/) or [MTCNN](https://github.com/ipazc/mtcnn).,mtcnn,SOFTWARE
Monitoring the training process via tensorboard: ``` tensorboard --logdir=result/<custom_model_name> --port=10001 ``` 3.,tensorboard,SOFTWARE
Monitoring the training process via tensorboard: ``` tensorboard --logdir=result/<custom_model_name> --port=10001 ``` 3.,tensorboard,SOFTWARE
The whole process is tensorflow-based which allows gradient back-propagation for other tasks. ### 2020.6 ### Upload a [pre-trained model](https://drive.google.com/file/d/1fPsvLKghlCK8rknb9GPiKwIq9HIqWWwV/view?,tensorflow,SOFTWARE
"To get 5 facial landmarks, you can choose any open source face detector that returns them, such as [dlib](http://dlib.net/) or [MTCNN](https://github.com/ipazc/mtcnn).",dlib,SOFTWARE
"To get 5 facial landmarks, you can choose any open source face detector that returns them, such as [dlib](http://dlib.net/) or [MTCNN](https://github.com/ipazc/mtcnn).",dlib,SOFTWARE
"To get 5 facial landmarks, you can choose any open source face detector that returns them, such as [dlib](http://dlib.net/) or [MTCNN](https://github.com/ipazc/mtcnn).",MTCNN,SOFTWARE
"To get 5 facial landmarks, you can choose any open source face detector that returns them, such as [dlib](http://dlib.net/) or [MTCNN](https://github.com/ipazc/mtcnn).",mtcnn,SOFTWARE
/images/intro.png)  This repository contains the **PyTorch implementation** of our RAL'2023 paper [Uncertainty-aware Panoptic Segmentation](https://arxiv.org/abs/2206.14554).,PyTorch,SOFTWARE
"The repository builds on [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS), [mmdetection](https://github.com/open-mmlab/mmdetection) and [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) codebases.",pytorch,SOFTWARE
"The repository builds on [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS), [mmdetection](https://github.com/open-mmlab/mmdetection) and [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) codebases.",pytorch,SOFTWARE
"If you find the code useful for your research, please consider citing our paper: ``` @article{sirohi2023uncertainty,   title={Uncertainty-aware panoptic segmentation},   author={Sirohi, Kshitij and Marvi, Sajad and B{\""u}scher, Daniel and Burgard, Wolfram},   journal={IEEE Robotics and Automation Letters},   volume={8},   number={5},   pages={2629--2636},   year={2023},   publisher={IEEE} } ```  ## System Requirements * Linux  * Python 3.7 * PyTorch 1.7 * CUDA 10.2 * GCC 7 or 8  **IMPORTANT NOTE**: These requirements are not necessarily mandatory.",Python 3.7,SOFTWARE
"If you find the code useful for your research, please consider citing our paper: ``` @article{sirohi2023uncertainty,   title={Uncertainty-aware panoptic segmentation},   author={Sirohi, Kshitij and Marvi, Sajad and B{\""u}scher, Daniel and Burgard, Wolfram},   journal={IEEE Robotics and Automation Letters},   volume={8},   number={5},   pages={2629--2636},   year={2023},   publisher={IEEE} } ```  ## System Requirements * Linux  * Python 3.7 * PyTorch 1.7 * CUDA 10.2 * GCC 7 or 8  **IMPORTANT NOTE**: These requirements are not necessarily mandatory.",PyTorch 1.7,SOFTWARE
"If you find the code useful for your research, please consider citing our paper: ``` @article{sirohi2023uncertainty,   title={Uncertainty-aware panoptic segmentation},   author={Sirohi, Kshitij and Marvi, Sajad and B{\""u}scher, Daniel and Burgard, Wolfram},   journal={IEEE Robotics and Automation Letters},   volume={8},   number={5},   pages={2629--2636},   year={2023},   publisher={IEEE} } ```  ## System Requirements * Linux  * Python 3.7 * PyTorch 1.7 * CUDA 10.2 * GCC 7 or 8  **IMPORTANT NOTE**: These requirements are not necessarily mandatory.",CUDA 10.2,SOFTWARE
"If you find the code useful for your research, please consider citing our paper: ``` @article{sirohi2023uncertainty,   title={Uncertainty-aware panoptic segmentation},   author={Sirohi, Kshitij and Marvi, Sajad and B{\""u}scher, Daniel and Burgard, Wolfram},   journal={IEEE Robotics and Automation Letters},   volume={8},   number={5},   pages={2629--2636},   year={2023},   publisher={IEEE} } ```  ## System Requirements * Linux  * Python 3.7 * PyTorch 1.7 * CUDA 10.2 * GCC 7 or 8  **IMPORTANT NOTE**: These requirements are not necessarily mandatory.",GCC 7,SOFTWARE
Install EvPSNet implementation ```bash cd .. python setup.py develop ``` ## Prepare datasets It is recommended to symlink the dataset root to `$EvPSNet/data`.,python,SOFTWARE
"If your folder structure is different, you may need to change the corresponding paths in config files.  ``` EvPSNet ‚îú‚îÄ‚îÄ mmdet ‚îú‚îÄ‚îÄ tools ‚îú‚îÄ‚îÄ configs ‚îî‚îÄ‚îÄ data     ‚îî‚îÄ‚îÄ cityscapes         ‚îú‚îÄ‚îÄ annotations         ‚îú‚îÄ‚îÄ train         ‚îú‚îÄ‚îÄ val         ‚îú‚îÄ‚îÄ stuffthingmaps         ‚îú‚îÄ‚îÄ cityscapes_panoptic_val.json         ‚îî‚îÄ‚îÄ cityscapes_panoptic_val ``` The cityscapes annotations have to be converted into the aforementioned format using `tools/convert_datasets/cityscapes.py`: ```shell python tools/convert_cityscapes.py ROOT_DIRECTORY_OF_CITYSCAPES .",python,SOFTWARE
/data/cityscapes/ cd .. git clone https://github.com/mcordts/cityscapesScripts.git cd cityscapesScripts/cityscapesscripts/preparation python createPanopticImgs.py --dataset-folder path_to_cityscapes_gtFine_folder --output-folder ../../..,python,SOFTWARE
Train with a single GPU: ``` python tools/train.py configs/EvPSNet_unc_singlegpu.py --work_dir work_dirs/checkpoints --validate  ``` Train with multiple GPUS: ``` .,python,SOFTWARE
/tools/dist_train.sh configs/EvPSNet_unc_mutigpu.py ${GPU_NUM} --work_dir work_dirs/checkpoints --validate  ``` * --resume_from ${CHECKPOINT_FILE}: Resume from a previous checkpoint file. ### Evaluation Procedure Test with a single GPU: ``` python tools/test.py configs/EvPSNet_unc_singlegpu.py ${CHECKPOINT_FILE} --eval panoptic ``` Test with multiple GPUS: ``` .,python,SOFTWARE
* This is an impmentation of EvPSNet in PyTorch.,PyTorch,SOFTWARE
"We especially thank the authors of: - [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS) - [mmdetection](https://github.com/open-mmlab/mmdetection) - [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) - [seamseg](https://github.com/mapillary/seamseg.git)  ## Contacts * [Kshitij Sirohi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Sajad Marvi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Daniel B√ºscher](http://www2.informatik.uni-freiburg.de/~buescher/)  ## License For academic usage, the code is released under the [GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html) license.",pytorch,SOFTWARE
"We especially thank the authors of: - [EfficientPS](https://github.com/DeepSceneSeg/EfficientPS) - [mmdetection](https://github.com/open-mmlab/mmdetection) - [gen-efficientnet-pytorch](https://github.com/rwightman/gen-efficientnet-pytorch) - [seamseg](https://github.com/mapillary/seamseg.git)  ## Contacts * [Kshitij Sirohi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Sajad Marvi](http://www2.informatik.uni-freiburg.de/~sirohik/) * [Daniel B√ºscher](http://www2.informatik.uni-freiburg.de/~buescher/)  ## License For academic usage, the code is released under the [GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html) license.",pytorch,SOFTWARE
)            | 100 languages |   ASSIN2        |   0.8680     |   0.8680    | | PTT5 (Carmo et al,PTT5,SOFTWARE
)             | EN & PT       |   ASSIN2        |   0.8850     |   0.8860    | | BERTimbau Large (Souza et al,BERTimbau Large,SOFTWARE
)  | EN & PT       |   ASSIN2        |   0.9000     |   0.9000    | | BERT-pt (ours)                  | EN & PT       |  MNLI + ASSIN2  |   0.9207     |   0.9207    |                       ## How to Translate   We made available the following data and the respectives notebooks with translation code: - [SQuAD](https://colab.research.google.com/drive/1CSNwfWJCwhFgYTtjxsDvUdN1DMuU_P4-?,colab,SOFTWARE
"We shall update this README accordingly.   #### Machine, Tools and Requirements  Machine specification:    * `Machine: Dell 5810`   * `RAM: 64GB`   * `Processor: Intel 16-Core Xeon (3.10GHz)`   * `GPU: NVIDIA P4000 (8GB VRAM)`   * `OS: Ubuntu Linux 16.04.6 LTS (64bit)`  Tools used in this work:    * `Editor: vi/vim`   * `Execution: Linux-Shell`   * `MATLAB R2018b (64bit)`   * `Prolog compiler: YAP 6.2.2`   * `ILP Engine: Aleph`   * `CUDA 10.1.105`   * `Python 3.7.6`  Python Libraries:     * `torch 1.4.0`   * `torch_geometric 1.2.1`   * `numpy 1.18.1`   #### Source Data  There are 73 problems obtained from NCI.",vi/vim,SOFTWARE
"We shall update this README accordingly.   #### Machine, Tools and Requirements  Machine specification:    * `Machine: Dell 5810`   * `RAM: 64GB`   * `Processor: Intel 16-Core Xeon (3.10GHz)`   * `GPU: NVIDIA P4000 (8GB VRAM)`   * `OS: Ubuntu Linux 16.04.6 LTS (64bit)`  Tools used in this work:    * `Editor: vi/vim`   * `Execution: Linux-Shell`   * `MATLAB R2018b (64bit)`   * `Prolog compiler: YAP 6.2.2`   * `ILP Engine: Aleph`   * `CUDA 10.1.105`   * `Python 3.7.6`  Python Libraries:     * `torch 1.4.0`   * `torch_geometric 1.2.1`   * `numpy 1.18.1`   #### Source Data  There are 73 problems obtained from NCI.",Linux-Shell,SOFTWARE
"We shall update this README accordingly.   #### Machine, Tools and Requirements  Machine specification:    * `Machine: Dell 5810`   * `RAM: 64GB`   * `Processor: Intel 16-Core Xeon (3.10GHz)`   * `GPU: NVIDIA P4000 (8GB VRAM)`   * `OS: Ubuntu Linux 16.04.6 LTS (64bit)`  Tools used in this work:    * `Editor: vi/vim`   * `Execution: Linux-Shell`   * `MATLAB R2018b (64bit)`   * `Prolog compiler: YAP 6.2.2`   * `ILP Engine: Aleph`   * `CUDA 10.1.105`   * `Python 3.7.6`  Python Libraries:     * `torch 1.4.0`   * `torch_geometric 1.2.1`   * `numpy 1.18.1`   #### Source Data  There are 73 problems obtained from NCI.",MATLAB R2018b,SOFTWARE
"We shall update this README accordingly.   #### Machine, Tools and Requirements  Machine specification:    * `Machine: Dell 5810`   * `RAM: 64GB`   * `Processor: Intel 16-Core Xeon (3.10GHz)`   * `GPU: NVIDIA P4000 (8GB VRAM)`   * `OS: Ubuntu Linux 16.04.6 LTS (64bit)`  Tools used in this work:    * `Editor: vi/vim`   * `Execution: Linux-Shell`   * `MATLAB R2018b (64bit)`   * `Prolog compiler: YAP 6.2.2`   * `ILP Engine: Aleph`   * `CUDA 10.1.105`   * `Python 3.7.6`  Python Libraries:     * `torch 1.4.0`   * `torch_geometric 1.2.1`   * `numpy 1.18.1`   #### Source Data  There are 73 problems obtained from NCI.",YAP 6.2.2,SOFTWARE
"We shall update this README accordingly.   #### Machine, Tools and Requirements  Machine specification:    * `Machine: Dell 5810`   * `RAM: 64GB`   * `Processor: Intel 16-Core Xeon (3.10GHz)`   * `GPU: NVIDIA P4000 (8GB VRAM)`   * `OS: Ubuntu Linux 16.04.6 LTS (64bit)`  Tools used in this work:    * `Editor: vi/vim`   * `Execution: Linux-Shell`   * `MATLAB R2018b (64bit)`   * `Prolog compiler: YAP 6.2.2`   * `ILP Engine: Aleph`   * `CUDA 10.1.105`   * `Python 3.7.6`  Python Libraries:     * `torch 1.4.0`   * `torch_geometric 1.2.1`   * `numpy 1.18.1`   #### Source Data  There are 73 problems obtained from NCI.",Aleph,SOFTWARE
"We shall update this README accordingly.   #### Machine, Tools and Requirements  Machine specification:    * `Machine: Dell 5810`   * `RAM: 64GB`   * `Processor: Intel 16-Core Xeon (3.10GHz)`   * `GPU: NVIDIA P4000 (8GB VRAM)`   * `OS: Ubuntu Linux 16.04.6 LTS (64bit)`  Tools used in this work:    * `Editor: vi/vim`   * `Execution: Linux-Shell`   * `MATLAB R2018b (64bit)`   * `Prolog compiler: YAP 6.2.2`   * `ILP Engine: Aleph`   * `CUDA 10.1.105`   * `Python 3.7.6`  Python Libraries:     * `torch 1.4.0`   * `torch_geometric 1.2.1`   * `numpy 1.18.1`   #### Source Data  There are 73 problems obtained from NCI.",CUDA 10.1.105,SOFTWARE
"We shall update this README accordingly.   #### Machine, Tools and Requirements  Machine specification:    * `Machine: Dell 5810`   * `RAM: 64GB`   * `Processor: Intel 16-Core Xeon (3.10GHz)`   * `GPU: NVIDIA P4000 (8GB VRAM)`   * `OS: Ubuntu Linux 16.04.6 LTS (64bit)`  Tools used in this work:    * `Editor: vi/vim`   * `Execution: Linux-Shell`   * `MATLAB R2018b (64bit)`   * `Prolog compiler: YAP 6.2.2`   * `ILP Engine: Aleph`   * `CUDA 10.1.105`   * `Python 3.7.6`  Python Libraries:     * `torch 1.4.0`   * `torch_geometric 1.2.1`   * `numpy 1.18.1`   #### Source Data  There are 73 problems obtained from NCI.",torch 1.4.0,SOFTWARE
"We shall update this README accordingly.   #### Machine, Tools and Requirements  Machine specification:    * `Machine: Dell 5810`   * `RAM: 64GB`   * `Processor: Intel 16-Core Xeon (3.10GHz)`   * `GPU: NVIDIA P4000 (8GB VRAM)`   * `OS: Ubuntu Linux 16.04.6 LTS (64bit)`  Tools used in this work:    * `Editor: vi/vim`   * `Execution: Linux-Shell`   * `MATLAB R2018b (64bit)`   * `Prolog compiler: YAP 6.2.2`   * `ILP Engine: Aleph`   * `CUDA 10.1.105`   * `Python 3.7.6`  Python Libraries:     * `torch 1.4.0`   * `torch_geometric 1.2.1`   * `numpy 1.18.1`   #### Source Data  There are 73 problems obtained from NCI.",torch_geometric 1.2.1,SOFTWARE
"We shall update this README accordingly.   #### Machine, Tools and Requirements  Machine specification:    * `Machine: Dell 5810`   * `RAM: 64GB`   * `Processor: Intel 16-Core Xeon (3.10GHz)`   * `GPU: NVIDIA P4000 (8GB VRAM)`   * `OS: Ubuntu Linux 16.04.6 LTS (64bit)`  Tools used in this work:    * `Editor: vi/vim`   * `Execution: Linux-Shell`   * `MATLAB R2018b (64bit)`   * `Prolog compiler: YAP 6.2.2`   * `ILP Engine: Aleph`   * `CUDA 10.1.105`   * `Python 3.7.6`  Python Libraries:     * `torch 1.4.0`   * `torch_geometric 1.2.1`   * `numpy 1.18.1`   #### Source Data  There are 73 problems obtained from NCI.",numpy 1.18.1,SOFTWARE
"Temporarity, this is hosted in the Google Drive: [DataForVEGNN](https://drive.google.com/file/d/1eJrq_kvD2UmWiWe6F3C3Ire0fbv6CNrm/view?",Google Drive:,SOFTWARE
"` bash run.bash `  The script ""resultsummary.bash"" summarises the results for each dataset.",bash,SOFTWARE
"For example:  ` bash resultsummary.bash Result_dir_name `  To store the results in a file:  ` bash resultsummary.bash Result_dir_name > Result_file_name.txt `  To store only dataset and accuracy, one can *gawk* out the columns and then redirect to some file.",bash,SOFTWARE
"For example:  ` bash resultsummary.bash Result_dir_name `  To store the results in a file:  ` bash resultsummary.bash Result_dir_name > Result_file_name.txt `  To store only dataset and accuracy, one can *gawk* out the columns and then redirect to some file.",bash,SOFTWARE
` bash createressum.bash --help `   **added 2:** Added a Python script to load a (trained) saved model and print the structure  ` python modelsummary.py `  **added 3:** Code to load the saved models for deployment.,bash,SOFTWARE
` bash createressum.bash --help `   **added 2:** Added a Python script to load a (trained) saved model and print the structure  ` python modelsummary.py `  **added 3:** Code to load the saved models for deployment.,python,SOFTWARE
` bash test.bash `  Some results from this are saved in the file `testedsavedmodels.out`.,bash,SOFTWARE
` bash createressum_[enter].bash --help `   #### Note on Reproducibility   The implementations have been carried out using [PyTorch](https://pytorch.org/docs/stable/index.html).,bash,SOFTWARE
` bash createressum_[enter].bash --help `   #### Note on Reproducibility   The implementations have been carried out using [PyTorch](https://pytorch.org/docs/stable/index.html).,PyTorch,SOFTWARE
` bash createressum_[enter].bash --help `   #### Note on Reproducibility   The implementations have been carried out using [PyTorch](https://pytorch.org/docs/stable/index.html).,pytorch,SOFTWARE
"Although we believe that this should be sufficient to reproduce all the experiments; we note the [official statement](https://pytorch.org/docs/stable/notes/randomness.html) from PyTorch on reproducibility: ""Completely reproducible results are not guaranteed across PyTorch releases, individual commits or different platforms.",pytorch,SOFTWARE
"Although we believe that this should be sufficient to reproduce all the experiments; we note the [official statement](https://pytorch.org/docs/stable/notes/randomness.html) from PyTorch on reproducibility: ""Completely reproducible results are not guaranteed across PyTorch releases, individual commits or different platforms.",PyTorch,SOFTWARE
"Although we believe that this should be sufficient to reproduce all the experiments; we note the [official statement](https://pytorch.org/docs/stable/notes/randomness.html) from PyTorch on reproducibility: ""Completely reproducible results are not guaranteed across PyTorch releases, individual commits or different platforms.",PyTorch,SOFTWARE
"# ImGAGN: Imbalanced Networks Embedding via Generative Adversarial Graph Networks  This is our Pytorch implementation for the [paper](https://arxiv.org/abs/2106.02817):  > Liang Qu, Huaisheng Zhu, Ruiqi Zheng, Yuhui Shi, and Hongzhi Yin. 2021.",Pytorch,SOFTWARE
"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes‚Äô attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541).",GraphGenerator,SOFTWARE
"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes‚Äô attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541).",PyTorch >= 0.4,SOFTWARE
"It introduces a novel generator for graph structure data, named **GraphGenerator**, which can simulate both the minority class nodes‚Äô attribute distribution and network topological structure distribution by generating a set of synthetic minority nodes such that the number of nodes in different classes can be balanced.   ## Requirements  + PyTorch >= 0.4  + Python >= 3.6  ## Usage  ### Dataset  The datasets can be downloaded from [Cora](https://relational.fit.cvut.cz/dataset/CORA), [Citeseer](https://linqs.soe.ucsc.edu/data), [Pubmed](https://linqs.soe.ucsc.edu/data) and [DBLP](https://www.aminer.cn/citation#b541).",Python >= 3.6,SOFTWARE
"- **Cora** folder contains four files:    - ""edge.cora"": each line represents a edge between two nodes with the follwing format:      ```     1397 1470     1397 362     ... ...     ```    - ""feature.cora"": each line represents the features (e.g., one-hot feature) of nodes with the following format:      ```     0 0 0 0 1 0 0 0,...,0     0 1 0 1 0 0 0 0,...,0     ...     ```    - ""train.cora"": each line represents the node ID in training set with the following format:      ```     0     2     ...     ```    - ""test.cora"": each line represents the node ID in test set with the following format:      ```     2159     2160     ...     ```  ### *Example Usage*  ``` cd ImGAGN python train.py ```  ## Citation  ``` @misc{qu2021imgagnimbalanced,       title={ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks},        author={Liang Qu and Huaisheng Zhu and Ruiqi Zheng and Yuhui Shi and Hongzhi Yin},       year={2021},       eprint={2106.02817},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```",ImGAGN,SOFTWARE
# Welcome to **Wildlife ML**!,Wildlife ML,SOFTWARE
## What can `wildlife-ml` do?,wildlife-ml,SOFTWARE
"First, we leverage the predictive power of the amazing [Megadetector](https://github.com/microsoft/CameraTraps) (**MD**) to pre-select images that contain wildlife.",Megadetector,SOFTWARE
"First, we leverage the predictive power of the amazing [Megadetector](https://github.com/microsoft/CameraTraps) (**MD**) to pre-select images that contain wildlife.",MD,SOFTWARE
"Additionally, MD provides bounding boxes of the detected wildlife, which allows the algorithm of our next stage to directly focus on the element in question.",MD,SOFTWARE
`.xx` symbolizes that it doesn't matter what format the image files have as long as the format is compatible with `PIL`.  ###  02: Prediction of Bounding Boxes  #### Running the Megadetector  This package includes an implementation of the [Megadetector](https://github.com/microsoft/CameraTraps) for detecting objects in camera traps.,PIL,SOFTWARE
`.xx` symbolizes that it doesn't matter what format the image files have as long as the format is compatible with `PIL`.  ###  02: Prediction of Bounding Boxes  #### Running the Megadetector  This package includes an implementation of the [Megadetector](https://github.com/microsoft/CameraTraps) for detecting objects in camera traps.,Megadetector,SOFTWARE
"It is accessible as a python object:  ```python from wildlifeml import MegaDetector  md = MegaDetector(batch_size=32, confidence_threshold=0.1) md.predict_directory(directory='<directory>', output_file='<output_file>') ```  `<directory>` should be a directory that contains images, where bounding boxes should be predicted.",wildlifeml,SOFTWARE
"It is accessible as a python object:  ```python from wildlifeml import MegaDetector  md = MegaDetector(batch_size=32, confidence_threshold=0.1) md.predict_directory(directory='<directory>', output_file='<output_file>') ```  `<directory>` should be a directory that contains images, where bounding boxes should be predicted.",MegaDetector,SOFTWARE
"We also offer a CLI option:  ```shell wildlifeml-get-bbox -d <directory> ```  Access help over the `--help` flag.  #### The MD index file  By default, the MD saves its results in `images_megadetector.json`.",wildlifeml,SOFTWARE
"We also offer a CLI option:  ```shell wildlifeml-get-bbox -d <directory> ```  Access help over the `--help` flag.  #### The MD index file  By default, the MD saves its results in `images_megadetector.json`.",MD,SOFTWARE
"We also offer a CLI option:  ```shell wildlifeml-get-bbox -d <directory> ```  Access help over the `--help` flag.  #### The MD index file  By default, the MD saves its results in `images_megadetector.json`.",MD,SOFTWARE
The keys in MD file are composed as `<img_path>_<idx_of_bbox>`.,MD,SOFTWARE
Each entry has a `category`. `1` implies a bounding box with wildlife. `-1` indicates that the megadetector has not found a bounding box.  #### Mapping bounding boxes to images  The above structure has one distinct weakness: We don't know how many bounding boxes an image have or what keys of the MD file point to each image.,megadetector,SOFTWARE
Each entry has a `category`. `1` implies a bounding box with wildlife. `-1` indicates that the megadetector has not found a bounding box.  #### Mapping bounding boxes to images  The above structure has one distinct weakness: We don't know how many bounding boxes an image have or what keys of the MD file point to each image.,MD,SOFTWARE
"""img_id1.xx_nnn""     ],     ....... } ```  The `BBoxMapper` requires a path to the MD file.",MD,SOFTWARE
```python from wildlifeml.data import BBoxMapper mapper = BBoxMapper(detector_file_path='<path_to_images_megadetector.json>') key_map = mapper.get_keymap() ```  ### 03: Creating a Wildlife Dataset  #### Initializing a Dataset  Our dataset builds on the Keras `Sequence` utility and thus supports multi-threaded loading during training.,wildlifeml,SOFTWARE
```python from wildlifeml.data import BBoxMapper mapper = BBoxMapper(detector_file_path='<path_to_images_megadetector.json>') key_map = mapper.get_keymap() ```  ### 03: Creating a Wildlife Dataset  #### Initializing a Dataset  Our dataset builds on the Keras `Sequence` utility and thus supports multi-threaded loading during training.,Keras,SOFTWARE
"This corresponds to a list that contains the identifiers provided by the MD file, e.g:  ```python train_keys = ['img_id1.xx_001', 'img_id1.xx_002', 'img_id2.xx_001'] ```  Using a key based initialization, one can easily derive cross validation splits, test sets or other variants based on one MD file.",MD,SOFTWARE
"This corresponds to a list that contains the identifiers provided by the MD file, e.g:  ```python train_keys = ['img_id1.xx_001', 'img_id1.xx_002', 'img_id2.xx_001'] ```  Using a key based initialization, one can easily derive cross validation splits, test sets or other variants based on one MD file.",MD,SOFTWARE
Other required parameters are:  - `image_dir`: The image directory - `detector_file_path`: Path to the MD file - `batch_size`: Batch size - `bbox_map`: Result obtained from `BBoxMapper`  For training the `label_file_path` is also required.,MD,SOFTWARE
"All in all, an instantiation of `WildlifeDataset` is done like:  ```python from wildlifeml.data import WildlifeDataset  wd = WildlifeDataset(     keys=train_keys,     image_dir='<path_to_image_dir>',     detector_file_path='<path_to_images_megadetector.json>',     batch_size=8,     bbox_map=key_map,     label_file_path='<path_to_labels.csv>', ) ```  `do_cropping` is by default set to `True` and leads to the image being directly cropped to their respective bounding boxes when being loaded in the data process.  ### 04: Training a classifier  `wildlife-ml` contains the `WildlifeTrainer` that is an interface for directly training a classifier on the MD processed wildlife data.",wildlifeml,SOFTWARE
"All in all, an instantiation of `WildlifeDataset` is done like:  ```python from wildlifeml.data import WildlifeDataset  wd = WildlifeDataset(     keys=train_keys,     image_dir='<path_to_image_dir>',     detector_file_path='<path_to_images_megadetector.json>',     batch_size=8,     bbox_map=key_map,     label_file_path='<path_to_labels.csv>', ) ```  `do_cropping` is by default set to `True` and leads to the image being directly cropped to their respective bounding boxes when being loaded in the data process.  ### 04: Training a classifier  `wildlife-ml` contains the `WildlifeTrainer` that is an interface for directly training a classifier on the MD processed wildlife data.",wildlife-ml,SOFTWARE
"All in all, an instantiation of `WildlifeDataset` is done like:  ```python from wildlifeml.data import WildlifeDataset  wd = WildlifeDataset(     keys=train_keys,     image_dir='<path_to_image_dir>',     detector_file_path='<path_to_images_megadetector.json>',     batch_size=8,     bbox_map=key_map,     label_file_path='<path_to_labels.csv>', ) ```  `do_cropping` is by default set to `True` and leads to the image being directly cropped to their respective bounding boxes when being loaded in the data process.  ### 04: Training a classifier  `wildlife-ml` contains the `WildlifeTrainer` that is an interface for directly training a classifier on the MD processed wildlife data.",MD,SOFTWARE
The trainer first loads a pretrained `model_backbone` from the Keras hub and then conducts training in two stages.,Keras,SOFTWARE
The trainer has following mandatory parameters:  - `batch_size`: Batch size - `loss_func`: TF or Keras loss function - `num_classes`: The number of classes in the dataset - `transfer_epochs`: How many epochs to train in the `transfer`-phase. - `finetune_epochs`: How many epochs to train in the `finetune`-phase. - `transfer_optimizer`: Keras optimizer to use in the `transfer`-phase. - `finetune_optimizer`: Keras optimizer to use in the `finetune`-phase. - `finetune_layers`: Number of layers to unfreeze for the `finetune`-phase.,Keras,SOFTWARE
The trainer has following mandatory parameters:  - `batch_size`: Batch size - `loss_func`: TF or Keras loss function - `num_classes`: The number of classes in the dataset - `transfer_epochs`: How many epochs to train in the `transfer`-phase. - `finetune_epochs`: How many epochs to train in the `finetune`-phase. - `transfer_optimizer`: Keras optimizer to use in the `transfer`-phase. - `finetune_optimizer`: Keras optimizer to use in the `finetune`-phase. - `finetune_layers`: Number of layers to unfreeze for the `finetune`-phase.,Keras,SOFTWARE
The trainer has following mandatory parameters:  - `batch_size`: Batch size - `loss_func`: TF or Keras loss function - `num_classes`: The number of classes in the dataset - `transfer_epochs`: How many epochs to train in the `transfer`-phase. - `finetune_epochs`: How many epochs to train in the `finetune`-phase. - `transfer_optimizer`: Keras optimizer to use in the `transfer`-phase. - `finetune_optimizer`: Keras optimizer to use in the `finetune`-phase. - `finetune_layers`: Number of layers to unfreeze for the `finetune`-phase.,Keras,SOFTWARE
"Thus, a minimal training initialization is for example:  ```python from tensorflow.keras.losses import SparseCategoricalCrossentropy from tensorflow.keras.optimizers import Adam  from wildlifeml.training.trainer import WildlifeTrainer  trainer = WildlifeTrainer(     batch_size=8,     loss_func=SparseCategoricalCrossentropy(),     num_classes=10,     transfer_epochs=10,     finetune_epochs=10,     transfer_optimizer=Adam(),     finetune_optimizer=Adam(),     finetune_layers=3 ) ```  The training itself is triggered over the `fit` function and providing a training and validation `WildlifeDataset`.",tensorflow,SOFTWARE
"Thus, a minimal training initialization is for example:  ```python from tensorflow.keras.losses import SparseCategoricalCrossentropy from tensorflow.keras.optimizers import Adam  from wildlifeml.training.trainer import WildlifeTrainer  trainer = WildlifeTrainer(     batch_size=8,     loss_func=SparseCategoricalCrossentropy(),     num_classes=10,     transfer_epochs=10,     finetune_epochs=10,     transfer_optimizer=Adam(),     finetune_optimizer=Adam(),     finetune_layers=3 ) ```  The training itself is triggered over the `fit` function and providing a training and validation `WildlifeDataset`.",tensorflow,SOFTWARE
"Thus, a minimal training initialization is for example:  ```python from tensorflow.keras.losses import SparseCategoricalCrossentropy from tensorflow.keras.optimizers import Adam  from wildlifeml.training.trainer import WildlifeTrainer  trainer = WildlifeTrainer(     batch_size=8,     loss_func=SparseCategoricalCrossentropy(),     num_classes=10,     transfer_epochs=10,     finetune_epochs=10,     transfer_optimizer=Adam(),     finetune_optimizer=Adam(),     finetune_layers=3 ) ```  The training itself is triggered over the `fit` function and providing a training and validation `WildlifeDataset`.",wildlifeml,SOFTWARE
"```python trainer.fit(train_dataset, val_dataset) ```  ### 05: Evaluating a model  Due to our cascaded MD approach.",MD,SOFTWARE
The evaluation of the model requires a bit more care than a usual Keras model.,Keras,SOFTWARE
"As the MD can provide multiple bounding boxes, the final decision is made by a confidence-weighted vote.",MD,SOFTWARE
"This is contained in the  `Evaluator`, which computes metrics by incorporating the decision of the MD and respecting the multiple-boxes-per-image dilemma.",MD,SOFTWARE
Required parameters for the evaluator are:  - `label_file_path`: Path pointing to the labels - `detector_file_path`: Path to the MD file - `dataset`: `WildlifeDataset` to evaluate - `num_classes`: Number of classes in the dataset.,MD,SOFTWARE
"If the `Evaluator` is supposed to discard boxes that do not meet the MD confidence threshold as empty, it is advisable to also set the `conf_threshold` parameter.",MD,SOFTWARE
"An example is:  ```python from wildlifeml.training.evaluator import Evaluator  evaluator = Evaluator(     detector_file_path='<path_to_images_megadetector.json>',     label_file_path='<path_to_labels.csv>',     dataset=training_dataset,     num_classes=10,     conf_threshold=0.1,     empty_class_id=99 ) ```  For a trained model, which is contained in a `WildlifeTrainer`, the accuracy, precision, recall and f1 score is computed as:  ```python evaluator.evaluate(trainer) metrics = evaluator.compute_metrics() ```  If you wish to extract the predictions and ground-truth labels for all individual observations, use `evaluator.get_details()`.  ### 06: Active Learning  Apart from fitting a model in a fully supervised way, we offer an active learning pipeline.",wildlifeml,SOFTWARE
Active learning in `wildlife-ml` is realized over the `ActiveLearner` object.,wildlife-ml,SOFTWARE
"Minimally the `ActiveLearner` needs:  - `trainer`: `WildlifeTrainer` as model container and handler. - `pool_dataset`: `WildlifeDataset` without labels - `label_file_path`: Path pointing to the file where labels should be saved - `label_file_path`: Path pointing to the file where labels should be saved - `conf_threshold`: Minimal confidence for bounding boxes in the MD file - `empty_class_id`: idx of the class that symbolizes an empty image  In case a test set exists, it is recommended to fill the `test_dataset` argument as well.",MD,SOFTWARE
"An example is:  ```python from wildlifeml import ActiveLearner  a_learner = ActiveLearner(     trainer=trainer,     pool_dataset=train_dataset,     label_file_path='<path_to_labels.csv>',     conf_threshold=0.1,     empty_class_id=10,     test_dataset=test_dataset ) ```  Execution of the active learning loop is triggered over:  ```python a_learner.run() ```  After `.run()` has finished, the directory `active-wildlife` will contain a directory `images` and a file `active_labels.csv`.",wildlifeml,SOFTWARE
"In particular, we have used the DTW algorithm provided by the [Librosa](https://github.com/librosa/librosa).",Librosa,SOFTWARE
"In particular, we have used the DTW algorithm provided by the [Librosa](https://github.com/librosa/librosa).",librosa,SOFTWARE
"More specifically, the symbolic notation has been converted to audio signal using the [Midi2audio library](https://github.com/bzamecnik/midi2audio).",Midi2audio,SOFTWARE
"More specifically, the symbolic notation has been converted to audio signal using the [Midi2audio library](https://github.com/bzamecnik/midi2audio).",midi2audio,SOFTWARE
"All these types can be extracted either considering rests or not.   ### Harmonic Features  For harmonic features, we implemented an algorithm to extract several harmonic features, including: * the pitch of the song * the dissonances present in the song * the chords that are formed in polyphonic compositions * the recurrent harmonic sequences, using a mirror approach to that proposed in the previous section.   ### Structural Features  For the structural features of the song, i.e. the structure and sections that make up the song, we reused the [MSAF library](https://pythonhosted.org/msaf/tutorial.html), using it directly on the audio signal.   ## HaMSE Ontology Starting from the lack of ontologies that deal with musicological research in an extensive way, we propose a novel ontological model.",MSAF,SOFTWARE
"All these types can be extracted either considering rests or not.   ### Harmonic Features  For harmonic features, we implemented an algorithm to extract several harmonic features, including: * the pitch of the song * the dissonances present in the song * the chords that are formed in polyphonic compositions * the recurrent harmonic sequences, using a mirror approach to that proposed in the previous section.   ### Structural Features  For the structural features of the song, i.e. the structure and sections that make up the song, we reused the [MSAF library](https://pythonhosted.org/msaf/tutorial.html), using it directly on the audio signal.   ## HaMSE Ontology Starting from the lack of ontologies that deal with musicological research in an extensive way, we propose a novel ontological model.",msaf,SOFTWARE
"<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/jury"" alt=""Python versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/jury"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/jury?",Jury,SOFTWARE
"<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/jury"" alt=""Python versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/jury"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/jury?",pypi,SOFTWARE
"<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/jury"" alt=""Python versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/jury"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/jury?",jury,SOFTWARE
"<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/jury"" alt=""Python versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/jury"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/jury?",pypi,SOFTWARE
"<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/jury"" alt=""Python versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/jury"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/jury?",jury,SOFTWARE
"<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/jury"" alt=""Python versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/jury"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/jury?",Python,SOFTWARE
"<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/jury"" alt=""Python versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/jury"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/jury?",jury,SOFTWARE
"<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/jury"" alt=""Python versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/jury"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/jury?",jury,SOFTWARE
"<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/jury"" alt=""Python versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/jury"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/jury?",pypi,SOFTWARE
"<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/jury"" alt=""Python versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/jury"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/jury?",jury,SOFTWARE
"<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/jury"" alt=""Python versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/jury"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/jury?",pypi,SOFTWARE
"<h1 align=""center"">Jury</h1>  <p align=""center""> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/pyversions/jury"" alt=""Python versions""></a> <a href=""https://pepy.tech/project/jury""><img src=""https://pepy.tech/badge/jury"" alt=""downloads""></a> <a href=""https://pypi.org/project/jury""><img src=""https://img.shields.io/pypi/v/jury?",jury,SOFTWARE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",PyPI,SOFTWARE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",jury,SOFTWARE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",jury,SOFTWARE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",jury,SOFTWARE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",jury,SOFTWARE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",jury,SOFTWARE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",jury,SOFTWARE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",pypi,SOFTWARE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",jury,SOFTWARE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",jury,SOFTWARE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",black,SOFTWARE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",jury,SOFTWARE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",pypi,SOFTWARE
"color=blue"" alt=""PyPI version""></a> <a href=""https://github.com/obss/jury/releases/latest""><img alt=""Latest Release"" src=""https://img.shields.io/github/release-date/obss/jury""></a> <a href=""https://colab.research.google.com/github/obss/jury/blob/main/examples/jury_evaluate.ipynb"" target=""_blank""><img alt=""Open in Colab"" src=""https://colab.research.google.com/assets/colab-badge.svg""></a> <br> <a href=""https://github.com/obss/jury/actions""><img alt=""Build status"" src=""https://github.com/obss/jury/actions/workflows/ci.yml/badge.svg""></a> <a href=""https://libraries.io/pypi/jury""><img alt=""Dependencies"" src=""https://img.shields.io/librariesio/github/obss/jury""></a> <a href=""https://github.com/psf/black""><img alt=""Code style: black"" src=""https://img.shields.io/badge/code%20style-black-000000.svg""></a> <a href=""https://github.com/obss/jury/blob/main/LICENSE""><img alt=""License: MIT"" src=""https://img.shields.io/pypi/l/jury""></a> <br> <a href=""https://doi.org/10.48550/arXiv.2310.02040""><img src=""https://img.shields.io/badge/DOI-10.48550%2FarXiv.2310.02040-blue"" alt=""DOI""></a> </p>  A comprehensive toolkit for evaluating NLP experiments offering various automated metrics.",jury,SOFTWARE
Jury offers a smooth and easy-to-use interface.,Jury,SOFTWARE
"It uses a more advanced version of [evaluate](https://github.com/huggingface/evaluate/) design for underlying metric computation, so that adding custom metric is easy as extending proper class.",evaluate,SOFTWARE
"It uses a more advanced version of [evaluate](https://github.com/huggingface/evaluate/) design for underlying metric computation, so that adding custom metric is easy as extending proper class.",evaluate,SOFTWARE
Main advantages that Jury offers are:  - Easy to use for any NLP project. - Unified structure for computation input across all metrics. - Calculate many metrics at once. - Metrics calculations can be handled concurrently to save processing time. - It seamlessly supports evaluation for multiple predictions/multiple references.,Jury,SOFTWARE
[The plagiarised paper](https://aclanthology.org/2022.coling-1.306.pdf) has been retracted. * (2023.10.03) Jury paper is out currently is on [arxiv](https://arxiv.org/abs/2310.02040).,Jury,SOFTWARE
"Please cite this paper if your work use Jury, and if your publication material will be submitted to the venues after this date",Jury,SOFTWARE
"usp=sharing) document that poses a claim about plagiarism of the work, *jury*, presented in this codebase.  ## Available Metrics  The table below shows the current support status for available metrics",jury,SOFTWARE
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,evaluate,SOFTWARE
| Metric                                                                        | Jury Support       | HF/evaluate Support | |-------------------------------------------------------------------------------|--------------------|---------------------| | Accuracy-Numeric                                                              | :heavy_check_mark: | :white_check_mark:  | | Accuracy-Text                                                                 | :heavy_check_mark: | :x:                 | | Bartscore                                                                     | :heavy_check_mark: | :x:                 | | Bertscore                                                                     | :heavy_check_mark: | :white_check_mark:  | | Bleu                                                                          | :heavy_check_mark: | :white_check_mark:  | | Bleurt                                                                        | :heavy_check_mark: | :white_check_mark:  | | CER                                                                           | :heavy_check_mark: | :white_check_mark:  | | CHRF                                                                          | :heavy_check_mark: | :white_check_mark:  | | COMET                                                                         | :heavy_check_mark: | :white_check_mark:  | | F1-Numeric                                                                    | :heavy_check_mark: | :white_check_mark:  | | F1-Text                                                                       | :heavy_check_mark: | :x:                 | | METEOR                                                                        | :heavy_check_mark: | :white_check_mark:  | | Precision-Numeric                                                             | :heavy_check_mark: | :white_check_mark:  | | Precision-Text                                                                | :heavy_check_mark: | :x:                 | | Prism                                                                         | :heavy_check_mark: | :x:                 | | Recall-Numeric                                                                | :heavy_check_mark: | :white_check_mark:  | | Recall-Text                                                                   | :heavy_check_mark: | :x:                 | | ROUGE                                                                         | :heavy_check_mark: | :white_check_mark:  | | SacreBleu                                                                     | :heavy_check_mark: | :white_check_mark:  | | Seqeval                                                                       | :heavy_check_mark: | :white_check_mark:  | | Squad                                                                         | :heavy_check_mark: | :white_check_mark:  | | TER                                                                           | :heavy_check_mark: | :white_check_mark:  | | WER                                                                           | :heavy_check_mark: | :white_check_mark:  | | [Other metrics](https://github.com/huggingface/evaluate/tree/master/metrics)* | :white_check_mark: | :white_check_mark:  |  _*_ Placeholder for the rest of the metrics available in `evaluate` package apart from those which are present in the  table.,evaluate,SOFTWARE
"**Notes**  * The entry :heavy_check_mark: represents that full Jury support is available meaning that all combinations of input  types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple  references) are supported  * The entry :white_check_mark: means that this metric is supported (for Jury through the `evaluate`), so that it  can (and should) be used just like the `evaluate` metric as instructed in `evaluate` implementation although  unfortunately full Jury support for those metrics are not yet available.  ## Request for a New Metric  For the request of a new metric please [open an issue](https://github.com/obss/jury/issues/new?",Jury,SOFTWARE
"**Notes**  * The entry :heavy_check_mark: represents that full Jury support is available meaning that all combinations of input  types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple  references) are supported  * The entry :white_check_mark: means that this metric is supported (for Jury through the `evaluate`), so that it  can (and should) be used just like the `evaluate` metric as instructed in `evaluate` implementation although  unfortunately full Jury support for those metrics are not yet available.  ## Request for a New Metric  For the request of a new metric please [open an issue](https://github.com/obss/jury/issues/new?",Jury,SOFTWARE
"**Notes**  * The entry :heavy_check_mark: represents that full Jury support is available meaning that all combinations of input  types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple  references) are supported  * The entry :white_check_mark: means that this metric is supported (for Jury through the `evaluate`), so that it  can (and should) be used just like the `evaluate` metric as instructed in `evaluate` implementation although  unfortunately full Jury support for those metrics are not yet available.  ## Request for a New Metric  For the request of a new metric please [open an issue](https://github.com/obss/jury/issues/new?",evaluate,SOFTWARE
"**Notes**  * The entry :heavy_check_mark: represents that full Jury support is available meaning that all combinations of input  types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple  references) are supported  * The entry :white_check_mark: means that this metric is supported (for Jury through the `evaluate`), so that it  can (and should) be used just like the `evaluate` metric as instructed in `evaluate` implementation although  unfortunately full Jury support for those metrics are not yet available.  ## Request for a New Metric  For the request of a new metric please [open an issue](https://github.com/obss/jury/issues/new?",evaluate,SOFTWARE
"**Notes**  * The entry :heavy_check_mark: represents that full Jury support is available meaning that all combinations of input  types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple  references) are supported  * The entry :white_check_mark: means that this metric is supported (for Jury through the `evaluate`), so that it  can (and should) be used just like the `evaluate` metric as instructed in `evaluate` implementation although  unfortunately full Jury support for those metrics are not yet available.  ## Request for a New Metric  For the request of a new metric please [open an issue](https://github.com/obss/jury/issues/new?",evaluate,SOFTWARE
"**Notes**  * The entry :heavy_check_mark: represents that full Jury support is available meaning that all combinations of input  types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple  references) are supported  * The entry :white_check_mark: means that this metric is supported (for Jury through the `evaluate`), so that it  can (and should) be used just like the `evaluate` metric as instructed in `evaluate` implementation although  unfortunately full Jury support for those metrics are not yet available.  ## Request for a New Metric  For the request of a new metric please [open an issue](https://github.com/obss/jury/issues/new?",Jur,SOFTWARE
"**Notes**  * The entry :heavy_check_mark: represents that full Jury support is available meaning that all combinations of input  types (single prediction & single reference, single prediction & multiple references, multiple predictions & multiple  references) are supported  * The entry :white_check_mark: means that this metric is supported (for Jury through the `evaluate`), so that it  can (and should) be used just like the `evaluate` metric as instructed in `evaluate` implementation although  unfortunately full Jury support for those metrics are not yet available.  ## Request for a New Metric  For the request of a new metric please [open an issue](https://github.com/obss/jury/issues/new?",jury,SOFTWARE
"Also, PRs addressing new metric  supports are welcomed :).  ## <div align=""center""> Installation </div>  Through pip,      pip install jury  or build from source,      git clone https://github.com/obss/jury.git     cd jury     python setup.py install  **NOTE:** There may be malfunctions of some metrics depending on `sacrebleu` package on Windows machines which is  mainly due to the package `pywin32`.",pip,SOFTWARE
"Also, PRs addressing new metric  supports are welcomed :).  ## <div align=""center""> Installation </div>  Through pip,      pip install jury  or build from source,      git clone https://github.com/obss/jury.git     cd jury     python setup.py install  **NOTE:** There may be malfunctions of some metrics depending on `sacrebleu` package on Windows machines which is  mainly due to the package `pywin32`.",pip,SOFTWARE
"Also, PRs addressing new metric  supports are welcomed :).  ## <div align=""center""> Installation </div>  Through pip,      pip install jury  or build from source,      git clone https://github.com/obss/jury.git     cd jury     python setup.py install  **NOTE:** There may be malfunctions of some metrics depending on `sacrebleu` package on Windows machines which is  mainly due to the package `pywin32`.",jury,SOFTWARE
"Also, PRs addressing new metric  supports are welcomed :).  ## <div align=""center""> Installation </div>  Through pip,      pip install jury  or build from source,      git clone https://github.com/obss/jury.git     cd jury     python setup.py install  **NOTE:** There may be malfunctions of some metrics depending on `sacrebleu` package on Windows machines which is  mainly due to the package `pywin32`.",jury,SOFTWARE
"Also, PRs addressing new metric  supports are welcomed :).  ## <div align=""center""> Installation </div>  Through pip,      pip install jury  or build from source,      git clone https://github.com/obss/jury.git     cd jury     python setup.py install  **NOTE:** There may be malfunctions of some metrics depending on `sacrebleu` package on Windows machines which is  mainly due to the package `pywin32`.",python,SOFTWARE
"Also, PRs addressing new metric  supports are welcomed :).  ## <div align=""center""> Installation </div>  Through pip,      pip install jury  or build from source,      git clone https://github.com/obss/jury.git     cd jury     python setup.py install  **NOTE:** There may be malfunctions of some metrics depending on `sacrebleu` package on Windows machines which is  mainly due to the package `pywin32`.",sacrebleu,SOFTWARE
"Also, PRs addressing new metric  supports are welcomed :).  ## <div align=""center""> Installation </div>  Through pip,      pip install jury  or build from source,      git clone https://github.com/obss/jury.git     cd jury     python setup.py install  **NOTE:** There may be malfunctions of some metrics depending on `sacrebleu` package on Windows machines which is  mainly due to the package `pywin32`.",pywin32,SOFTWARE
"For this, we fixed pywin32 version on our setup config for Windows platforms.",pywin32,SOFTWARE
"However, if pywin32 causes trouble in your environment we strongly recommend using `conda` manager install the package  as `conda install pywin32`.  ## <div align=""center""> Usage </div>  ### API Usage  It is only two lines of code to evaluate generated outputs.",pywin32,SOFTWARE
"However, if pywin32 causes trouble in your environment we strongly recommend using `conda` manager install the package  as `conda install pywin32`.  ## <div align=""center""> Usage </div>  ### API Usage  It is only two lines of code to evaluate generated outputs.",conda,SOFTWARE
"However, if pywin32 causes trouble in your environment we strongly recommend using `conda` manager install the package  as `conda install pywin32`.  ## <div align=""center""> Usage </div>  ### API Usage  It is only two lines of code to evaluate generated outputs.",pywin32,SOFTWARE
"```python from jury import Jury  scorer = Jury() predictions = [     [""the cat is on the mat"", ""There is cat playing on the mat""],      [""Look!",jury,SOFTWARE
"```python from jury import Jury  scorer = Jury() predictions = [     [""the cat is on the mat"", ""There is cat playing on the mat""],      [""Look!",Jury,SOFTWARE
"```python from jury import Jury  scorer = Jury() predictions = [     [""the cat is on the mat"", ""There is cat playing on the mat""],      [""Look!",Jury,SOFTWARE
"```python scorer = Jury(metrics=[""bleu"", ""meteor""]) scores = scorer(predictions, references) ```  #### Use of Metrics standalone  You can directly import metrics from `jury.metrics` as classes, and then instantiate and use as desired.",Jury,SOFTWARE
"```python scorer = Jury(metrics=[""bleu"", ""meteor""]) scores = scorer(predictions, references) ```  #### Use of Metrics standalone  You can directly import metrics from `jury.metrics` as classes, and then instantiate and use as desired.",jury,SOFTWARE
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",jury,SOFTWARE
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",jury,SOFTWARE
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",jury,SOFTWARE
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",jury,SOFTWARE
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",evaluate,SOFTWARE
"```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references) ```  The additional parameters can either be specified on `compute()`  ```python from jury.metrics import Bleu  bleu = Bleu.construct() score = bleu.compute(predictions=predictions, references=references, max_order=4) ```  , or alternatively on instantiation  ```python from jury.metrics import Bleu bleu = Bleu.construct(compute_kwargs={""max_order"": 1}) score = bleu.compute(predictions=predictions, references=references) ```  Note that you can seemlessly access both `jury` and `evaluate` metrics through `jury.load_metric`.",jury,SOFTWARE
"```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.",jury,SOFTWARE
"```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.",jury,SOFTWARE
"```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.",jury,SOFTWARE
"```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.",jury,SOFTWARE
"```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.",evaluate,SOFTWARE
"```python import jury  bleu = jury.load_metric(""bleu"") bleu_1 = jury.load_metric(""bleu"", resulting_name=""bleu_1"", compute_kwargs={""max_order"": 1}) # metrics not available in `jury` but in `evaluate` wer = jury.load_metric(""competition_math"") # It falls back to `evaluate` package with a warning ```  ### CLI Usage  You can specify predictions file and references file paths and get the resulting scores.",jury,SOFTWARE
jury eval --predictions /path/to/predictions.txt --references /path/to/references.txt --reduce_fn max --export /path/to/export.txt  You can also provide prediction folders and reference folders to evaluate multiple experiments.,jury,SOFTWARE
"jury eval --predictions /path/to/predictions_folder --references /path/to/references_folder --reduce_fn max --export /path/to/export.txt  If you want to specify metrics, and do not want to use default, specify it in config file (json) in `metrics` key.",jury,SOFTWARE
"```json {   ""predictions"": ""/path/to/predictions.txt"",   ""references"": ""/path/to/references.txt"",   ""reduce_fn"": ""max"",   ""metrics"": [     ""bleu"",     ""meteor""   ] } ```  Then, you can call jury eval with `config` argument.",jury,SOFTWARE
"jury eval --config path/to/config.json  ### Custom Metrics  You can use custom metrics with inheriting `jury.metrics.Metric`, you can see current metrics implemented on Jury from [jury/metrics](https://github.com/obss/jury/tree/master/jury/metrics).",jury,SOFTWARE
"jury eval --config path/to/config.json  ### Custom Metrics  You can use custom metrics with inheriting `jury.metrics.Metric`, you can see current metrics implemented on Jury from [jury/metrics](https://github.com/obss/jury/tree/master/jury/metrics).",jury,SOFTWARE
"jury eval --config path/to/config.json  ### Custom Metrics  You can use custom metrics with inheriting `jury.metrics.Metric`, you can see current metrics implemented on Jury from [jury/metrics](https://github.com/obss/jury/tree/master/jury/metrics).",Jury,SOFTWARE
"jury eval --config path/to/config.json  ### Custom Metrics  You can use custom metrics with inheriting `jury.metrics.Metric`, you can see current metrics implemented on Jury from [jury/metrics](https://github.com/obss/jury/tree/master/jury/metrics).",jury,SOFTWARE
"jury eval --config path/to/config.json  ### Custom Metrics  You can use custom metrics with inheriting `jury.metrics.Metric`, you can see current metrics implemented on Jury from [jury/metrics](https://github.com/obss/jury/tree/master/jury/metrics).",jury,SOFTWARE
"jury eval --config path/to/config.json  ### Custom Metrics  You can use custom metrics with inheriting `jury.metrics.Metric`, you can see current metrics implemented on Jury from [jury/metrics](https://github.com/obss/jury/tree/master/jury/metrics).",jury,SOFTWARE
"Jury falls back to `evaluate` implementation of metrics for the ones that are currently not supported by Jury, you can see the metrics available for `evaluate` on [evaluate/metrics](https://github.com/huggingface/evaluate/tree/master/metrics).",Jury,SOFTWARE
"Jury falls back to `evaluate` implementation of metrics for the ones that are currently not supported by Jury, you can see the metrics available for `evaluate` on [evaluate/metrics](https://github.com/huggingface/evaluate/tree/master/metrics).",Jury,SOFTWARE
"Jury falls back to `evaluate` implementation of metrics for the ones that are currently not supported by Jury, you can see the metrics available for `evaluate` on [evaluate/metrics](https://github.com/huggingface/evaluate/tree/master/metrics).",evaluate,SOFTWARE
"Jury falls back to `evaluate` implementation of metrics for the ones that are currently not supported by Jury, you can see the metrics available for `evaluate` on [evaluate/metrics](https://github.com/huggingface/evaluate/tree/master/metrics).",evaluate,SOFTWARE
Jury itself uses `evaluate.Metric` as a base class to drive its own base class as `jury.metrics.Metric`.,Jury,SOFTWARE
Jury itself uses `evaluate.Metric` as a base class to drive its own base class as `jury.metrics.Metric`.,jury,SOFTWARE
"The interface is similar; however, Jury makes the metrics to take a unified input type by handling the inputs for each metrics, and allows supporting several input types as;  - single prediction & single reference - single prediction & multiple reference - multiple prediction & multiple reference  As a custom metric both base classes can be used; however, we strongly recommend using `jury.metrics.Metric` as it has several advantages such as supporting computations for the input types above or unifying the type of the input.",Jury,SOFTWARE
"The interface is similar; however, Jury makes the metrics to take a unified input type by handling the inputs for each metrics, and allows supporting several input types as;  - single prediction & single reference - single prediction & multiple reference - multiple prediction & multiple reference  As a custom metric both base classes can be used; however, we strongly recommend using `jury.metrics.Metric` as it has several advantages such as supporting computations for the input types above or unifying the type of the input.",jury,SOFTWARE
"```python from jury.metrics import MetricForTask  class CustomMetric(MetricForTask):     def _compute_single_pred_single_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_single_pred_multi_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_multi_pred_multi_ref(             self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError ```  For more details, have a look at base metric implementation [jury.metrics.Metric](.",jury,SOFTWARE
"```python from jury.metrics import MetricForTask  class CustomMetric(MetricForTask):     def _compute_single_pred_single_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_single_pred_multi_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_multi_pred_multi_ref(             self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError ```  For more details, have a look at base metric implementation [jury.metrics.Metric](.",MetricForTask,SOFTWARE
"```python from jury.metrics import MetricForTask  class CustomMetric(MetricForTask):     def _compute_single_pred_single_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_single_pred_multi_ref(         self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError      def _compute_multi_pred_multi_ref(             self, predictions, references, reduce_fn = None, **kwargs     ):         raise NotImplementedError ```  For more details, have a look at base metric implementation [jury.metrics.Metric](.",jury,SOFTWARE
"/jury/metrics/_base.py)  ## <div align=""center""> Contributing </div>  PRs are welcomed as always :)  ### Installation      git clone https://github.com/obss/jury.git     cd jury     pip install -e "".",git,SOFTWARE
"/jury/metrics/_base.py)  ## <div align=""center""> Contributing </div>  PRs are welcomed as always :)  ### Installation      git clone https://github.com/obss/jury.git     cd jury     pip install -e "".",jury,SOFTWARE
"/jury/metrics/_base.py)  ## <div align=""center""> Contributing </div>  PRs are welcomed as always :)  ### Installation      git clone https://github.com/obss/jury.git     cd jury     pip install -e "".",jury,SOFTWARE
"/jury/metrics/_base.py)  ## <div align=""center""> Contributing </div>  PRs are welcomed as always :)  ### Installation      git clone https://github.com/obss/jury.git     cd jury     pip install -e "".",pip,SOFTWARE
"The file `requirements-dev.txt` includes packages  which are currently only available through a git source, or they are PYPI packages with no recent release or  incompatible with Jury, so that they are added as git sources or pointing to specific commits.",Jury,SOFTWARE
pip install -r requirements-dev.txt  ### Tests  To tests simply run.,pip,SOFTWARE
"python tests/run_tests.py  ### Code Style  To check code style,      python tests/run_code_style.py check  To format codebase,      python tests/run_code_style.py format   ## <div align=""center""> Citation </div>  If you use this package in your work, please cite it as:      @misc{cavusoglu2023jury,       title={Jury: A Comprehensive Evaluation Toolkit},        author={Devrim Cavusoglu and Ulas Sert and Secil Sen and Sinan Altinuc},       year={2023},       eprint={2310.02040},       archivePrefix={arXiv},       primaryClass={cs.CL},       doi={10.48550/arXiv.2310.02040}     }  ## Community Interaction  We use the GitHub Issue Tracker to track issues in general.",python,SOFTWARE
"python tests/run_tests.py  ### Code Style  To check code style,      python tests/run_code_style.py check  To format codebase,      python tests/run_code_style.py format   ## <div align=""center""> Citation </div>  If you use this package in your work, please cite it as:      @misc{cavusoglu2023jury,       title={Jury: A Comprehensive Evaluation Toolkit},        author={Devrim Cavusoglu and Ulas Sert and Secil Sen and Sinan Altinuc},       year={2023},       eprint={2310.02040},       archivePrefix={arXiv},       primaryClass={cs.CL},       doi={10.48550/arXiv.2310.02040}     }  ## Community Interaction  We use the GitHub Issue Tracker to track issues in general.",python,SOFTWARE
"python tests/run_tests.py  ### Code Style  To check code style,      python tests/run_code_style.py check  To format codebase,      python tests/run_code_style.py format   ## <div align=""center""> Citation </div>  If you use this package in your work, please cite it as:      @misc{cavusoglu2023jury,       title={Jury: A Comprehensive Evaluation Toolkit},        author={Devrim Cavusoglu and Ulas Sert and Secil Sen and Sinan Altinuc},       year={2023},       eprint={2310.02040},       archivePrefix={arXiv},       primaryClass={cs.CL},       doi={10.48550/arXiv.2310.02040}     }  ## Community Interaction  We use the GitHub Issue Tracker to track issues in general.",python,SOFTWARE
"python tests/run_tests.py  ### Code Style  To check code style,      python tests/run_code_style.py check  To format codebase,      python tests/run_code_style.py format   ## <div align=""center""> Citation </div>  If you use this package in your work, please cite it as:      @misc{cavusoglu2023jury,       title={Jury: A Comprehensive Evaluation Toolkit},        author={Devrim Cavusoglu and Ulas Sert and Secil Sen and Sinan Altinuc},       year={2023},       eprint={2310.02040},       archivePrefix={arXiv},       primaryClass={cs.CL},       doi={10.48550/arXiv.2310.02040}     }  ## Community Interaction  We use the GitHub Issue Tracker to track issues in general.",jury,SOFTWARE
"python tests/run_tests.py  ### Code Style  To check code style,      python tests/run_code_style.py check  To format codebase,      python tests/run_code_style.py format   ## <div align=""center""> Citation </div>  If you use this package in your work, please cite it as:      @misc{cavusoglu2023jury,       title={Jury: A Comprehensive Evaluation Toolkit},        author={Devrim Cavusoglu and Ulas Sert and Secil Sen and Sinan Altinuc},       year={2023},       eprint={2310.02040},       archivePrefix={arXiv},       primaryClass={cs.CL},       doi={10.48550/arXiv.2310.02040}     }  ## Community Interaction  We use the GitHub Issue Tracker to track issues in general.",Jury,SOFTWARE
assignees=&labels=&projects=&template=bug_report.md&title=) | | New Metric Request             | [Request Metric Implementation](https://github.com/obss/jury/issues/new?,jury,SOFTWARE
"assignees=&labels=&projects=&template=new-metric.md&title=) | | All other issues and questions | [General Issues](https://github.com/obss/jury/issues/new)                                                            |  ## <div align=""center""> License </div>  Licensed under the [MIT](LICENSE) License.",jury,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",conda,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",conda,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",jupyter,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",scikit-image,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",cython,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",opencv,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",seaborn,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",nltk,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",pycairo,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",h5py,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",python,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",nltk,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",NLTK,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",pytorch,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",pytorch,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",torchVision,SOFTWARE
"We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.  ## Requirements - Setup a conda environment and install some prerequisite packages like this ```bash conda create -n retrieval python=3.6    # Create a virtual environment source activate retrieval              # Activate virtual environment conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies python -m nltk.downloader all        # Install NLTK data ``` - Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext   ## Data  - Download the images of the Visual Genome dataset if you have not done so ```Shell .",torchtext,SOFTWARE
[alt text](ICLR-conference-material/NJODE_poster.png)    ## Requirements  This code was executed using Python 3.7.,Python 3.7,SOFTWARE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",Anaconda,SOFTWARE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",anaconda,SOFTWARE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",Anaconda,SOFTWARE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",conda,SOFTWARE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",conda,SOFTWARE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",anaconda,SOFTWARE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",conda,SOFTWARE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",theano,SOFTWARE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",pip,SOFTWARE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",tensorflow,SOFTWARE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",git,SOFTWARE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",python,SOFTWARE
"[badge](https://github.com/prasunroy/air-writing/blob/master/assets/badge_2.svg)  ## Installation #### Step 1: Install [Anaconda](https://www.anaconda.com/download/) distribution of python 2.7+ or 3.5+ (recommended) #### Step 2: Update Anaconda ``` conda update conda conda update anaconda ``` #### Step 3: Install dependencies ``` conda install theano pip install keras numpy opencv-python pyqt5 ``` >To switch backend from ""tensorflow"" (default) to ""theano"" read the [Keras Documentation](https://keras.io/backend/). #### Step 4: Clone repository and launch application ``` git clone https://github.com/prasunroy/air-writing.git cd air-writing python app.py ``` <p align='center'>   <img src='https://github.com/prasunroy/air-writing/raw/master/assets/image.png' /> </p>  ## References  >[Git Logo](https://github.com/prasunroy/air-writing/raw/master/assets/button_repo.png) is designed by [Jason Long](https://github.com/jasonlong) made available under [Creative Commons Attribution 3.0 Unported License](https://creativecommons.org/licenses/by/3.0/deed.en).  ## Citation ``` @inproceedings{roy2018cnn,   title     = {A CNN Based Framework for Unistroke Numeral Recognition in Air-Writing},   author    = {Roy, Prasun and Ghosh, Subhankar and Pal, Umapada},   booktitle = {International Conference on Frontiers in Handwriting Recognition (ICFHR)},   year      = {2018} } ```  ## License MIT License  Copyright (c) 2018 Prasun Roy  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.",Git,SOFTWARE
"Then create a conda enviroment by running: ``` conda env create -f environment.yml -n YourEnvironmentName ``` Activate the new enviornment: ``` source activate YourEnvironmentName ``` To validate correct installation `cd` to `src` and run  ``` python run_simulation.py ``` If you encounter any issues, please let us know so that we can help in getting the simulation up and running.   ### Configuring Your Simulation This library is intended to be modified as needed for your use case.",conda,SOFTWARE
"Then create a conda enviroment by running: ``` conda env create -f environment.yml -n YourEnvironmentName ``` Activate the new enviornment: ``` source activate YourEnvironmentName ``` To validate correct installation `cd` to `src` and run  ``` python run_simulation.py ``` If you encounter any issues, please let us know so that we can help in getting the simulation up and running.   ### Configuring Your Simulation This library is intended to be modified as needed for your use case.",conda,SOFTWARE
"Then create a conda enviroment by running: ``` conda env create -f environment.yml -n YourEnvironmentName ``` Activate the new enviornment: ``` source activate YourEnvironmentName ``` To validate correct installation `cd` to `src` and run  ``` python run_simulation.py ``` If you encounter any issues, please let us know so that we can help in getting the simulation up and running.   ### Configuring Your Simulation This library is intended to be modified as needed for your use case.",python,SOFTWARE
"####################################################################  Source code and datasets of Para-DPMM model for single cell transcriptomic clustering to reproduce results in paper ""[Parallel Clustering of Single Cell Transcriptomic Data with Split-Merge Sampling on Dirichlet Process Mixtures](https://arxiv.org/pdf/1812.10048.pdf)"", Author: Tiehang Duan; Jos√© P.",Para-DPMM,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",Matlab2015a,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",gcc/4.8.4,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",gcc/4.6.X,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",gcc/4.7.X,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",GNU Scientific Library,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",gsl,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",Eigen,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",eigen,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",Eigen,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",eigen,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",Matlab,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",Matlab,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",gcc/6.1.0,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",gsl/2.3,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",MATLAB/r2017b,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",gcc/6.1.0,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",gsl/2.3,SOFTWARE
"Usage (two options):  ## Option 1: Run Source Code   Enter the ""Para_DPMM_Source_Code"" Directory  We tested the code with Matlab2015a and gcc/4.8.4 (gcc/4.6.X and gcc/4.7.X should also work), the installation of following two libraries is needed:  1) GNU Scientific Library: http://www.gnu.org/software/gsl/ 2) Eigen library: http://eigen.tuxfamily.org/   Steps:  (1) Install the packages mentioned above (for the Eigen library, you only need to place the unzipped files inside the ""eigen"" directory);  (2) Start Matlab, enter the ""main"" sub directory in Matlab;  (3) In Matlab, run compile_MEX.m;  (4) Run Para_DPMM.m, follow the guidelines given in the program (an example):       1> Please enter dataset path:  data_matrix_1_S_Set.mat            2> Please enter number of processors:  16            3> Please set the value of alpha:  1            4> Please enter computing time limit (seconds):  20  (5) Result: The training iteration and computation time log is saved in Para_DPMM_output.txt, the clustering result is saved in Para_DPMM_result.mat, where z is the clustering result of Para_DPMM model, label is the ground truth cluster label, AR is Adjusted Random Index, RI is Random Index benchmark, MI is ""Mirkin's"" index and HI is ""Hubert's"" index.     ## Option 2: Run Compiled Package  The execution of compiled package is tested on HPC clusters with module gcc/6.1.0, gsl/2.3 and MATLAB/r2017b loaded;   Steps:  ### (1) On HPC command line:        1> module load gcc/6.1.0        2> module load gsl/2.3        3> module load MATLAB/r2017b        4> .",MATLAB/r2017b,SOFTWARE
Note: The Para-DPMM project depend heavily on the open source Dirichlet Process Mixtures package(http://people.csail.mit.edu/jchang7/code.php) written by Jason Chang.  ###############################################################################,Dirichlet Process Mixtures,SOFTWARE
"# ModelSketchBook ‚Äî Getting Started  [Paper](https://hci.stanford.edu/publications/2023/Lam_ModelSketching_CHI23.pdf) | [DOI](https://doi.org/10.1145/3544548.3581290) |  [Video](https://youtu.be/-zaeXENVTfk) | [Sample NB](https://github.com/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) |  <a target=""_blank"" href=""https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb"">   <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/> </a>  <p align=""center""> <img src="".",ModelSketchBook,SOFTWARE
"# ModelSketchBook ‚Äî Getting Started  [Paper](https://hci.stanford.edu/publications/2023/Lam_ModelSketching_CHI23.pdf) | [DOI](https://doi.org/10.1145/3544548.3581290) |  [Video](https://youtu.be/-zaeXENVTfk) | [Sample NB](https://github.com/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) |  <a target=""_blank"" href=""https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb"">   <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/> </a>  <p align=""center""> <img src="".",ModelSketchBook,SOFTWARE
"# ModelSketchBook ‚Äî Getting Started  [Paper](https://hci.stanford.edu/publications/2023/Lam_ModelSketching_CHI23.pdf) | [DOI](https://doi.org/10.1145/3544548.3581290) |  [Video](https://youtu.be/-zaeXENVTfk) | [Sample NB](https://github.com/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) |  <a target=""_blank"" href=""https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb"">   <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/> </a>  <p align=""center""> <img src="".",ModelSketchBook,SOFTWARE
"/docs/media/ModelSketchBook.png"" alt=""ModelSketchBook logo"" width=""75%""> </p>  **ModelSketchBook** is a Python package introduced as part of an ACM CHI 2023 paper:  **Model Sketching: Centering Concepts in Early-Stage Machine Learning Model Design**.",ModelSketchBook,SOFTWARE
"/docs/media/ModelSketchBook.png"" alt=""ModelSketchBook logo"" width=""75%""> </p>  **ModelSketchBook** is a Python package introduced as part of an ACM CHI 2023 paper:  **Model Sketching: Centering Concepts in Early-Stage Machine Learning Model Design**.",ModelSketchBook,SOFTWARE
"ModelSketchBook instantiates a vision of **model sketching**, a technical framework for rapidly iterating over a machine learning model's decision-making logic.",ModelSketchBook,SOFTWARE
"/docs/media/model_sketching_header.png"" alt=""Summary of the model sketching process from concepts to sketch models to evaluating and iterating"" width=""100%"">  With **ModelSketchBook**, you can **create concepts** using zero-shot methods,  <p align=""center""> <img src="".",ModelSketchBook,SOFTWARE
"It has not yet been tested in a wide variety of development setups, but see our [sample Colab notebook](https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) for a working example.  ## Installation First, you'll need to install this [package](https://pypi.org/project/model-sketch-book/) using PyPI. ``` pip install model_sketch_book ```  ## 1: Basic setup ### Imports Then import the package into a Python notebook (i.e., a Jupyter or Colab notebook). ``` import model_sketch_book as msb ```  ### Set up the sketchbook **Data**.",Colab,SOFTWARE
"It has not yet been tested in a wide variety of development setups, but see our [sample Colab notebook](https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) for a working example.  ## Installation First, you'll need to install this [package](https://pypi.org/project/model-sketch-book/) using PyPI. ``` pip install model_sketch_book ```  ## 1: Basic setup ### Imports Then import the package into a Python notebook (i.e., a Jupyter or Colab notebook). ``` import model_sketch_book as msb ```  ### Set up the sketchbook **Data**.",ModelSketchBook,SOFTWARE
"It has not yet been tested in a wide variety of development setups, but see our [sample Colab notebook](https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) for a working example.  ## Installation First, you'll need to install this [package](https://pypi.org/project/model-sketch-book/) using PyPI. ``` pip install model_sketch_book ```  ## 1: Basic setup ### Imports Then import the package into a Python notebook (i.e., a Jupyter or Colab notebook). ``` import model_sketch_book as msb ```  ### Set up the sketchbook **Data**.",pypi,SOFTWARE
"It has not yet been tested in a wide variety of development setups, but see our [sample Colab notebook](https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) for a working example.  ## Installation First, you'll need to install this [package](https://pypi.org/project/model-sketch-book/) using PyPI. ``` pip install model_sketch_book ```  ## 1: Basic setup ### Imports Then import the package into a Python notebook (i.e., a Jupyter or Colab notebook). ``` import model_sketch_book as msb ```  ### Set up the sketchbook **Data**.",model-sketch-book,SOFTWARE
"It has not yet been tested in a wide variety of development setups, but see our [sample Colab notebook](https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) for a working example.  ## Installation First, you'll need to install this [package](https://pypi.org/project/model-sketch-book/) using PyPI. ``` pip install model_sketch_book ```  ## 1: Basic setup ### Imports Then import the package into a Python notebook (i.e., a Jupyter or Colab notebook). ``` import model_sketch_book as msb ```  ### Set up the sketchbook **Data**.",PyPI,SOFTWARE
"It has not yet been tested in a wide variety of development setups, but see our [sample Colab notebook](https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) for a working example.  ## Installation First, you'll need to install this [package](https://pypi.org/project/model-sketch-book/) using PyPI. ``` pip install model_sketch_book ```  ## 1: Basic setup ### Imports Then import the package into a Python notebook (i.e., a Jupyter or Colab notebook). ``` import model_sketch_book as msb ```  ### Set up the sketchbook **Data**.",pip,SOFTWARE
"It has not yet been tested in a wide variety of development setups, but see our [sample Colab notebook](https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) for a working example.  ## Installation First, you'll need to install this [package](https://pypi.org/project/model-sketch-book/) using PyPI. ``` pip install model_sketch_book ```  ## 1: Basic setup ### Imports Then import the package into a Python notebook (i.e., a Jupyter or Colab notebook). ``` import model_sketch_book as msb ```  ### Set up the sketchbook **Data**.",model_sketch_book,SOFTWARE
"It has not yet been tested in a wide variety of development setups, but see our [sample Colab notebook](https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) for a working example.  ## Installation First, you'll need to install this [package](https://pypi.org/project/model-sketch-book/) using PyPI. ``` pip install model_sketch_book ```  ## 1: Basic setup ### Imports Then import the package into a Python notebook (i.e., a Jupyter or Colab notebook). ``` import model_sketch_book as msb ```  ### Set up the sketchbook **Data**.",Jupyter,SOFTWARE
"It has not yet been tested in a wide variety of development setups, but see our [sample Colab notebook](https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) for a working example.  ## Installation First, you'll need to install this [package](https://pypi.org/project/model-sketch-book/) using PyPI. ``` pip install model_sketch_book ```  ## 1: Basic setup ### Imports Then import the package into a Python notebook (i.e., a Jupyter or Colab notebook). ``` import model_sketch_book as msb ```  ### Set up the sketchbook **Data**.",Colab,SOFTWARE
"It has not yet been tested in a wide variety of development setups, but see our [sample Colab notebook](https://colab.research.google.com/github/StanfordHCI/ModelSketchBook/blob/main/example_nb/23_04_ModelSketchBook_example.ipynb) for a working example.  ## Installation First, you'll need to install this [package](https://pypi.org/project/model-sketch-book/) using PyPI. ``` pip install model_sketch_book ```  ## 1: Basic setup ### Imports Then import the package into a Python notebook (i.e., a Jupyter or Colab notebook). ``` import model_sketch_book as msb ```  ### Set up the sketchbook **Data**.",model_sketch_book,SOFTWARE
ModelSketchBook is designed to work with datasets in Pandas DataFrame format.,ModelSketchBook,SOFTWARE
ModelSketchBook is designed to work with datasets in Pandas DataFrame format.,Pandas,SOFTWARE
You can load CSVs to dataframes using the Pandas [`read_csv` function](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).,Pandas,SOFTWARE
You can load CSVs to dataframes using the Pandas [`read_csv` function](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).,pandas,SOFTWARE
You can load CSVs to dataframes using the Pandas [`read_csv` function](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).,pandas,SOFTWARE
Strings may be truncated to a max token length for prompts to GPT. - `InputType.Image`: (string) URLs to remotely-hosted images,GPT,SOFTWARE
Please supply your own OpenAI organization and API key if you would like to use GPT-based concepts.,GPT,SOFTWARE
"Otherwise, you can provide empty strings.  ``` sb = msb.create_model_sketchbook(     goal='Example sketchbook goal here',     schema={         # Specify your data schema here         ""picture_url"": msb.InputType.Image,         ""neighborhood_overview"": msb.InputType.Text,         ""description"": msb.InputType.Text,         ""overall_rating"": msb.InputType.GroundTruth,  # Required     },     credentials={         ""organization"": ""org-INSERT"",         ""api_key"": ""sk-INSERT""     } ) ```  ### Add your dataset(s) Then, add your dataset from a Pandas dataframe (recommended: 40-50 rows).",Pandas,SOFTWARE
"Currently, text concepts can be served by GPT-3 (using the `text-davinci-002` model) or OpenCLIP (using the `ViT-B-32-quickgelu` model), and image concepts can only be served by OpenCLIP.  ### Create sketches Then, you can combine concepts together into sketches with the following function.",GPT-3,SOFTWARE
"Currently, text concepts can be served by GPT-3 (using the `text-davinci-002` model) or OpenCLIP (using the `ViT-B-32-quickgelu` model), and image concepts can only be served by OpenCLIP.  ### Create sketches Then, you can combine concepts together into sketches with the following function.",text-davinci-002,SOFTWARE
"Currently, text concepts can be served by GPT-3 (using the `text-davinci-002` model) or OpenCLIP (using the `ViT-B-32-quickgelu` model), and image concepts can only be served by OpenCLIP.  ### Create sketches Then, you can combine concepts together into sketches with the following function.",OpenCLIP,SOFTWARE
"Currently, text concepts can be served by GPT-3 (using the `text-davinci-002` model) or OpenCLIP (using the `ViT-B-32-quickgelu` model), and image concepts can only be served by OpenCLIP.  ### Create sketches Then, you can combine concepts together into sketches with the following function.",ViT-B-32-quickgelu,SOFTWARE
"Currently, text concepts can be served by GPT-3 (using the `text-davinci-002` model) or OpenCLIP (using the `ViT-B-32-quickgelu` model), and image concepts can only be served by OpenCLIP.  ### Create sketches Then, you can combine concepts together into sketches with the following function.",OpenCLIP,SOFTWARE
"By contributing to this project, you agree to abide by its terms.  ## License  `model_sketch_book` was created by Michelle Lam.",model_sketch_book,SOFTWARE
It is licensed under the terms of the MIT license.  ## Credits  `model_sketch_book` was created with [`cookiecutter`](https://cookiecutter.readthedocs.io/en/latest/) and the `py-pkgs-cookiecutter` [template](https://github.com/py-pkgs/py-pkgs-cookiecutter).,model_sketch_book,SOFTWARE
It is licensed under the terms of the MIT license.  ## Credits  `model_sketch_book` was created with [`cookiecutter`](https://cookiecutter.readthedocs.io/en/latest/) and the `py-pkgs-cookiecutter` [template](https://github.com/py-pkgs/py-pkgs-cookiecutter).,cookiecutter,SOFTWARE
It is licensed under the terms of the MIT license.  ## Credits  `model_sketch_book` was created with [`cookiecutter`](https://cookiecutter.readthedocs.io/en/latest/) and the `py-pkgs-cookiecutter` [template](https://github.com/py-pkgs/py-pkgs-cookiecutter).,cookiecutter,SOFTWARE
It is licensed under the terms of the MIT license.  ## Credits  `model_sketch_book` was created with [`cookiecutter`](https://cookiecutter.readthedocs.io/en/latest/) and the `py-pkgs-cookiecutter` [template](https://github.com/py-pkgs/py-pkgs-cookiecutter).,py-pkgs-cookiecutter,SOFTWARE
See full demo on [YouTube](https://www.youtube.com/watch?,YouTube,SOFTWARE
"[demo](https://user-images.githubusercontent.com/9694230/51806092-db766c00-2275-11e9-8de0-888bed0fc9e8.gif)   ## Requirements Python 3.7 The most important packages are pytorch, torchvision, numpy, pillow and matplotlib.",Python 3.7,SOFTWARE
"[demo](https://user-images.githubusercontent.com/9694230/51806092-db766c00-2275-11e9-8de0-888bed0fc9e8.gif)   ## Requirements Python 3.7 The most important packages are pytorch, torchvision, numpy, pillow and matplotlib.",pytorch,SOFTWARE
"[demo](https://user-images.githubusercontent.com/9694230/51806092-db766c00-2275-11e9-8de0-888bed0fc9e8.gif)   ## Requirements Python 3.7 The most important packages are pytorch, torchvision, numpy, pillow and matplotlib.",torchvision,SOFTWARE
"[demo](https://user-images.githubusercontent.com/9694230/51806092-db766c00-2275-11e9-8de0-888bed0fc9e8.gif)   ## Requirements Python 3.7 The most important packages are pytorch, torchvision, numpy, pillow and matplotlib.",numpy,SOFTWARE
"[demo](https://user-images.githubusercontent.com/9694230/51806092-db766c00-2275-11e9-8de0-888bed0fc9e8.gif)   ## Requirements Python 3.7 The most important packages are pytorch, torchvision, numpy, pillow and matplotlib.",pillow,SOFTWARE
"[demo](https://user-images.githubusercontent.com/9694230/51806092-db766c00-2275-11e9-8de0-888bed0fc9e8.gif)   ## Requirements Python 3.7 The most important packages are pytorch, torchvision, numpy, pillow and matplotlib.",matplotlib,SOFTWARE
(Works with Pytorch 1.1)   ## Dataset The [Kitti dataset](www.cvlibs.net/datasets/kitti/) has been used.,Pytorch 1.1,SOFTWARE
"|--validation ```   ## Run Code To run the code:  `python main.py --data_path /path/to/data/ --lr_policy plateau`  Flags: - Set flag ""input_type"" to rgb or depth. - Set flag ""pretrained"" to true or false to use a model pretrained on Cityscapes for the global branch. - See `python main.py --help` for more information.",python,SOFTWARE
"|--validation ```   ## Run Code To run the code:  `python main.py --data_path /path/to/data/ --lr_policy plateau`  Flags: - Set flag ""input_type"" to rgb or depth. - Set flag ""pretrained"" to true or false to use a model pretrained on Cityscapes for the global branch. - See `python main.py --help` for more information.",python,SOFTWARE
or   `source Shell/train.sh $datapath`  checkout more details in the bash file.  ## Trained models Our network architecture is based on [ERFNet](https://github.com/Eromera/erfnet_pytorch).,pytorch,SOFTWARE
# Overview  - The repository dependencies are saved as conda environment (environment.yaml) file,conda,SOFTWARE
- The Deep Learning models/layers are build with TF 2.X. - Setup instruction for the repository are given here: [Install requirements](https://github.com/Cardio-AI/3d-mri-domain-adaptation#setup-instructions-tested-with-osx-and-ubuntu) - An overview of all files and there usage is given here: [Repository Structure](https://github.com/Cardio-AI/3d-mri-domain-adaptation#repository-structure)  # Paper:  Please cite the following paper if you use/modify or adapt part of the code from this repository:  S.,TF 2.X,SOFTWARE
"- Link to original paper: [IEEE TMI paper link](https://ieeexplore.ieee.org/document/9328840) (accepted 13.1.2021) - A pre-print version is available here: [arxiv-Preprint](http://arxiv.org/abs/2101.07653)  Bibtex:  >@article{Koehler_2021, <br> >   title={Unsupervised Domain Adaptation from Axial to Short-Axis Multi-Slice Cardiac MR Images by Incorporating Pretrained Task Networks},<br> >   ISSN={1558-254X},<br> >   url={http://dx.doi.org/10.1109/TMI.2021.3052972},<br> >   DOI={10.1109/tmi.2021.3052972},<br> >   journal={IEEE Transactions on Medical Imaging},<br> >   publisher={Institute of Electrical and Electronics Engineers (IEEE)},<br> >   author={Koehler, Sven and Hussain, Tarique and Blair, Zach and Huffaker, Tyler and Ritzmann, Florian and Tandon, Animesh and Pickardt, Thomas and Sarikouch, Samir and Sarikouch, Samir and Latus, Heiner and Greil, Gerald and Wolf, Ivo and Engelhardt, Sandy}, >   year={2021},<br> >   pages={1‚Äì1}<br> >}   # Institutions:  >- [Heidelberg University Hospital, Artificial Intelligence in Cardiovascular Medicine (AICM) Group](https://www.klinikum.uni-heidelberg.de/chirurgische-klinik-zentrum/herzchirurgie/forschung/ag-artificial-intelligence-in-cardiovascular-medicine) >- [German Competence network for Congenital heart defects](https://www.kompetenznetz-ahf.de/en/about-us/competence-network/) >- [UT Southwestern Medical Center, Pediatric Cardiology](https://www.utsouthwestern.edu/education/medical-school/departments/pediatrics/divisions/cardiology/) >- [Department of Computer Science, University Of Applied Science Mannheim](https://www.informatik.hs-mannheim.de/wir/menschen/professoren/prof-dr-ivo-wolf.html)   # How to use:  This repository splits the source code into:  - interactive notebooks (/notebooks),  - python source modules (/src) and  - the experiment related files such as the experiment configs (/reports) or trained models (/models).",python,SOFTWARE
"Each experiment includes the following artefacts: - One config file, which represents all experimental hyper-parameters which are neccessary to reproduce the experiment or to load it for later predictions - Three tensorboard logfiles per training to keep track of the trainings, evaluation and visual output progress",tensorboard,SOFTWARE
- Model graph description either as json or tensorflow protobuf file and the corresponding model weights as h5 file.,tensorflow,SOFTWARE
The tensorflow model and layer definitions are within /src/models.,tensorflow,SOFTWARE
‚îÇ   ‚îú‚îÄ‚îÄ configs        <- Experiment config files as json with all hyperparameters and paths     ‚îÇ   ‚îú‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting     ‚îÇ   ‚îú‚îÄ‚îÄ history        <- Tensorboard trainings history files     ‚îÇ   ‚îî‚îÄ‚îÄ tensorboard_logs  <- Trainings-scalars and images of the intermediate predictions     ‚îÇ     ‚îú‚îÄ‚îÄ environment.yaml   <- Conda requirements file.,Tensorboard,SOFTWARE
‚îÇ   ‚îú‚îÄ‚îÄ configs        <- Experiment config files as json with all hyperparameters and paths     ‚îÇ   ‚îú‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting     ‚îÇ   ‚îú‚îÄ‚îÄ history        <- Tensorboard trainings history files     ‚îÇ   ‚îî‚îÄ‚îÄ tensorboard_logs  <- Trainings-scalars and images of the intermediate predictions     ‚îÇ     ‚îú‚îÄ‚îÄ environment.yaml   <- Conda requirements file.,tensorboard,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",Python 3.6,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",anaconda,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",Anaconda,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",nvidia drivers,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",cuda,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",cudnn,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",tensorflow,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",Tensorflow,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",git,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",conda,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",conda,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",conda,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",pip,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",jupyter lab,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",python,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",nodejs,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",conda,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",conda,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",nodejs,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",jupyterlab-manager,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",jupyter,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",jupyter-widgets/jupyterlab-manager,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",jupyterlab,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",JupyterLab,SOFTWARE
"Medical Imaging: Image-Guided Procedures (2020).  ## Setup instructions (Ubuntu)  ### Preconditions:  - Python 3.6 locally installed  (e.g.:  <a target=""_blank"" href=""https://www.anaconda.com/download/#macos"">Anaconda</a>) - Installed nvidia drivers, cuda and cudnn  (e.g.:  <a target=""_blank"" href=""https://www.tensorflow.org/install/gpu"">Tensorflow</a>)  ### Local setup Clone repository ``` git clone %repo-name% cd %repo-name% ``` Create a conda environment from environment.yaml (environment name will be ax2sax) ``` conda env create --file environment.yaml ```  Activate environment ``` conda activate ax2sax ``` Install a helper to automatically change the working directory to the project root directory ``` pip install --extra-index-url https://test.pypi.org/simple/ ProjectRoot ``` Create a jupyter kernel from the activated environment, this kernel will be visible in the jupyter lab ``` python -m ipykernel install --user --name ax2sax --display-name ""ax2sax kernel"" ```   ### Enable interactive widgets in Jupyterlab  Pre-condition: nodejs installed globally or into the conda environment. e.g.: ``` conda install -c conda-forge nodejs ``` Install the jupyterlab-manager which enables the use of interactive widgets ``` jupyter labextension install @jupyter-widgets/jupyterlab-manager ```  Further infos on how to enable the jupyterlab-extensions: [JupyterLab](https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension)",jupyterlab,SOFTWARE
# NELA-GT-2019  This repository contain examples of how to use the NELA-GT-2019 data set with Python 3.,Python 3,SOFTWARE
"Horne and Sibel Adalƒ±},     year={2020},     eprint={2003.08444},     archivePrefix={arXiv},     primaryClass={cs.CY} } ``` ## Data  Metadata|| ---|--- Dataset name|`NELA-GT-2019` Formats|`Sqlite3`,`JSON` No. of articles|`1118821` No. of sources|`261` Collection period|`2019-01-01` to `2019-12-31`  ### Fields  Each data point collected corresponds to an article and contains the fields described below.",Sqlite3,SOFTWARE
Visual results on the first 5k images from COCO test set of our ***COCO 2016 challenge entry***: [OneDrive](https://onedrive.live.com/?,OneDrive,SOFTWARE
Slides in [ImageNet ILSVRC and COCO workshop 2016](http://image-net.org/challenges/ilsvrc+coco2016): [OneDrive](https://onedrive.live.com/?,OneDrive,SOFTWARE
<img src='data/readme_img.png' width='800'>   ### Disclaimer  This is an official implementation for [Fully Convolutional Instance-aware Semantic Segmentation](https://arxiv.org/abs/1611.07709) (FCIS) based on MXNet.,MXNet,SOFTWARE
"* This repository used code from [MXNet rcnn example](https://github.com/dmlc/mxnet/tree/master/example/rcnn) and [mx-rfcn](https://github.com/giorking/mx-rfcn).   ### License  ¬© Microsoft, 2017.",MXNet,SOFTWARE
MXNet from [the offical repository](https://github.com/dmlc/mxnet).,MXNet,SOFTWARE
"Due to the rapid development of MXNet, it is recommended to checkout this version if you encounter any issues.",MXNet,SOFTWARE
We may maintain this repository periodically if MXNet adds important feature in future release.  2.,MXNet,SOFTWARE
"Python packages might missing: cython, opencv-python >= 3.2.0, easydict.",cython,SOFTWARE
"Python packages might missing: cython, opencv-python >= 3.2.0, easydict.",opencv-python >= 3.2.0,SOFTWARE
"If `pip` is set up on your system, those packages should be able to be fetched and installed by running  ```  pip install Cython  pip install opencv-python==3.2.0.6  pip install easydict==1.6  pip install hickle  ``` 3.",pip,SOFTWARE
"If `pip` is set up on your system, those packages should be able to be fetched and installed by running  ```  pip install Cython  pip install opencv-python==3.2.0.6  pip install easydict==1.6  pip install hickle  ``` 3.",pip,SOFTWARE
"If `pip` is set up on your system, those packages should be able to be fetched and installed by running  ```  pip install Cython  pip install opencv-python==3.2.0.6  pip install easydict==1.6  pip install hickle  ``` 3.",Cython,SOFTWARE
"If `pip` is set up on your system, those packages should be able to be fetched and installed by running  ```  pip install Cython  pip install opencv-python==3.2.0.6  pip install easydict==1.6  pip install hickle  ``` 3.",pip,SOFTWARE
"If `pip` is set up on your system, those packages should be able to be fetched and installed by running  ```  pip install Cython  pip install opencv-python==3.2.0.6  pip install easydict==1.6  pip install hickle  ``` 3.",opencv-python==3.2.0.6,SOFTWARE
"If `pip` is set up on your system, those packages should be able to be fetched and installed by running  ```  pip install Cython  pip install opencv-python==3.2.0.6  pip install easydict==1.6  pip install hickle  ``` 3.",pip,SOFTWARE
"If `pip` is set up on your system, those packages should be able to be fetched and installed by running  ```  pip install Cython  pip install opencv-python==3.2.0.6  pip install easydict==1.6  pip install hickle  ``` 3.",easydict==1.6,SOFTWARE
"If `pip` is set up on your system, those packages should be able to be fetched and installed by running  ```  pip install Cython  pip install opencv-python==3.2.0.6  pip install easydict==1.6  pip install hickle  ``` 3.",pip,SOFTWARE
"If `pip` is set up on your system, those packages should be able to be fetched and installed by running  ```  pip install Cython  pip install opencv-python==3.2.0.6  pip install easydict==1.6  pip install hickle  ``` 3.",hickle,SOFTWARE
"For Windows users, Visual Studio 2015 is needed to compile cython module.   ### Requirements: Hardware  Any NVIDIA GPUs with at least 5GB memory should be OK  ### Installation  1.",Windows,SOFTWARE
"For Windows users, Visual Studio 2015 is needed to compile cython module.   ### Requirements: Hardware  Any NVIDIA GPUs with at least 5GB memory should be OK  ### Installation  1.",Visual Studio 2015,SOFTWARE
"For Windows users, Visual Studio 2015 is needed to compile cython module.   ### Requirements: Hardware  Any NVIDIA GPUs with at least 5GB memory should be OK  ### Installation  1.",cython,SOFTWARE
Install MXNet:    **Note: The MXNet's Custom Op cannot execute parallelly using multi-gpus after this [PR](https://github.com/apache/incubator-mxnet/pull/6928).,MXNet,SOFTWARE
Install MXNet:    **Note: The MXNet's Custom Op cannot execute parallelly using multi-gpus after this [PR](https://github.com/apache/incubator-mxnet/pull/6928).,MXNet,SOFTWARE
"We strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.6).**   ***Quick start***   3.1 Install MXNet and all dependencies by   ```  pip install -r requirements.txt  ```  If there is no other error message, MXNet should be installed successfully.",MXNet@(commit 998378a),SOFTWARE
"We strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.6).**   ***Quick start***   3.1 Install MXNet and all dependencies by   ```  pip install -r requirements.txt  ```  If there is no other error message, MXNet should be installed successfully.",MXNet,SOFTWARE
"We strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.6).**   ***Quick start***   3.1 Install MXNet and all dependencies by   ```  pip install -r requirements.txt  ```  If there is no other error message, MXNet should be installed successfully.",pip,SOFTWARE
"We strongly suggest the user rollback to version [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) for training (following Section 3.2 - 3.6).**   ***Quick start***   3.1 Install MXNet and all dependencies by   ```  pip install -r requirements.txt  ```  If there is no other error message, MXNet should be installed successfully.",MXNet,SOFTWARE
"***Build from source (alternative way)***   3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by  ```  git clone --recursive https://github.com/dmlc/mxnet.git  git checkout 998378a  git submodule init  git submodule update  ```  3.3 Copy channel operators in `$(FCIS_ROOT)/fcis/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by  ```  cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/     ```  3.4 Compile MXNet  ```  cd ${MXNET_ROOT}  make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1  ```  3.5 Install the MXNet Python binding by    ***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***  ```  cd python  sudo python setup.py install  ```  3.6 For advanced users, you may put your Python packge into `.",MXNet,SOFTWARE
"***Build from source (alternative way)***   3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by  ```  git clone --recursive https://github.com/dmlc/mxnet.git  git checkout 998378a  git submodule init  git submodule update  ```  3.3 Copy channel operators in `$(FCIS_ROOT)/fcis/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by  ```  cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/     ```  3.4 Compile MXNet  ```  cd ${MXNET_ROOT}  make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1  ```  3.5 Install the MXNet Python binding by    ***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***  ```  cd python  sudo python setup.py install  ```  3.6 For advanced users, you may put your Python packge into `.",MXNet@(commit 998378a),SOFTWARE
"***Build from source (alternative way)***   3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by  ```  git clone --recursive https://github.com/dmlc/mxnet.git  git checkout 998378a  git submodule init  git submodule update  ```  3.3 Copy channel operators in `$(FCIS_ROOT)/fcis/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by  ```  cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/     ```  3.4 Compile MXNet  ```  cd ${MXNET_ROOT}  make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1  ```  3.5 Install the MXNet Python binding by    ***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***  ```  cd python  sudo python setup.py install  ```  3.6 For advanced users, you may put your Python packge into `.",git,SOFTWARE
"***Build from source (alternative way)***   3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by  ```  git clone --recursive https://github.com/dmlc/mxnet.git  git checkout 998378a  git submodule init  git submodule update  ```  3.3 Copy channel operators in `$(FCIS_ROOT)/fcis/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by  ```  cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/     ```  3.4 Compile MXNet  ```  cd ${MXNET_ROOT}  make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1  ```  3.5 Install the MXNet Python binding by    ***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***  ```  cd python  sudo python setup.py install  ```  3.6 For advanced users, you may put your Python packge into `.",git,SOFTWARE
"***Build from source (alternative way)***   3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by  ```  git clone --recursive https://github.com/dmlc/mxnet.git  git checkout 998378a  git submodule init  git submodule update  ```  3.3 Copy channel operators in `$(FCIS_ROOT)/fcis/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by  ```  cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/     ```  3.4 Compile MXNet  ```  cd ${MXNET_ROOT}  make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1  ```  3.5 Install the MXNet Python binding by    ***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***  ```  cd python  sudo python setup.py install  ```  3.6 For advanced users, you may put your Python packge into `.",git,SOFTWARE
"***Build from source (alternative way)***   3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by  ```  git clone --recursive https://github.com/dmlc/mxnet.git  git checkout 998378a  git submodule init  git submodule update  ```  3.3 Copy channel operators in `$(FCIS_ROOT)/fcis/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by  ```  cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/     ```  3.4 Compile MXNet  ```  cd ${MXNET_ROOT}  make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1  ```  3.5 Install the MXNet Python binding by    ***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***  ```  cd python  sudo python setup.py install  ```  3.6 For advanced users, you may put your Python packge into `.",git,SOFTWARE
"***Build from source (alternative way)***   3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by  ```  git clone --recursive https://github.com/dmlc/mxnet.git  git checkout 998378a  git submodule init  git submodule update  ```  3.3 Copy channel operators in `$(FCIS_ROOT)/fcis/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by  ```  cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/     ```  3.4 Compile MXNet  ```  cd ${MXNET_ROOT}  make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1  ```  3.5 Install the MXNet Python binding by    ***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***  ```  cd python  sudo python setup.py install  ```  3.6 For advanced users, you may put your Python packge into `.",MXNet,SOFTWARE
"***Build from source (alternative way)***   3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by  ```  git clone --recursive https://github.com/dmlc/mxnet.git  git checkout 998378a  git submodule init  git submodule update  ```  3.3 Copy channel operators in `$(FCIS_ROOT)/fcis/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by  ```  cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/     ```  3.4 Compile MXNet  ```  cd ${MXNET_ROOT}  make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1  ```  3.5 Install the MXNet Python binding by    ***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***  ```  cd python  sudo python setup.py install  ```  3.6 For advanced users, you may put your Python packge into `.",MXNet,SOFTWARE
"***Build from source (alternative way)***   3.2 Clone MXNet and checkout to [MXNet@(commit 998378a)](https://github.com/dmlc/mxnet/tree/998378a) by  ```  git clone --recursive https://github.com/dmlc/mxnet.git  git checkout 998378a  git submodule init  git submodule update  ```  3.3 Copy channel operators in `$(FCIS_ROOT)/fcis/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by  ```  cp -r $(FCIS_ROOT)/fcis/operator_cxx/channel_operator* $(MXNET_ROOT)/src/operator/contrib/     ```  3.4 Compile MXNet  ```  cd ${MXNET_ROOT}  make -j $(nproc) USE_OPENCV=1 USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1  ```  3.5 Install the MXNet Python binding by    ***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***  ```  cd python  sudo python setup.py install  ```  3.6 For advanced users, you may put your Python packge into `.",MXNet,SOFTWARE
"/external/mxnet/$(YOUR_MXNET_PACKAGE)`, and modify `MXNET_VERSION` in `.",mxnet,SOFTWARE
Thus you can switch among different versions of MXNet quickly.  ### Demo  1.,MXNet,SOFTWARE
"To run the demo with our trained model (on COCO trainval35k), please download the model manually from [OneDrive](https://1drv.ms/u/s!",OneDrive,SOFTWARE
"Am-5JzdW2XHzhqMJZmVOEDgfde8_tg) (Chinese users can also get it from [BaiduYun](https://pan.baidu.com/s/1geOHioV) with code `tmd4`), and put it under folder `model/`.",BaiduYun,SOFTWARE
Run  ```  python .,python,SOFTWARE
Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!,ImageNet-pretrained ResNet-v1-101,SOFTWARE
Please download ImageNet-pretrained ResNet-v1-101 model manually from [OneDrive](https://1drv.ms/u/s!,OneDrive,SOFTWARE
"For example, to train and test FCIS on COCO with ResNet-v1-101, use the following command     ```     python experiments/fcis/fcis_end2end_train_test.py --cfg experiments/fcis/cfgs/resnet_v1_101_coco_fcis_end2end_ohem.yaml     ```     A cache folder would be created automatically to save the model and the log under `output/fcis/coco/` or `output/fcis/voc/`. 4.",python,SOFTWARE
Code has been tested under:  - Ubuntu 14.04 with a Maxwell Titan X GPU and Intel Xeon CPU E5-2620 v2 @ 2.10GHz - Windows Server 2012 R2 with 8 K40 GPUs and Intel Xeon CPU E5-2650 v2 @ 2.60GHz - Windows Server 2012 R2 with 4 Pascal Titan X GPUs and Intel Xeon CPU E5-2650 v4 @ 2.30GHz,Ubuntu 14.04,SOFTWARE
Code has been tested under:  - Ubuntu 14.04 with a Maxwell Titan X GPU and Intel Xeon CPU E5-2620 v2 @ 2.10GHz - Windows Server 2012 R2 with 8 K40 GPUs and Intel Xeon CPU E5-2650 v2 @ 2.60GHz - Windows Server 2012 R2 with 4 Pascal Titan X GPUs and Intel Xeon CPU E5-2650 v4 @ 2.30GHz,Windows Server 2012 R2,SOFTWARE
Code has been tested under:  - Ubuntu 14.04 with a Maxwell Titan X GPU and Intel Xeon CPU E5-2620 v2 @ 2.10GHz - Windows Server 2012 R2 with 8 K40 GPUs and Intel Xeon CPU E5-2650 v2 @ 2.60GHz - Windows Server 2012 R2 with 4 Pascal Titan X GPUs and Intel Xeon CPU E5-2650 v4 @ 2.30GHz,Windows Server 2012 R2,SOFTWARE
Use the `Word Cluster Analysis` notebook in the code folder to inspect the content of these files,Word Cluster Analysis,SOFTWARE
Usage example for S2ORC:  ``` python write_mask_preds.py --data_dir /data/actual_data --out_dir /output --dataset s2orc --model scholarBERT --max_tokens_per_batch 16384 --write_specific_replacements --vocab_path /data/wsi_vocab_set_98_50.txt --overwrite_cache --files_range 0-24 ```  Usage example for Wikipedia:  ``` python write_mask_preds.py --data_dir /data/actual_data --out_dir /output --dataset wikipedia --model scholarBERT --max_tokens_per_batch 16384 --write_specific_replacements --vocab_path /data/wsi_vocab_set_98_50.txt --overwrite_cache --files_range 0-24 ```  In the `WSIatScale` folder run `wsi_pipeline.sh` to do WSI pipeline for the entire vocab.,python,SOFTWARE
Usage example for S2ORC:  ``` python write_mask_preds.py --data_dir /data/actual_data --out_dir /output --dataset s2orc --model scholarBERT --max_tokens_per_batch 16384 --write_specific_replacements --vocab_path /data/wsi_vocab_set_98_50.txt --overwrite_cache --files_range 0-24 ```  Usage example for Wikipedia:  ``` python write_mask_preds.py --data_dir /data/actual_data --out_dir /output --dataset wikipedia --model scholarBERT --max_tokens_per_batch 16384 --write_specific_replacements --vocab_path /data/wsi_vocab_set_98_50.txt --overwrite_cache --files_range 0-24 ```  In the `WSIatScale` folder run `wsi_pipeline.sh` to do WSI pipeline for the entire vocab.,python,SOFTWARE
"- `create_inverted_index.py`: create inverted index  ``` python create_inverted_index.py --replacements_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --vocab_path /home/lucyl/language-map-of-science/logs/sense_vocab/wsi_vocab_set_98_50.txt --outdir /home/lucyl/language-map-of-science/logs/inverted_index --input_ids_path /home/lucyl/language-map-of-science/data/input_paper_ids/journal_analysis.txt ```  - `cluster_reps_per_token.py`: cluster the reps  Lemmatized, specifying resolution:   ``` python cluster_reps_per_token.py --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --out_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  After clustering for the whole dataset, use `Wiktionary Validation.ipynb` notebook to get FOS to words json.",python,SOFTWARE
"- `create_inverted_index.py`: create inverted index  ``` python create_inverted_index.py --replacements_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --vocab_path /home/lucyl/language-map-of-science/logs/sense_vocab/wsi_vocab_set_98_50.txt --outdir /home/lucyl/language-map-of-science/logs/inverted_index --input_ids_path /home/lucyl/language-map-of-science/data/input_paper_ids/journal_analysis.txt ```  - `cluster_reps_per_token.py`: cluster the reps  Lemmatized, specifying resolution:   ``` python cluster_reps_per_token.py --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --out_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  After clustering for the whole dataset, use `Wiktionary Validation.ipynb` notebook to get FOS to words json.",python,SOFTWARE
"Cluster only wiktionary words, lemmatized, specifying resolution:   ``` python cluster_reps_per_token.py --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --dataset s2orc --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --out_dir /home/lucyl/language-map-of-science/logs/word_clusters_eval --lemmatize True --wiki_eval True --resolution 0.0 ```  Can check the coverage of words that appear in FOS in `Wiktionary Validation.ipynb`",python,SOFTWARE
"- `assign_clusters_to_tokens.py`: assign everyone to a cluster  Lemmatized, specifying resolution:   ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_lemmed --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  Assign only wiktionary words, lemmatized, specifying resolution:  ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_eval --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_eval --lemmatize True --wiki_eval True --resolution 0.5 ```  Sense NPMI - run `get_documentID_maps()` in `get_docID_to_group.py` - `word_sense.py`, for journals, fos, and wiktionary evaluation - `Wiktionary Validation.ipynb` is the notebook that contains Wiktionary evaluation results.",python,SOFTWARE
"- `assign_clusters_to_tokens.py`: assign everyone to a cluster  Lemmatized, specifying resolution:   ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_lemmed --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_lemmed --lemmatize True --resolution 0.0 ```  Assign only wiktionary words, lemmatized, specifying resolution:  ``` python assign_clusters_to_tokens.py --out_dir /home/lucyl/language-map-of-science/logs/sense_assignments_eval --index_dir /home/lucyl/language-map-of-science/logs/inverted_index --dataset s2orc --data_dir /home/lucyl/language-map-of-science/logs/replacements/replacements --cluster_dir /home/lucyl/language-map-of-science/logs/word_clusters_eval --lemmatize True --wiki_eval True --resolution 0.5 ```  Sense NPMI - run `get_documentID_maps()` in `get_docID_to_group.py` - `word_sense.py`, for journals, fos, and wiktionary evaluation - `Wiktionary Validation.ipynb` is the notebook that contains Wiktionary evaluation results.",python,SOFTWARE
**Social implication experiments** - `Domain Language Analysis` is a notebook that generates data for some of the tables of example jargon in the paper.,Domain Language Analysis,SOFTWARE
"- `get_discipline_specific.py`: get discipline specific journals and their papers, for the audience design experiment - `jargonyness_per_paper.py`: calculate amount of jargon per abstract  Example usage:  ``` python jargonyness_per_paper.py --cutoff 0.1 --exp_name general_specific  python jargonyness_per_paper.py --cutoff 0.1 --exp_name regression_sample ```  - `expected_max_npmi.py`: expected max NPMI over token positions in abstract, for audience design experiment - `Paper Jargon Rate.ipynb`: audience design plots - `get_paper_time_and_place.py`: get FOS and year of potential papers that may cite the papers in regression study - `General Dataset Statistics`: get data used for regression  - `regression_variables.py`: get some of the simpler regression variables - `citations_per_journal.py`: for calculating the average number of citations per journal, a regression variable - `Paper Success Regression.ipynb`: notebook that runs regressions - `get_fos_citation_matrix.py`: for calculating similarity among disciplines, part of interdisciplinarity regression  **Citation graph**  Future work may want to run analysis on the S2ORC citation graph.",python,SOFTWARE
"- `get_discipline_specific.py`: get discipline specific journals and their papers, for the audience design experiment - `jargonyness_per_paper.py`: calculate amount of jargon per abstract  Example usage:  ``` python jargonyness_per_paper.py --cutoff 0.1 --exp_name general_specific  python jargonyness_per_paper.py --cutoff 0.1 --exp_name regression_sample ```  - `expected_max_npmi.py`: expected max NPMI over token positions in abstract, for audience design experiment - `Paper Jargon Rate.ipynb`: audience design plots - `get_paper_time_and_place.py`: get FOS and year of potential papers that may cite the papers in regression study - `General Dataset Statistics`: get data used for regression  - `regression_variables.py`: get some of the simpler regression variables - `citations_per_journal.py`: for calculating the average number of citations per journal, a regression variable - `Paper Success Regression.ipynb`: notebook that runs regressions - `get_fos_citation_matrix.py`: for calculating similarity among disciplines, part of interdisciplinarity regression  **Citation graph**  Future work may want to run analysis on the S2ORC citation graph.",python,SOFTWARE
"The below script supports the conversion of S2ORC data to a `graph-tool` network, where nodes are papers labeled with paper ID",graph-tool,SOFTWARE
See [environment.yml](environment.yml) for complete environment settings. - python 3.7.6 - pytorch 1.1.0 - torchvision 0.3.0 - tensorboard 2.1.0 - spacy 2.2.3  ## Data Preparation Follow instructions in `data/README.md` to setup `data` directory.,python 3.7.6,SOFTWARE
See [environment.yml](environment.yml) for complete environment settings. - python 3.7.6 - pytorch 1.1.0 - torchvision 0.3.0 - tensorboard 2.1.0 - spacy 2.2.3  ## Data Preparation Follow instructions in `data/README.md` to setup `data` directory.,pytorch 1.1.0,SOFTWARE
See [environment.yml](environment.yml) for complete environment settings. - python 3.7.6 - pytorch 1.1.0 - torchvision 0.3.0 - tensorboard 2.1.0 - spacy 2.2.3  ## Data Preparation Follow instructions in `data/README.md` to setup `data` directory.,torchvision 0.3.0,SOFTWARE
See [environment.yml](environment.yml) for complete environment settings. - python 3.7.6 - pytorch 1.1.0 - torchvision 0.3.0 - tensorboard 2.1.0 - spacy 2.2.3  ## Data Preparation Follow instructions in `data/README.md` to setup `data` directory.,tensorboard 2.1.0,SOFTWARE
See [environment.yml](environment.yml) for complete environment settings. - python 3.7.6 - pytorch 1.1.0 - torchvision 0.3.0 - tensorboard 2.1.0 - spacy 2.2.3  ## Data Preparation Follow instructions in `data/README.md` to setup `data` directory.,spacy 2.2.3,SOFTWARE
We provide altered version of [MAttNet](https://github.com/ChopinSharp/MAttNet) and [CM-A-E](https://github.com/ChopinSharp/CM-Erase-REG) for downstream REG task evaluation.,MAttNet,SOFTWARE
We provide altered version of [MAttNet](https://github.com/ChopinSharp/MAttNet) and [CM-A-E](https://github.com/ChopinSharp/CM-Erase-REG) for downstream REG task evaluation.,MAttNet,SOFTWARE
We provide altered version of [MAttNet](https://github.com/ChopinSharp/MAttNet) and [CM-A-E](https://github.com/ChopinSharp/CM-Erase-REG) for downstream REG task evaluation.,CM-A-E,SOFTWARE
We provide altered version of [MAttNet](https://github.com/ChopinSharp/MAttNet) and [CM-A-E](https://github.com/ChopinSharp/CM-Erase-REG) for downstream REG task evaluation.,CM-Erase-REG,SOFTWARE
[[Google Drive]](https://drive.google.com/drive/folders/1BPqWW0LrAEBFna7b-ORF2TcrY7K_DDvM?,Google Drive,SOFTWARE
"usp=sharing) [[Baidu Disk]](https://pan.baidu.com/s/1G4k7APKSUs-_5StXoYaNrA) (extraction code: 5a9r)  ## Citation ``` @inproceedings{chen2021ref,   title={Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding},   author={Chen, Long and Ma, Wenbo and Xiao, Jun and Zhang, Hanwang and Chang, Shih-Fu},   booktitle={AAAI},   year={2021} } ```",Baidu Disk,SOFTWARE
style=plastic&logo=themodelsresource)](https://huggingface.co/fses91/VNEGNN-MODEL) [!,VNEGNN,SOFTWARE
"style=plastic&logo=internetcomputer)](https://linktr.ee/vnegnn)  ## News  üî•  ***April 10 2024***:  *The trained VNEGNN model and processed dataset are released, as in the paper!",vnegnn,SOFTWARE
"style=plastic&logo=internetcomputer)](https://linktr.ee/vnegnn)  ## News  üî•  ***April 10 2024***:  *The trained VNEGNN model and processed dataset are released, as in the paper!",VNEGNN,SOFTWARE
[](https://img.shields.io/badge/PyTorch-2.1.2-ee4c2c?,PyTorch-2.1.2,SOFTWARE
logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/) [!,pytorch,SOFTWARE
[lightning](https://img.shields.io/badge/-Lightning_2.2.1-792ee5?,lightning,SOFTWARE
[lightning](https://img.shields.io/badge/-Lightning_2.2.1-792ee5?,Lightning_2.2.1,SOFTWARE
logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/) [!,pytorchlightning,SOFTWARE
logo=pytorchlightning&logoColor=white)](https://pytorchlightning.ai/) [!,pytorchlightning,SOFTWARE
[hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/) [!,hydra,SOFTWARE
[hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/) [!,Hydra_1.3,SOFTWARE
[hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/) [!,hydra,SOFTWARE
[black](https://img.shields.io/badge/Code%20Style-Black-black.svg?,black,SOFTWARE
[black](https://img.shields.io/badge/Code%20Style-Black-black.svg?,Black,SOFTWARE
[black](https://img.shields.io/badge/Code%20Style-Black-black.svg?,black,SOFTWARE
labelColor=gray)](https://black.readthedocs.io/en/stable/) [!,black,SOFTWARE
[isort](https://img.shields.io/badge/%20imports-isort-%231674b1?,isort,SOFTWARE
[isort](https://img.shields.io/badge/%20imports-isort-%231674b1?,isort,SOFTWARE
"style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)  To reproduce the results please use Python 3.9, PyTorch version 2.1.2, Cuda 12.1, PyG version 2.3.0.",isort,SOFTWARE
"style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)  To reproduce the results please use Python 3.9, PyTorch version 2.1.2, Cuda 12.1, PyG version 2.3.0.",PyTorch version 2.1.2,SOFTWARE
"style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)  To reproduce the results please use Python 3.9, PyTorch version 2.1.2, Cuda 12.1, PyG version 2.3.0.",Cuda 12.1,SOFTWARE
"style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)  To reproduce the results please use Python 3.9, PyTorch version 2.1.2, Cuda 12.1, PyG version 2.3.0.",PyG version 2.3.0,SOFTWARE
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",git,SOFTWARE
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",conda,SOFTWARE
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",pytorch==2.1.2,SOFTWARE
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",torchvision==0.16.2,SOFTWARE
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",torchaudio==2.1.2,SOFTWARE
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",pytorch-cuda=12.1,SOFTWARE
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",pytorch,SOFTWARE
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",nvidia,SOFTWARE
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",pyg==2.4.0,SOFTWARE
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",pyg,SOFTWARE
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",conda,SOFTWARE
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",conda,SOFTWARE
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",python,SOFTWARE
"Clone the repo:  ``` git clone https://github.com/ml-jku/vnegnn ```  Setup dependencies:  ``` conda create --name vnegnn python=3.9 conda activate vnegnn conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia conda install pyg==2.4.0 -c pyg conda env update --name vnegnn --file environment.yaml ```  Activate the environment:  ``` conda activate vnegnn ```  ## Usage Examples  Following commands can be executed in the directory of the cloned repository.  ### Predict  Provide your protein in `.pdb` format.  ``` python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0               # run prediction on GPU  python predict.py -i protein.pdb -o output -c model.ckpt -d cuda:0 -v            # run prediction on GPU and create visualization ```  #### Predict Output  The model outputs:  - `prediction.csv`: holding the predicted positions (x,y,z) of the virtual nodes and the corresponding ranks - `visualization.pse`: PyMOL session file containing the protein structure with the predicted virtual node positions, only created if the `-v` flag is used.a  ## Data  In our training and evaluation process, we adopted the methodology outlined in [Equipocket](https://arxiv.org/abs/2302.12177) with a modification: we utilized a single validation set in place of the 5-fold cross-validation, due to computational limitations.",python,SOFTWARE
The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,python,SOFTWARE
The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,python,SOFTWARE
The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,python,SOFTWARE
The `lmdb` and avoids processing all the data again if you want to create a graph with different parameters.  ``` python process_dataset.py --input_path data/scpdb/scpdb_subset_puresnet/raw --output_path data/scpdb/scpdb_subset_puresnet/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/coach420/raw --output_path data/bindingsite_test_data/coach420/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k/raw --output_path data/bindingsite_test_data/holo4k/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/holo4k_split_chain/raw --output_path data/bindingsite_test_data/holo4k_split_chain/raw --device cuda:0 --n_jobs 8 python process_dataset.py --input_path data/bindingsite_test_data/pdbbind2020/raw --output_path data/bindingsite_test_data/pdbbind2020/raw --device cuda:0 --n_jobs 8 ```  ### Train  Train the *heterogenous* or *homogenous* model with the parameters as used in the paper.,python,SOFTWARE
"```python python train.py --config-name=config_binding_hetero        # heterogenous model, top performing model in the paper ```  For training on [SLURM](https://www.schedmd.com/) cluster with [submitit](https://github.com/facebookincubator/submitit)  used the `conf/hydra/meluxina.yaml` as blueprint.",python,SOFTWARE
"```python python train.py --config-name=config_binding_hetero        # heterogenous model, top performing model in the paper ```  For training on [SLURM](https://www.schedmd.com/) cluster with [submitit](https://github.com/facebookincubator/submitit)  used the `conf/hydra/meluxina.yaml` as blueprint.",SLURM,SOFTWARE
"```python python train.py --config-name=config_binding_hetero        # heterogenous model, top performing model in the paper ```  For training on [SLURM](https://www.schedmd.com/) cluster with [submitit](https://github.com/facebookincubator/submitit)  used the `conf/hydra/meluxina.yaml` as blueprint.",hydra,SOFTWARE
"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and G√ºnter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",python,SOFTWARE
"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and G√ºnter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",hydra,SOFTWARE
"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and G√ºnter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",SLURM,SOFTWARE
"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and G√ºnter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",hydra,SOFTWARE
"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and G√ºnter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",EGNNClassifierGlobalNodeHetero,SOFTWARE
"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and G√ºnter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",hydra,SOFTWARE
"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and G√ºnter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",pytorch_lightning,SOFTWARE
"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and G√ºnter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",OmegaConf,SOFTWARE
"```python python train.py --config-name=config_binding_hetero hydra=meluxina --multirun        # homogenous model traind on SLURM hydra=meluxina ```  ### Run on test dataset  ```python # Run this from the root of the project from src.models.egnn import EGNNClassifierGlobalNodeHetero from hydra.utils import instantiate import pytorch_lightning as pl  path_to_config = ""/path/to_config/"" cfg = OmegaConf.create(run.config)      datamodule = instantiate(cfg.datamodule) ckpt_file = ""/path/to/your/checkpoint.ckpt""      model = EGNNClassifierGlobalNodeHetero.load_from_checkpoint(     ckpt_file,     strict=False,     segmentation_loss=instantiate(cfg.model.segmentation_loss),     global_node_pos_loss=instantiate(cfg.model.global_node_pos_loss), ) model.eval()  trainer = pl.Trainer(devices=1) loader = datamodule.named_test_loaders[""test_coach420""] # name of dataloader [""test_coach420"", ""test_pdbbind2020"", ""test_holo4k""] trainer.test(model, dataloaders=loader)                 # CAUTION: for the test_holo4k dataset, if you evalute it like this you will get lower scores than reported in the paper, # For the results in the paper we splitted the proteins into chains, run the predictions and combined them (clean code for this procedure will be released in future) # The intuition behind this step is explained in the paper. ```  ## Citation  ``` @misc{sestak2024vnegnn,       title={VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes Enhance Protein Binding Site Identification},        author={Florian Sestak and Lisa Schneckenreiter and Johannes Brandstetter and Sepp Hochreiter and Andreas Mayr and G√ºnter Klambauer},       year={2024},       eprint={2404.07194},       archivePrefix={arXiv},       primaryClass={cs.LG} } ```  ## License  MIT  ## Acknowledgements  The ELLIS Unit Linz, the LIT AI Lab, the Institute for Ma- chine Learning, are supported by the Federal State Upper Austria.",EGNNClassifierGlobalNodeHetero,SOFTWARE
# EntQA  This repo provides the code for our ICLR 2022 paper [EntQA: Entitly Linking as Question Answering](https://arxiv.org/pdf/2110.02369.pdf)  ## Setup  ``` conda create --name entqa python=3.8 conda activate entqa pip install -r requirements.txt conda install -c pytorch faiss-gpu cudatoolkit=11.0  ```  ## Download data & preprocess All the preprocessed data can be downloaded [here](https://drive.google.com/drive/folders/1DQvfjKOuOoUE3YcYrg2GIvODaOEZXMdH?,conda,SOFTWARE
# EntQA  This repo provides the code for our ICLR 2022 paper [EntQA: Entitly Linking as Question Answering](https://arxiv.org/pdf/2110.02369.pdf)  ## Setup  ``` conda create --name entqa python=3.8 conda activate entqa pip install -r requirements.txt conda install -c pytorch faiss-gpu cudatoolkit=11.0  ```  ## Download data & preprocess All the preprocessed data can be downloaded [here](https://drive.google.com/drive/folders/1DQvfjKOuOoUE3YcYrg2GIvODaOEZXMdH?,conda,SOFTWARE
# EntQA  This repo provides the code for our ICLR 2022 paper [EntQA: Entitly Linking as Question Answering](https://arxiv.org/pdf/2110.02369.pdf)  ## Setup  ``` conda create --name entqa python=3.8 conda activate entqa pip install -r requirements.txt conda install -c pytorch faiss-gpu cudatoolkit=11.0  ```  ## Download data & preprocess All the preprocessed data can be downloaded [here](https://drive.google.com/drive/folders/1DQvfjKOuOoUE3YcYrg2GIvODaOEZXMdH?,pip,SOFTWARE
# EntQA  This repo provides the code for our ICLR 2022 paper [EntQA: Entitly Linking as Question Answering](https://arxiv.org/pdf/2110.02369.pdf)  ## Setup  ``` conda create --name entqa python=3.8 conda activate entqa pip install -r requirements.txt conda install -c pytorch faiss-gpu cudatoolkit=11.0  ```  ## Download data & preprocess All the preprocessed data can be downloaded [here](https://drive.google.com/drive/folders/1DQvfjKOuOoUE3YcYrg2GIvODaOEZXMdH?,conda,SOFTWARE
# EntQA  This repo provides the code for our ICLR 2022 paper [EntQA: Entitly Linking as Question Answering](https://arxiv.org/pdf/2110.02369.pdf)  ## Setup  ``` conda create --name entqa python=3.8 conda activate entqa pip install -r requirements.txt conda install -c pytorch faiss-gpu cudatoolkit=11.0  ```  ## Download data & preprocess All the preprocessed data can be downloaded [here](https://drive.google.com/drive/folders/1DQvfjKOuOoUE3YcYrg2GIvODaOEZXMdH?,pytorch,SOFTWARE
# EntQA  This repo provides the code for our ICLR 2022 paper [EntQA: Entitly Linking as Question Answering](https://arxiv.org/pdf/2110.02369.pdf)  ## Setup  ``` conda create --name entqa python=3.8 conda activate entqa pip install -r requirements.txt conda install -c pytorch faiss-gpu cudatoolkit=11.0  ```  ## Download data & preprocess All the preprocessed data can be downloaded [here](https://drive.google.com/drive/folders/1DQvfjKOuOoUE3YcYrg2GIvODaOEZXMdH?,cudatoolkit=11.0,SOFTWARE
Download BLINK pretrained retriever model [here](https://github.com/facebookresearch/BLINK)  \ 3.,BLINK,SOFTWARE
Download BLINK pretrained retriever model [here](https://github.com/facebookresearch/BLINK)  \ 3.,BLINK,SOFTWARE
"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.",python,SOFTWARE
"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.",python,SOFTWARE
"usp=sharing) and put it under /raw_aida/ for remapping outdated entities of AIDA datasets to KILT wikipedia entity titles \ 5. preprocess AIDA data and KILT kb by ``` python preprocess_data.py \ --raw_dir /raw_aida/  --out_aida_dir /retriever_input/  \ --raw_kb_path /raw_kb/[kilt wikipedia file name] --out_kb_path /kb/entities_kilt.json \ --max_ent_len 128  --instance_length 32 --stride 16 --pos_prop 1 --title_map_dir /raw_aida/  ```  ## Train Retriever   Train retriever by  ``` python run_retriever.py \ --model /model_retriever/retriever.pt  --data_dir /retriever_input/   --kb_dir /kb/ \ --k 100 --num_cands 64  --pretrained_path /blink/BLINK/models/ --gpus 0,1,2,3  --max_len 42   \ --mention_bsz 4096 --entity_bsz 2048  --epochs 4  --B 4  --lr 2e-6  --rands_ratio 0.9   \ --logging_step 100 --warmup_proportion 0.2  --out_dir /reader_input/    --gradient_accumulation_steps 2  --type_loss sum_log_nce   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy \ --blink  --add_topic ``` It takes 10 hours on 4 A100 GPUs to finish the retriever training experiments.",BLINK,SOFTWARE
"Use the above training scripts and set `--epochs` to be 0 for evaluation. ### Retrieval Results | val Recall@100 | test Recall@100 | val LRAP | test LRAP | val passage-level Recall@100 | test passage-level Recall@100| |:----------------:|:-----------------:|:----------:|:-----------:|:---------------------:|:---------------------:| |     98.17%     |     96.62%      |  83.98%  |  82.65%   |      97.03%         |        94.59%       |   **Recall@k** is the percentage of total number of positive entities retrieved by the topk candidates with respect to the total number of gold entities for all the query passages. \ **passage-level Recall@k** is the percentage of the number of passages with all the gold entities retrieved in the topk candidates with respect to the number of passages. \ **LRAP** is [Label ranking average precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_average_precision_score.html) which measures the multi-label ranking performance.    ## Train Reader   Train reader by  ``` python run_reader.py  \ --model /model_reader/reader.pt   --data_dir /reader_input/  \ --C 64  --B 2  --L 180  --C_val 100  --gpus 0,1   --val_bsz 32 \ --gradient_accumulation_steps 2  --warmup_proportion 0.06  \ --epochs 4  --lr 1e-5 --thresd  0.05  --logging_steps 100  \ --k 3  --stride 16 --max_passage_len 32  --filter_span  \ --type_encoder squad2_electra_large  \ --type_span_loss sum_log  --type_rank_loss sum_log  \ --do_rerank  --add_topic  --results_dir /reader_results/  --kb_dir /kb/  ``` It takes about 6 hours on 2 A100 GPUs to finish the reader training experiment.",python,SOFTWARE
One one terminal/screen run GERBIL by: ``` cd gerbil .,terminal,SOFTWARE
One one terminal/screen run GERBIL by: ``` cd gerbil .,GERBIL,SOFTWARE
On another terminal/screen run: ``` cd gerbil-SpotWrapNifWS4Test/ mvn clean -Dmaven.tomcat.port=1235 tomcat:run  ``` 4.,terminal,SOFTWARE
"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",terminal,SOFTWARE
"On a third terminal/screen run: ``` python gerbil_experiments/server.py  \ --gpus 0,1,2,3  --log_path  /logs/log.txt --blink_dir //blink_model/  \ --retriever_path /model_retriever/retriever.pt   \ --cands_embeds_path /candidates_embeds/candidate_embeds.npy   \ --ents_path /kb/entities_kilt.json  \ --reader_path /model_reader/reader.pt   \ --add_topic  --do_rerank  --bsz_retriever 8192    ``` Open the url http://localhost:1234/gerbil - Configure experiment - Select A2KB as experiment type - Select strong evaluation as Matching - In URI field write: http://localhost:1235/gerbil-spotWrapNifWS4Test/myalgorithm - Name: whatever you wish - Press add annotator button - Select the datasets that you want to evaluate the model on - Run experiment  ### GERBIL Results | AIDA testb| MSNBC| Der|K50|R128|R500|OKE15|OKE16|AVG| |:---------:|:--------:|:----:|:---:|:----:|:----:|:-----:|:-----:|:----:| |85.82%|72.09%|52.85%|64.46%|54.05%|41.93%|61.10%|51.34%|60.46%|",python,SOFTWARE
"# Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites  ## About  The Forschungszentrum J√ºlich Machine Learning Library  It is currently being developed and maintained at the [Applied Machine Learning](https://www.fz-juelich.de/en/inm/inm-7/research-groups/applied-machine-learning-aml) group at [Forschungszentrum Juelich](https://www.fz-juelich.de/en), Germany.   ## Overview  **PrettYharmonize** is a Python package developed to address data leakage in the harmonization of biomedical datasets with site-specific variability, particularly under scenarios where class balance differs across data collection sites.",PrettYharmonize,SOFTWARE
"[Workflow of PrettYharmonize](figures/PrettYharmonize_workflow.png) *Figure 1: Workflow for the PrettYharmonize data harmonization approach, outlining key stages in data processing.*  For more details, see our paper on arXiv: [Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites](https://arxiv.org/abs/2410.19643).  ## Installation  To set up the environment for PrettYharmonize, follow these steps:  1.",PrettYharmonize,SOFTWARE
"[Workflow of PrettYharmonize](figures/PrettYharmonize_workflow.png) *Figure 1: Workflow for the PrettYharmonize data harmonization approach, outlining key stages in data processing.*  For more details, see our paper on arXiv: [Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites](https://arxiv.org/abs/2410.19643).  ## Installation  To set up the environment for PrettYharmonize, follow these steps:  1.",PrettYharmonize,SOFTWARE
"[Workflow of PrettYharmonize](figures/PrettYharmonize_workflow.png) *Figure 1: Workflow for the PrettYharmonize data harmonization approach, outlining key stages in data processing.*  For more details, see our paper on arXiv: [Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites](https://arxiv.org/abs/2410.19643).  ## Installation  To set up the environment for PrettYharmonize, follow these steps:  1.",PrettYharmonize,SOFTWARE
"[Workflow of PrettYharmonize](figures/PrettYharmonize_workflow.png) *Figure 1: Workflow for the PrettYharmonize data harmonization approach, outlining key stages in data processing.*  For more details, see our paper on arXiv: [Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites](https://arxiv.org/abs/2410.19643).  ## Installation  To set up the environment for PrettYharmonize, follow these steps:  1.",PrettYharmonize,SOFTWARE
"**Clone the repository:**     ```bash    git clone https://github.com/juaml/PrettYharmonize.git    cd PrettYharmonize   ## Citation ```bibtex If you use PrettYharmonize in your work, please cite the following: @article{nieto2024impact,   title={Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites},   author={Nieto, Nicol{\'a}s and Eickhoff, Simon B and Jung, Christian and Reuter, Martin and Diers, Kersten and Kelm, Malte and Lichtenberg, Artur and Raimondo, Federico and Patil, Kaustubh R},   journal={arXiv preprint arXiv:2410.19643},   year={2024} } ```  ## Licensing  preattyharmonize is released under the AGPL v3 license:  preattyharmonize, FZJuelich AML machine learning library.",git,SOFTWARE
"**Clone the repository:**     ```bash    git clone https://github.com/juaml/PrettYharmonize.git    cd PrettYharmonize   ## Citation ```bibtex If you use PrettYharmonize in your work, please cite the following: @article{nieto2024impact,   title={Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites},   author={Nieto, Nicol{\'a}s and Eickhoff, Simon B and Jung, Christian and Reuter, Martin and Diers, Kersten and Kelm, Malte and Lichtenberg, Artur and Raimondo, Federico and Patil, Kaustubh R},   journal={arXiv preprint arXiv:2410.19643},   year={2024} } ```  ## Licensing  preattyharmonize is released under the AGPL v3 license:  preattyharmonize, FZJuelich AML machine learning library.",PrettYharmonize,SOFTWARE
"**Clone the repository:**     ```bash    git clone https://github.com/juaml/PrettYharmonize.git    cd PrettYharmonize   ## Citation ```bibtex If you use PrettYharmonize in your work, please cite the following: @article{nieto2024impact,   title={Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites},   author={Nieto, Nicol{\'a}s and Eickhoff, Simon B and Jung, Christian and Reuter, Martin and Diers, Kersten and Kelm, Malte and Lichtenberg, Artur and Raimondo, Federico and Patil, Kaustubh R},   journal={arXiv preprint arXiv:2410.19643},   year={2024} } ```  ## Licensing  preattyharmonize is released under the AGPL v3 license:  preattyharmonize, FZJuelich AML machine learning library.",PrettYharmonize,SOFTWARE
"**Clone the repository:**     ```bash    git clone https://github.com/juaml/PrettYharmonize.git    cd PrettYharmonize   ## Citation ```bibtex If you use PrettYharmonize in your work, please cite the following: @article{nieto2024impact,   title={Impact of Leakage on Data Harmonization in Machine Learning Pipelines in Class Imbalance Across Sites},   author={Nieto, Nicol{\'a}s and Eickhoff, Simon B and Jung, Christian and Reuter, Martin and Diers, Kersten and Kelm, Malte and Lichtenberg, Artur and Raimondo, Federico and Patil, Kaustubh R},   journal={arXiv preprint arXiv:2410.19643},   year={2024} } ```  ## Licensing  preattyharmonize is released under the AGPL v3 license:  preattyharmonize, FZJuelich AML machine learning library.",PrettYharmonize,SOFTWARE
"# Open Datasheets  The Open Datasheets framework is a simple, standardized no-code way to document datasets.",Open Datasheets,SOFTWARE
"# Open Datasheets  The Open Datasheets framework is a simple, standardized no-code way to document datasets.",Open Datasheets,SOFTWARE
"By integrating directly with GitHub, the framework allows you to create, edit, and export your Open Datasheets directly to GitHub.",Open Datasheets,SOFTWARE
"For more detailed information, you can refer to our paper titled ""[Open Datasheets: Machine-readable Documentation for Open Datasets and Responsible AI Assessments](https://arxiv.org/abs/2312.06153)""",Open Datasheets,SOFTWARE
"# Try the No-Code App We have created a no-code web tool to help you create, edit, generate, and export an Open Datasheets `datapackage.json` file for your datasets directly to/from GitHub.",Open Datasheets,SOFTWARE
You can try it out here: [https://microsoft.github.io/opendatasheets](https://microsoft.github.io/opendatasheets).,opendatasheets,SOFTWARE
You can try it out here: [https://microsoft.github.io/opendatasheets](https://microsoft.github.io/opendatasheets).,opendatasheets,SOFTWARE
"With the no-code tool, you have the ability to: - Create a new Open Datasheets datapackage from scratch. - Generate a new Open Datasheets datapackage from data located on GitHub or a local file. - Modify an existing Open Datasheets datapackage file from GitHub or a local file. - Save an Open Datasheets datapackage file to GitHub or locally.",Open Datasheets,SOFTWARE
"With the no-code tool, you have the ability to: - Create a new Open Datasheets datapackage from scratch. - Generate a new Open Datasheets datapackage from data located on GitHub or a local file. - Modify an existing Open Datasheets datapackage file from GitHub or a local file. - Save an Open Datasheets datapackage file to GitHub or locally.",Open Datasheets,SOFTWARE
"With the no-code tool, you have the ability to: - Create a new Open Datasheets datapackage from scratch. - Generate a new Open Datasheets datapackage from data located on GitHub or a local file. - Modify an existing Open Datasheets datapackage file from GitHub or a local file. - Save an Open Datasheets datapackage file to GitHub or locally.",Open Datasheets,SOFTWARE
"With the no-code tool, you have the ability to: - Create a new Open Datasheets datapackage from scratch. - Generate a new Open Datasheets datapackage from data located on GitHub or a local file. - Modify an existing Open Datasheets datapackage file from GitHub or a local file. - Save an Open Datasheets datapackage file to GitHub or locally.",Open Datasheets,SOFTWARE
"/assets/demo.gif)  # Top level schema  | Property | Description | Key Elements for Fostering Responsible AI | | --- | --- | --- | | name | The name of the data package. |  | | title | The title of the data package. | | | description | The description of the data package. |  | | version | The version of the data package. |  | | created | The date when the data package was created. |  | | licenses | The licenses under which the data package is distributed. |  | | sources | The sources from which the data package is derived. |  | | resources | The resources included in the data package. |  | | contributors | The contributors to the data package. | | | privacy | The privacy considerations for the data package. | &#x2705; | | security | The security considerations for the data package. | &#x2705; | | procedures | The collection, processing, and update procedures for the data package. | &#x2705; | | use | The permitted use of the data package. | &#x2705; |  For the complete schema, please refer to the [Open Datasheets schema](.",Open Datasheets,SOFTWARE
"For more detailed information about the responsible AI properties, such as collection procedures, privacy, and security considerations, please refer to the [Open Datasheets paper](https://arxiv.org/pdf/2312.06153).  ## Contributing  This project welcomes contributions and suggestions.",Open Datasheets,SOFTWARE
Other required packages can be installed with `python3 -m pip install -r requirements.txt` from the main repository folder.,python3,SOFTWARE
Other required packages can be installed with `python3 -m pip install -r requirements.txt` from the main repository folder.,pip,SOFTWARE
"- English test_none dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_none.tsv    - Spanish test_none dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_none.tsv    - Portuguese test_none dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_none.tsv  The *test_gold* files contain the sentences, target complex words, and gold annotations<br/>   - English test_gold dataset (373 instances)<br/>  /datasets/test/tsar2022_en_test_gold.tsv    - Spanish test_gold dataset (368 instances)<br/>  /datasets/test/tsar2022_es_test_gold.tsv    - Portuguese test_gold dataset (374 instances)<br/>  /datasets/test/tsar2022_pt_test_gold.tsv   ## Results of the Evaluation Benchmark  The official results for each language (en, es, and pt) can be found in this directory:<br/>  /results/official  The following 10 metrics are reported in the official results: -  MAP@1/Potential@1/Precision@1 -  MAP@3 -  MAP@5 -  MAP@10 -  Potential@3 -  Potential@5 -  Potential@10 -  Accuracy@1@top_gold_1 -  Accuracy@2@top_gold_1 -  Accuracy@3@top_gold_1    The extended results for each language (en, es, and pt) can be found in this directory:<br/>  /results/extended<br/>   The following metrics are reported in the extended results: -  Potential@K  K={1..10}  -  MAP@K  K={1..10} -  Precision@K  K={1..10}  (macro-average) -  Recall@K  K={1..10}     (macro-average) -  Accuracy@K@top_gold_1   K={1..10}     ## Evaluation Scripts   ### tsar_eval.py  This script evaluates the following metric:      -  MAP@1/Potential@1/Precision@1     -  MAP@3     -  MAP@5     -  MAP@10     -  Potential@3     -  Potential@5     -  Potential@10     -  Accuracy@1@top_gold_1     -  Accuracy@2@top_gold_1     -  Accuracy@3@top_gold_1          Script options and help  ```console Evaluation Script for the TSAR-2022 Lexical Simplification Shared Task  Usage: tsar_eval.py <options>  Options:   -h, --help            show this help message and exit   --gold_file=<PATH>    The path to the file with the gold annotated instances   --predictions_file=<PATH>                         The path to file with the predictions   --output_file=<PATH>  path to the output file   --verbose             Verbose output mode ```   usage example  ```console python3 .",python3,SOFTWARE
[Pytorch 2.0](https://img.shields.io/badge/PyTorch-2.0+-lightblue) !,Pytorch 2.0,SOFTWARE
[Pytorch 2.0](https://img.shields.io/badge/PyTorch-2.0+-lightblue) !,PyTorch-2.0,SOFTWARE
[transformers](https://img.shields.io/badge/transformers-4.36.0.dev0%2B-lightblue) !,transformers,SOFTWARE
[transformers](https://img.shields.io/badge/transformers-4.36.0.dev0%2B-lightblue) !,transformers-4.36.0.dev0,SOFTWARE
"[accelerate](https://img.shields.io/badge/accelerate-0.22+-lightblue) </center>  <p align=""center"">    üìÉ  <a href=""https://arxiv.org/abs/2311.13951"" target=""_blank"">Paper</a> ‚Ä¢ üåê  <a href=""https://mllm-bench.llmzoo.com/"" target=""_blank"">Website</a> ‚Ä¢ ü§ó  <a href=""huggingface.com"" target=""_blank"">HuggingFace</a>    <p align=""center""> <img src="".",accelerate,SOFTWARE
"[accelerate](https://img.shields.io/badge/accelerate-0.22+-lightblue) </center>  <p align=""center"">    üìÉ  <a href=""https://arxiv.org/abs/2311.13951"" target=""_blank"">Paper</a> ‚Ä¢ üåê  <a href=""https://mllm-bench.llmzoo.com/"" target=""_blank"">Website</a> ‚Ä¢ ü§ó  <a href=""huggingface.com"" target=""_blank"">HuggingFace</a>    <p align=""center""> <img src="".",accelerate-0.22,SOFTWARE
üéâ üéâ üéâ   ## Leaderboard We present the results of voting using LLaVA-v1.5-13B as anchor.,LLaVA-v1.5-13B,SOFTWARE
The numbers denote *win/tie/lose* of a benchmarked model over LLaVA-v1.5-13B.,LLaVA-v1.5-13B,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,LLaVA-v1.5-13B,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,GPT-4o,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,Claude-3,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,GPT-4V,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,LLaVA-v1.6-34B,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,LLaVA-v1.6-Vicuna-13B,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,LLaVA-v1.6-Vicuna-7B,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,ALLaVA-3B-Longer,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,Gemini-1.0-Pro,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,Qwen-VL-Chat,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,LVIS,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,mPLUG-Owl2,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,LLaVA-v1.5-7B,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,MiniGPT-v2,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,InstructBLIP,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,Cheetor,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,SEED-LLaMA,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,kosmos2,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,Yi-VL-6B,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,Fuyu-8B,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,LWM,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,OpenFlamingo,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,BLIP2,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,bash,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,pip,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,transformers,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,bash,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,pip,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,git,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,transformers,SOFTWARE
| **Rank** | **Models**       | **Perception**  | **Understanding** | **Applying** | **Analyzing** | **Evaluation** | **Creation** | **Win Rates over LLaVA-v1.5-13B** | |------|-----------------------|-------------|---------------|----------|-----------|------------|----------|-----------------------| | üèÖÔ∏è   | GPT-4o             | 64/5/1     | 98/11/1        | 50/8/2  | 86/9/5   | 40/0/0     | 38/1/1   | 0.90                  | | ü•à   | Claude-3              | 56/13/1     | 98/9/3        | 45/11/4  | 83/14/3   | 33/5/2     | 33/6/1   | 0.83                  | | ü•â   | GPT-4V                | 56/10/4     | 101/6/3       | 29/12/19 | 73/22/5   | 33/2/5     | 2/0/38   | 0.70                  | | 4  | LLaVA-v1.6-34B        | 46/17/7     | 78/22/10      | 36/15/9  | 61/28/11  | 33/3/4     | 24/10/6  | 0.66                  | | 5    | LLaVA-v1.6-Vicuna-13B | 40/21/9     | 65/33/12      | 35/19/6  | 51/26/23  | 33/5/2     | 27/9/4   | 0.60                  | | 6   | LLaVA-v1.6-Vicuna-7B  | 31/25/14    | 56/37/17      | 26/23/11 | 40/31/29  | 22/10/8    | 19/10/11 | 0.46                  | | 7    | ALLaVA-3B-Longer      | 22/21/27    | 57/30/23      | 23/17/20 | 44/30/26  | 16/10/14   | 17/12/11 | 0.43                  | | 8    | Gemini-1.0-Pro        | 45/10/15    | 36/35/39      | 24/19/17 | 33/28/39  | 9/8/23     | 16/8/16  | 0.39                  | | 9    | Qwen-VL-Chat          | 34/22/14    | 38/36/36      | 26/18/16 | 35/29/36  | 15/6/19    | 9/12/19  | 0.37                  | | 10    | LVIS                  | 22/28/20    | 32/39/39      | 11/27/22 | 33/36/31  | 14/9/17    | 9/16/15  | 0.29                  | | 11   | mPLUG-Owl2            | 16/24/30    | 30/34/46      | 17/17/26 | 23/38/39  | 15/8/17    | 11/14/15 | 0.27                  | | 12   | LLaVA-v1.5-7B         | 19/22/29    | 27/47/36      | 13/29/18 | 21/43/36  | 9/14/17    | 8/13/19  | 0.23                  | | 13   | MiniGPT-v2            | 12/25/33    | 24/32/54      | 11/25/24 | 17/38/45  | 9/9/22     | 6/6/28   | 0.19                  | | 14   | InstructBLIP          | 15/16/39    | 13/36/61      | 6/23/31  | 13/29/58  | 10/7/23    | 4/9/27   | 0.15                  | | 15   | Cheetor               | 12/20/38    | 7/27/76       | 10/22/28 | 16/23/61  | 4/4/32     | 3/4/33   | 0.12                  | | 16   | SEED-LLaMA            | 16/15/39    | 5/25/80       | 10/21/29 | 7/25/68   | 3/7/30     | 3/3/34   | 0.10                  | | 17   | kosmos2               | 6/22/42     | 6/18/86       | 6/15/39  | 10/20/70  | 1/4/35     | 2/3/35   | 0.07                  | | 18   | Yi-VL-6B              | 4/17/49     | 8/22/80       | 5/27/28  | 5/29/66   | 3/9/28     | 3/9/28   | 0.07                  | | 19   | Fuyu-8B               | 7/19/44     | 7/27/76       | 6/14/40  | 4/22/74   | 3/7/30     | 0/6/34   | 0.06                  | | 20   | LWM                   | 2/18/50     | 5/15/90       | 4/21/35  | 2/18/80   | 3/2/35     | 2/6/32   | 0.04                  | | 21   | OpenFlamingo          | 8/13/49     | 2/8/100       | 3/14/43  | 2/21/77   | 1/2/37     | 1/5/34   | 0.04                  | | 22   | BLIP2                 | 3/13/54     | 2/15/93       | 6/8/46   | 0/22/78   | 0/1/39     | 0/2/38   | 0.03                  |    ## Usage ### Environment Setup <details><summary>Click to expand</summary>     Install required packages: ```bash pip install -r requirements.txt ``` Update `transformers` (we used `4.36.0.dev0`): ```bash pip install git+https://github.com/huggingface/transformers ```  </details>    ### Answer Generation <details><summary>Click to expand</summary>  - Configurate `accelerate` settings.,accelerate,SOFTWARE
- Run `bash generate.sh`.,bash,SOFTWARE
- Run `bash evaluate.sh`,bash,SOFTWARE
- Run `cd scripts & bash evaluate4elo.sh` for elo rating,bash,SOFTWARE
"Ashley, Vincent Herrmann, Zachary Friggstad, and J√ºrgen Schmidhuber.   ## Installation  This project is implemented in [Python](https://www.python.org/) and uses models learned with [PyTorch](https://pytorch.org).",PyTorch,SOFTWARE
"Ashley, Vincent Herrmann, Zachary Friggstad, and J√ºrgen Schmidhuber.   ## Installation  This project is implemented in [Python](https://www.python.org/) and uses models learned with [PyTorch](https://pytorch.org).",pytorch,SOFTWARE
"Before installation, first, ensure you have a recent version of Python and pip, then clone the repository using the `--sparse` option: ```bash git clone --sparse git@github.com:dylanashley/story-distiller.git ```  Afterwards, install the Python dependencies using pip: ```bash pip install -r requirements.txt ```  To use the full suite of file types supported by the tool, it is also necessary to separately install the [ffmpeg](https://ffmpeg.org/) tool by following the instructions for your operating system provided on [their website](https://ffmpeg.org/download.html).",Python,SOFTWARE
"Before installation, first, ensure you have a recent version of Python and pip, then clone the repository using the `--sparse` option: ```bash git clone --sparse git@github.com:dylanashley/story-distiller.git ```  Afterwards, install the Python dependencies using pip: ```bash pip install -r requirements.txt ```  To use the full suite of file types supported by the tool, it is also necessary to separately install the [ffmpeg](https://ffmpeg.org/) tool by following the instructions for your operating system provided on [their website](https://ffmpeg.org/download.html).",pip,SOFTWARE
"Before installation, first, ensure you have a recent version of Python and pip, then clone the repository using the `--sparse` option: ```bash git clone --sparse git@github.com:dylanashley/story-distiller.git ```  Afterwards, install the Python dependencies using pip: ```bash pip install -r requirements.txt ```  To use the full suite of file types supported by the tool, it is also necessary to separately install the [ffmpeg](https://ffmpeg.org/) tool by following the instructions for your operating system provided on [their website](https://ffmpeg.org/download.html).",git,SOFTWARE
"Before installation, first, ensure you have a recent version of Python and pip, then clone the repository using the `--sparse` option: ```bash git clone --sparse git@github.com:dylanashley/story-distiller.git ```  Afterwards, install the Python dependencies using pip: ```bash pip install -r requirements.txt ```  To use the full suite of file types supported by the tool, it is also necessary to separately install the [ffmpeg](https://ffmpeg.org/) tool by following the instructions for your operating system provided on [their website](https://ffmpeg.org/download.html).",git,SOFTWARE
"Before installation, first, ensure you have a recent version of Python and pip, then clone the repository using the `--sparse` option: ```bash git clone --sparse git@github.com:dylanashley/story-distiller.git ```  Afterwards, install the Python dependencies using pip: ```bash pip install -r requirements.txt ```  To use the full suite of file types supported by the tool, it is also necessary to separately install the [ffmpeg](https://ffmpeg.org/) tool by following the instructions for your operating system provided on [their website](https://ffmpeg.org/download.html).",pip,SOFTWARE
"Before installation, first, ensure you have a recent version of Python and pip, then clone the repository using the `--sparse` option: ```bash git clone --sparse git@github.com:dylanashley/story-distiller.git ```  Afterwards, install the Python dependencies using pip: ```bash pip install -r requirements.txt ```  To use the full suite of file types supported by the tool, it is also necessary to separately install the [ffmpeg](https://ffmpeg.org/) tool by following the instructions for your operating system provided on [their website](https://ffmpeg.org/download.html).",pip,SOFTWARE
"Before installation, first, ensure you have a recent version of Python and pip, then clone the repository using the `--sparse` option: ```bash git clone --sparse git@github.com:dylanashley/story-distiller.git ```  Afterwards, install the Python dependencies using pip: ```bash pip install -r requirements.txt ```  To use the full suite of file types supported by the tool, it is also necessary to separately install the [ffmpeg](https://ffmpeg.org/) tool by following the instructions for your operating system provided on [their website](https://ffmpeg.org/download.html).",ffmpeg,SOFTWARE
"Before installation, first, ensure you have a recent version of Python and pip, then clone the repository using the `--sparse` option: ```bash git clone --sparse git@github.com:dylanashley/story-distiller.git ```  Afterwards, install the Python dependencies using pip: ```bash pip install -r requirements.txt ```  To use the full suite of file types supported by the tool, it is also necessary to separately install the [ffmpeg](https://ffmpeg.org/) tool by following the instructions for your operating system provided on [their website](https://ffmpeg.org/download.html).",ffmpeg,SOFTWARE
"Before installation, first, ensure you have a recent version of Python and pip, then clone the repository using the `--sparse` option: ```bash git clone --sparse git@github.com:dylanashley/story-distiller.git ```  Afterwards, install the Python dependencies using pip: ```bash pip install -r requirements.txt ```  To use the full suite of file types supported by the tool, it is also necessary to separately install the [ffmpeg](https://ffmpeg.org/) tool by following the instructions for your operating system provided on [their website](https://ffmpeg.org/download.html).",ffmpeg,SOFTWARE
"You can now run the tool by directly executing the `__main__.py` file, or‚Äîif you're using Linux or macOS‚Äîyou can use the makefile to compile and then install an executable Python zip archive: ```bash make all sudo make install ```   ## Command-line Tool Usage  To run the program, execute it while passing the audio files as command-line arguments: ```bash sdistil files [files ...] >> playlist.txt ```  [librosa](https://librosa.org/doc/latest/index.html) is used to process the audio files, so most common audio file types are supported.",Linux,SOFTWARE
"You can now run the tool by directly executing the `__main__.py` file, or‚Äîif you're using Linux or macOS‚Äîyou can use the makefile to compile and then install an executable Python zip archive: ```bash make all sudo make install ```   ## Command-line Tool Usage  To run the program, execute it while passing the audio files as command-line arguments: ```bash sdistil files [files ...] >> playlist.txt ```  [librosa](https://librosa.org/doc/latest/index.html) is used to process the audio files, so most common audio file types are supported.",macOS,SOFTWARE
"You can now run the tool by directly executing the `__main__.py` file, or‚Äîif you're using Linux or macOS‚Äîyou can use the makefile to compile and then install an executable Python zip archive: ```bash make all sudo make install ```   ## Command-line Tool Usage  To run the program, execute it while passing the audio files as command-line arguments: ```bash sdistil files [files ...] >> playlist.txt ```  [librosa](https://librosa.org/doc/latest/index.html) is used to process the audio files, so most common audio file types are supported.",bash,SOFTWARE
"You can now run the tool by directly executing the `__main__.py` file, or‚Äîif you're using Linux or macOS‚Äîyou can use the makefile to compile and then install an executable Python zip archive: ```bash make all sudo make install ```   ## Command-line Tool Usage  To run the program, execute it while passing the audio files as command-line arguments: ```bash sdistil files [files ...] >> playlist.txt ```  [librosa](https://librosa.org/doc/latest/index.html) is used to process the audio files, so most common audio file types are supported.",bash,SOFTWARE
"You can now run the tool by directly executing the `__main__.py` file, or‚Äîif you're using Linux or macOS‚Äîyou can use the makefile to compile and then install an executable Python zip archive: ```bash make all sudo make install ```   ## Command-line Tool Usage  To run the program, execute it while passing the audio files as command-line arguments: ```bash sdistil files [files ...] >> playlist.txt ```  [librosa](https://librosa.org/doc/latest/index.html) is used to process the audio files, so most common audio file types are supported.",librosa,SOFTWARE
"You can now run the tool by directly executing the `__main__.py` file, or‚Äîif you're using Linux or macOS‚Äîyou can use the makefile to compile and then install an executable Python zip archive: ```bash make all sudo make install ```   ## Command-line Tool Usage  To run the program, execute it while passing the audio files as command-line arguments: ```bash sdistil files [files ...] >> playlist.txt ```  [librosa](https://librosa.org/doc/latest/index.html) is used to process the audio files, so most common audio file types are supported.",librosa,SOFTWARE
"raw=true)   ## Web App Usage  To run the web app, simply execute the `app.py` file: ```bash .",bash,SOFTWARE
"In particular, it includes - the scripts needed to learn the PyTorch models for extracting the narrative essence from either music albums (`album_extractor.py`) or movie frames (`movie_extractor.py`) and compute the lower bounds for the mutual information of the different features, - the scripts for learning a set of template curves from scalar descriptions of items in a set of collections (`template_learner.py`), - the code that can fit the scalar descriptions of items in a collection to a given template curve (`fit_values` in `__main__.py`), - the preprocessed album data used to train the original PyTorch models for the music albums (`data/`), and - the learned PyTorch models and template curves (`results/`).",PyTorch,SOFTWARE
"In particular, it includes - the scripts needed to learn the PyTorch models for extracting the narrative essence from either music albums (`album_extractor.py`) or movie frames (`movie_extractor.py`) and compute the lower bounds for the mutual information of the different features, - the scripts for learning a set of template curves from scalar descriptions of items in a set of collections (`template_learner.py`), - the code that can fit the scalar descriptions of items in a collection to a given template curve (`fit_values` in `__main__.py`), - the preprocessed album data used to train the original PyTorch models for the music albums (`data/`), and - the learned PyTorch models and template curves (`results/`).",PyTorch,SOFTWARE
"In particular, it includes - the scripts needed to learn the PyTorch models for extracting the narrative essence from either music albums (`album_extractor.py`) or movie frames (`movie_extractor.py`) and compute the lower bounds for the mutual information of the different features, - the scripts for learning a set of template curves from scalar descriptions of items in a set of collections (`template_learner.py`), - the code that can fit the scalar descriptions of items in a collection to a given template curve (`fit_values` in `__main__.py`), - the preprocessed album data used to train the original PyTorch models for the music albums (`data/`), and - the learned PyTorch models and template curves (`results/`).",PyTorch,SOFTWARE
"Note that to obtain the preprocessed data and the learned PyTorch models and template curves, you will have to clone the repository without using the `--sparse` option: ```bash git clone git@github.com:dylanashley/story-distiller.git ```  At this time, the raw data used to train the movie frame extractor is not possible to release due to copyright issues.",PyTorch,SOFTWARE
"Note that to obtain the preprocessed data and the learned PyTorch models and template curves, you will have to clone the repository without using the `--sparse` option: ```bash git clone git@github.com:dylanashley/story-distiller.git ```  At this time, the raw data used to train the movie frame extractor is not possible to release due to copyright issues.",git,SOFTWARE
"Note that to obtain the preprocessed data and the learned PyTorch models and template curves, you will have to clone the repository without using the `--sparse` option: ```bash git clone git@github.com:dylanashley/story-distiller.git ```  At this time, the raw data used to train the movie frame extractor is not possible to release due to copyright issues.",git,SOFTWARE
Preprocessing to extract CLIP features from images can be done straightforwardly using the [official implementation](https://github.com/openai/CLIP).,CLIP,SOFTWARE
Preprocessing to extract CLIP features from images can be done straightforwardly using the [official implementation](https://github.com/openai/CLIP).,CLIP,SOFTWARE
"# MeanSum: A Model for Unsupervised Neural Multi-Document Abstractive Summarization  Corresponding paper, accepted to ICML 2019: [https://arxiv.org/abs/1810.05739](https://arxiv.org/abs/1810.05739).  ## Requirements  Main requirements: - python 3 - torch 0.4.0  Rest of python packages in ```requirements.txt```.",MeanSum,SOFTWARE
"# MeanSum: A Model for Unsupervised Neural Multi-Document Abstractive Summarization  Corresponding paper, accepted to ICML 2019: [https://arxiv.org/abs/1810.05739](https://arxiv.org/abs/1810.05739).  ## Requirements  Main requirements: - python 3 - torch 0.4.0  Rest of python packages in ```requirements.txt```.",python 3,SOFTWARE
"# MeanSum: A Model for Unsupervised Neural Multi-Document Abstractive Summarization  Corresponding paper, accepted to ICML 2019: [https://arxiv.org/abs/1810.05739](https://arxiv.org/abs/1810.05739).  ## Requirements  Main requirements: - python 3 - torch 0.4.0  Rest of python packages in ```requirements.txt```.",torch 0.4.0,SOFTWARE
"# MeanSum: A Model for Unsupervised Neural Multi-Document Abstractive Summarization  Corresponding paper, accepted to ICML 2019: [https://arxiv.org/abs/1810.05739](https://arxiv.org/abs/1810.05739).  ## Requirements  Main requirements: - python 3 - torch 0.4.0  Rest of python packages in ```requirements.txt```.",python,SOFTWARE
"Tested in Docker, image = ```pytorch/pytorch:0.4_cuda9_cudnn7```.  ## General setup   Execute inside ```scripts/```:  ##### Create directories that aren't part of the Git repo (checkpoints/, outputs/):  ``` bash setup_dirs.sh ```  ##### Install python packages:  ``` bash install_python_pkgs.sh ```  ##### The default parameters for Tensorboard(x?)",Docker,SOFTWARE
"Tested in Docker, image = ```pytorch/pytorch:0.4_cuda9_cudnn7```.  ## General setup   Execute inside ```scripts/```:  ##### Create directories that aren't part of the Git repo (checkpoints/, outputs/):  ``` bash setup_dirs.sh ```  ##### Install python packages:  ``` bash install_python_pkgs.sh ```  ##### The default parameters for Tensorboard(x?)",pytorch,SOFTWARE
"Tested in Docker, image = ```pytorch/pytorch:0.4_cuda9_cudnn7```.  ## General setup   Execute inside ```scripts/```:  ##### Create directories that aren't part of the Git repo (checkpoints/, outputs/):  ``` bash setup_dirs.sh ```  ##### Install python packages:  ``` bash install_python_pkgs.sh ```  ##### The default parameters for Tensorboard(x?)",pytorch,SOFTWARE
"Tested in Docker, image = ```pytorch/pytorch:0.4_cuda9_cudnn7```.  ## General setup   Execute inside ```scripts/```:  ##### Create directories that aren't part of the Git repo (checkpoints/, outputs/):  ``` bash setup_dirs.sh ```  ##### Install python packages:  ``` bash install_python_pkgs.sh ```  ##### The default parameters for Tensorboard(x?)",Git,SOFTWARE
"Tested in Docker, image = ```pytorch/pytorch:0.4_cuda9_cudnn7```.  ## General setup   Execute inside ```scripts/```:  ##### Create directories that aren't part of the Git repo (checkpoints/, outputs/):  ``` bash setup_dirs.sh ```  ##### Install python packages:  ``` bash install_python_pkgs.sh ```  ##### The default parameters for Tensorboard(x?)",bash,SOFTWARE
"Tested in Docker, image = ```pytorch/pytorch:0.4_cuda9_cudnn7```.  ## General setup   Execute inside ```scripts/```:  ##### Create directories that aren't part of the Git repo (checkpoints/, outputs/):  ``` bash setup_dirs.sh ```  ##### Install python packages:  ``` bash install_python_pkgs.sh ```  ##### The default parameters for Tensorboard(x?)",bash,SOFTWARE
"Tested in Docker, image = ```pytorch/pytorch:0.4_cuda9_cudnn7```.  ## General setup   Execute inside ```scripts/```:  ##### Create directories that aren't part of the Git repo (checkpoints/, outputs/):  ``` bash setup_dirs.sh ```  ##### Install python packages:  ``` bash install_python_pkgs.sh ```  ##### The default parameters for Tensorboard(x?)",Tensorboard,SOFTWARE
Update by:  ``` python update_tensorboard.py ```    ## Downloading data and pretrained models  ### Data  1.,python,SOFTWARE
"Run script to pre-process script and create train, val, test splits:     ```     bash scripts/preprocess_data.sh     ``` 3.",bash,SOFTWARE
"Hence, for a machine with a single GPU, you will give gpus=0 ``` python train_sum.py --mode=test --gpus=0 --batch_size=16 --notes=<run_name> ```  Training summarization model (using pre-trained language model and default hyperparams).",python,SOFTWARE
"The automated metrics results will be in ```checkpoints/sum/mlstm/yelp/<hparams>_<additional_notes>```.: ``` python train_sum.py --batch_size=16 --gpus=0,1,2,3 --notes=<additional_notes>  ```",python,SOFTWARE
It can be fetched using: ``` gsutil cp gs://sfr-books-dataset-chapters-research/all_chapterized_books.zip . ```  or downloaded directly [here](https://storage.cloud.google.com/sfr-books-dataset-chapters-research/all_chapterized_books.zip).  #### 2.,gsutil,SOFTWARE
"Additionally, `get_works.py` can be used to collect an exhaustive set of summaries from that source.  ``` cd scripts/data_collection/cliffnotes/ python get_summaries.py ```  #### 3.",python,SOFTWARE
Perform basic cleanup operations and setup the summary text for splitting and further cleaning operations     ```     cd scripts/data_cleaning_scripts/     python basic_clean.py     ```  2.,python,SOFTWARE
"Chapters 1-3 summary separated into 3 different sections - Chapter 1 summary, Chapter 2 summary, Chapter 3 summary)     ```     python split_aggregate_chaps_all_sources.py     ```  3.",python,SOFTWARE
"The main cleanup script separates out analysis/commentary/notes from the summary text, removes prefixes etc.     ```     python clean_summaries.py     ```  #### Data Alignments Generating paragraph alignments from the chapter-level-summary-alignments, is performed individually for the train/test/val splits:  Gather the data from the summaries and book chapters into a single jsonl.",python,SOFTWARE
The script needs to be run separately for each split as the matched file ``` cd paragraph-level-summary-alignments python gather_data.py --matched_file /path/to/chapter_summary_aligned_{train/test/val}_split.jsonl --split_paragraphs ```  Generate alignments of the paragraphs with sentences from the summary using the bi-encoder **paraphrase-distilroberta-base-v1** ``` python align_data_bi_encoder_paraphrase.py --data_path /path/to/chapter_summary_aligned_{train/test/val}_split.jsonl.gathered --stable_alignment ```  ## Troubleshooting 1.,python,SOFTWARE
The script needs to be run separately for each split as the matched file ``` cd paragraph-level-summary-alignments python gather_data.py --matched_file /path/to/chapter_summary_aligned_{train/test/val}_split.jsonl --split_paragraphs ```  Generate alignments of the paragraphs with sentences from the summary using the bi-encoder **paraphrase-distilroberta-base-v1** ``` python align_data_bi_encoder_paraphrase.py --data_path /path/to/chapter_summary_aligned_{train/test/val}_split.jsonl.gathered --stable_alignment ```  ## Troubleshooting 1.,paraphrase-distilroberta-base-v1,SOFTWARE
The script needs to be run separately for each split as the matched file ``` cd paragraph-level-summary-alignments python gather_data.py --matched_file /path/to/chapter_summary_aligned_{train/test/val}_split.jsonl --split_paragraphs ```  Generate alignments of the paragraphs with sentences from the summary using the bi-encoder **paraphrase-distilroberta-base-v1** ``` python align_data_bi_encoder_paraphrase.py --data_path /path/to/chapter_summary_aligned_{train/test/val}_split.jsonl.gathered --stable_alignment ```  ## Troubleshooting 1.,python,SOFTWARE
"[[arXiv](https://arxiv.org/abs/2003.06658)] [[Poster](https://www.shininglab.ai/assets/posters/Revisit%20Systematic%20Generalization%20via%20Meaningful%20Learning.pdf)]  ## Directory + **main/config.py** - Configurations + **main/res** - Resources including model check points, datasets, experiment records, and results + **main/src** - Source code including model structures and utility functions ``` Systematic-Generalization-via-Meaningful-Learning ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ main ‚îÇ   ‚îú‚îÄ‚îÄ config.py ‚îÇ   ‚îú‚îÄ‚îÄ res ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ check_points ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scan ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geography ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ advising ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geo_vars.txt ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adv_vars.txt ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ iwslt14 ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ iwslt15 ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prepare-iwslt14.sh ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prepare-iwslt15.sh ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ log ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ result ‚îÇ   ‚îú‚îÄ‚îÄ src ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils ‚îÇ   ‚îî‚îÄ‚îÄ train.py ‚îî‚îÄ‚îÄ requirements.txt ```  ## Dependencies + python >= 3.10.6 + tqdm >= 4.64.1 + numpy >= 1.23.4 + torch >= 1.13.0  ## Data All datasets can be downloaded [here](https://drive.google.com/drive/folders/19vFBn5C-nTdjxMeuMgw-BvsPNsLF6DpV?",python >= 3.10.6,SOFTWARE
"[[arXiv](https://arxiv.org/abs/2003.06658)] [[Poster](https://www.shininglab.ai/assets/posters/Revisit%20Systematic%20Generalization%20via%20Meaningful%20Learning.pdf)]  ## Directory + **main/config.py** - Configurations + **main/res** - Resources including model check points, datasets, experiment records, and results + **main/src** - Source code including model structures and utility functions ``` Systematic-Generalization-via-Meaningful-Learning ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ main ‚îÇ   ‚îú‚îÄ‚îÄ config.py ‚îÇ   ‚îú‚îÄ‚îÄ res ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ check_points ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scan ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geography ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ advising ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geo_vars.txt ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adv_vars.txt ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ iwslt14 ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ iwslt15 ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prepare-iwslt14.sh ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prepare-iwslt15.sh ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ log ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ result ‚îÇ   ‚îú‚îÄ‚îÄ src ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils ‚îÇ   ‚îî‚îÄ‚îÄ train.py ‚îî‚îÄ‚îÄ requirements.txt ```  ## Dependencies + python >= 3.10.6 + tqdm >= 4.64.1 + numpy >= 1.23.4 + torch >= 1.13.0  ## Data All datasets can be downloaded [here](https://drive.google.com/drive/folders/19vFBn5C-nTdjxMeuMgw-BvsPNsLF6DpV?",tqdm >= 4.64.1,SOFTWARE
"[[arXiv](https://arxiv.org/abs/2003.06658)] [[Poster](https://www.shininglab.ai/assets/posters/Revisit%20Systematic%20Generalization%20via%20Meaningful%20Learning.pdf)]  ## Directory + **main/config.py** - Configurations + **main/res** - Resources including model check points, datasets, experiment records, and results + **main/src** - Source code including model structures and utility functions ``` Systematic-Generalization-via-Meaningful-Learning ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ main ‚îÇ   ‚îú‚îÄ‚îÄ config.py ‚îÇ   ‚îú‚îÄ‚îÄ res ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ check_points ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scan ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geography ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ advising ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geo_vars.txt ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adv_vars.txt ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ iwslt14 ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ iwslt15 ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prepare-iwslt14.sh ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prepare-iwslt15.sh ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ log ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ result ‚îÇ   ‚îú‚îÄ‚îÄ src ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils ‚îÇ   ‚îî‚îÄ‚îÄ train.py ‚îî‚îÄ‚îÄ requirements.txt ```  ## Dependencies + python >= 3.10.6 + tqdm >= 4.64.1 + numpy >= 1.23.4 + torch >= 1.13.0  ## Data All datasets can be downloaded [here](https://drive.google.com/drive/folders/19vFBn5C-nTdjxMeuMgw-BvsPNsLF6DpV?",numpy >= 1.23.4,SOFTWARE
"[[arXiv](https://arxiv.org/abs/2003.06658)] [[Poster](https://www.shininglab.ai/assets/posters/Revisit%20Systematic%20Generalization%20via%20Meaningful%20Learning.pdf)]  ## Directory + **main/config.py** - Configurations + **main/res** - Resources including model check points, datasets, experiment records, and results + **main/src** - Source code including model structures and utility functions ``` Systematic-Generalization-via-Meaningful-Learning ‚îú‚îÄ‚îÄ README.md ‚îú‚îÄ‚îÄ main ‚îÇ   ‚îú‚îÄ‚îÄ config.py ‚îÇ   ‚îú‚îÄ‚îÄ res ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ check_points ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scan ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geography ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ advising ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geo_vars.txt ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adv_vars.txt ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ iwslt14 ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ iwslt15 ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prepare-iwslt14.sh ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prepare-iwslt15.sh ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ log ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ result ‚îÇ   ‚îú‚îÄ‚îÄ src ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils ‚îÇ   ‚îî‚îÄ‚îÄ train.py ‚îî‚îÄ‚îÄ requirements.txt ```  ## Dependencies + python >= 3.10.6 + tqdm >= 4.64.1 + numpy >= 1.23.4 + torch >= 1.13.0  ## Data All datasets can be downloaded [here](https://drive.google.com/drive/folders/19vFBn5C-nTdjxMeuMgw-BvsPNsLF6DpV?",torch >= 1.13.0,SOFTWARE
"A virtual environment is recommended. ``` $ cd Systematic-Generalization-via-Meaningful-Learning $ cd main $ pip install pip --upgrade $ pip install -r requirements.txt ```  ## Run Before training, please double check **config.py** to ensure training configurations. ``` $ vim config.py $ python train.py ```  ## Outputs If everything goes well, there should be a similar progressing shown as below. ``` Initialize...",pip,SOFTWARE
"A virtual environment is recommended. ``` $ cd Systematic-Generalization-via-Meaningful-Learning $ cd main $ pip install pip --upgrade $ pip install -r requirements.txt ```  ## Run Before training, please double check **config.py** to ensure training configurations. ``` $ vim config.py $ python train.py ```  ## Outputs If everything goes well, there should be a similar progressing shown as below. ``` Initialize...",pip,SOFTWARE
"A virtual environment is recommended. ``` $ cd Systematic-Generalization-via-Meaningful-Learning $ cd main $ pip install pip --upgrade $ pip install -r requirements.txt ```  ## Run Before training, please double check **config.py** to ensure training configurations. ``` $ vim config.py $ python train.py ```  ## Outputs If everything goes well, there should be a similar progressing shown as below. ``` Initialize...",pip,SOFTWARE
"A virtual environment is recommended. ``` $ cd Systematic-Generalization-via-Meaningful-Learning $ cd main $ pip install pip --upgrade $ pip install -r requirements.txt ```  ## Run Before training, please double check **config.py** to ensure training configurations. ``` $ vim config.py $ python train.py ```  ## Outputs If everything goes well, there should be a similar progressing shown as below. ``` Initialize...",vim,SOFTWARE
"A virtual environment is recommended. ``` $ cd Systematic-Generalization-via-Meaningful-Learning $ cd main $ pip install pip --upgrade $ pip install -r requirements.txt ```  ## Run Before training, please double check **config.py** to ensure training configurations. ``` $ vim config.py $ python train.py ```  ## Outputs If everything goes well, there should be a similar progressing shown as below. ``` Initialize...",python,SOFTWARE
"Loss:1.7061: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 132/132 [00:14<00:00,  9.37it/s] Train Epoch 0 Total Step 132 Loss:1.9179 ... ```  ## NMT We use [fairseq](https://github.com/facebookresearch/fairseq) for NMT tasks in Section 4.1.",fairseq,SOFTWARE
"Loss:1.7061: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 132/132 [00:14<00:00,  9.37it/s] Train Epoch 0 Total Step 132 Loss:1.9179 ... ```  ## NMT We use [fairseq](https://github.com/facebookresearch/fairseq) for NMT tasks in Section 4.1.",fairseq,SOFTWARE
"# Improving Deep Metric Learning by Divide and Conquer ## About  PyTorch implementation for the paper _Improving Deep Metric  Learning by Divide and Conquer_ accepted to **TPAMI** (Sep. 2021), which is our follow-up paper of [_Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019)_](https://github.com/CompVis/metric-learning-divide-and-conquer)  **Links**: * arxiv: https://arxiv.org/abs/2109.04003 or * TPAMI early access: https://ieeexplore.ieee.org/document/9540303   ## Requirements  * PyTorch 1.1.0 * Faiss-GPU >= 1.5.0, [Link](https://github.com/facebookresearch/faiss) * albumentations >= 0.4.5, [Link](https://github.com/albumentations-team/albumentations)   ## Usage ### Training:  Training is done by calling `python train.py` and setting the respective params, all of which are listed and explained  in `/experiment/margin_loss_resnet50.py` (the default setup for all our experiments).",PyTorch,SOFTWARE
"# Improving Deep Metric Learning by Divide and Conquer ## About  PyTorch implementation for the paper _Improving Deep Metric  Learning by Divide and Conquer_ accepted to **TPAMI** (Sep. 2021), which is our follow-up paper of [_Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019)_](https://github.com/CompVis/metric-learning-divide-and-conquer)  **Links**: * arxiv: https://arxiv.org/abs/2109.04003 or * TPAMI early access: https://ieeexplore.ieee.org/document/9540303   ## Requirements  * PyTorch 1.1.0 * Faiss-GPU >= 1.5.0, [Link](https://github.com/facebookresearch/faiss) * albumentations >= 0.4.5, [Link](https://github.com/albumentations-team/albumentations)   ## Usage ### Training:  Training is done by calling `python train.py` and setting the respective params, all of which are listed and explained  in `/experiment/margin_loss_resnet50.py` (the default setup for all our experiments).",PyTorch 1.1.0,SOFTWARE
"# Improving Deep Metric Learning by Divide and Conquer ## About  PyTorch implementation for the paper _Improving Deep Metric  Learning by Divide and Conquer_ accepted to **TPAMI** (Sep. 2021), which is our follow-up paper of [_Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019)_](https://github.com/CompVis/metric-learning-divide-and-conquer)  **Links**: * arxiv: https://arxiv.org/abs/2109.04003 or * TPAMI early access: https://ieeexplore.ieee.org/document/9540303   ## Requirements  * PyTorch 1.1.0 * Faiss-GPU >= 1.5.0, [Link](https://github.com/facebookresearch/faiss) * albumentations >= 0.4.5, [Link](https://github.com/albumentations-team/albumentations)   ## Usage ### Training:  Training is done by calling `python train.py` and setting the respective params, all of which are listed and explained  in `/experiment/margin_loss_resnet50.py` (the default setup for all our experiments).",Faiss-GPU >= 1.5.0,SOFTWARE
"# Improving Deep Metric Learning by Divide and Conquer ## About  PyTorch implementation for the paper _Improving Deep Metric  Learning by Divide and Conquer_ accepted to **TPAMI** (Sep. 2021), which is our follow-up paper of [_Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019)_](https://github.com/CompVis/metric-learning-divide-and-conquer)  **Links**: * arxiv: https://arxiv.org/abs/2109.04003 or * TPAMI early access: https://ieeexplore.ieee.org/document/9540303   ## Requirements  * PyTorch 1.1.0 * Faiss-GPU >= 1.5.0, [Link](https://github.com/facebookresearch/faiss) * albumentations >= 0.4.5, [Link](https://github.com/albumentations-team/albumentations)   ## Usage ### Training:  Training is done by calling `python train.py` and setting the respective params, all of which are listed and explained  in `/experiment/margin_loss_resnet50.py` (the default setup for all our experiments).",faiss,SOFTWARE
"# Improving Deep Metric Learning by Divide and Conquer ## About  PyTorch implementation for the paper _Improving Deep Metric  Learning by Divide and Conquer_ accepted to **TPAMI** (Sep. 2021), which is our follow-up paper of [_Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019)_](https://github.com/CompVis/metric-learning-divide-and-conquer)  **Links**: * arxiv: https://arxiv.org/abs/2109.04003 or * TPAMI early access: https://ieeexplore.ieee.org/document/9540303   ## Requirements  * PyTorch 1.1.0 * Faiss-GPU >= 1.5.0, [Link](https://github.com/facebookresearch/faiss) * albumentations >= 0.4.5, [Link](https://github.com/albumentations-team/albumentations)   ## Usage ### Training:  Training is done by calling `python train.py` and setting the respective params, all of which are listed and explained  in `/experiment/margin_loss_resnet50.py` (the default setup for all our experiments).",albumentations >= 0.4.5,SOFTWARE
"# Improving Deep Metric Learning by Divide and Conquer ## About  PyTorch implementation for the paper _Improving Deep Metric  Learning by Divide and Conquer_ accepted to **TPAMI** (Sep. 2021), which is our follow-up paper of [_Divide and Conquer the Embedding Space for Metric Learning (CVPR 2019)_](https://github.com/CompVis/metric-learning-divide-and-conquer)  **Links**: * arxiv: https://arxiv.org/abs/2109.04003 or * TPAMI early access: https://ieeexplore.ieee.org/document/9540303   ## Requirements  * PyTorch 1.1.0 * Faiss-GPU >= 1.5.0, [Link](https://github.com/facebookresearch/faiss) * albumentations >= 0.4.5, [Link](https://github.com/albumentations-team/albumentations)   ## Usage ### Training:  Training is done by calling `python train.py` and setting the respective params, all of which are listed and explained  in `/experiment/margin_loss_resnet50.py` (the default setup for all our experiments).",python,SOFTWARE
"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",python,SOFTWARE
"**A basic sample run using default parameters would like this**:  ``` python train.py --experiment margin_loss_resnet50 \                 --dataset=sop -i=$NAME -seed=4 \                 --sz-embedding=512 --mod-epoch=2 --nb-clusters=32 --nb-epochs=180 \                 --batch-size=80 --num-samples-per-class=2 --backend=faiss-gpu \                 --lr-gamma=0.3 --mod-epoch-freeze=1 --sampler=distanceweighted \                 --weight-decay=1e-4 --batch-sampler=adaptbalanced \                 --force-full-embedding-epoch=80 --mask-lr-mult=100 \                 --masking-lambda=0.01 --mask-wd-mult=1 --dataset-dir=$datadir ```  - **--experiment margin_loss_resnet50** please keep this untouched, otherwise the args won't be read correctly. - **--dataset** specify the dataset that you want to train the model for, choose one of `--dataset=cub` (CUB200-2011), `cars` (CARS196), `sop` (Standford Online Porducts), `inshop` (In-Shop cloths retireval) or `vid` (PKU Vehicle id). - **--nb_clusters**: could be maximumly possible set to 16.",faiss-gpu,SOFTWARE
"Besides, please take the total number of different classes in the dataset and the sampling strategy used into account when you setting this value. - **--dataset-dir**: the path to the datasets, check the *Datasets* section below. - **--sampler**: batchminer used to sample pairs or triplets (in embedding space) to create learning signal, check `/metriclearning/sampler` for details. - **--batch-sampler**: data sampler used to generate training batches, check `/dataset/sampler.py` for details. - **--nb-epochs**: the maximum training epochs. - **--mod-epoch**: division frequency in the paper - the number of training epochs between consecutive divisions. - **--wandb-enabled**: by setting this flag, you will enable the [Weights&Biases](https://wandb.ai/site) logging.",wandb,SOFTWARE
"Besides, please take the total number of different classes in the dataset and the sampling strategy used into account when you setting this value. - **--dataset-dir**: the path to the datasets, check the *Datasets* section below. - **--sampler**: batchminer used to sample pairs or triplets (in embedding space) to create learning signal, check `/metriclearning/sampler` for details. - **--batch-sampler**: data sampler used to generate training batches, check `/dataset/sampler.py` for details. - **--nb-epochs**: the maximum training epochs. - **--mod-epoch**: division frequency in the paper - the number of training epochs between consecutive divisions. - **--wandb-enabled**: by setting this flag, you will enable the [Weights&Biases](https://wandb.ai/site) logging.",wandb,SOFTWARE
"‚îî‚îÄ‚îÄ‚îÄcabinet_final |   ‚îÇ   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",PyTorch (0.4.1),SOFTWARE
"‚îî‚îÄ‚îÄ‚îÄcabinet_final |   ‚îÇ   xxx.jpg |       ... | bicycle.txt | ... | cabinet_final.txt ```  * For In-shop Clothes and PKU Vehicle id datasets, simply download them from the above given links and unzip their folder as they originally are.   ## Results  __CUB200__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 68.60 | 54.98 | 67.39 -- 77.43 --  84.82 --  90.83     learn |  Margin/Distance    | 69.84 | 55.11 | __67.71__ --  78.14 --  85.80 --  91.46   __Cars196__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 2 -- 4 -- 8 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 70.57 | 65.89 | __87.22__ -- 92.19 --  95.39 --  97.43     learn |  Margin/Distance    | 70.10 | 65.54 | 86.90 --  92.34 --  95.41 --  97.45   __In-Shop Clothes__  Variants | Loss/Sampling    |   NMI  |  mARP  | Recall @ 1 -- 10 -- 20 -- 30 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 89.76 | 87.86| 89.84 -- 97.56 -- 98.21 -- 98.51 learn |  Margin/Distance    | 89.88 | 88.50| __90.49__ -- 97.48 -- 98.23 -- 98.51  __Online Products__  Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 10 -- 100 ---------|--------------- |--------|------|----------------- fixed |  Margin/Distance    | 89.59 | 79.32| 79.50 -- 90.44 -- 95.08     learn |  Margin/Distance    | 89.68 | 79.60| __79.80__ --  90.46 --  95.26   __VID (Large eval set)__   Variants | Loss/Sampling |   NMI  |  mARP  | Recall @ 1 -- 5 ------|---------------      |--------|------|----------------- fixed |  Margin/Distance | 90.78 | 92.89 | __94.01__ -- 96.18 learn |  Margin/Distance    | 90.70 | 92.68 | 93.93 -- 95.99    _Disclaimer: The results above are slightly different from those on the paper, as they were obtained before the code refactoring and with PyTorch (0.4.1) and Faiss (1.4.0).",Faiss (1.4.0),SOFTWARE
"Some deviations in results based on different PyTorch/Cuda/Faiss versions and hardware (e.g. between P100 and RTX GPUs) are to be expected._   ## Related Repos  * Divide and Conquer the Embedding Space for Metric Learning (our previous paper on CVPR 2019):  https://github.com/CompVis/metric-learning-divide-and-conquer * An easy-to-use repo to start your DML research, containing collections of models, losses, and samplers implemented in PyTorch: https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch  ## BibTex If you use this code in your research, please cite the following papers:  ``` @InProceedings{dcesml,   title={Divide and Conquer the Embedding Space for Metric Learning},   author={Sanakoyeu, Artsiom and Tschernezki, Vadim and B\""uchler, Uta and Ommer, Bj\""orn},   booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},   year={2019} }  @article{sanakoyeu2021improving,   title={Improving Deep Metric Learning by Divide and Conquer},    author={Artsiom Sanakoyeu and Pingchuan Ma and Vadim Tschernezki and Bj√∂rn Ommer},   journal={IEEE Transactions on pattern analysis and machine intelligence},   year={2021},   publisher={IEEE} } ```",PyTorch,SOFTWARE
"Some deviations in results based on different PyTorch/Cuda/Faiss versions and hardware (e.g. between P100 and RTX GPUs) are to be expected._   ## Related Repos  * Divide and Conquer the Embedding Space for Metric Learning (our previous paper on CVPR 2019):  https://github.com/CompVis/metric-learning-divide-and-conquer * An easy-to-use repo to start your DML research, containing collections of models, losses, and samplers implemented in PyTorch: https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch  ## BibTex If you use this code in your research, please cite the following papers:  ``` @InProceedings{dcesml,   title={Divide and Conquer the Embedding Space for Metric Learning},   author={Sanakoyeu, Artsiom and Tschernezki, Vadim and B\""uchler, Uta and Ommer, Bj\""orn},   booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},   year={2019} }  @article{sanakoyeu2021improving,   title={Improving Deep Metric Learning by Divide and Conquer},    author={Artsiom Sanakoyeu and Pingchuan Ma and Vadim Tschernezki and Bj√∂rn Ommer},   journal={IEEE Transactions on pattern analysis and machine intelligence},   year={2021},   publisher={IEEE} } ```",Cuda,SOFTWARE
"Some deviations in results based on different PyTorch/Cuda/Faiss versions and hardware (e.g. between P100 and RTX GPUs) are to be expected._   ## Related Repos  * Divide and Conquer the Embedding Space for Metric Learning (our previous paper on CVPR 2019):  https://github.com/CompVis/metric-learning-divide-and-conquer * An easy-to-use repo to start your DML research, containing collections of models, losses, and samplers implemented in PyTorch: https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch  ## BibTex If you use this code in your research, please cite the following papers:  ``` @InProceedings{dcesml,   title={Divide and Conquer the Embedding Space for Metric Learning},   author={Sanakoyeu, Artsiom and Tschernezki, Vadim and B\""uchler, Uta and Ommer, Bj\""orn},   booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},   year={2019} }  @article{sanakoyeu2021improving,   title={Improving Deep Metric Learning by Divide and Conquer},    author={Artsiom Sanakoyeu and Pingchuan Ma and Vadim Tschernezki and Bj√∂rn Ommer},   journal={IEEE Transactions on pattern analysis and machine intelligence},   year={2021},   publisher={IEEE} } ```",Faiss,SOFTWARE
"Some deviations in results based on different PyTorch/Cuda/Faiss versions and hardware (e.g. between P100 and RTX GPUs) are to be expected._   ## Related Repos  * Divide and Conquer the Embedding Space for Metric Learning (our previous paper on CVPR 2019):  https://github.com/CompVis/metric-learning-divide-and-conquer * An easy-to-use repo to start your DML research, containing collections of models, losses, and samplers implemented in PyTorch: https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch  ## BibTex If you use this code in your research, please cite the following papers:  ``` @InProceedings{dcesml,   title={Divide and Conquer the Embedding Space for Metric Learning},   author={Sanakoyeu, Artsiom and Tschernezki, Vadim and B\""uchler, Uta and Ommer, Bj\""orn},   booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},   year={2019} }  @article{sanakoyeu2021improving,   title={Improving Deep Metric Learning by Divide and Conquer},    author={Artsiom Sanakoyeu and Pingchuan Ma and Vadim Tschernezki and Bj√∂rn Ommer},   journal={IEEE Transactions on pattern analysis and machine intelligence},   year={2021},   publisher={IEEE} } ```",PyTorch,SOFTWARE
"Some deviations in results based on different PyTorch/Cuda/Faiss versions and hardware (e.g. between P100 and RTX GPUs) are to be expected._   ## Related Repos  * Divide and Conquer the Embedding Space for Metric Learning (our previous paper on CVPR 2019):  https://github.com/CompVis/metric-learning-divide-and-conquer * An easy-to-use repo to start your DML research, containing collections of models, losses, and samplers implemented in PyTorch: https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch  ## BibTex If you use this code in your research, please cite the following papers:  ``` @InProceedings{dcesml,   title={Divide and Conquer the Embedding Space for Metric Learning},   author={Sanakoyeu, Artsiom and Tschernezki, Vadim and B\""uchler, Uta and Ommer, Bj\""orn},   booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},   year={2019} }  @article{sanakoyeu2021improving,   title={Improving Deep Metric Learning by Divide and Conquer},    author={Artsiom Sanakoyeu and Pingchuan Ma and Vadim Tschernezki and Bj√∂rn Ommer},   journal={IEEE Transactions on pattern analysis and machine intelligence},   year={2021},   publisher={IEEE} } ```",PyTorch,SOFTWARE
"To replicate the development environment with the [Anaconda](https://www.anaconda.com/products/individual) distribution, first create an empty conda environment by running: <br /> ```conda create --name fisher-information python=3.8.3```  2.",Anaconda,SOFTWARE
"To replicate the development environment with the [Anaconda](https://www.anaconda.com/products/individual) distribution, first create an empty conda environment by running: <br /> ```conda create --name fisher-information python=3.8.3```  2.",anaconda,SOFTWARE
"To replicate the development environment with the [Anaconda](https://www.anaconda.com/products/individual) distribution, first create an empty conda environment by running: <br /> ```conda create --name fisher-information python=3.8.3```  2.",conda,SOFTWARE
"To replicate the development environment with the [Anaconda](https://www.anaconda.com/products/individual) distribution, first create an empty conda environment by running: <br /> ```conda create --name fisher-information python=3.8.3```  2.",python=3.8.3,SOFTWARE
"To activate the environment, run: ```conda activate fisher-information```  3.",conda,SOFTWARE
Install pip by running: ```conda install pip```  4.,pip,SOFTWARE
Install pip by running: ```conda install pip```  4.,conda,SOFTWARE
Install pip by running: ```conda install pip```  4.,pip,SOFTWARE
Run the following to install the required packages from the [requirements.txt](/requirements.txt) file: <br />    ```pip install -r requirements.txt```  You should now be able to run the code.,pip,SOFTWARE
"Please ensure you are running a version of Python >= 3.8.0 \ If you are running an old version of Anaconda, you may need to reinstall with a newer version for this.  ## Contact Jos Cooper - jos.cooper@stfc.ac.uk \ James Durant - james.durant@warwick.ac.uk \ Lucas Wilkins - lucas@lucaswilkins.com  ## Acknowledgements This work has been partially supported by the STFC Facilities Programme Fund through the ISIS Neutron and Muon Source, and Scientific Computing Department of Rutherford Appleton Laboratory, Science and Technology Facilities Council, and by the Wave 1 of The UKRI Strategic Priorities Fund under the EPSRC Grant EP/T001569/1, particularly the ""AI for Science"" theme within that grant and The Alan Turing Institute.",Python >= 3.8.0,SOFTWARE
"Please ensure you are running a version of Python >= 3.8.0 \ If you are running an old version of Anaconda, you may need to reinstall with a newer version for this.  ## Contact Jos Cooper - jos.cooper@stfc.ac.uk \ James Durant - james.durant@warwick.ac.uk \ Lucas Wilkins - lucas@lucaswilkins.com  ## Acknowledgements This work has been partially supported by the STFC Facilities Programme Fund through the ISIS Neutron and Muon Source, and Scientific Computing Department of Rutherford Appleton Laboratory, Science and Technology Facilities Council, and by the Wave 1 of The UKRI Strategic Priorities Fund under the EPSRC Grant EP/T001569/1, particularly the ""AI for Science"" theme within that grant and The Alan Turing Institute.",Anaconda,SOFTWARE
"Hereinafter we describe the procedure to follow to infuse the disease knowledge formalized by the  [disease ontology MONDO](https://mondo.monarchinitiative.org/) in four widespread embedding-LLMs  and evaluate the ontology-enhanced embedding-LLMs against two widespread sentence-similarity  datasets: [BIOSSES](https://huggingface.co/datasets/biosses) (biomedical domain) and the five test sets of SemEval Sentence Similarity challenges released yearly of [2012](https://huggingface.co/datasets/mteb/sts12-sts), [2013](https://huggingface.co/datasets/mteb/sts13-sts), [2014](https://huggingface.co/datasets/mteb/sts14-sts), [2015](https://huggingface.co/datasets/mteb/sts15-sts),  [2016](https://huggingface.co/datasets/mteb/sts16-sts).    ### 1) Set-up environment - Create and activate a virtual environment based on Python 3.10.x. - Clone the repository, set the repo-folder as the working directory and install python requirements: `pip install -r requirements.txt` - Create a resource-folder - e.g.",pip,SOFTWARE
"The python scripts included in this repository  should be able to read files from and write files to the resorce-folder.  ### 2) Preload MONDO ontology Run the [onto/load.py](onto/load.py) script: in the resource-folder a MONDO ontology pickle object file  exploited to store the contents of the ontology to support ontological knowledge infusion, will be  created.  ### 3) Generate synthetic definitions of MONDO ontology concepts - Add your OpenAI access credentials to prompt GPT-3.5-turbo in the module [synth_data_gen/constants.py](synth_data_gen/constants.py).",OpenAI,SOFTWARE
"The python scripts included in this repository  should be able to read files from and write files to the resorce-folder.  ### 2) Preload MONDO ontology Run the [onto/load.py](onto/load.py) script: in the resource-folder a MONDO ontology pickle object file  exploited to store the contents of the ontology to support ontological knowledge infusion, will be  created.  ### 3) Generate synthetic definitions of MONDO ontology concepts - Add your OpenAI access credentials to prompt GPT-3.5-turbo in the module [synth_data_gen/constants.py](synth_data_gen/constants.py).",GPT-3.5-turbo,SOFTWARE
The module [synth_data_gen/prompt_openai.py](synth_data_gen/prompt_openai.py) wraps interactions with OpenAI GPT-3.5-turbo model. - Run [synth_data_gen/generate_synth_data.py](synth_data_gen/generate_synth_data.py) script: this script will generate a synthetic  concept definition from each synonym of each concept of the MONDO ontology.,OpenAI GPT-3.5-turbo,SOFTWARE
"This script will perform the following actions: - read real definitions of concepts from the MONDO ontology, together with synthetically generated ones   (as described at step 3) - ontology-driven generation of positive pairs of definitions by synonym substitution - ontology-driven generation of hard-negative pairs of definitions by: (i) indexing all definitions  in a collection of the [Chroma DB embedding database](https://www.trychroma.com/); (ii) identifying  for each definition the associated hard-negative one(s), relying on the is-a relations specified by  the MONDO ontology - infuse disease knowledge from the MONDO ontology in the embedding-LLM selected through command-line  **ARGUMENT 1**: this is obtained by relying on a contrastive learning framework relying on  InfoNCE loss (for more details refer the [paper on ontological knowledge infusion](https://arxiv.org/abs/2405.20527))",Chroma DB,SOFTWARE
[HamilToniQ_logo](.,HamilToniQ,SOFTWARE
/figures/HamilToniQ_logo.png)HamilToniQ: An Open-Source Benchmark Toolkit for Quantum Computers  Table of Contents:  1.,HamilToniQ,SOFTWARE
/figures/HamilToniQ_logo.png)HamilToniQ: An Open-Source Benchmark Toolkit for Quantum Computers  Table of Contents:  1.,HamilToniQ,SOFTWARE
"[How to cite](#cite)  <a name=""introduction""></a>  ## Introduction   HamilToniQ is an application-oriented benchmarking toolkit for the comprehensive evaluation of QPUs.",HamilToniQ,SOFTWARE
"<a name=""quickstart""></a>  ## Quick Start   ### Installation  Install the *HamilToniQ* toolkit by running the following code in the Terminal.",HamilToniQ,SOFTWARE
"```shell cd /path/to/your/directory git clone https://github.com/FelixXu35/hamiltoniq.git cd hamiltoniq pip install -e . ```  ### Bechmark a backend  Simply copy and run the following Python code:  ```python from hamiltoniq.bechmark import Toniq  toniq = Toniq() backend = <your_backend> n_qubits = <your_prefered_number_of_qubits> n_layers = <your_prefered_number_of_layers> n_cores = <number_of_cores_in_your_PC>  score = toniq.simulator_run(backend=backend, n_qubits=n_qubits, n_layers=n_layers, n_cores=n_cores) ```  An example is given in [this notebook](.",git,SOFTWARE
"```shell cd /path/to/your/directory git clone https://github.com/FelixXu35/hamiltoniq.git cd hamiltoniq pip install -e . ```  ### Bechmark a backend  Simply copy and run the following Python code:  ```python from hamiltoniq.bechmark import Toniq  toniq = Toniq() backend = <your_backend> n_qubits = <your_prefered_number_of_qubits> n_layers = <your_prefered_number_of_layers> n_cores = <number_of_cores_in_your_PC>  score = toniq.simulator_run(backend=backend, n_qubits=n_qubits, n_layers=n_layers, n_cores=n_cores) ```  An example is given in [this notebook](.",hamiltoniq,SOFTWARE
"```shell cd /path/to/your/directory git clone https://github.com/FelixXu35/hamiltoniq.git cd hamiltoniq pip install -e . ```  ### Bechmark a backend  Simply copy and run the following Python code:  ```python from hamiltoniq.bechmark import Toniq  toniq = Toniq() backend = <your_backend> n_qubits = <your_prefered_number_of_qubits> n_layers = <your_prefered_number_of_layers> n_cores = <number_of_cores_in_your_PC>  score = toniq.simulator_run(backend=backend, n_qubits=n_qubits, n_layers=n_layers, n_cores=n_cores) ```  An example is given in [this notebook](.",hamiltoniq,SOFTWARE
"```shell cd /path/to/your/directory git clone https://github.com/FelixXu35/hamiltoniq.git cd hamiltoniq pip install -e . ```  ### Bechmark a backend  Simply copy and run the following Python code:  ```python from hamiltoniq.bechmark import Toniq  toniq = Toniq() backend = <your_backend> n_qubits = <your_prefered_number_of_qubits> n_layers = <your_prefered_number_of_layers> n_cores = <number_of_cores_in_your_PC>  score = toniq.simulator_run(backend=backend, n_qubits=n_qubits, n_layers=n_layers, n_cores=n_cores) ```  An example is given in [this notebook](.",pip,SOFTWARE
"```shell cd /path/to/your/directory git clone https://github.com/FelixXu35/hamiltoniq.git cd hamiltoniq pip install -e . ```  ### Bechmark a backend  Simply copy and run the following Python code:  ```python from hamiltoniq.bechmark import Toniq  toniq = Toniq() backend = <your_backend> n_qubits = <your_prefered_number_of_qubits> n_layers = <your_prefered_number_of_layers> n_cores = <number_of_cores_in_your_PC>  score = toniq.simulator_run(backend=backend, n_qubits=n_qubits, n_layers=n_layers, n_cores=n_cores) ```  An example is given in [this notebook](.",hamiltoniq,SOFTWARE
"/hamiltoniq/H_Scores/qubit_3.png"" alt=""n_qubits=3"" width=""700"" /></p>  ##### 4 qubits  <p align=center><img src="".",hamiltoniq,SOFTWARE
"/hamiltoniq/H_Scores/qubit_4.png"" alt=""n_qubits=4"" width=""700"" /></p>  ##### 5 qubits  <p align=center><img src="".",hamiltoniq,SOFTWARE
"/hamiltoniq/H_Scores/qubit_5.png"" alt=""n_qubits=4"" width=""700"" /></p>  ##### 6 qubits  <p align=center><img src="".",hamiltoniq,SOFTWARE
"/hamiltoniq/H_Scores/qubit_6.png"" alt=""n_qubits=4"" width=""700"" /></p>  <a name=""architecture""></a>  ## Architecture  For more technical details please visit our [arXiv Paper](https://arxiv.org/abs/2404.13971).",hamiltoniq,SOFTWARE
"The HamilToniQ‚Äôs benchmarking workflow, shown in the figure below, commences with the characterization of QPUs, where each QPU is classified according to its type, topology, and multi-QPU system.",HamilToniQ,SOFTWARE
"These metrics provide a quantitative and objective measure of the QPU‚Äôs performance, reflecting the effectiveness of the benchmarking process implemented by HamilToniQ.",HamilToniQ,SOFTWARE
"/figures/benchmarking_scheme.png"" alt=""scheme"" width=""400"" /></p>  HamilToniQ primarily comprises two components: the reference part, also known as ground truth, and the scoring part, as depicted in the figure below.",HamilToniQ,SOFTWARE
"/figures/benchmarking_flow.png"" alt=""flow"" width=""400"" /></p>  <a name=""cite""></a>  ## How to cite  If you used this package or framework for your research, please cite:  ```text @article{xu2024hamiltoniq,   title={HamilToniQ: An Open-Source Benchmark Toolkit for Quantum Computers},   author={Xu, Xiaotian and Chen, Kuan-Cheng and Wille, Robert},   journal={arXiv preprint arXiv:2404.13971},   year={2024} } ```",HamilToniQ,SOFTWARE
# [AAAI 2024] NuScenes-QA  Official repository for the AAAI 2024 paper **[NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario](https://arxiv.org/pdf/2305.14836.pdf)**.  !,NuScenes-QA,SOFTWARE
# [AAAI 2024] NuScenes-QA  Official repository for the AAAI 2024 paper **[NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario](https://arxiv.org/pdf/2305.14836.pdf)**.  !,NuScenes-QA,SOFTWARE
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",python >= 3.5,SOFTWARE
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",CUDA >= 9.0,SOFTWARE
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",PyTorch >= 1.4.0,SOFTWARE
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",SpaCy == 2.1.0,SOFTWARE
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",SpaCy,SOFTWARE
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",pip,SOFTWARE
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",python3,SOFTWARE
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",python3,SOFTWARE
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",MMDetection3D,SOFTWARE
"The folder structure should be organized as follows before training.  ``` NuScenes-QA +-- configs/ |   +-- butd.yaml                     |   +-- mcan_small.yaml +-- data/ |   +-- questions/    # downloaded |   |   +-- NuScenes_train_questions.json |   |   +-- NuScenes_val_questions.json |   +-- features/     # downloaded or extracted |   |   +-- CenterPoint/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- BEVDet/ |   |   |   +-- xxx.npz |   |   |   +-- ... |   |   +-- MSMDFusion/ |   |   |   +-- xxx.npz |   |   |   +-- ... +-- src/ +-- run.py ```  ### Installation  The following packages are required to build the project:  ```bash python >= 3.5 CUDA >= 9.0 PyTorch >= 1.4.0 SpaCy == 2.1.0 ```  For the SpaCy, you can install it by:  ```bash wget https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz pip install en_core_web_lg-2.1.0.tar.gz ```  ### Training   The following script will start training a `man_small` model with `CenterPoint` feature on `2` GPUs:  ```bash python3 run.py --RUN='train' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --GPU='0, 1' ```  All checkpoint files and the training logs will be saved to the following paths respectively:  ```bash outputs/ckpts/ckpt_<VERSION>/epoch<EPOCH_INDEX>.pkl outputs/log/log_run_<VERSION>.txt ```  ### Testing  For testing, you can use the following script:  ```bash python3 run.py --RUN='val' --MODEL='mcan_small' --VIS_FEAT='CenterPoint' --CKPT_PATH'path/to/ckpt.pkl' ```  The evaluation results and the answers for all questions will ba saved to the following paths respectively:  ```bash outputs/log/log_run_xxx.txt outputs/result/result_run_xxx.txt ```  ## :star: Others If you have any questions about the dataset and its generation or the object-level feature extraction, feel free to cantact me with `twqian19@fudan.edu.cn`.   ## :book: Citation If you find our paper and project useful, please consider citing: ```bibtex @article{qian2023nuscenes,   title={NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario},   author={Qian, Tianwen and Chen, Jingjing and Zhuo, Linhai and Jiao, Yang and Jiang, Yu-Gang},   journal={arXiv preprint arXiv:2305.14836},   year={2023} } ```  ## Acknowlegement  We sincerely thank the authors of [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) and [OpenVQA](https://github.com/MILVLG/openvqa) for open sourcing their methods.",OpenVQA,SOFTWARE
"A longitudinal study on recent misinformation about COVID-19                                                       | Annotator  |   2021 | Personal and Ubiquitous Computing                                                                                         | Springer Science and Business Media Deutschland GmbH | https://www.doi.org/10.1007/s00779-021-01604-6    | | The many dimensions of truthfulness: Crowdsourcing misinformation assessments on a multidimensional scale                                            | Annotator  |   2021 | Information Processing and Management                                                                                     | Elsevier Ltd                                         | https://www.doi.org/10.1016/j.ipm.2021.102710     | | ‚ÄòIt infuriates me': examining young adults‚Äô reactions to and recommendations to fight misinformation about COVID-19                                  | Evaluator  |   2021 | Journal of Youth Studies                                                                                                  | Routledge                                            | https://www.doi.org/10.1080/13676261.2021.1965108 | | The Effects of a News Literacy Video and Real-Time Corrections to Video Misinformation Related to Sunscreen and Skin Cancer                          | Evaluator  |   2021 | Health Communication                                                                                                      | Routledge                                            | https://www.doi.org/10.1080/10410236.2021.1910165 | | Scaling up fact-checking using the wisdom of crowds                                                                                                  | Evaluator  |   2021 | Science Advances                                                                                                          | American Association for the Advancement of Science  | https://www.doi.org/10.1126/sciadv.abf4393        | | You‚Äôre definitely wrong, maybe: Correction style has minimal effect on corrections of misinformation online                                          | Evaluator  |   2021 | Media and Communication                                                                                                   | Cogitatio Press                                      | https://www.doi.org/10.17645/mac.v9i1.3519        | | The Role of Influence of Presumed Influence and Anticipated Guilt in Evoking Social Correction of COVID-19 Misinformation                            | Evaluator  |   2021 | Health Communication                                                                                                      | Routledge                                            | https://www.doi.org/10.1080/10410236.2021.1888452 | | Evaluating Rumor Debunking Effectiveness During the COVID-19 Pandemic Crisis: Utilizing User Stance in Comments on Sina Weibo                        | Evaluator  |   2021 | Frontiers in Public Health                                                                                                | Frontiers Media S.A",Sina Weibo,SOFTWARE
"| https://www.doi.org/10.1108/MRR-05-2020-0286      | | ""There is No Corona; It‚Äôs a Conspiracy"": Addressing the Perceptions of People about COVID-19 through the Narrative of Their Comments on Social Media | Creator    |   2021 | Journal of Consumer Health on the Internet                                                                                | Routledge                                            | https://www.doi.org/10.1080/15398285.2020.1867412 | | Raising the flag: Monitoring user perceived disinformation on reddit                                                                                 | Creator    |   2021 | Information (Switzerland)                                                                                                 | MDPI AG                                              | https://www.doi.org/10.3390/info12010004          | | SAMS: Human-in-the-loop approach to combat the sharing of digital misinformation                                                                     | Creator    |   2021 | CEUR Workshop Proceedings                                                                                                 | CEUR-WS                                              | https://arodes.hes-so.ch/record/8922              | | Fighting disaster misinformation in Latin America: the #19S Mexican earthquake case study                                                            | Creator    |   2021 | Personal and Ubiquitous Computing                                                                                         | Springer Science and Business Media Deutschland GmbH | https://www.doi.org/10.1007/s00779-020-01411-5    | | Dynamics of social corrections to peers sharing COVID-19 misinformation on WhatsApp in Brazil                                                        | Creator    |   2021 | Journal of the American Medical Informatics Association : JAMIA                                                           | NLM (Medline)                                        | https://www.doi.org/10.1093/jamia/ocab219         |   ## 2020 | Title                                                                                                                                                                                    | Category   |   Year | Venue                                                                           | Publisher                                              | URL                                                            | |:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------|-------:|:--------------------------------------------------------------------------------|:-------------------------------------------------------|:---------------------------------------------------------------| | Investigating Differences in Crowdsourced News Credibility Assessment: Raters, Tasks, and Expert Criteria                                                                                | Annotator  |   2020 | Proceedings of the ACM on Human-Computer Interaction                            | Association for Computing Machinery                    | https://www.doi.org/10.1145/3415164                            | | WhistleBlower: Towards A Decentralized and Open Platform for Spotting Fake News                                                                                                          | Annotator  |   2020 | Proceedings - 2020 IEEE International Conference on Blockchain, Blockchain 2020 | Institute of Electrical and Electronics Engineers Inc. | https://www.doi.org/10.1109/Blockchain50366.2020.00026         | | A reliable weighting scheme for the aggregation of crowd intelligence to detect fake news                                                                                                | Annotator  |   2020 | Information (Switzerland)                                                       | MDPI AG                                                | https://www.doi.org/10.3390/INFO11060319                       | | ‚ÄúIs It the Message or the Messenger?‚Äù",WhatsApp,SOFTWARE
Install the codesearch library: `pip install .` 2.,codesearch,SOFTWARE
Install the codesearch library: `pip install .` 2.,pip,SOFTWARE
"Install the tree-sitter parsers (for preprocessing the code snippets): e.g., `codesearch install_parsers python java` or simply `codesearch install_parsers` to install parsers for all supported languages.",codesearch,SOFTWARE
"Install the tree-sitter parsers (for preprocessing the code snippets): e.g., `codesearch install_parsers python java` or simply `codesearch install_parsers` to install parsers for all supported languages.",codesearch,SOFTWARE
"By default, parsers are installed under the `codesearch/parsers` directory this can be customized by setting the `TREE_SITTER_DIR` variable. 3.",codesearch,SOFTWARE
"Install spacy (for preprocessing descriptions/code comments): `python -m spacy download en_core_web_md`   ## Code structure  ``` codesearch ‚îú‚îÄ‚îÄ codesearch          // Contains the library modules: model code, utilities to download and evaluate models, etc. ‚îú‚îÄ‚îÄ nbs                 // Contains examples notebooks and notebooks to reproduce the experiments ‚îú‚îÄ‚îÄ tests               // Contains some unit tests, mostly for verifying the code preprocessing ```  ## Models  We provide some pretrained embedding models to create a retrieval system.",spacy,SOFTWARE
"Install spacy (for preprocessing descriptions/code comments): `python -m spacy download en_core_web_md`   ## Code structure  ``` codesearch ‚îú‚îÄ‚îÄ codesearch          // Contains the library modules: model code, utilities to download and evaluate models, etc. ‚îú‚îÄ‚îÄ nbs                 // Contains examples notebooks and notebooks to reproduce the experiments ‚îú‚îÄ‚îÄ tests               // Contains some unit tests, mostly for verifying the code preprocessing ```  ## Models  We provide some pretrained embedding models to create a retrieval system.",python,SOFTWARE
"Install spacy (for preprocessing descriptions/code comments): `python -m spacy download en_core_web_md`   ## Code structure  ``` codesearch ‚îú‚îÄ‚îÄ codesearch          // Contains the library modules: model code, utilities to download and evaluate models, etc. ‚îú‚îÄ‚îÄ nbs                 // Contains examples notebooks and notebooks to reproduce the experiments ‚îú‚îÄ‚îÄ tests               // Contains some unit tests, mostly for verifying the code preprocessing ```  ## Models  We provide some pretrained embedding models to create a retrieval system.",spacy,SOFTWARE
"Install spacy (for preprocessing descriptions/code comments): `python -m spacy download en_core_web_md`   ## Code structure  ``` codesearch ‚îú‚îÄ‚îÄ codesearch          // Contains the library modules: model code, utilities to download and evaluate models, etc. ‚îú‚îÄ‚îÄ nbs                 // Contains examples notebooks and notebooks to reproduce the experiments ‚îú‚îÄ‚îÄ tests               // Contains some unit tests, mostly for verifying the code preprocessing ```  ## Models  We provide some pretrained embedding models to create a retrieval system.",codesearch,SOFTWARE
"Install spacy (for preprocessing descriptions/code comments): `python -m spacy download en_core_web_md`   ## Code structure  ``` codesearch ‚îú‚îÄ‚îÄ codesearch          // Contains the library modules: model code, utilities to download and evaluate models, etc. ‚îú‚îÄ‚îÄ nbs                 // Contains examples notebooks and notebooks to reproduce the experiments ‚îú‚îÄ‚îÄ tests               // Contains some unit tests, mostly for verifying the code preprocessing ```  ## Models  We provide some pretrained embedding models to create a retrieval system.",codesearch,SOFTWARE
"The pretrained models also expose a consistent interface to embed snippets and queries:  #### Example: Query a snippet collection with a pretrained embedding model  ```python from codesearch.utils import load_model from codesearch.embedding_retrieval import EmbeddingRetrievalModel  query = ""plot a bar chart"" snippets = [{                           # a dummy snippet collection with 1 snippet     ""id"": ""1"",     ""description"": ""Hello world"",     ""code"": ""print('hello world')"",     ""language"": ""python""     }]  embedding_model = load_model(""use-embedder-pacs"") retrieval_model = EmbeddingRetrievalModel(embedding_model) retrieval_model.add_snippets(snippets) retrieval_model.query(query) ```  #### Example: Embed snippets or queries with a pre-trained embedding model  ```python from codesearch.utils import load_model  model_name = ""use-embedder-pacs"" queries = [""plot a bar chart""] snippets = [{     ""description"": ""Hello world"",     ""code"": ""print('hello world')"",     ""language"": ""python""     }]  embedding_model = load_model(model_name) query_embs = embedding_model.embed_queries(queries) snippet_embs = embedding_model.embed_snippets(snippets) ```  ### Available models  Below you find a table with the pretrained models.",codesearch,SOFTWARE
"| name                       | inputs             | training data                                          | notebook                    | |----------------------------|--------------------|--------------------------------------------------------|-----------------------------| | ncs-embedder-so-ds-feb20      | code               | so-ds-feb20                                            | nbs/ncs/ncs.ipynb           | | ncs-embedder-staqc-py      | code               | staqc-py-cleaned                              | nbs/ncs/ncs.ipynb           | | tnbow-embedder-so-ds-feb20 | description        | so-python-question-titles-feb20                        | nbs/tnbow/tnbow.ipynb       | | use-embedder-pacs          | description        | so-duplicates-pacsv1-train                             | nbs/tuse/tuse_tuned.ipynb   | | ensemble-embedder-pacs     | description + code | staqc-py-cleaned + so-duplicates-pacs-train | nbs/ensemble/ensemble.ipynb |  ## Datasets  This project provides a consistent interface to download and load datasets related to code search.  ### Snippet collections  ####  Example: Load a snippet collection  ```python from codesearch.data import load_snippet_collection collection_name = ""so-ds-feb20"" snippets = load_snippet_collection(collection_name) ```  #### Available snippet collections In the table below you find which snippet collections can be loaded.",codesearch,SOFTWARE
"Stack Overflow dumps can be found here: https://archive.org/details/stackexchange, [LICENSE](https://creativecommons.org/licenses/by-sa/4.0/)                                                             | | staqc-py-cleaned                     | Derived from the Python StaQC snippets (additional cleaning was done as decribed in the paper).",StaQC,SOFTWARE
"To download and load the title pairs from Stack Overflow duplicate posts run:  ```python from codesearch.data import load_train_dataset duplicate_records = load_train_dataset(""so-duplicates-pacs-train"") ```  These duplicate records have been filtered to ensure that there is no overlap with the `so-ds-feb20` and `staqc-py` evaluation datasets.",codesearch,SOFTWARE
"To download a text file with Stack Overflow post titles tagged with Python (used for the TNBOW baseline) run:   ```python from codesearch.data import load_train_dataset filename = load_train_dataset(""so-python-question-titles-feb20"") ```  ## Demo notebook   You can run the demo notebook `nbs/demo/demo.ipynb` to quickly try out any of the pretrained models on one of the snippet collections.  ## Benchmark on PACS  To replicate the results of our paper or evaluate your own model on the PACS benchmark, have a look at `nbs/evaluate.ipynb` and `codesearch/benchmark.ipynb`.",codesearch,SOFTWARE
"To download a text file with Stack Overflow post titles tagged with Python (used for the TNBOW baseline) run:   ```python from codesearch.data import load_train_dataset filename = load_train_dataset(""so-python-question-titles-feb20"") ```  ## Demo notebook   You can run the demo notebook `nbs/demo/demo.ipynb` to quickly try out any of the pretrained models on one of the snippet collections.  ## Benchmark on PACS  To replicate the results of our paper or evaluate your own model on the PACS benchmark, have a look at `nbs/evaluate.ipynb` and `codesearch/benchmark.ipynb`.",codesearch,SOFTWARE
"A custom embedding model class should implement the `embed_snippets` and `embed_queries` functions (similar to `codesearch/tuse/tuse_embedder.py`, `codesearch/tnbow/tnbow_embedder.py`, `codesearch/ncs/ncs_embedder.py` etc.).  #### Example: Benchmark a model on PACS  ```python from codesearch.benchmark import benchmark_on_pacs  benchmark_on_pacs(     model_path=model_path, # one of the pretrained model names or a path to a model that can be loaded with `codesearch.utils.load_model`     output_dir=output_dir ) ```",codesearch,SOFTWARE
"A custom embedding model class should implement the `embed_snippets` and `embed_queries` functions (similar to `codesearch/tuse/tuse_embedder.py`, `codesearch/tnbow/tnbow_embedder.py`, `codesearch/ncs/ncs_embedder.py` etc.).  #### Example: Benchmark a model on PACS  ```python from codesearch.benchmark import benchmark_on_pacs  benchmark_on_pacs(     model_path=model_path, # one of the pretrained model names or a path to a model that can be loaded with `codesearch.utils.load_model`     output_dir=output_dir ) ```",codesearch,SOFTWARE
"A custom embedding model class should implement the `embed_snippets` and `embed_queries` functions (similar to `codesearch/tuse/tuse_embedder.py`, `codesearch/tnbow/tnbow_embedder.py`, `codesearch/ncs/ncs_embedder.py` etc.).  #### Example: Benchmark a model on PACS  ```python from codesearch.benchmark import benchmark_on_pacs  benchmark_on_pacs(     model_path=model_path, # one of the pretrained model names or a path to a model that can be loaded with `codesearch.utils.load_model`     output_dir=output_dir ) ```",codesearch,SOFTWARE
"A custom embedding model class should implement the `embed_snippets` and `embed_queries` functions (similar to `codesearch/tuse/tuse_embedder.py`, `codesearch/tnbow/tnbow_embedder.py`, `codesearch/ncs/ncs_embedder.py` etc.).  #### Example: Benchmark a model on PACS  ```python from codesearch.benchmark import benchmark_on_pacs  benchmark_on_pacs(     model_path=model_path, # one of the pretrained model names or a path to a model that can be loaded with `codesearch.utils.load_model`     output_dir=output_dir ) ```",codesearch,SOFTWARE
"A custom embedding model class should implement the `embed_snippets` and `embed_queries` functions (similar to `codesearch/tuse/tuse_embedder.py`, `codesearch/tnbow/tnbow_embedder.py`, `codesearch/ncs/ncs_embedder.py` etc.).  #### Example: Benchmark a model on PACS  ```python from codesearch.benchmark import benchmark_on_pacs  benchmark_on_pacs(     model_path=model_path, # one of the pretrained model names or a path to a model that can be loaded with `codesearch.utils.load_model`     output_dir=output_dir ) ```",codesearch,SOFTWARE
# TAG-CF: Test-time Aggregation for CF  Source code for the paper **[How Does Message Passing Improve Collaborative Filtering?],TAG-CF,SOFTWARE
The paper proposes TAG-CF which is a test-time aggregation framework that can be utilized as a plug-and-play module to enhance the performance of matrix factorization models.  ## 1.,TAG-CF,SOFTWARE
Installation  Please install all dependencies using the command: ``` conda create --name <env> --file requirements.txt ```  ## 2.,conda,SOFTWARE
"Results This is a list containing performance of MF trained by BPR and DirectAU, along with the relative performance improvement brought by TAG-CF.",BPR,SOFTWARE
"Results This is a list containing performance of MF trained by BPR and DirectAU, along with the relative performance improvement brought by TAG-CF.",DirectAU,SOFTWARE
"Results This is a list containing performance of MF trained by BPR and DirectAU, along with the relative performance improvement brought by TAG-CF.",TAG-CF,SOFTWARE
"To play around with ```m``` and ```n``` in TAG-CF, feel free to accordingly change individual config files.",TAG-CF,SOFTWARE
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,TAGCF,SOFTWARE
||DirectAU|+TAGCF|Impr. (%)|BPR|+TAGCF|Impr. (%) |:-|:-:|:-:|:-:|:-:|:-:|:-:| ||||Amazon-Book Recall@10 | 0.0619 | 0.0630 | 1.83% | 0.0396 | 0.0454 | 14.55% Recall@20 | 0.0959 | 0.0972 | 1.32% | 0.0652 | 0.0737 | 13.04% NDCG@10   | 0.0527 | 0.0536 | 1.60% | 0.0330 | 0.0384 | 16.47% NDCG@20   | 0.0641 | 0.0650 | 1.54% | 0.0416 | 0.0479 | 15.06% ||||Yelp-2018 Recall@10 | 0.0618 | 0.0633 | 2.44% | 0.0456 | 0.0475 | 4.25% Recall@20 | 0.0997 | 0.1018 | 2.09% | 0.0761 | 0.0793 | 4.15% NDCG@10   | 0.0517 | 0.0532 | 2.80% | 0.0385 | 0.0403 | 4.68% NDCG@20   | 0.0647 | 0.0663 | 2.56% | 0.0489 | 0.0513 | 4.71% ||||Gowalla Recall@10 | 0.1045 | 0.1072 | 1.72% | 0.0869 | 0.0914 | 5.15% Recall@20 | 0.1528 | 0.1566 | 1.71% | 0.1272 | 0.1333 | 4.73% NDCG@10   | 0.0825 | 0.0839 | 2.66% | 0.0690 | 0.0719 | 4.16% NDCG@20   | 0.0976 | 0.0993 | 2.50% | 0.0816 | 0.0850 | 4.17% ||||MovieLens-1M Recall@10 | 0.1223 | 0.1602 | 31.00% | 0.1276 | 0.1319 | 3.35% Recall@20 | 0.1899 | 0.2452 | 29.14% | 0.2179 | 0.2187 | 0.35% NDCG@10   | 0.1521 | 0.2085 | 37.03% | 0.1528 | 0.1571 | 2.82% NDCG@20   | 0.1667 | 0.2233 | 33.93% | 0.1800 | 0.1821 | 1.15%  ## 5.,TAGCF,SOFTWARE
All the software requirements are already pre-installed in the Docker image below.,Docker,SOFTWARE
"Note that `DGL 0.8` is not released yet when I did this work, so I installed `DGL 0.8` manually from the source code.",DGL 0.8,SOFTWARE
"Note that `DGL 0.8` is not released yet when I did this work, so I installed `DGL 0.8` manually from the source code.",DGL 0.8,SOFTWARE
PyTorch version should be equal to or greater than 1.11.0.,PyTorch,SOFTWARE
```bash    docker pull lizytalk/dejavu    ``` 2.,docker,SOFTWARE
```bash    docker pull lizytalk/dejavu    ``` 2.,lizytalk/dejavu,SOFTWARE
Pull the code from GitHub    ```bash    git pull https://github.com/NetManAIOps/DejaVu.git DejaVu    ``` 3.,git,SOFTWARE
"I use the command `realpath` in the example commands below, which is not bundled in macOS and Windows.",realpath,SOFTWARE
"On macOS, you can install it by `brew install coreutils`. 5.",brew,SOFTWARE
"On macOS, you can install it by `brew install coreutils`. 5.",coreutils,SOFTWARE
Start a Docker container with our image and enter its shell    ```bash    docker run -it --rm -v $(realpath DejaVu):/workspace lizytalk/dejavu bash    ``` 6.,Docker,SOFTWARE
Start a Docker container with our image and enter its shell    ```bash    docker run -it --rm -v $(realpath DejaVu):/workspace lizytalk/dejavu bash    ``` 6.,docker,SOFTWARE
Start a Docker container with our image and enter its shell    ```bash    docker run -it --rm -v $(realpath DejaVu):/workspace lizytalk/dejavu bash    ``` 6.,lizytalk/dejavu,SOFTWARE
Start a Docker container with our image and enter its shell    ```bash    docker run -it --rm -v $(realpath DejaVu):/workspace lizytalk/dejavu bash    ``` 6.,bash,SOFTWARE
Run `direnv allow` in the shell of the Docker container to set the environment variables. 7.,direnv,SOFTWARE
Run `direnv allow` in the shell of the Docker container to set the environment variables. 7.,Docker,SOFTWARE
Note that the pickle files are not compatible in different Python and Pandas versions.,Pandas,SOFTWARE
"To download and extract dataset V2, run:  ```shell wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz mkdir data2 mv .",wget,SOFTWARE
"To download and extract dataset V2, run:  ```shell wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz mkdir data2 mv .",tensorflow,SOFTWARE
/speech_commands_v0.02.tar.gz cd ../ ```  And similarly for V1:  ```shell wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz mkdir data1 mv .,tensorflow,SOFTWARE
/speech_commands_v0.01.tar.gz cd ../ ```  ### Install dependencies  Set up a new virtual environment:  ```shell pip install virtualenv virtualenv --system-site-packages -p python3 .,pip,SOFTWARE
/speech_commands_v0.01.tar.gz cd ../ ```  ### Install dependencies  Set up a new virtual environment:  ```shell pip install virtualenv virtualenv --system-site-packages -p python3 .,virtualenv,SOFTWARE
/speech_commands_v0.01.tar.gz cd ../ ```  ### Install dependencies  Set up a new virtual environment:  ```shell pip install virtualenv virtualenv --system-site-packages -p python3 .,virtualenv,SOFTWARE
/speech_commands_v0.01.tar.gz cd ../ ```  ### Install dependencies  Set up a new virtual environment:  ```shell pip install virtualenv virtualenv --system-site-packages -p python3 .,python3,SOFTWARE
"/venv3/bin/activate ```  To install dependencies, run  ```shell pip install -r requirements.txt ```  Tested using Tensorflow 2.4.0rc1 with CUDA 11.",pip,SOFTWARE
"/venv3/bin/activate ```  To install dependencies, run  ```shell pip install -r requirements.txt ```  Tested using Tensorflow 2.4.0rc1 with CUDA 11.",Tensorflow 2.4.0rc1,SOFTWARE
"/venv3/bin/activate ```  To install dependencies, run  ```shell pip install -r requirements.txt ```  Tested using Tensorflow 2.4.0rc1 with CUDA 11.",CUDA 11,SOFTWARE
**Note**: Installing the correct Tensorflow version is important for reproducibility!,Tensorflow,SOFTWARE
Using more recent versions of Tensorflow results in small accuracy differences each time the model is evaluated.,Tensorflow,SOFTWARE
"The model-specific arguments for KWT are:  ```shell --num_layers 12 \ #number of sequential transformer encoders --heads 3 \ #number of attentions heads --d_model 192 \ #embedding dimension --mlp_dim 768 \ #mlp-dimension --dropout1 0. \ #dropout in mlp/multi-head attention blocks --attention_type 'time' \ #attention type: 'time', 'freq', 'both' or 'patch' --patch_size '1,40' \ #spectrogram patch_size, if patch attention is used --prenorm False \ # if False, use postnorm ```  ## Training with distillation  We employ hard distillation from a convolutional model (Att-MH-RNN), similar to the approach in [DeIT](https://github.com/facebookresearch/deit).",KWT,SOFTWARE
"The model-specific arguments for KWT are:  ```shell --num_layers 12 \ #number of sequential transformer encoders --heads 3 \ #number of attentions heads --d_model 192 \ #embedding dimension --mlp_dim 768 \ #mlp-dimension --dropout1 0. \ #dropout in mlp/multi-head attention blocks --attention_type 'time' \ #attention type: 'time', 'freq', 'both' or 'patch' --patch_size '1,40' \ #spectrogram patch_size, if patch attention is used --prenorm False \ # if False, use postnorm ```  ## Training with distillation  We employ hard distillation from a convolutional model (Att-MH-RNN), similar to the approach in [DeIT](https://github.com/facebookresearch/deit).",Att-MH-RNN,SOFTWARE
"The model-specific arguments for KWT are:  ```shell --num_layers 12 \ #number of sequential transformer encoders --heads 3 \ #number of attentions heads --d_model 192 \ #embedding dimension --mlp_dim 768 \ #mlp-dimension --dropout1 0. \ #dropout in mlp/multi-head attention blocks --attention_type 'time' \ #attention type: 'time', 'freq', 'both' or 'patch' --patch_size '1,40' \ #spectrogram patch_size, if patch attention is used --prenorm False \ # if False, use postnorm ```  ## Training with distillation  We employ hard distillation from a convolutional model (Att-MH-RNN), similar to the approach in [DeIT](https://github.com/facebookresearch/deit).",DeIT,SOFTWARE
"The model-specific arguments for KWT are:  ```shell --num_layers 12 \ #number of sequential transformer encoders --heads 3 \ #number of attentions heads --d_model 192 \ #embedding dimension --mlp_dim 768 \ #mlp-dimension --dropout1 0. \ #dropout in mlp/multi-head attention blocks --attention_type 'time' \ #attention type: 'time', 'freq', 'both' or 'patch' --patch_size '1,40' \ #spectrogram patch_size, if patch attention is used --prenorm False \ # if False, use postnorm ```  ## Training with distillation  We employ hard distillation from a convolutional model (Att-MH-RNN), similar to the approach in [DeIT](https://github.com/facebookresearch/deit).",deit,SOFTWARE
Some source files are derived from the [KWS streaming repository](https://github.com/google-research/google-research/tree/master/kws_streaming) by Google Research.,KWS streaming repository,SOFTWARE
Some source files are derived from the [KWS streaming repository](https://github.com/google-research/google-research/tree/master/kws_streaming) by Google Research.,kws_streaming,SOFTWARE
"The naming convention we follow for the pruned models is straightforward; for instance, `ERES10_FPGM` refers to the EResFD model pruned with 10% sparsity using the FPGM technique. - `torchscript/`: All the required files for android deployment of the EResFD model (and its pruned versions) using the torchscript framework.  ## Prerequisites  Before running the pruning scripts, the user needs to prepare the necessary dataset:  ### WIDER FACE Dataset  The models are trained and evaluated using the WIDER FACE dataset.",ERES10_FPGM,SOFTWARE
"The naming convention we follow for the pruned models is straightforward; for instance, `ERES10_FPGM` refers to the EResFD model pruned with 10% sparsity using the FPGM technique. - `torchscript/`: All the required files for android deployment of the EResFD model (and its pruned versions) using the torchscript framework.  ## Prerequisites  Before running the pruning scripts, the user needs to prepare the necessary dataset:  ### WIDER FACE Dataset  The models are trained and evaluated using the WIDER FACE dataset.",EResFD,SOFTWARE
"The naming convention we follow for the pruned models is straightforward; for instance, `ERES10_FPGM` refers to the EResFD model pruned with 10% sparsity using the FPGM technique. - `torchscript/`: All the required files for android deployment of the EResFD model (and its pruned versions) using the torchscript framework.  ## Prerequisites  Before running the pruning scripts, the user needs to prepare the necessary dataset:  ### WIDER FACE Dataset  The models are trained and evaluated using the WIDER FACE dataset.",torchscript,SOFTWARE
"The naming convention we follow for the pruned models is straightforward; for instance, `ERES10_FPGM` refers to the EResFD model pruned with 10% sparsity using the FPGM technique. - `torchscript/`: All the required files for android deployment of the EResFD model (and its pruned versions) using the torchscript framework.  ## Prerequisites  Before running the pruning scripts, the user needs to prepare the necessary dataset:  ### WIDER FACE Dataset  The models are trained and evaluated using the WIDER FACE dataset.",EResFD,SOFTWARE
"The naming convention we follow for the pruned models is straightforward; for instance, `ERES10_FPGM` refers to the EResFD model pruned with 10% sparsity using the FPGM technique. - `torchscript/`: All the required files for android deployment of the EResFD model (and its pruned versions) using the torchscript framework.  ## Prerequisites  Before running the pruning scripts, the user needs to prepare the necessary dataset:  ### WIDER FACE Dataset  The models are trained and evaluated using the WIDER FACE dataset.",torchscript,SOFTWARE
"Additionally, the code was developed using Python 3.11.7, CUDA 11.4 and Ubuntu 20.04.06 LTS.",Python 3.11.7,SOFTWARE
"Additionally, the code was developed using Python 3.11.7, CUDA 11.4 and Ubuntu 20.04.06 LTS.",Ubuntu 20.04.06 LTS,SOFTWARE
"To ensure compatibility and proper functionality of the pruning scripts, please install the specific versions of the python packages listed in the requirements.txt file, using the following command:  ```bash pip install -r requirements.txt ```  ## Running the Scripts for Pruning a Face Detector  The pruning script executes the model pruning process as outlined in Section 4.2 of our paper.",pip,SOFTWARE
"E.g., for pruning with the Geometric Median (FPGM) algorithm the EResFD model:   ```bash   python fpgm.py --pruning_rate 0.1 --pruned_eres '.",EResFD,SOFTWARE
"E.g., for pruning with the Geometric Median (FPGM) algorithm the EResFD model:   ```bash   python fpgm.py --pruning_rate 0.1 --pruned_eres '.",python,SOFTWARE
[Version](https://badge.fury.io/py/karateclub.svg?,karateclub,SOFTWARE
[License](https://img.shields.io/github/license/benedekrozemberczki/karateclub.svg) [!,karateclub,SOFTWARE
[repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/karateclub.svg)](https://github.com/benedekrozemberczki/karateclub/archive/master.zip)  [!,karateclub,SOFTWARE
[repo size](https://img.shields.io/github/repo-size/benedekrozemberczki/karateclub.svg)](https://github.com/benedekrozemberczki/karateclub/archive/master.zip)  [!,karateclub,SOFTWARE
[build badge](https://github.com/benedekrozemberczki/karateclub/workflows/CI/badge.svg)](https://github.com/benedekrozemberczki/karateclub/actions?,karateclub,SOFTWARE
[build badge](https://github.com/benedekrozemberczki/karateclub/workflows/CI/badge.svg)](https://github.com/benedekrozemberczki/karateclub/actions?,karateclub,SOFTWARE
[coverage badge](https://codecov.io/gh/benedekrozemberczki/karateclub/branch/master/graph/badge.svg)](https://codecov.io/github/benedekrozemberczki/karateclub?,karateclub,SOFTWARE
[coverage badge](https://codecov.io/gh/benedekrozemberczki/karateclub/branch/master/graph/badge.svg)](https://codecov.io/github/benedekrozemberczki/karateclub?,karateclub,SOFTWARE
"branch=master) <p align=""center"">   <img width=""90%"" src=""https://github.com/benedekrozemberczki/karateclub/blob/master/karatelogo.jpg?",karateclub,SOFTWARE
"sanitize=true"" /> </p>  ------------------------------------------------------   **Karate Club** is an unsupervised machine learning extension library for [NetworkX](https://networkx.github.io/).",Karate Club,SOFTWARE
"sanitize=true"" /> </p>  ------------------------------------------------------   **Karate Club** is an unsupervised machine learning extension library for [NetworkX](https://networkx.github.io/).",NetworkX,SOFTWARE
"sanitize=true"" /> </p>  ------------------------------------------------------   **Karate Club** is an unsupervised machine learning extension library for [NetworkX](https://networkx.github.io/).",networkx,SOFTWARE
"Please look at the **[Documentation](https://karateclub.readthedocs.io/)**, relevant **[Paper](https://arxiv.org/abs/2003.04819)**, **[Promo Video](https://www.youtube.com/watch?",karateclub,SOFTWARE
"v=t212-ntxu2U)**, and **[External Resources](https://karateclub.readthedocs.io/en/latest/notes/resources.html)**.",karateclub,SOFTWARE
*Karate Club* consists of state-of-the-art methods to do unsupervised learning on graph structured data.,Karate Club,SOFTWARE
"The newly introduced graph classification datasets are available at [SNAP](https://snap.stanford.edu/data/#disjointgraphs), [TUD Graph Kernel Datasets](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets), and [GraphLearning.io](https://chrsmrrs.github.io/datasets/).  --------------------------------------------------------------  **Citing**  If you find *Karate Club* and the new datasets useful in your research, please consider citing the following paper:  ```bibtex @inproceedings{karateclub,        title = {{Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs}},        author = {Benedek Rozemberczki and Oliver Kiss and Rik Sarkar},        year = {2020},        pages = {3125‚Äì3132},        booktitle = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)},        organization = {ACM}, } ``` ----------------------------------------------------------------  **A simple example**  *Karate Club* makes the use of modern community detection techniques quite easy (see [here](https://karateclub.readthedocs.io/en/latest/notes/introduction.html) for the accompanying tutorial).",Karate Club,SOFTWARE
"The newly introduced graph classification datasets are available at [SNAP](https://snap.stanford.edu/data/#disjointgraphs), [TUD Graph Kernel Datasets](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets), and [GraphLearning.io](https://chrsmrrs.github.io/datasets/).  --------------------------------------------------------------  **Citing**  If you find *Karate Club* and the new datasets useful in your research, please consider citing the following paper:  ```bibtex @inproceedings{karateclub,        title = {{Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs}},        author = {Benedek Rozemberczki and Oliver Kiss and Rik Sarkar},        year = {2020},        pages = {3125‚Äì3132},        booktitle = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)},        organization = {ACM}, } ``` ----------------------------------------------------------------  **A simple example**  *Karate Club* makes the use of modern community detection techniques quite easy (see [here](https://karateclub.readthedocs.io/en/latest/notes/introduction.html) for the accompanying tutorial).",Karate Club,SOFTWARE
"The newly introduced graph classification datasets are available at [SNAP](https://snap.stanford.edu/data/#disjointgraphs), [TUD Graph Kernel Datasets](https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets), and [GraphLearning.io](https://chrsmrrs.github.io/datasets/).  --------------------------------------------------------------  **Citing**  If you find *Karate Club* and the new datasets useful in your research, please consider citing the following paper:  ```bibtex @inproceedings{karateclub,        title = {{Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs}},        author = {Benedek Rozemberczki and Oliver Kiss and Rik Sarkar},        year = {2020},        pages = {3125‚Äì3132},        booktitle = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)},        organization = {ACM}, } ``` ----------------------------------------------------------------  **A simple example**  *Karate Club* makes the use of modern community detection techniques quite easy (see [here](https://karateclub.readthedocs.io/en/latest/notes/introduction.html) for the accompanying tutorial).",karateclub,SOFTWARE
"For example, this is all it takes to use on a Watts-Strogatz graph [Ego-splitting](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf):  ```python import networkx as nx from karateclub import EgoNetSplitter  g = nx.newman_watts_strogatz_graph(1000, 20, 0.05)  splitter = EgoNetSplitter(1.0)  splitter.fit(g)  print(splitter.get_memberships()) ```  ----------------------------------------------------------------  **Models included**  In detail, the following community detection and embedding methods were implemented.",python,SOFTWARE
"For example, this is all it takes to use on a Watts-Strogatz graph [Ego-splitting](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf):  ```python import networkx as nx from karateclub import EgoNetSplitter  g = nx.newman_watts_strogatz_graph(1000, 20, 0.05)  splitter = EgoNetSplitter(1.0)  splitter.fit(g)  print(splitter.get_memberships()) ```  ----------------------------------------------------------------  **Models included**  In detail, the following community detection and embedding methods were implemented.",networkx,SOFTWARE
"For example, this is all it takes to use on a Watts-Strogatz graph [Ego-splitting](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf):  ```python import networkx as nx from karateclub import EgoNetSplitter  g = nx.newman_watts_strogatz_graph(1000, 20, 0.05)  splitter = EgoNetSplitter(1.0)  splitter.fit(g)  print(splitter.get_memberships()) ```  ----------------------------------------------------------------  **Models included**  In detail, the following community detection and embedding methods were implemented.",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"**Overlapping Community Detection**  * **[DANMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.danmf.DANMF)** from Ye *et al.*: [Deep Autoencoder-like Nonnegative Matrix Factorization for Community Detection](https://github.com/benedekrozemberczki/DANMF/blob/master/18DANMF.pdf) (CIKM 2018)  * **[M-NMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.mnmf.M_NMF)** from Wang *et al.*: [Community Preserving Network Embedding](https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14589) (AAAI 2017)  * **[Ego-Splitting](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.ego_splitter.EgoNetSplitter)** from Epasto *et al.*: [Ego-splitting Framework: from Non-Overlapping to Overlapping Clusters](https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf) (KDD 2017)  * **[NNSED](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.nnsed.NNSED)** from Sun *et al.*: [A Non-negative Symmetric Encoder-Decoder Approach for Community Detection](http://www.bigdatalab.ac.cn/~shenhuawei/publications/2017/cikm-sun.pdf) (CIKM 2017)  * **[BigClam](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.bigclam.BigClam)** from Yang and Leskovec: [Overlapping Community Detection at Scale: A Nonnegative Matrix Factorization Approach](http://infolab.stanford.edu/~crucis/pubs/paper-nmfagm.pdf) (WSDM 2013)  * **[SymmNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.overlapping.symmnmf.SymmNMF)** from Kuang *et al.*: [Symmetric Nonnegative Matrix Factorization for Graph Clustering](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) (SDM 2012)  **Non-Overlapping Community Detection**  * **[GEMSEC](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.gemsec.GEMSEC)** from Rozemberczki *et al.*: [GEMSEC: Graph Embedding with Self Clustering](https://arxiv.org/abs/1802.03997) (ASONAM 2019)  * **[EdMot](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.edmot.EdMot)** from Li *et al.*: [EdMot: An Edge Enhancement Approach for Motif-aware Community Detection](https://arxiv.org/abs/1906.04560) (KDD 2019)  * **[SCD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.scd.SCD)** from Prat-Perez *et al.*: [High Quality, Scalable and Parallel Community Detectionfor Large Real Graphs](http://wwwconference.org/proceedings/www2014/proceedings/p225.pdf) (WWW 2014)  * **[Label Propagation](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.community_detection.non_overlapping.label_propagation.LabelPropagation)** from Raghavan *et al.*: [Near Linear Time Algorithm to Detect Community Structures in Large-Scale Networks](https://arxiv.org/abs/0709.2938) (Physics Review E 2007)   **Proximity Preserving Node Embedding**  * **[GraRep](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.grarep.GraRep)** from Cao *et al.*: [GraRep: Learning Graph Representations with Global Structural Information](https://dl.acm.org/citation.cfm?",karateclub,SOFTWARE
"id=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!",karateclub,SOFTWARE
"id=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!",karateclub,SOFTWARE
"id=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!",karateclub,SOFTWARE
"id=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!",karateclub,SOFTWARE
"id=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!",karateclub,SOFTWARE
"id=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!",karateclub,SOFTWARE
"id=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!",karateclub,SOFTWARE
"id=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!",karateclub,SOFTWARE
"id=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!",karateclub,SOFTWARE
"id=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!",karateclub,SOFTWARE
"id=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!",karateclub,SOFTWARE
"id=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!",karateclub,SOFTWARE
"id=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!",karateclub,SOFTWARE
"id=2806512) (CIKM 2015)  * **[DeepWalk](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.deepwalk.DeepWalk)** from Perozzi *et al.*: [DeepWalk: Online Learning of Social Representations](https://arxiv.org/abs/1403.6652) (KDD 2014)  * **[Node2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.node2vec.Node2Vec)** from Grover *et al.*: [node2vec: Scalable Feature Learning for Networks](https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf) (KDD 2016)  * **[SocioDim](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.sociodim.SocioDim)** from Tang *et al.*: [Relational Learning via Latent Social Dimensions](ttp://www.public.asu.edu/~huanliu/papers/kdd09.pdf) (KDD 2009)  * **[GLEE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.geometriclaplacianeigenmaps.GLEE)** from Torres *et al.*: [GLEE: Geometric Laplacian Eigenmap Embedding](https://arxiv.org/abs/1905.09763) (Journal of Complex Networks 2020)  * **[BoostNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.boostne.BoostNE)** from Li *et al.*: [Multi-Level Network Embedding with Boosted Low-Rank Matrix Approximation](https://arxiv.org/abs/1808.08627) (ASONAM 2019)  * **[NodeSketch](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nodesketch.NodeSketch)**  from Yang *et al.*: [NodeSketch: Highly-Efficient Graph Embeddings via Recursive Sketching](https://exascale.info/assets/pdf/yang2019nodesketch.pdf) (KDD 2019)  * **[Diff2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.diff2vec.Diff2Vec)** from Rozemberczki and Sarkar: [Fast Sequence Based Embedding with Diffusion Graphs](https://arxiv.org/abs/2001.07463) (CompleNet 2018)  * **[NetMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.netmf.NetMF)** from Qiu *et al.*: [Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE, and Node2Vec](https://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf) (WSDM 2018)  * **[RandNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.randne.RandNE)** from Zhang *et al.*: [Billion-scale Network Embedding with Iterative Random Projection](https://arxiv.org/abs/1805.02396) (ICDM 2018)  * **[Walklets](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.walklets.Walklets)** from Perozzi *et al.*: [Don't Walk, Skip!",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"Online Learning of Multi-scale Network Embeddings](https://arxiv.org/abs/1605.02115) (ASONAM 2017)  * **[HOPE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.hope.HOPE)** from Ou *et al.*: [Asymmetric Transitivity Preserving Graph Embedding](https://dl.acm.org/doi/abs/10.1145/2939672.2939751) (KDD 2016)  * **[NMF-ADMM](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.nmfadmm.NMFADMM)** from Sun and F√©votte: [Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence](http://statweb.stanford.edu/~dlsun/papers/nmf_admm.pdf) (ICASSP 2014)  * **[Laplacian Eigenmaps](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.neighbourhood.laplacianeigenmaps.LaplacianEigenmaps)** from Belkin and Niyogi: [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](https://papers.nips.cc/paper/1961-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering) (NIPS 2001)  **Structural Node Level Embedding**  * **[GraphWave](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.graphwave.GraphWave)** from Donnat *et al.*: [Learning Structural Node Embeddings via Diffusion Wavelets](https://arxiv.org/abs/1710.10321) (KDD 2018)  * **[Role2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.structural.role2vec.Role2vec)** from Ahmed *et al.*: [Learning Role-based Graph Embeddings](https://arxiv.org/abs/1802.02896) (IJCAI StarAI 2018)  **Attributed Node Level Embedding**  * **[FEATHER-N](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.feathernode.FeatherNode)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[TADW](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tadw.TADW)** from Yang *et al.*: [Network Representation Learning with Rich Text Information](https://www.ijcai.org/Proceedings/15/Papers/299.pdf) (IJCAI 2015)  * **[MUSAE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.musae.MUSAE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[AE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.ae.AE)** from Rozemberczki *et al.*: [Multi-Scale Attributed Node Embedding](https://arxiv.org/abs/1909.13021) (Arxiv 2019)  * **[FSCNMF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.fscnmf.FSCNMF)** from Bandyopadhyay *et al.*: [Fusing Structure and Content via Non-negative Matrix Factorization for Embedding Information Networks](https://arxiv.org/pdf/1804.05313.pdf) (ArXiV 2018)  * **[SINE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.sine.SINE)** from Zhang *et al.*: [SINE: Scalable Incomplete Network Embedding](https://arxiv.org/pdf/1810.06768.pdf) (ICDM 2018)  * **[BANE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.bane.BANE)** from Yang *et al.*: [Binarized Attributed Network Embedding](https://ieeexplore.ieee.org/document/8626170) (ICDM 2018)  * **[TENE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.tene.TENE)** from Yang *et al.*: [Enhanced Network Embedding with Text Information](https://ieeexplore.ieee.org/document/8545577) (ICPR 2018)  * **[ASNE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.attributed.asne.ASNE)** from Liao *et al.*: [Attributed Social Network Embedding](https://arxiv.org/abs/1705.04969) (TKDE 2018)  **Meta Node Embedding**  * **[NEU](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.node_embedding.meta.neu.NEU)** from Yang *et al.*: [Fast Network Embedding Enhancement via High Order Proximity Approximation](https://www.ijcai.org/Proceedings/2017/0544.pdf) (IJCAI 2017)  **Graph Level Embedding**  * **[FEATHER-G](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.feathergraph.FeatherGraph)** from Rozemberczki *et al.*: [Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models](https://arxiv.org/abs/2005.07959) (CIKM 2020)  * **[Graph2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.graph2vec.Graph2Vec)** from Narayanan *et al.*: [Graph2Vec: Learning Distributed Representations of Graphs](https://arxiv.org/abs/1707.05005) (MLGWorkshop 2017)  * **[NetLSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.netlsd.NetLSD)** from Tsitsulin *et al.*: [NetLSD: Hearing the Shape of a Graph](https://arxiv.org/abs/1805.10712) (KDD 2018)  * **[WaveletCharacteristic](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.waveletcharacteristic.WaveletCharacteristic)** from Wang *et al.*: [Graph Embedding via Diffusion-Wavelets-Based Node Feature Distribution Characterization](https://arxiv.org/abs/2109.07016) (CIKM 2021)  * **[IGE](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ige.IGE)** from Galland *et al.*: [Invariant Embedding for Graph Classification](https://graphreason.github.io/papers/16.pdf) (ICML 2019 LRGSD Workshop)  * **[LDP](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.ldp.LDP)** from Cai *et al.*: [A Simple Yet Effective Baseline for Non-Attributed Graph Classification](https://arxiv.org/abs/1811.03508) (ICLR 2019)  * **[GeoScattering](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.geoscattering.GeoScattering)** from Gao *et al.*: [Geometric Scattering for Graph Data Analysis](http://proceedings.mlr.press/v97/gao19e.html) (ICML 2019)  * **[GL2Vec](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.gl2vec.GL2Vec)** from Chen and Koga: [GL2Vec: Graph Embedding Enriched by Line Graphs with Edge Features](https://link.springer.com/chapter/10.1007/978-3-030-36718-3_1) (ICONIP 2019)  * **[SF](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.sf.SF)** from de Lara and Pineau: [A Simple Baseline Algorithm for Graph Classification](https://arxiv.org/abs/1810.09155) (NeurIPS RRL Workshop 2018)  * **[FGSD](https://karateclub.readthedocs.io/en/latest/modules/root.html#karateclub.graph_embedding.fgsd.FGSD)** from Verma and Zhang: [Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs](https://papers.nips.cc/paper/6614-hunt-for-the-unique-stable-sparse-and-fast-feature-learning-on-graphs.pdf) (NeurIPS 2017)  Head over to our [documentation](https://karateclub.readthedocs.io) to find out more about installation and data handling, a full list of implemented methods, and datasets.",karateclub,SOFTWARE
"For a quick start, check out our [examples](https://github.com/benedekrozemberczki/karateclub/tree/master/examples.py).",karateclub,SOFTWARE
"If you notice anything unexpected, please open an [issue](https://github.com/benedekrozemberczki/karateclub/issues) and let us know.",karateclub,SOFTWARE
"If you are missing a specific method, feel free to open a [feature request](https://github.com/benedekrozemberczki/karateclub/issues).",karateclub,SOFTWARE
We are motivated to constantly make Karate Club even better.   --------------------------------------------------------------------------------  **Installation**  Karate Club can be installed with the following pip command.,Karate Club,SOFTWARE
We are motivated to constantly make Karate Club even better.   --------------------------------------------------------------------------------  **Installation**  Karate Club can be installed with the following pip command.,Karate Club,SOFTWARE
"```sh $ pip install karateclub ```  As we create new releases frequently, upgrading the package casually might be beneficial.",pip,SOFTWARE
"```sh $ pip install karateclub ```  As we create new releases frequently, upgrading the package casually might be beneficial.",karateclub,SOFTWARE
```sh $ pip install karateclub --upgrade ```  --------------------------------------------------------------------------------  **Running examples**  As part of the documentation we provide a number of use cases to show how the clusterings and embeddings can be utilized for downstream learning.,pip,SOFTWARE
```sh $ pip install karateclub --upgrade ```  --------------------------------------------------------------------------------  **Running examples**  As part of the documentation we provide a number of use cases to show how the clusterings and embeddings can be utilized for downstream learning.,karateclub,SOFTWARE
"In order to run one of the examples, the Graph2Vec snippet:  ```sh $ cd examples/whole_graph_embedding/ $ python graph2vec_example.py ```  --------------------------------------------------------------------------------  **Running tests**  From the project's root-level directory:  ```sh $ pytest ```  --------------------------------------------------------------------------------  **License**  - [GNU General Public License v3.0](https://github.com/benedekrozemberczki/karateclub/blob/master/LICENSE)",python,SOFTWARE
"# megaman: Manifold Learning for Millions of Points  <img src=""https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/word2vec_rmetric_plot_no_digits.png"" height=200><img src=""https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/spectra_D4000.png"" height=200><img src=""https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/spectra_Halpha.png"" height=200>  [!",megaman,SOFTWARE
"# megaman: Manifold Learning for Millions of Points  <img src=""https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/word2vec_rmetric_plot_no_digits.png"" height=200><img src=""https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/spectra_D4000.png"" height=200><img src=""https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/spectra_Halpha.png"" height=200>  [!",megaman,SOFTWARE
"# megaman: Manifold Learning for Millions of Points  <img src=""https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/word2vec_rmetric_plot_no_digits.png"" height=200><img src=""https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/spectra_D4000.png"" height=200><img src=""https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/spectra_Halpha.png"" height=200>  [!",megaman,SOFTWARE
"# megaman: Manifold Learning for Millions of Points  <img src=""https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/word2vec_rmetric_plot_no_digits.png"" height=200><img src=""https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/spectra_D4000.png"" height=200><img src=""https://raw.githubusercontent.com/mmp2/megaman/master/doc/images/spectra_Halpha.png"" height=200>  [!",megaman,SOFTWARE
[Anaconda-Server Badge](https://anaconda.org/conda-forge/megaman/badges/downloads.svg)](https://anaconda.org/conda-forge/megaman) [!,Anaconda,SOFTWARE
[Anaconda-Server Badge](https://anaconda.org/conda-forge/megaman/badges/downloads.svg)](https://anaconda.org/conda-forge/megaman) [!,anaconda,SOFTWARE
[Anaconda-Server Badge](https://anaconda.org/conda-forge/megaman/badges/downloads.svg)](https://anaconda.org/conda-forge/megaman) [!,megaman,SOFTWARE
[Anaconda-Server Badge](https://anaconda.org/conda-forge/megaman/badges/downloads.svg)](https://anaconda.org/conda-forge/megaman) [!,anaconda,SOFTWARE
[Anaconda-Server Badge](https://anaconda.org/conda-forge/megaman/badges/downloads.svg)](https://anaconda.org/conda-forge/megaman) [!,megaman,SOFTWARE
[build status](http://img.shields.io/travis/mmp2/megaman/master.svg?,megaman,SOFTWARE
style=flat)](https://travis-ci.org/mmp2/megaman) [!,megaman,SOFTWARE
[version status](http://img.shields.io/pypi/v/megaman.svg?,megaman,SOFTWARE
style=flat)](https://pypi.python.org/pypi/megaman) [!,python,SOFTWARE
style=flat)](https://pypi.python.org/pypi/megaman) [!,megaman,SOFTWARE
style=flat)](https://github.com/mmp2/megaman/blob/master/LICENSE)  ``megaman`` is a scalable manifold learning package implemented in python.,megaman,SOFTWARE
style=flat)](https://github.com/mmp2/megaman/blob/master/LICENSE)  ``megaman`` is a scalable manifold learning package implemented in python.,megaman,SOFTWARE
It has a front-end API designed to be familiar to [scikit-learn](http://scikit-learn.org/) but harnesses the C++ Fast Library for Approximate Nearest Neighbors (FLANN) and the Sparse Symmetric Positive Definite (SSPD) solver Locally Optimal Block Precodition Gradient (LOBPCG) method to scale manifold learning algorithms to large data sets.,scikit-learn,SOFTWARE
It has a front-end API designed to be familiar to [scikit-learn](http://scikit-learn.org/) but harnesses the C++ Fast Library for Approximate Nearest Neighbors (FLANN) and the Sparse Symmetric Positive Definite (SSPD) solver Locally Optimal Block Precodition Gradient (LOBPCG) method to scale manifold learning algorithms to large data sets.,scikit-learn,SOFTWARE
It has a front-end API designed to be familiar to [scikit-learn](http://scikit-learn.org/) but harnesses the C++ Fast Library for Approximate Nearest Neighbors (FLANN) and the Sparse Symmetric Positive Definite (SSPD) solver Locally Optimal Block Precodition Gradient (LOBPCG) method to scale manifold learning algorithms to large data sets.,Fast Library for Approximate Nearest Neighbors,SOFTWARE
It has a front-end API designed to be familiar to [scikit-learn](http://scikit-learn.org/) but harnesses the C++ Fast Library for Approximate Nearest Neighbors (FLANN) and the Sparse Symmetric Positive Definite (SSPD) solver Locally Optimal Block Precodition Gradient (LOBPCG) method to scale manifold learning algorithms to large data sets.,FLANN,SOFTWARE
On a personal computer megaman can embed 1 million data points with hundreds of dimensions in 10 minutes. megaman is designed for researchers and as such caches intermediary steps and indices to allow for fast re-computation with new parameters.,megaman,SOFTWARE
On a personal computer megaman can embed 1 million data points with hundreds of dimensions in 10 minutes. megaman is designed for researchers and as such caches intermediary steps and indices to allow for fast re-computation with new parameters.,megaman,SOFTWARE
"Package documentation can be found at http://mmp2.github.io/megaman/  If you use our software please cite the following JMLR paper:  McQueen, Meila, VanderPlas, & Zhang, ""Megaman: Scalable Manifold Learning in Python"", Journal of Machine Learning Research, Vol 17 no. 14, 2016. http://jmlr.org/papers/v17/16-109.html  You can also find our arXiv paper at http://arxiv.org/abs/1603.02763  ## Examples  - [Tutorial Notebook]( https://github.com/mmp2/megaman/blob/master/examples/megaman_tutorial.ipynb)  ## Installation and Examples in Google Colab  Below it's a tutorial to install megaman on Google Colab, through Conda environment.",megaman,SOFTWARE
"Package documentation can be found at http://mmp2.github.io/megaman/  If you use our software please cite the following JMLR paper:  McQueen, Meila, VanderPlas, & Zhang, ""Megaman: Scalable Manifold Learning in Python"", Journal of Machine Learning Research, Vol 17 no. 14, 2016. http://jmlr.org/papers/v17/16-109.html  You can also find our arXiv paper at http://arxiv.org/abs/1603.02763  ## Examples  - [Tutorial Notebook]( https://github.com/mmp2/megaman/blob/master/examples/megaman_tutorial.ipynb)  ## Installation and Examples in Google Colab  Below it's a tutorial to install megaman on Google Colab, through Conda environment.",megaman,SOFTWARE
"Package documentation can be found at http://mmp2.github.io/megaman/  If you use our software please cite the following JMLR paper:  McQueen, Meila, VanderPlas, & Zhang, ""Megaman: Scalable Manifold Learning in Python"", Journal of Machine Learning Research, Vol 17 no. 14, 2016. http://jmlr.org/papers/v17/16-109.html  You can also find our arXiv paper at http://arxiv.org/abs/1603.02763  ## Examples  - [Tutorial Notebook]( https://github.com/mmp2/megaman/blob/master/examples/megaman_tutorial.ipynb)  ## Installation and Examples in Google Colab  Below it's a tutorial to install megaman on Google Colab, through Conda environment.",megaman,SOFTWARE
"Package documentation can be found at http://mmp2.github.io/megaman/  If you use our software please cite the following JMLR paper:  McQueen, Meila, VanderPlas, & Zhang, ""Megaman: Scalable Manifold Learning in Python"", Journal of Machine Learning Research, Vol 17 no. 14, 2016. http://jmlr.org/papers/v17/16-109.html  You can also find our arXiv paper at http://arxiv.org/abs/1603.02763  ## Examples  - [Tutorial Notebook]( https://github.com/mmp2/megaman/blob/master/examples/megaman_tutorial.ipynb)  ## Installation and Examples in Google Colab  Below it's a tutorial to install megaman on Google Colab, through Conda environment.",Google Colab,SOFTWARE
"Package documentation can be found at http://mmp2.github.io/megaman/  If you use our software please cite the following JMLR paper:  McQueen, Meila, VanderPlas, & Zhang, ""Megaman: Scalable Manifold Learning in Python"", Journal of Machine Learning Research, Vol 17 no. 14, 2016. http://jmlr.org/papers/v17/16-109.html  You can also find our arXiv paper at http://arxiv.org/abs/1603.02763  ## Examples  - [Tutorial Notebook]( https://github.com/mmp2/megaman/blob/master/examples/megaman_tutorial.ipynb)  ## Installation and Examples in Google Colab  Below it's a tutorial to install megaman on Google Colab, through Conda environment.",megaman,SOFTWARE
"Package documentation can be found at http://mmp2.github.io/megaman/  If you use our software please cite the following JMLR paper:  McQueen, Meila, VanderPlas, & Zhang, ""Megaman: Scalable Manifold Learning in Python"", Journal of Machine Learning Research, Vol 17 no. 14, 2016. http://jmlr.org/papers/v17/16-109.html  You can also find our arXiv paper at http://arxiv.org/abs/1603.02763  ## Examples  - [Tutorial Notebook]( https://github.com/mmp2/megaman/blob/master/examples/megaman_tutorial.ipynb)  ## Installation and Examples in Google Colab  Below it's a tutorial to install megaman on Google Colab, through Conda environment.",Google Colab,SOFTWARE
"Package documentation can be found at http://mmp2.github.io/megaman/  If you use our software please cite the following JMLR paper:  McQueen, Meila, VanderPlas, & Zhang, ""Megaman: Scalable Manifold Learning in Python"", Journal of Machine Learning Research, Vol 17 no. 14, 2016. http://jmlr.org/papers/v17/16-109.html  You can also find our arXiv paper at http://arxiv.org/abs/1603.02763  ## Examples  - [Tutorial Notebook]( https://github.com/mmp2/megaman/blob/master/examples/megaman_tutorial.ipynb)  ## Installation and Examples in Google Colab  Below it's a tutorial to install megaman on Google Colab, through Conda environment.",Conda,SOFTWARE
It also provides tutorial of using megaman to build spectral embedding on uniform swiss roll dataset,megaman,SOFTWARE
usp=sharing) - [You can download the Jupyter Notebook version here]( https://github.com/mmp2/megaman/blob/master/examples/megaman_install_usage_colab.ipynb)  ## ~~Installation with Conda~~  <!,Jupyter Notebook,SOFTWARE
usp=sharing) - [You can download the Jupyter Notebook version here]( https://github.com/mmp2/megaman/blob/master/examples/megaman_install_usage_colab.ipynb)  ## ~~Installation with Conda~~  <!,megaman,SOFTWARE
usp=sharing) - [You can download the Jupyter Notebook version here]( https://github.com/mmp2/megaman/blob/master/examples/megaman_install_usage_colab.ipynb)  ## ~~Installation with Conda~~  <!,megaman,SOFTWARE
usp=sharing) - [You can download the Jupyter Notebook version here]( https://github.com/mmp2/megaman/blob/master/examples/megaman_install_usage_colab.ipynb)  ## ~~Installation with Conda~~  <!,Conda,SOFTWARE
"-- The easiest way to install ``megaman`` and its dependencies is with [conda](http://conda.pydata.org/miniconda.html), the cross-platform package manager for the scientific Python ecosystem.",megaman,SOFTWARE
"-- The easiest way to install ``megaman`` and its dependencies is with [conda](http://conda.pydata.org/miniconda.html), the cross-platform package manager for the scientific Python ecosystem.",conda,SOFTWARE
"-- The easiest way to install ``megaman`` and its dependencies is with [conda](http://conda.pydata.org/miniconda.html), the cross-platform package manager for the scientific Python ecosystem.",conda,SOFTWARE
"To install megaman and its dependencies, run  ``` $ conda install megaman --channel=conda-forge ```  Currently builds are available for OSX and Linux, on Python 2.7, 3.4, and 3.5.",megaman,SOFTWARE
"To install megaman and its dependencies, run  ``` $ conda install megaman --channel=conda-forge ```  Currently builds are available for OSX and Linux, on Python 2.7, 3.4, and 3.5.",conda,SOFTWARE
"To install megaman and its dependencies, run  ``` $ conda install megaman --channel=conda-forge ```  Currently builds are available for OSX and Linux, on Python 2.7, 3.4, and 3.5.",megaman,SOFTWARE
"For other operating systems, see the full install instructions below. -->  Due to the change of API, `$ conda install -c conda-forge megaman` is no longer supported.",megaman,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",megaman,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",megaman,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",numpy,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",numpy,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",scipy,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",scipy,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",scikit-learn,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",scikit-learn,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",FLANN,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",flann,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",pyflann,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",flann,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",cython,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",cython,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",gcc,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",g++,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",pyamg,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",pyamg,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",nose,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",nose,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",h5py,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",h5py,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",plotly,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",conda,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",python=3.5,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",python=2.7,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",python=3.6,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",conda,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",nose,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",cython,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",numpy,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",scipy,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",scikit-learn,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",pyflann,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",pyamg,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",h5py,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",plotly,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",megaman,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",megaman,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",megaman,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",python,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",megaman,SOFTWARE
"Please see the full install instructions below to build `megaman` from source.  ## Installation from source  To install megaman from source requires the following:  - [python](http://python.org) tested with versions 2.7, 3.5 and 3.6 - [numpy](http://numpy.org) version 1.8 or higher - [scipy](http://scipy.org) version 0.16.0 or higher - [scikit-learn](http://scikit-learn.org) - [FLANN](http://www.cs.ubc.ca/research/flann/) - [pyflann](http://www.cs.ubc.ca/research/flann/) which offers another method of computing distance matrices (this is bundled with the FLANN source code) - [cython](http://cython.org/) - a C++ compiler such as ``gcc``/``g++``  Optional requirements include  - [pyamg](http://pyamg.org/), which allows for faster decompositions of large matrices - [nose](https://nose.readthedocs.org/) for running the unit tests - [h5py](http://www.h5py.org) for reading testing .mat files - [plotly](https://plot.ly) an graphing library for interactive plot   These requirements can be installed on Linux and MacOSX using the following conda command:  ```shell $ conda create -n manifold_env python=3.5 -y # can also use python=2.7 or python=3.6  $ source activate manifold_env $ conda install --channel=conda-forge -y pip nose coverage cython numpy scipy \                                          scikit-learn pyflann pyamg h5py plotly ```  Clone this repository and `cd` into source repository  ```shell $ cd /tmp/ $ git clone https://github.com/mmp2/megaman.git $ cd megaman ```  Finally, within the source repository, run this command to install the ``megaman`` package itself: ```shell $ python setup.py install ```  ## Unit Tests megaman uses ``nose`` for unit tests.",nose,SOFTWARE
"With ``nose`` installed, type ``` $ make test ``` to run the unit tests.",nose,SOFTWARE
"``megaman`` is tested on Python versions 2.7, 3.4, and 3.5.  ## Authors - [James McQueen](http://www.stat.washington.edu/people/jmcq/) - [Marina Meila](http://www.stat.washington.edu/mmp/) - [Zhongyue Zhang](https://github.com/Jerryzcn) - [Jake VanderPlas](http://www.vanderplas.com) - [Yu-Chia Chen](https://github.com/yuchaz)  ## Other Contributors  - Xiao Wang: lazy rmetric, Nystrom Extension - [Hangliang Ren (Harry)](https://github.com/Harryahh): Installation tutorials, Spectral Embedding  ## Future Work  See this issues list for what we have planned for upcoming releases:  [Future Work](https://github.com/mmp2/megaman/issues/47)",megaman,SOFTWARE
"``megaman`` is tested on Python versions 2.7, 3.4, and 3.5.  ## Authors - [James McQueen](http://www.stat.washington.edu/people/jmcq/) - [Marina Meila](http://www.stat.washington.edu/mmp/) - [Zhongyue Zhang](https://github.com/Jerryzcn) - [Jake VanderPlas](http://www.vanderplas.com) - [Yu-Chia Chen](https://github.com/yuchaz)  ## Other Contributors  - Xiao Wang: lazy rmetric, Nystrom Extension - [Hangliang Ren (Harry)](https://github.com/Harryahh): Installation tutorials, Spectral Embedding  ## Future Work  See this issues list for what we have planned for upcoming releases:  [Future Work](https://github.com/mmp2/megaman/issues/47)",Python,SOFTWARE
"``megaman`` is tested on Python versions 2.7, 3.4, and 3.5.  ## Authors - [James McQueen](http://www.stat.washington.edu/people/jmcq/) - [Marina Meila](http://www.stat.washington.edu/mmp/) - [Zhongyue Zhang](https://github.com/Jerryzcn) - [Jake VanderPlas](http://www.vanderplas.com) - [Yu-Chia Chen](https://github.com/yuchaz)  ## Other Contributors  - Xiao Wang: lazy rmetric, Nystrom Extension - [Hangliang Ren (Harry)](https://github.com/Harryahh): Installation tutorials, Spectral Embedding  ## Future Work  See this issues list for what we have planned for upcoming releases:  [Future Work](https://github.com/mmp2/megaman/issues/47)",megaman,SOFTWARE
# NetCov NetCov is an open-source tool that can be used with [Batfish](https://github.com/batfish/batfish) to analyze test coverage for network configurations.,NetCov,SOFTWARE
# NetCov NetCov is an open-source tool that can be used with [Batfish](https://github.com/batfish/batfish) to analyze test coverage for network configurations.,NetCov,SOFTWARE
# NetCov NetCov is an open-source tool that can be used with [Batfish](https://github.com/batfish/batfish) to analyze test coverage for network configurations.,Batfish,SOFTWARE
# NetCov NetCov is an open-source tool that can be used with [Batfish](https://github.com/batfish/batfish) to analyze test coverage for network configurations.,batfish,SOFTWARE
"Given a set of Batfish queries, it analyzes which lines of configurations has/has not been covered.",Batfish,SOFTWARE
"NetCov is written in Python and can be used in concert with [pybatfish](https://pybatfish.readthedocs.io/en/latest/notebooks/interacting.html), Batfish's Python API.",NetCov,SOFTWARE
"NetCov is written in Python and can be used in concert with [pybatfish](https://pybatfish.readthedocs.io/en/latest/notebooks/interacting.html), Batfish's Python API.",pybatfish,SOFTWARE
"NetCov is written in Python and can be used in concert with [pybatfish](https://pybatfish.readthedocs.io/en/latest/notebooks/interacting.html), Batfish's Python API.",pybatfish,SOFTWARE
"NetCov is written in Python and can be used in concert with [pybatfish](https://pybatfish.readthedocs.io/en/latest/notebooks/interacting.html), Batfish's Python API.",Batfish,SOFTWARE
"[<img src=""screenshot_demo_video.png""  width=""500"">](https://youtube.com/video/FcBD2LhxqOQ)  ## Features  NetCov supports coverage tracking for the following [Batfish questions](https://pybatfish.readthedocs.io/en/latest/questions.html):  - Reachability test via [traceroute](https://pybatfish.readthedocs.io/en/latest/notebooks/forwarding.html#Traceroute) question - Routing policy evaluation via [testRoutePolicies](https://pybatfish.readthedocs.io/en/latest/notebooks/routingProtocols.html#Test-Route-Policies) question - Direct inspection of routing tables via [routes](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#Routes) and [bgpRib](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#BGP-RIB) questions - Querying interfaces via [interfaceProperties](https://pybatfish.readthedocs.io/en/latest/notebooks/configProperties.html#Interface-Properties) question   NetCov supports configuration syntax of the following vendors:  |                    | Interface          | BGP neighbor       | Routing Policy     | Prefix list        | Community list     | As-path list       | |--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------| | Cisco              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Juniper            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Arista             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Palo Alto Networks | :heavy_check_mark: |                    |                    |                    |                    |                    | | SONiC              |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |   If you‚Äôd like support for additional [vendors](https://pybatfish.readthedocs.io/en/latest/supported_devices.html) (which are supported by Batfish but unsupported by NetCov) or currently-unsupported configuration features, let us know via [GitHub issue](https://github.com/UWNetworksLab/netcov/issues).",NetCov,SOFTWARE
"[<img src=""screenshot_demo_video.png""  width=""500"">](https://youtube.com/video/FcBD2LhxqOQ)  ## Features  NetCov supports coverage tracking for the following [Batfish questions](https://pybatfish.readthedocs.io/en/latest/questions.html):  - Reachability test via [traceroute](https://pybatfish.readthedocs.io/en/latest/notebooks/forwarding.html#Traceroute) question - Routing policy evaluation via [testRoutePolicies](https://pybatfish.readthedocs.io/en/latest/notebooks/routingProtocols.html#Test-Route-Policies) question - Direct inspection of routing tables via [routes](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#Routes) and [bgpRib](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#BGP-RIB) questions - Querying interfaces via [interfaceProperties](https://pybatfish.readthedocs.io/en/latest/notebooks/configProperties.html#Interface-Properties) question   NetCov supports configuration syntax of the following vendors:  |                    | Interface          | BGP neighbor       | Routing Policy     | Prefix list        | Community list     | As-path list       | |--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------| | Cisco              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Juniper            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Arista             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Palo Alto Networks | :heavy_check_mark: |                    |                    |                    |                    |                    | | SONiC              |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |   If you‚Äôd like support for additional [vendors](https://pybatfish.readthedocs.io/en/latest/supported_devices.html) (which are supported by Batfish but unsupported by NetCov) or currently-unsupported configuration features, let us know via [GitHub issue](https://github.com/UWNetworksLab/netcov/issues).",Batfish,SOFTWARE
"[<img src=""screenshot_demo_video.png""  width=""500"">](https://youtube.com/video/FcBD2LhxqOQ)  ## Features  NetCov supports coverage tracking for the following [Batfish questions](https://pybatfish.readthedocs.io/en/latest/questions.html):  - Reachability test via [traceroute](https://pybatfish.readthedocs.io/en/latest/notebooks/forwarding.html#Traceroute) question - Routing policy evaluation via [testRoutePolicies](https://pybatfish.readthedocs.io/en/latest/notebooks/routingProtocols.html#Test-Route-Policies) question - Direct inspection of routing tables via [routes](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#Routes) and [bgpRib](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#BGP-RIB) questions - Querying interfaces via [interfaceProperties](https://pybatfish.readthedocs.io/en/latest/notebooks/configProperties.html#Interface-Properties) question   NetCov supports configuration syntax of the following vendors:  |                    | Interface          | BGP neighbor       | Routing Policy     | Prefix list        | Community list     | As-path list       | |--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------| | Cisco              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Juniper            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Arista             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Palo Alto Networks | :heavy_check_mark: |                    |                    |                    |                    |                    | | SONiC              |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |   If you‚Äôd like support for additional [vendors](https://pybatfish.readthedocs.io/en/latest/supported_devices.html) (which are supported by Batfish but unsupported by NetCov) or currently-unsupported configuration features, let us know via [GitHub issue](https://github.com/UWNetworksLab/netcov/issues).",pybatfish,SOFTWARE
"[<img src=""screenshot_demo_video.png""  width=""500"">](https://youtube.com/video/FcBD2LhxqOQ)  ## Features  NetCov supports coverage tracking for the following [Batfish questions](https://pybatfish.readthedocs.io/en/latest/questions.html):  - Reachability test via [traceroute](https://pybatfish.readthedocs.io/en/latest/notebooks/forwarding.html#Traceroute) question - Routing policy evaluation via [testRoutePolicies](https://pybatfish.readthedocs.io/en/latest/notebooks/routingProtocols.html#Test-Route-Policies) question - Direct inspection of routing tables via [routes](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#Routes) and [bgpRib](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#BGP-RIB) questions - Querying interfaces via [interfaceProperties](https://pybatfish.readthedocs.io/en/latest/notebooks/configProperties.html#Interface-Properties) question   NetCov supports configuration syntax of the following vendors:  |                    | Interface          | BGP neighbor       | Routing Policy     | Prefix list        | Community list     | As-path list       | |--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------| | Cisco              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Juniper            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Arista             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Palo Alto Networks | :heavy_check_mark: |                    |                    |                    |                    |                    | | SONiC              |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |   If you‚Äôd like support for additional [vendors](https://pybatfish.readthedocs.io/en/latest/supported_devices.html) (which are supported by Batfish but unsupported by NetCov) or currently-unsupported configuration features, let us know via [GitHub issue](https://github.com/UWNetworksLab/netcov/issues).",pybatfish,SOFTWARE
"[<img src=""screenshot_demo_video.png""  width=""500"">](https://youtube.com/video/FcBD2LhxqOQ)  ## Features  NetCov supports coverage tracking for the following [Batfish questions](https://pybatfish.readthedocs.io/en/latest/questions.html):  - Reachability test via [traceroute](https://pybatfish.readthedocs.io/en/latest/notebooks/forwarding.html#Traceroute) question - Routing policy evaluation via [testRoutePolicies](https://pybatfish.readthedocs.io/en/latest/notebooks/routingProtocols.html#Test-Route-Policies) question - Direct inspection of routing tables via [routes](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#Routes) and [bgpRib](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#BGP-RIB) questions - Querying interfaces via [interfaceProperties](https://pybatfish.readthedocs.io/en/latest/notebooks/configProperties.html#Interface-Properties) question   NetCov supports configuration syntax of the following vendors:  |                    | Interface          | BGP neighbor       | Routing Policy     | Prefix list        | Community list     | As-path list       | |--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------| | Cisco              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Juniper            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Arista             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Palo Alto Networks | :heavy_check_mark: |                    |                    |                    |                    |                    | | SONiC              |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |   If you‚Äôd like support for additional [vendors](https://pybatfish.readthedocs.io/en/latest/supported_devices.html) (which are supported by Batfish but unsupported by NetCov) or currently-unsupported configuration features, let us know via [GitHub issue](https://github.com/UWNetworksLab/netcov/issues).",pybatfish,SOFTWARE
"[<img src=""screenshot_demo_video.png""  width=""500"">](https://youtube.com/video/FcBD2LhxqOQ)  ## Features  NetCov supports coverage tracking for the following [Batfish questions](https://pybatfish.readthedocs.io/en/latest/questions.html):  - Reachability test via [traceroute](https://pybatfish.readthedocs.io/en/latest/notebooks/forwarding.html#Traceroute) question - Routing policy evaluation via [testRoutePolicies](https://pybatfish.readthedocs.io/en/latest/notebooks/routingProtocols.html#Test-Route-Policies) question - Direct inspection of routing tables via [routes](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#Routes) and [bgpRib](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#BGP-RIB) questions - Querying interfaces via [interfaceProperties](https://pybatfish.readthedocs.io/en/latest/notebooks/configProperties.html#Interface-Properties) question   NetCov supports configuration syntax of the following vendors:  |                    | Interface          | BGP neighbor       | Routing Policy     | Prefix list        | Community list     | As-path list       | |--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------| | Cisco              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Juniper            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Arista             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Palo Alto Networks | :heavy_check_mark: |                    |                    |                    |                    |                    | | SONiC              |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |   If you‚Äôd like support for additional [vendors](https://pybatfish.readthedocs.io/en/latest/supported_devices.html) (which are supported by Batfish but unsupported by NetCov) or currently-unsupported configuration features, let us know via [GitHub issue](https://github.com/UWNetworksLab/netcov/issues).",pybatfish,SOFTWARE
"[<img src=""screenshot_demo_video.png""  width=""500"">](https://youtube.com/video/FcBD2LhxqOQ)  ## Features  NetCov supports coverage tracking for the following [Batfish questions](https://pybatfish.readthedocs.io/en/latest/questions.html):  - Reachability test via [traceroute](https://pybatfish.readthedocs.io/en/latest/notebooks/forwarding.html#Traceroute) question - Routing policy evaluation via [testRoutePolicies](https://pybatfish.readthedocs.io/en/latest/notebooks/routingProtocols.html#Test-Route-Policies) question - Direct inspection of routing tables via [routes](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#Routes) and [bgpRib](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#BGP-RIB) questions - Querying interfaces via [interfaceProperties](https://pybatfish.readthedocs.io/en/latest/notebooks/configProperties.html#Interface-Properties) question   NetCov supports configuration syntax of the following vendors:  |                    | Interface          | BGP neighbor       | Routing Policy     | Prefix list        | Community list     | As-path list       | |--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------| | Cisco              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Juniper            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Arista             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Palo Alto Networks | :heavy_check_mark: |                    |                    |                    |                    |                    | | SONiC              |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |   If you‚Äôd like support for additional [vendors](https://pybatfish.readthedocs.io/en/latest/supported_devices.html) (which are supported by Batfish but unsupported by NetCov) or currently-unsupported configuration features, let us know via [GitHub issue](https://github.com/UWNetworksLab/netcov/issues).",pybatfish,SOFTWARE
"[<img src=""screenshot_demo_video.png""  width=""500"">](https://youtube.com/video/FcBD2LhxqOQ)  ## Features  NetCov supports coverage tracking for the following [Batfish questions](https://pybatfish.readthedocs.io/en/latest/questions.html):  - Reachability test via [traceroute](https://pybatfish.readthedocs.io/en/latest/notebooks/forwarding.html#Traceroute) question - Routing policy evaluation via [testRoutePolicies](https://pybatfish.readthedocs.io/en/latest/notebooks/routingProtocols.html#Test-Route-Policies) question - Direct inspection of routing tables via [routes](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#Routes) and [bgpRib](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#BGP-RIB) questions - Querying interfaces via [interfaceProperties](https://pybatfish.readthedocs.io/en/latest/notebooks/configProperties.html#Interface-Properties) question   NetCov supports configuration syntax of the following vendors:  |                    | Interface          | BGP neighbor       | Routing Policy     | Prefix list        | Community list     | As-path list       | |--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------| | Cisco              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Juniper            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Arista             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Palo Alto Networks | :heavy_check_mark: |                    |                    |                    |                    |                    | | SONiC              |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |   If you‚Äôd like support for additional [vendors](https://pybatfish.readthedocs.io/en/latest/supported_devices.html) (which are supported by Batfish but unsupported by NetCov) or currently-unsupported configuration features, let us know via [GitHub issue](https://github.com/UWNetworksLab/netcov/issues).",pybatfish,SOFTWARE
"[<img src=""screenshot_demo_video.png""  width=""500"">](https://youtube.com/video/FcBD2LhxqOQ)  ## Features  NetCov supports coverage tracking for the following [Batfish questions](https://pybatfish.readthedocs.io/en/latest/questions.html):  - Reachability test via [traceroute](https://pybatfish.readthedocs.io/en/latest/notebooks/forwarding.html#Traceroute) question - Routing policy evaluation via [testRoutePolicies](https://pybatfish.readthedocs.io/en/latest/notebooks/routingProtocols.html#Test-Route-Policies) question - Direct inspection of routing tables via [routes](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#Routes) and [bgpRib](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#BGP-RIB) questions - Querying interfaces via [interfaceProperties](https://pybatfish.readthedocs.io/en/latest/notebooks/configProperties.html#Interface-Properties) question   NetCov supports configuration syntax of the following vendors:  |                    | Interface          | BGP neighbor       | Routing Policy     | Prefix list        | Community list     | As-path list       | |--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------| | Cisco              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Juniper            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Arista             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Palo Alto Networks | :heavy_check_mark: |                    |                    |                    |                    |                    | | SONiC              |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |   If you‚Äôd like support for additional [vendors](https://pybatfish.readthedocs.io/en/latest/supported_devices.html) (which are supported by Batfish but unsupported by NetCov) or currently-unsupported configuration features, let us know via [GitHub issue](https://github.com/UWNetworksLab/netcov/issues).",NetCov,SOFTWARE
"[<img src=""screenshot_demo_video.png""  width=""500"">](https://youtube.com/video/FcBD2LhxqOQ)  ## Features  NetCov supports coverage tracking for the following [Batfish questions](https://pybatfish.readthedocs.io/en/latest/questions.html):  - Reachability test via [traceroute](https://pybatfish.readthedocs.io/en/latest/notebooks/forwarding.html#Traceroute) question - Routing policy evaluation via [testRoutePolicies](https://pybatfish.readthedocs.io/en/latest/notebooks/routingProtocols.html#Test-Route-Policies) question - Direct inspection of routing tables via [routes](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#Routes) and [bgpRib](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#BGP-RIB) questions - Querying interfaces via [interfaceProperties](https://pybatfish.readthedocs.io/en/latest/notebooks/configProperties.html#Interface-Properties) question   NetCov supports configuration syntax of the following vendors:  |                    | Interface          | BGP neighbor       | Routing Policy     | Prefix list        | Community list     | As-path list       | |--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------| | Cisco              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Juniper            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Arista             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Palo Alto Networks | :heavy_check_mark: |                    |                    |                    |                    |                    | | SONiC              |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |   If you‚Äôd like support for additional [vendors](https://pybatfish.readthedocs.io/en/latest/supported_devices.html) (which are supported by Batfish but unsupported by NetCov) or currently-unsupported configuration features, let us know via [GitHub issue](https://github.com/UWNetworksLab/netcov/issues).",pybatfish,SOFTWARE
"[<img src=""screenshot_demo_video.png""  width=""500"">](https://youtube.com/video/FcBD2LhxqOQ)  ## Features  NetCov supports coverage tracking for the following [Batfish questions](https://pybatfish.readthedocs.io/en/latest/questions.html):  - Reachability test via [traceroute](https://pybatfish.readthedocs.io/en/latest/notebooks/forwarding.html#Traceroute) question - Routing policy evaluation via [testRoutePolicies](https://pybatfish.readthedocs.io/en/latest/notebooks/routingProtocols.html#Test-Route-Policies) question - Direct inspection of routing tables via [routes](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#Routes) and [bgpRib](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#BGP-RIB) questions - Querying interfaces via [interfaceProperties](https://pybatfish.readthedocs.io/en/latest/notebooks/configProperties.html#Interface-Properties) question   NetCov supports configuration syntax of the following vendors:  |                    | Interface          | BGP neighbor       | Routing Policy     | Prefix list        | Community list     | As-path list       | |--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------| | Cisco              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Juniper            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Arista             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Palo Alto Networks | :heavy_check_mark: |                    |                    |                    |                    |                    | | SONiC              |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |   If you‚Äôd like support for additional [vendors](https://pybatfish.readthedocs.io/en/latest/supported_devices.html) (which are supported by Batfish but unsupported by NetCov) or currently-unsupported configuration features, let us know via [GitHub issue](https://github.com/UWNetworksLab/netcov/issues).",Batfish,SOFTWARE
"[<img src=""screenshot_demo_video.png""  width=""500"">](https://youtube.com/video/FcBD2LhxqOQ)  ## Features  NetCov supports coverage tracking for the following [Batfish questions](https://pybatfish.readthedocs.io/en/latest/questions.html):  - Reachability test via [traceroute](https://pybatfish.readthedocs.io/en/latest/notebooks/forwarding.html#Traceroute) question - Routing policy evaluation via [testRoutePolicies](https://pybatfish.readthedocs.io/en/latest/notebooks/routingProtocols.html#Test-Route-Policies) question - Direct inspection of routing tables via [routes](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#Routes) and [bgpRib](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#BGP-RIB) questions - Querying interfaces via [interfaceProperties](https://pybatfish.readthedocs.io/en/latest/notebooks/configProperties.html#Interface-Properties) question   NetCov supports configuration syntax of the following vendors:  |                    | Interface          | BGP neighbor       | Routing Policy     | Prefix list        | Community list     | As-path list       | |--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------| | Cisco              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Juniper            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Arista             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Palo Alto Networks | :heavy_check_mark: |                    |                    |                    |                    |                    | | SONiC              |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |   If you‚Äôd like support for additional [vendors](https://pybatfish.readthedocs.io/en/latest/supported_devices.html) (which are supported by Batfish but unsupported by NetCov) or currently-unsupported configuration features, let us know via [GitHub issue](https://github.com/UWNetworksLab/netcov/issues).",NetCov,SOFTWARE
"[<img src=""screenshot_demo_video.png""  width=""500"">](https://youtube.com/video/FcBD2LhxqOQ)  ## Features  NetCov supports coverage tracking for the following [Batfish questions](https://pybatfish.readthedocs.io/en/latest/questions.html):  - Reachability test via [traceroute](https://pybatfish.readthedocs.io/en/latest/notebooks/forwarding.html#Traceroute) question - Routing policy evaluation via [testRoutePolicies](https://pybatfish.readthedocs.io/en/latest/notebooks/routingProtocols.html#Test-Route-Policies) question - Direct inspection of routing tables via [routes](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#Routes) and [bgpRib](https://pybatfish.readthedocs.io/en/latest/notebooks/routingTables.html#BGP-RIB) questions - Querying interfaces via [interfaceProperties](https://pybatfish.readthedocs.io/en/latest/notebooks/configProperties.html#Interface-Properties) question   NetCov supports configuration syntax of the following vendors:  |                    | Interface          | BGP neighbor       | Routing Policy     | Prefix list        | Community list     | As-path list       | |--------------------|--------------------|--------------------|--------------------|--------------------|--------------------|--------------------| | Cisco              | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Juniper            | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Arista             | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | | Palo Alto Networks | :heavy_check_mark: |                    |                    |                    |                    |                    | | SONiC              |                    | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |   If you‚Äôd like support for additional [vendors](https://pybatfish.readthedocs.io/en/latest/supported_devices.html) (which are supported by Batfish but unsupported by NetCov) or currently-unsupported configuration features, let us know via [GitHub issue](https://github.com/UWNetworksLab/netcov/issues).",netcov,SOFTWARE
"NetCov reports configuration coverage as the percentage of configuration lines that are covered, such as:  <img src=""screenshot_aggregate.png""  width=""500"">  NetCov can also report fine-grained coverage results as colored annotations on source configurations (lines in blue are covered, lines in red are not covered):  <img src=""screenshot_annotation.png""  width=""500"">  You can find an example of the coverage report [here](https://rawcdn.githack.com/UWNetworksLab/netcov/main/examples/fattree4/coverage/HTML_REPORT/index.html).     ## Installing NetCov Install NetCov using `pip`.",NetCov,SOFTWARE
"NetCov reports configuration coverage as the percentage of configuration lines that are covered, such as:  <img src=""screenshot_aggregate.png""  width=""500"">  NetCov can also report fine-grained coverage results as colored annotations on source configurations (lines in blue are covered, lines in red are not covered):  <img src=""screenshot_annotation.png""  width=""500"">  You can find an example of the coverage report [here](https://rawcdn.githack.com/UWNetworksLab/netcov/main/examples/fattree4/coverage/HTML_REPORT/index.html).     ## Installing NetCov Install NetCov using `pip`.",NetCov,SOFTWARE
"NetCov reports configuration coverage as the percentage of configuration lines that are covered, such as:  <img src=""screenshot_aggregate.png""  width=""500"">  NetCov can also report fine-grained coverage results as colored annotations on source configurations (lines in blue are covered, lines in red are not covered):  <img src=""screenshot_annotation.png""  width=""500"">  You can find an example of the coverage report [here](https://rawcdn.githack.com/UWNetworksLab/netcov/main/examples/fattree4/coverage/HTML_REPORT/index.html).     ## Installing NetCov Install NetCov using `pip`.",netcov,SOFTWARE
"NetCov reports configuration coverage as the percentage of configuration lines that are covered, such as:  <img src=""screenshot_aggregate.png""  width=""500"">  NetCov can also report fine-grained coverage results as colored annotations on source configurations (lines in blue are covered, lines in red are not covered):  <img src=""screenshot_annotation.png""  width=""500"">  You can find an example of the coverage report [here](https://rawcdn.githack.com/UWNetworksLab/netcov/main/examples/fattree4/coverage/HTML_REPORT/index.html).     ## Installing NetCov Install NetCov using `pip`.",NetCov,SOFTWARE
"NetCov reports configuration coverage as the percentage of configuration lines that are covered, such as:  <img src=""screenshot_aggregate.png""  width=""500"">  NetCov can also report fine-grained coverage results as colored annotations on source configurations (lines in blue are covered, lines in red are not covered):  <img src=""screenshot_annotation.png""  width=""500"">  You can find an example of the coverage report [here](https://rawcdn.githack.com/UWNetworksLab/netcov/main/examples/fattree4/coverage/HTML_REPORT/index.html).     ## Installing NetCov Install NetCov using `pip`.",NetCov,SOFTWARE
"NetCov reports configuration coverage as the percentage of configuration lines that are covered, such as:  <img src=""screenshot_aggregate.png""  width=""500"">  NetCov can also report fine-grained coverage results as colored annotations on source configurations (lines in blue are covered, lines in red are not covered):  <img src=""screenshot_annotation.png""  width=""500"">  You can find an example of the coverage report [here](https://rawcdn.githack.com/UWNetworksLab/netcov/main/examples/fattree4/coverage/HTML_REPORT/index.html).     ## Installing NetCov Install NetCov using `pip`.",pip,SOFTWARE
```sh $ pip install netcov ```  NetCov leverages [LCOV](https://github.com/linux-test-project/lcov) to generate HTML report.,pip,SOFTWARE
```sh $ pip install netcov ```  NetCov leverages [LCOV](https://github.com/linux-test-project/lcov) to generate HTML report.,netcov,SOFTWARE
```sh $ pip install netcov ```  NetCov leverages [LCOV](https://github.com/linux-test-project/lcov) to generate HTML report.,NetCov,SOFTWARE
```sh $ pip install netcov ```  NetCov leverages [LCOV](https://github.com/linux-test-project/lcov) to generate HTML report.,LCOV,SOFTWARE
```sh $ pip install netcov ```  NetCov leverages [LCOV](https://github.com/linux-test-project/lcov) to generate HTML report.,lcov,SOFTWARE
"If you would like to use this feature, install LCOV using Homebrew (MacOS) or apt (Ubuntu):  Install `lcov` on MacOS using Homebrew: ```sh $ brew install lcov ``` Install `lcov` on Ubuntu using apt: ```sh $ sudo apt install lcov ```     ## Usage NetCov can be used seamlessly with `pybatfish`.",LCOV,SOFTWARE
"If you would like to use this feature, install LCOV using Homebrew (MacOS) or apt (Ubuntu):  Install `lcov` on MacOS using Homebrew: ```sh $ brew install lcov ``` Install `lcov` on Ubuntu using apt: ```sh $ sudo apt install lcov ```     ## Usage NetCov can be used seamlessly with `pybatfish`.",Homebrew,SOFTWARE
"If you would like to use this feature, install LCOV using Homebrew (MacOS) or apt (Ubuntu):  Install `lcov` on MacOS using Homebrew: ```sh $ brew install lcov ``` Install `lcov` on Ubuntu using apt: ```sh $ sudo apt install lcov ```     ## Usage NetCov can be used seamlessly with `pybatfish`.",apt,SOFTWARE
"If you would like to use this feature, install LCOV using Homebrew (MacOS) or apt (Ubuntu):  Install `lcov` on MacOS using Homebrew: ```sh $ brew install lcov ``` Install `lcov` on Ubuntu using apt: ```sh $ sudo apt install lcov ```     ## Usage NetCov can be used seamlessly with `pybatfish`.",lcov,SOFTWARE
"If you would like to use this feature, install LCOV using Homebrew (MacOS) or apt (Ubuntu):  Install `lcov` on MacOS using Homebrew: ```sh $ brew install lcov ``` Install `lcov` on Ubuntu using apt: ```sh $ sudo apt install lcov ```     ## Usage NetCov can be used seamlessly with `pybatfish`.",Homebrew,SOFTWARE
"If you would like to use this feature, install LCOV using Homebrew (MacOS) or apt (Ubuntu):  Install `lcov` on MacOS using Homebrew: ```sh $ brew install lcov ``` Install `lcov` on Ubuntu using apt: ```sh $ sudo apt install lcov ```     ## Usage NetCov can be used seamlessly with `pybatfish`.",brew,SOFTWARE
"If you would like to use this feature, install LCOV using Homebrew (MacOS) or apt (Ubuntu):  Install `lcov` on MacOS using Homebrew: ```sh $ brew install lcov ``` Install `lcov` on Ubuntu using apt: ```sh $ sudo apt install lcov ```     ## Usage NetCov can be used seamlessly with `pybatfish`.",lcov,SOFTWARE
"If you would like to use this feature, install LCOV using Homebrew (MacOS) or apt (Ubuntu):  Install `lcov` on MacOS using Homebrew: ```sh $ brew install lcov ``` Install `lcov` on Ubuntu using apt: ```sh $ sudo apt install lcov ```     ## Usage NetCov can be used seamlessly with `pybatfish`.",lcov,SOFTWARE
"If you would like to use this feature, install LCOV using Homebrew (MacOS) or apt (Ubuntu):  Install `lcov` on MacOS using Homebrew: ```sh $ brew install lcov ``` Install `lcov` on Ubuntu using apt: ```sh $ sudo apt install lcov ```     ## Usage NetCov can be used seamlessly with `pybatfish`.",apt,SOFTWARE
"If you would like to use this feature, install LCOV using Homebrew (MacOS) or apt (Ubuntu):  Install `lcov` on MacOS using Homebrew: ```sh $ brew install lcov ``` Install `lcov` on Ubuntu using apt: ```sh $ sudo apt install lcov ```     ## Usage NetCov can be used seamlessly with `pybatfish`.",apt,SOFTWARE
"If you would like to use this feature, install LCOV using Homebrew (MacOS) or apt (Ubuntu):  Install `lcov` on MacOS using Homebrew: ```sh $ brew install lcov ``` Install `lcov` on Ubuntu using apt: ```sh $ sudo apt install lcov ```     ## Usage NetCov can be used seamlessly with `pybatfish`.",lcov,SOFTWARE
"If you would like to use this feature, install LCOV using Homebrew (MacOS) or apt (Ubuntu):  Install `lcov` on MacOS using Homebrew: ```sh $ brew install lcov ``` Install `lcov` on Ubuntu using apt: ```sh $ sudo apt install lcov ```     ## Usage NetCov can be used seamlessly with `pybatfish`.",NetCov,SOFTWARE
"If you would like to use this feature, install LCOV using Homebrew (MacOS) or apt (Ubuntu):  Install `lcov` on MacOS using Homebrew: ```sh $ brew install lcov ``` Install `lcov` on Ubuntu using apt: ```sh $ sudo apt install lcov ```     ## Usage NetCov can be used seamlessly with `pybatfish`.",pybatfish,SOFTWARE
It provides a hooked version of pybatfish APIs that automatically tracks coverage during the execution of supported pybatfish questions.   ### Using NetCov for an existing pybatfish script/notebook  It takes only two simple steps to measure coverage for your existing pybatfish scripts/notebooks.  1.,pybatfish,SOFTWARE
It provides a hooked version of pybatfish APIs that automatically tracks coverage during the execution of supported pybatfish questions.   ### Using NetCov for an existing pybatfish script/notebook  It takes only two simple steps to measure coverage for your existing pybatfish scripts/notebooks.  1.,pybatfish,SOFTWARE
It provides a hooked version of pybatfish APIs that automatically tracks coverage during the execution of supported pybatfish questions.   ### Using NetCov for an existing pybatfish script/notebook  It takes only two simple steps to measure coverage for your existing pybatfish scripts/notebooks.  1.,NetCov,SOFTWARE
It provides a hooked version of pybatfish APIs that automatically tracks coverage during the execution of supported pybatfish questions.   ### Using NetCov for an existing pybatfish script/notebook  It takes only two simple steps to measure coverage for your existing pybatfish scripts/notebooks.  1.,pybatfish,SOFTWARE
It provides a hooked version of pybatfish APIs that automatically tracks coverage during the execution of supported pybatfish questions.   ### Using NetCov for an existing pybatfish script/notebook  It takes only two simple steps to measure coverage for your existing pybatfish scripts/notebooks.  1.,pybatfish,SOFTWARE
"For import, replace pybatfish client session with the one provided by NetCov: ```python #from pybatfish.client.session import Session from netcov import NetCovSession as Session ``` 2.",pybatfish,SOFTWARE
"For import, replace pybatfish client session with the one provided by NetCov: ```python #from pybatfish.client.session import Session from netcov import NetCovSession as Session ``` 2.",NetCov,SOFTWARE
"For import, replace pybatfish client session with the one provided by NetCov: ```python #from pybatfish.client.session import Session from netcov import NetCovSession as Session ``` 2.",pybatfish,SOFTWARE
"For import, replace pybatfish client session with the one provided by NetCov: ```python #from pybatfish.client.session import Session from netcov import NetCovSession as Session ``` 2.",netcov,SOFTWARE
"To generate HTML report, use `bf.cov.html_report()`.  ### Use NetCov for a new pybatfish script/notebook   We provide a [demo video](https://www.youtube.com/watch?",NetCov,SOFTWARE
"To generate HTML report, use `bf.cov.html_report()`.  ### Use NetCov for a new pybatfish script/notebook   We provide a [demo video](https://www.youtube.com/watch?",pybatfish,SOFTWARE
"If you are new to pybatfish, we recommend reading the [pybatfish doc](https://pybatfish.readthedocs.io/en/latest/notebooks/interacting.html) first.    ### Advanced  Sometimes not all information retrieved by Batfish questions are meant to be tested, for example, when you retrieve all RIB entries but only assert on a subset of them.",pybatfish,SOFTWARE
"If you are new to pybatfish, we recommend reading the [pybatfish doc](https://pybatfish.readthedocs.io/en/latest/notebooks/interacting.html) first.    ### Advanced  Sometimes not all information retrieved by Batfish questions are meant to be tested, for example, when you retrieve all RIB entries but only assert on a subset of them.",pybatfish,SOFTWARE
"If you are new to pybatfish, we recommend reading the [pybatfish doc](https://pybatfish.readthedocs.io/en/latest/notebooks/interacting.html) first.    ### Advanced  Sometimes not all information retrieved by Batfish questions are meant to be tested, for example, when you retrieve all RIB entries but only assert on a subset of them.",pybatfish,SOFTWARE
"If you are new to pybatfish, we recommend reading the [pybatfish doc](https://pybatfish.readthedocs.io/en/latest/notebooks/interacting.html) first.    ### Advanced  Sometimes not all information retrieved by Batfish questions are meant to be tested, for example, when you retrieve all RIB entries but only assert on a subset of them.",Batfish,SOFTWARE
"To help NetCov model coverage more accurately, you can pause coverage tracking and add tested information use a NetCov API: ``` # pause coverage tracking to avoid over-estimation bf.cov.pause() routes = bf.q.routes(nodes=""edge-0000"").answer().frame() bf.cov.resume()  # filter RIB entries to test tested = routes[routes[""Network""] == '0.0.0.0/0'].head(1)  # add tested route to coverage trace bf.cov.add_tested_routes(tested) ```  `bf.cov.result()` prints coverage metrics using `logging` module and writes to `stderr` by default.",NetCov,SOFTWARE
"To help NetCov model coverage more accurately, you can pause coverage tracking and add tested information use a NetCov API: ``` # pause coverage tracking to avoid over-estimation bf.cov.pause() routes = bf.q.routes(nodes=""edge-0000"").answer().frame() bf.cov.resume()  # filter RIB entries to test tested = routes[routes[""Network""] == '0.0.0.0/0'].head(1)  # add tested route to coverage trace bf.cov.add_tested_routes(tested) ```  `bf.cov.result()` prints coverage metrics using `logging` module and writes to `stderr` by default.",NetCov,SOFTWARE
"To save the coverage report to file, you can customize logger by: ```python import logging fh = logging.FileHandler('cov.log') logging.getLogger('netcov').addHandler(fh) ```  ## References ``` @inproceedings {netcov-nsdi-2023,   author = {Xieyang Xu and Weixin Deng and Ryan Beckett and Ratul Mahajan and David Walker},   title = {Test Coverage for Network Configurations},   booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},   year = {2023},   isbn = {978-1-939133-33-5},   address = {Boston, MA},   pages = {1717--1732},   url = {https://www.usenix.org/conference/nsdi23/presentation/xu},   publisher = {USENIX Association},   month = apr, }```",netcov,SOFTWARE
style=for-the-badge&logo=youtube&logoColor=white)](https://www.youtube.com/watch?,youtube,SOFTWARE
style=for-the-badge&logo=overleaf&logoColor=white)](#cite)   A PyTorch implementation of the Scaffolding Learning Regime (SLR) for training obstacle detection models for aquatic domains.,PyTorch,SOFTWARE
Clone the repository     ```bash     git clone https://github.com/lojzezust/SLR     cd SLR     ``` 2.,git,SOFTWARE
Install the requirements     ```bash     pip install -r requirements.txt     ``` 3.,pip,SOFTWARE
Install SLR.,SLR,SOFTWARE
```bash     pip install -e .     ``` 4.,pip,SOFTWARE
Download the [MaSTr1325 dataset](https://box.vicos.si/borja/viamaro/index.html) and corresponding [weak annotations](https://github.com/lojzezust/SLR/releases/download/weights_v2/mastr_slr.zip).,SLR,SOFTWARE
The weak annotation archive also includes automatically generated prior obstacle segmentation masks (i.e. using DEXTR). 2.,DEXTR,SOFTWARE
```bash     python tools/prepare_data.py     ```     The preparation script performs the following operations:     1.,python,SOFTWARE
"Creates a dataset file `all_weak.yaml`, which links the prepared dataset directories for training.  ### SLR Training  Use the utility script `tools/train_slr.sh` to train a model using the entire SLR pipeline.",SLR,SOFTWARE
"Creates a dataset file `all_weak.yaml`, which links the prepared dataset directories for training.  ### SLR Training  Use the utility script `tools/train_slr.sh` to train a model using the entire SLR pipeline.",SLR,SOFTWARE
- `MASTR_DIR`: Location of the dataset used for training. - `ARCHITECTURE`: Which architecture to use (use `python tools/train.py warmup --help` for more info). - `MODEL_NAME`: Name of the model.,python,SOFTWARE
"```bash export CUDA_VISIBLE_DEVICES=0,1 python tools/train.py warmup \ --architecture wasr_resnet101_imu \ --model-name wasr_slr_warmup \ --batch-size 4 ```  > [!",python,SOFTWARE
"```bash export CUDA_VISIBLE_DEVICES=0,1 python tools/generate_pseudo_labels.py \ --architecture wasr_resnet101_imu \ --weights-file output/logs/wasr_slr_warmup/version_0/checkpoints/last.ckpt \ --output-dir output/pseudo_labels/wasr_slr_warmup_v0 ```  This creates the pseudo-labels and stores them into `output/pseudo_labels/wasr_slr_warmup_v0`",python,SOFTWARE
"```bash export CUDA_VISIBLE_DEVICES=0,1 python tools/train.py finetune \ --architecture wasr_resnet101_imu \ --model-name wasr_slr \ --batch-size 4 \ --pretrained-weights output/logs/wasr_slr_warmup/version_0/checkpoints/last.ckpt \ --mask-dir output/pseudo_labels/wasr_slr_warmup_v0 ```  > [!",python,SOFTWARE
"```bash export CUDA_VISIBLE_DEVICES=0,1 python tools/general_inference.py \ --architecture wasr_resnet101 \ --weights-file output/logs/wasr_slr_v2_it1/version_0/checkpoints/last.ckpt \ --image-dir data/example_dir \ --output-dir output/predictions/test_predictions ```  Additionally, `--imu-dir` can be used to supply a directory with corresponding IMU horizon masks.",python,SOFTWARE
All models are trained on the MaSTr1325 dataset using SLR and weak annotations and evaluated on the [MODS benchmark](https://github.com/bborja/mods_evaluation).,SLR,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-contextualsentence-multim-compsci,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-contextualsentence-multim-compsci,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-contextualsentence-multim-biomed,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-contextualsentence-multim-biomed,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-contextualsentence-singlem-compsci,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-contextualsentence-singlem-compsci,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-contextualsentence-singlem-biomed,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-contextualsentence-singlem-biomed,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-biencoder-compsci-spec,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-biencoder-compsci-spec,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-biencoder-biomed-scib,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-biencoder-biomed-scib,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-biencoder-biomed-spec,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-biencoder-biomed-spec,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-sentence-embedder,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-sentence-embedder,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",transformers,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",transformers,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",allenai/aspire-contextualsentence-singlem-compsci,SOFTWARE
"[TODOs](#todos)   ### Artifacts <a name=""artifacts""></a>  #### Models <a name=""models""></a>  Models described in the paper are released as Hugging Face models:  `otAspire`:  - [`allenai/aspire-contextualsentence-multim-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-multim-compsci) - [`allenai/aspire-contextualsentence-multim-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-multim-biomed)  `tsAspire`:   - [`allenai/aspire-contextualsentence-singlem-compsci`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-compsci) - [`allenai/aspire-contextualsentence-singlem-biomed`](https://huggingface.co/allenai/aspire-contextualsentence-singlem-biomed)   `SPECTER-CoCite`:   - [`allenai/aspire-biencoder-compsci-spec`](https://huggingface.co/allenai/aspire-biencoder-compsci-spec) - [`allenai/aspire-biencoder-biomed-scib`](https://huggingface.co/allenai/aspire-biencoder-biomed-scib) - [`allenai/aspire-biencoder-biomed-spec`](https://huggingface.co/allenai/aspire-biencoder-biomed-spec)  `cosentbert`:   - [`allenai/aspire-sentence-embedder`](https://huggingface.co/allenai/aspire-sentence-embedder)  #### Model Usage Instructions <a name=""modelusage""></a>  ##### `tsAspire`  The `tsAspire` multi-vector model trained for single matches across documents can be used via the `transformers` library and some additional code to compute contextual sentence vectors as:  ```python from transformers import AutoTokenizer from examples.ex_aspire_consent import AspireConSent, prepare_abstracts  # Initialize the tokenizer and model. hf_model_name = 'allenai/aspire-contextualsentence-singlem-compsci' aspire_tok = AutoTokenizer.from_pretrained(hf_model_name) aspire_mv_model = AspireConSent(hf_model_name)   # Example input. ex_abstracts = [     {'TITLE': ""Multi-Vector Models with Textual Guidance for Fine-Grained Scientific""               "" Document Similarity"",      'ABSTRACT': [""We present a new scientific document similarity model based on ""                   ""matching fine-grained aspects of texts."",                   ""To train our model, we exploit a naturally-occurring source of ""                   ""supervision: sentences in the full-text of papers that cite multiple ""                   ""papers together (co-citations).""]},     {'TITLE': ""CSFCube -- A Test Collection of Computer Science Research Articles for ""               ""Faceted Query by Example"",      'ABSTRACT': [""Query by Example is a well-known information retrieval task in which""                   "" a document is chosen by the user as the search query and the goal is ""                   ""to retrieve relevant documents from a large collection."",                   ""However, a document often covers multiple aspects of a topic."",                   ""To address this scenario we introduce the task of faceted Query by ""                   ""Example in which users can also specify a finer grained aspect in ""                   ""addition to the input query document. ""]} ]  bert_batch, abs_lens, sent_token_idxs = prepare_abstracts(batch_abs=ex_abstracts,                                                           pt_lm_tokenizer=aspire_tok) clsreps, contextual_sent_reps = aspire_mv_model.forward(bert_batch=bert_batch,                                                         abs_lens=abs_lens,                                                         sent_tok_idxs=sent_token_idxs) ```  ##### `otAspire`  The `otAspire` multi-vector model trained for _multiple_ matching across documents can be used via the `transformers` library, and some additional code to compute contextual sentence vectors and to make multiple matches using optimal transport.",transformers,SOFTWARE
